import nltk

def tokenize(sentence):
    pass

def stem(word):
    pass

def bag_of_words(tokenized_sentence, all_words):
    pass