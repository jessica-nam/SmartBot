{
    "intents": [
        {
            "tag": "yield",
            "patterns": [
                "What does the \"yield\" keyword do?",
                "tell me the yield keyword?"
            ],
            "responses": [
                "To understand what yield does, you must understand what generators are. And before you can understand generators, you must understand iterables.yieldgeneratorsiterablesIterablesWhen you create a list, you can read its items one by one. Reading its items one by one is called iteration:>>> mylist = [1, 2, 3]\n>>> for i in mylist:\n...    print(i)\n1\n2\n3\n>>> mylist = [1, 2, 3]\n>>> for i in mylist:\n...    print(i)\n1\n2\n3\nmylist is an iterable. When you use a list comprehension, you create a list, and so an iterable:mylistiterable>>> mylist = [x*x for x in range(3)]\n>>> for i in mylist:\n...    print(i)\n0\n1\n4\n>>> mylist = [x*x for x in range(3)]\n>>> for i in mylist:\n...    print(i)\n0\n1\n4\nEverything you can use \"for... in...\" on is an iterable; lists, strings, files...for... in...listsstringsThese iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values.GeneratorsGenerators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly:you can only iterate over oncethey generate the values on the fly>>> mygenerator = (x*x for x in range(3))\n>>> for i in mygenerator:\n...    print(i)\n0\n1\n4\n>>> mygenerator = (x*x for x in range(3))\n>>> for i in mygenerator:\n...    print(i)\n0\n1\n4\nIt is just the same except you used () instead of []. BUT, you cannot perform for i in mygenerator a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one.()[]cannotfor i in mygeneratorYieldyield is a keyword that is used like return, except the function will return a generator.yieldreturn>>> def create_generator():\n...    mylist = range(3)\n...    for i in mylist:\n...        yield i*i\n...\n>>> mygenerator = create_generator() # create a generator\n>>> print(mygenerator) # mygenerator is an object!\n<generator object create_generator at 0xb7555c34>\n>>> for i in mygenerator:\n...     print(i)\n0\n1\n4\n>>> def create_generator():\n...    mylist = range(3)\n...    for i in mylist:\n...        yield i*i\n...\n>>> mygenerator = create_generator() # create a generator\n>>> print(mygenerator) # mygenerator is an object!\n<generator object create_generator at 0xb7555c34>\n>>> for i in mygenerator:\n...     print(i)\n0\n1\n4\nHere it's a useless example, but it's handy when you know your function will return a huge set of values that you will only need to read once.To master yield, you must understand that when you call the function, the code you have written in the function body does not run. The function only returns the generator object, this is a bit tricky.yieldwhen you call the function, the code you have written in the function body does not run.Then, your code will continue from where it left off each time for uses the generator.forNow the hard part:The first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it'll return the first value of the loop. Then, each subsequent call will run another iteration of the loop you have written in the function and return the next value. This will continue until the generator is considered empty, which happens when the function runs without hitting yield. That can be because the loop has come to an end, or because you no longer satisfy an \"if/else\".foryieldyield\"if/else\"Your code explainedGenerator:Generator:# Here you create the method of the node object that will return the generator\ndef _get_child_candidates(self, distance, min_dist, max_dist):\n\n    # Here is the code that will be called each time you use the generator object:\n\n    # If there is still a child of the node object on its left\n    # AND if the distance is ok, return the next child\n    if self._leftchild and distance - max_dist < self._median:\n        yield self._leftchild\n\n    # If there is still a child of the node object on its right\n    # AND if the distance is ok, return the next child\n    if self._rightchild and distance + max_dist >= self._median:\n        yield self._rightchild\n\n    # If the function arrives here, the generator will be considered empty\n    # there are no more than two values: the left and the right children\n# Here you create the method of the node object that will return the generator\ndef _get_child_candidates(self, distance, min_dist, max_dist):\n\n    # Here is the code that will be called each time you use the generator object:\n\n    # If there is still a child of the node object on its left\n    # AND if the distance is ok, return the next child\n    if self._leftchild and distance - max_dist < self._median:\n        yield self._leftchild\n\n    # If there is still a child of the node object on its right\n    # AND if the distance is ok, return the next child\n    if self._rightchild and distance + max_dist >= self._median:\n        yield self._rightchild\n\n    # If the function arrives here, the generator will be considered empty\n    # there are no more than two values: the left and the right children\nCaller:Caller:# Create an empty list and a list with the current object reference\nresult, candidates = list(), [self]\n\n# Loop on candidates (they contain only one element at the beginning)\nwhile candidates:\n\n    # Get the last candidate and remove it from the list\n    node = candidates.pop()\n\n    # Get the distance between obj and the candidate\n    distance = node._get_dist(obj)\n\n    # If the distance is ok, then you can fill in the result\n    if distance <= max_dist and distance >= min_dist:\n        result.extend(node._values)\n\n    # Add the children of the candidate to the candidate's list\n    # so the loop will keep running until it has looked\n    # at all the children of the children of the children, etc. of the candidate\n    candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))\n\nreturn result\n# Create an empty list and a list with the current object reference\nresult, candidates = list(), [self]\n\n# Loop on candidates (they contain only one element at the beginning)\nwhile candidates:\n\n    # Get the last candidate and remove it from the list\n    node = candidates.pop()\n\n    # Get the distance between obj and the candidate\n    distance = node._get_dist(obj)\n\n    # If the distance is ok, then you can fill in the result\n    if distance <= max_dist and distance >= min_dist:\n        result.extend(node._values)\n\n    # Add the children of the candidate to the candidate's list\n    # so the loop will keep running until it has looked\n    # at all the children of the children of the children, etc. of the candidate\n    candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))\n\nreturn result\nThis code contains several smart parts:\nThe loop iterates on a list, but the list expands while the loop is being iterated. It's a concise way to go through all these nested data even if it's a bit dangerous since you can end up with an infinite loop. In this case, candidates.extend(node._get_child_candidates(distance, min_dist, max_dist)) exhausts all the values of the generator, but while keeps creating new generator objects which will produce different values from the previous ones since it's not applied on the same node.\n\nThe extend() method is a list object method that expects an iterable and adds its values to the list.\n\nThe loop iterates on a list, but the list expands while the loop is being iterated. It's a concise way to go through all these nested data even if it's a bit dangerous since you can end up with an infinite loop. In this case, candidates.extend(node._get_child_candidates(distance, min_dist, max_dist)) exhausts all the values of the generator, but while keeps creating new generator objects which will produce different values from the previous ones since it's not applied on the same node.\nThe loop iterates on a list, but the list expands while the loop is being iterated. It's a concise way to go through all these nested data even if it's a bit dangerous since you can end up with an infinite loop. In this case, candidates.extend(node._get_child_candidates(distance, min_dist, max_dist)) exhausts all the values of the generator, but while keeps creating new generator objects which will produce different values from the previous ones since it's not applied on the same node.candidates.extend(node._get_child_candidates(distance, min_dist, max_dist))whileThe extend() method is a list object method that expects an iterable and adds its values to the list.\nThe extend() method is a list object method that expects an iterable and adds its values to the list.extend()Usually, we pass a list to it:>>> a = [1, 2]\n>>> b = [3, 4]\n>>> a.extend(b)\n>>> print(a)\n[1, 2, 3, 4]\n>>> a = [1, 2]\n>>> b = [3, 4]\n>>> a.extend(b)\n>>> print(a)\n[1, 2, 3, 4]\nBut in your code, it gets a generator, which is good because:\nYou don't need to read the values twice.\nYou may have a lot of children and you don't want them all stored in memory.\nYou don't need to read the values twice.You may have a lot of children and you don't want them all stored in memory.And it works because Python does not care if the argument of a method is a list or not. Python expects iterables so it will work with strings, lists, tuples, and generators! This is called duck typing and is one of the reasons why Python is so cool. But this is another story, for another question...You can stop here, or read a little bit to see an advanced use of a generator:Controlling a generator exhaustion>>> class Bank(): # Let's create a bank, building ATMs\n...    crisis = False\n...    def create_atm(self):\n...        while not self.crisis:\n...            yield \"$100\"\n>>> hsbc = Bank() # When everything's ok the ATM gives you as much as you want\n>>> corner_street_atm = hsbc.create_atm()\n>>> print(corner_street_atm.next())\n$100\n>>> print(corner_street_atm.next())\n$100\n>>> print([corner_street_atm.next() for cash in range(5)])\n['$100', '$100', '$100', '$100', '$100']\n>>> hsbc.crisis = True # Crisis is coming, no more money!\n>>> print(corner_street_atm.next())\n<type 'exceptions.StopIteration'>\n>>> wall_street_atm = hsbc.create_atm() # It's even true for new ATMs\n>>> print(wall_street_atm.next())\n<type 'exceptions.StopIteration'>\n>>> hsbc.crisis = False # The trouble is, even post-crisis the ATM remains empty\n>>> print(corner_street_atm.next())\n<type 'exceptions.StopIteration'>\n>>> brand_new_atm = hsbc.create_atm() # Build a new one to get back in business\n>>> for cash in brand_new_atm:\n...    print cash\n$100\n$100\n$100\n$100\n$100\n$100\n$100\n$100\n$100\n...\n>>> class Bank(): # Let's create a bank, building ATMs\n...    crisis = False\n...    def create_atm(self):\n...        while not self.crisis:\n...            yield \"$100\"\n>>> hsbc = Bank() # When everything's ok the ATM gives you as much as you want\n>>> corner_street_atm = hsbc.create_atm()\n>>> print(corner_street_atm.next())\n$100\n>>> print(corner_street_atm.next())\n$100\n>>> print([corner_street_atm.next() for cash in range(5)])\n['$100', '$100', '$100', '$100', '$100']\n>>> hsbc.crisis = True # Crisis is coming, no more money!\n>>> print(corner_street_atm.next())\n<type 'exceptions.StopIteration'>\n>>> wall_street_atm = hsbc.create_atm() # It's even true for new ATMs\n>>> print(wall_street_atm.next())\n<type 'exceptions.StopIteration'>\n>>> hsbc.crisis = False # The trouble is, even post-crisis the ATM remains empty\n>>> print(corner_street_atm.next())\n<type 'exceptions.StopIteration'>\n>>> brand_new_atm = hsbc.create_atm() # Build a new one to get back in business\n>>> for cash in brand_new_atm:\n...    print cash\n$100\n$100\n$100\n$100\n$100\n$100\n$100\n$100\n$100\n...\nNote: For Python 3, useprint(corner_street_atm.__next__()) or print(next(corner_street_atm))Note:print(corner_street_atm.__next__())print(next(corner_street_atm))It can be useful for various things like controlling access to a resource.Itertools, your best friendThe itertools module contains special functions to manipulate iterables. Ever wish to duplicate a generator?\nChain two generators? Group values in a nested list with a one-liner? Map / Zip without creating another list?Map / ZipThen just import itertools.import itertoolsAn example? Let's see the possible orders of arrival for a four-horse race:>>> horses = [1, 2, 3, 4]\n>>> races = itertools.permutations(horses)\n>>> print(races)\n<itertools.permutations object at 0xb754f1dc>\n>>> print(list(itertools.permutations(horses)))\n[(1, 2, 3, 4),\n (1, 2, 4, 3),\n (1, 3, 2, 4),\n (1, 3, 4, 2),\n (1, 4, 2, 3),\n (1, 4, 3, 2),\n (2, 1, 3, 4),\n (2, 1, 4, 3),\n (2, 3, 1, 4),\n (2, 3, 4, 1),\n (2, 4, 1, 3),\n (2, 4, 3, 1),\n (3, 1, 2, 4),\n (3, 1, 4, 2),\n (3, 2, 1, 4),\n (3, 2, 4, 1),\n (3, 4, 1, 2),\n (3, 4, 2, 1),\n (4, 1, 2, 3),\n (4, 1, 3, 2),\n (4, 2, 1, 3),\n (4, 2, 3, 1),\n (4, 3, 1, 2),\n (4, 3, 2, 1)]\n>>> horses = [1, 2, 3, 4]\n>>> races = itertools.permutations(horses)\n>>> print(races)\n<itertools.permutations object at 0xb754f1dc>\n>>> print(list(itertools.permutations(horses)))\n[(1, 2, 3, 4),\n (1, 2, 4, 3),\n (1, 3, 2, 4),\n (1, 3, 4, 2),\n (1, 4, 2, 3),\n (1, 4, 3, 2),\n (2, 1, 3, 4),\n (2, 1, 4, 3),\n (2, 3, 1, 4),\n (2, 3, 4, 1),\n (2, 4, 1, 3),\n (2, 4, 3, 1),\n (3, 1, 2, 4),\n (3, 1, 4, 2),\n (3, 2, 1, 4),\n (3, 2, 4, 1),\n (3, 4, 1, 2),\n (3, 4, 2, 1),\n (4, 1, 2, 3),\n (4, 1, 3, 2),\n (4, 2, 1, 3),\n (4, 2, 3, 1),\n (4, 3, 1, 2),\n (4, 3, 2, 1)]\nUnderstanding the inner mechanisms of iterationIteration is a process implying iterables (implementing the __iter__() method) and iterators (implementing the __next__() method).\nIterables are any objects you can get an iterator from. Iterators are objects that let you iterate on iterables.__iter__()__next__()There is more about it in this article about how for loops work.how for loops workfor",
                "Shortcut to understanding yieldyieldWhen you see a function with yield statements, apply this easy trick to understand what will happen:yield\nInsert a line result = [] at the start of the function.\nReplace each yield expr with result.append(expr).\nInsert a line return result at the bottom of the function.\nYay - no more yield statements! Read and figure out the code.\nCompare function to the original definition.\nInsert a line result = [] at the start of the function.result = []Replace each yield expr with result.append(expr).yield exprresult.append(expr)Insert a line return result at the bottom of the function.return resultYay - no more yield statements! Read and figure out the code.yieldCompare function to the original definition.This trick may give you an idea of the logic behind the function, but what actually happens with yield is significantly different than what happens in the list-based approach. In many cases, the yield approach will be a lot more memory efficient and faster too. In other cases, this trick will get you stuck in an infinite loop, even though the original function works just fine. Read on to learn more...yieldDon't confuse your Iterables, Iterators, and GeneratorsFirst, the iterator protocol - when you writeiterator protocolfor x in mylist:\n    ...loop body...\nfor x in mylist:\n    ...loop body...\nPython performs the following two steps:\nGets an iterator for mylist:\nCall iter(mylist) -> this returns an object with a next() method (or __next__() in Python 3).\n[This is the step most people forget to tell you about]\n\nUses the iterator to loop over items:\nKeep calling the next() method on the iterator returned from step 1. The return value from next() is assigned to x and the loop body is executed. If an exception StopIteration is raised from within next(), it means there are no more values in the iterator and the loop is exited.\n\nGets an iterator for mylist:\nCall iter(mylist) -> this returns an object with a next() method (or __next__() in Python 3).\n[This is the step most people forget to tell you about]\nGets an iterator for mylist:mylistCall iter(mylist) -> this returns an object with a next() method (or __next__() in Python 3).iter(mylist)next()__next__()[This is the step most people forget to tell you about]Uses the iterator to loop over items:\nKeep calling the next() method on the iterator returned from step 1. The return value from next() is assigned to x and the loop body is executed. If an exception StopIteration is raised from within next(), it means there are no more values in the iterator and the loop is exited.\nUses the iterator to loop over items:Keep calling the next() method on the iterator returned from step 1. The return value from next() is assigned to x and the loop body is executed. If an exception StopIteration is raised from within next(), it means there are no more values in the iterator and the loop is exited.next()next()xStopIterationnext()The truth is Python performs the above two steps anytime it wants to loop over the contents of an object - so it could be a for loop, but it could also be code like otherlist.extend(mylist) (where otherlist is a Python list).loop overotherlist.extend(mylist)otherlistHere mylist is an iterable because it implements the iterator protocol. In a user-defined class, you can implement the __iter__() method to make instances of your class iterable. This method should return an iterator. An iterator is an object with a next() method. It is possible to implement both __iter__() and next() on the same class, and have __iter__() return self. This will work for simple cases, but not when you want two iterators looping over the same object at the same time.mylistiterable__iter__()iteratornext()__iter__()next()__iter__()selfSo that's the iterator protocol, many objects implement this protocol:\nBuilt-in lists, dictionaries, tuples, sets, and files.\nUser-defined classes that implement __iter__().\nGenerators.\nBuilt-in lists, dictionaries, tuples, sets, and files.User-defined classes that implement __iter__().__iter__()Generators.Note that a for loop doesn't know what kind of object it's dealing with - it just follows the iterator protocol, and is happy to get item after item as it calls next(). Built-in lists return their items one by one, dictionaries return the keys one by one, files return the lines one by one, etc. And generators return... well that's where yield comes in:fornext()keyslinesyielddef f123():\n    yield 1\n    yield 2\n    yield 3\n\nfor item in f123():\n    print item\ndef f123():\n    yield 1\n    yield 2\n    yield 3\n\nfor item in f123():\n    print item\nInstead of yield statements, if you had three return statements in f123() only the first would get executed, and the function would exit. But f123() is no ordinary function. When f123() is called, it does not return any of the values in the yield statements! It returns a generator object. Also, the function does not really exit - it goes into a suspended state. When the for loop tries to loop over the generator object, the function resumes from its suspended state at the very next line after the yield it previously returned from, executes the next line of code, in this case, a yield statement, and returns that as the next item. This happens until the function exits, at which point the generator raises StopIteration, and the loop exits.yieldreturnf123()f123()f123()does notforyieldyieldStopIterationSo the generator object is sort of like an adapter - at one end it exhibits the iterator protocol, by exposing __iter__() and next() methods to keep the for loop happy. At the other end, however, it runs the function just enough to get the next value out of it, and puts it back in suspended mode.__iter__()next()forWhy Use Generators?Usually, you can write code that doesn't use generators but implements the same logic. One option is to use the temporary list 'trick' I mentioned before. That will not work in all cases, for e.g. if you have infinite loops, or it may make inefficient use of memory when you have a really long list. The other approach is to implement a new iterable class SomethingIter that keeps the state in instance members and performs the next logical step in its next() (or __next__() in Python 3) method. Depending on the logic, the code inside the next() method may end up looking very complex and prone to bugs. Here generators provide a clean and easy solution.next()__next__()next()",
                "Think of it this way:An iterator is just a fancy sounding term for an object that has a next() method.  So a yield-ed function ends up being something like this:next()Original version:def some_function():\n    for i in xrange(4):\n        yield i\n\nfor i in some_function():\n    print i\ndef some_function():\n    for i in xrange(4):\n        yield i\n\nfor i in some_function():\n    print i\nThis is basically what the Python interpreter does with the above code:class it:\n    def __init__(self):\n        # Start at -1 so that we get 0 when we add 1 below.\n        self.count = -1\n\n    # The __iter__ method will be called once by the 'for' loop.\n    # The rest of the magic happens on the object returned by this method.\n    # In this case it is the object itself.\n    def __iter__(self):\n        return self\n\n    # The next method will be called repeatedly by the 'for' loop\n    # until it raises StopIteration.\n    def next(self):\n        self.count += 1\n        if self.count < 4:\n            return self.count\n        else:\n            # A StopIteration exception is raised\n            # to signal that the iterator is done.\n            # This is caught implicitly by the 'for' loop.\n            raise StopIteration\n\ndef some_func():\n    return it()\n\nfor i in some_func():\n    print i\nclass it:\n    def __init__(self):\n        # Start at -1 so that we get 0 when we add 1 below.\n        self.count = -1\n\n    # The __iter__ method will be called once by the 'for' loop.\n    # The rest of the magic happens on the object returned by this method.\n    # In this case it is the object itself.\n    def __iter__(self):\n        return self\n\n    # The next method will be called repeatedly by the 'for' loop\n    # until it raises StopIteration.\n    def next(self):\n        self.count += 1\n        if self.count < 4:\n            return self.count\n        else:\n            # A StopIteration exception is raised\n            # to signal that the iterator is done.\n            # This is caught implicitly by the 'for' loop.\n            raise StopIteration\n\ndef some_func():\n    return it()\n\nfor i in some_func():\n    print i\nFor more insight as to what's happening behind the scenes, the for loop can be rewritten to this:foriterator = some_func()\ntry:\n    while 1:\n        print iterator.next()\nexcept StopIteration:\n    pass\niterator = some_func()\ntry:\n    while 1:\n        print iterator.next()\nexcept StopIteration:\n    pass\nDoes that make more sense or just confuse you more?  :)I should note that this is an oversimplification for illustrative purposes. :)is",
                "The yield keyword is reduced to two simple facts:yield\nIf the compiler detects the yield keyword anywhere inside a function, that function no longer returns via the return statement. Instead, it immediately returns a lazy \"pending list\" object called a generator\nA generator is iterable. What is an iterable? It's anything like a list or set or range or dict-view, with a built-in protocol for visiting each element in a certain order.\nIf the compiler detects the yield keyword anywhere inside a function, that function no longer returns via the return statement. Instead, it immediately returns a lazy \"pending list\" object called a generatoryieldanywherereturnInsteadInsteadimmediatelylazy \"pending list\" objectA generator is iterable. What is an iterable? It's anything like a list or set or range or dict-view, with a built-in protocol for visiting each element in a certain order.iterablelistsetrangebuilt-in protocol for visiting each element in a certain orderIn a nutshell: Most commonly, a generator is a lazy, incrementally-pending list, and yield statements allow you to use function notation to program the list values the generator should incrementally spit out. Furthermore, advanced usage lets you use generators as coroutines (see below).a generator is a lazy, incrementally-pending listyield statements allow you to use function notation to program the list valuesyieldFurthermore, advanced usage lets you use generators as coroutines (see below).generator = myYieldingFunction(...)  # basically a list (but lazy)\nx = list(generator)  # evaluate every element into a list\n\n   generator\n       v\n[x[0], ..., ???]\n\n         generator\n             v\n[x[0], x[1], ..., ???]\n\n               generator\n                   v\n[x[0], x[1], x[2], ..., ???]\n\n                       StopIteration exception\n[x[0], x[1], x[2]]     done\ngenerator = myYieldingFunction(...)  # basically a list (but lazy)\nx = list(generator)  # evaluate every element into a list\n\n   generator\n       v\n[x[0], ..., ???]\n\n         generator\n             v\n[x[0], x[1], ..., ???]\n\n               generator\n                   v\n[x[0], x[1], x[2], ..., ???]\n\n                       StopIteration exception\n[x[0], x[1], x[2]]     done\nBasically, whenever the yield statement is encountered, the function pauses and saves its state, then emits \"the next return value in the 'list'\" according to the python iterator protocol (to some syntactic construct like a for-loop that repeatedly calls next() and catches a StopIteration exception, etc.). You might have encountered generators with generator expressions; generator functions are more powerful because you can pass arguments back into the paused generator function, using them to implement coroutines. More on that later.yieldnext()StopIterationgenerator expressionsBasic Example ('list')Let's define a function makeRange that's just like Python's range. Calling makeRange(n) RETURNS A GENERATOR:makeRangerangemakeRange(n)def makeRange(n):\n    # return 0,1,2,...,n-1\n    i = 0\n    while i < n:\n        yield i\n        i += 1\n\n>>> makeRange(5)\n<generator object makeRange at 0x19e4aa0>\ndef makeRange(n):\n    # return 0,1,2,...,n-1\n    i = 0\n    while i < n:\n        yield i\n        i += 1\n\n>>> makeRange(5)\n<generator object makeRange at 0x19e4aa0>\nTo force the generator to immediately return its pending values, you can pass it into list() (just like you could any iterable):list()>>> list(makeRange(5))\n[0, 1, 2, 3, 4]\n>>> list(makeRange(5))\n[0, 1, 2, 3, 4]\nComparing example to \"just returning a list\"The above example can be thought of as merely creating a list which you append to and return:# return a list                  #  # return a generator\ndef makeRange(n):                #  def makeRange(n):\n    \"\"\"return [0,1,2,...,n-1]\"\"\" #      \"\"\"return 0,1,2,...,n-1\"\"\"\n    TO_RETURN = []               # \n    i = 0                        #      i = 0\n    while i < n:                 #      while i < n:\n        TO_RETURN += [i]         #          yield i\n        i += 1                   #          i += 1\n    return TO_RETURN             # \n\n>>> makeRange(5)\n[0, 1, 2, 3, 4]\n# return a list                  #  # return a generator\ndef makeRange(n):                #  def makeRange(n):\n    \"\"\"return [0,1,2,...,n-1]\"\"\" #      \"\"\"return 0,1,2,...,n-1\"\"\"\n    TO_RETURN = []               # \n    i = 0                        #      i = 0\n    while i < n:                 #      while i < n:\n        TO_RETURN += [i]         #          yield i\n        i += 1                   #          i += 1\n    return TO_RETURN             # \n\n>>> makeRange(5)\n[0, 1, 2, 3, 4]\nThere is one major difference, though; see the last section.How you might use generatorsAn iterable is the last part of a list comprehension, and all generators are iterable, so they're often used like so:#                  < ITERABLE >\n>>> [x+10 for x in makeRange(5)]\n[10, 11, 12, 13, 14]\n#                  < ITERABLE >\n>>> [x+10 for x in makeRange(5)]\n[10, 11, 12, 13, 14]\nTo get a better feel for generators, you can play around with the itertools module (be sure to use chain.from_iterable rather than chain when warranted). For example, you might even use generators to implement infinitely-long lazy lists like itertools.count(). You could implement your own def enumerate(iterable): zip(count(), iterable), or alternatively do so with the yield keyword in a while-loop.itertoolschain.from_iterablechainitertools.count()def enumerate(iterable): zip(count(), iterable)yieldPlease note: generators can actually be used for many more things, such as implementing coroutines or non-deterministic programming or other elegant things. However, the \"lazy lists\" viewpoint I present here is the most common use you will find.implementing coroutinesBehind the scenesThis is how the \"Python iteration protocol\" works. That is, what is going on when you do list(makeRange(5)). This is what I describe earlier as a \"lazy, incremental list\".list(makeRange(5))>>> x=iter(range(5))\n>>> next(x)  # calls x.__next__(); x.next() is deprecated\n0\n>>> next(x)\n1\n>>> next(x)\n2\n>>> next(x)\n3\n>>> next(x)\n4\n>>> next(x)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration\n>>> x=iter(range(5))\n>>> next(x)  # calls x.__next__(); x.next() is deprecated\n0\n>>> next(x)\n1\n>>> next(x)\n2\n>>> next(x)\n3\n>>> next(x)\n4\n>>> next(x)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration\nThe built-in function next() just calls the objects .__next__() function, which is a part of the \"iteration protocol\" and is found on all iterators. You can manually use the next() function (and other parts of the iteration protocol) to implement fancy things, usually at the expense of readability, so try to avoid doing that...next().__next__()next()CoroutinesCoroutine example:Coroutinedef interactiveProcedure():\n    userResponse = yield makeQuestionWebpage()\n    print('user response:', userResponse)\n    yield 'success'\n\ncoroutine = interactiveProcedure()\nwebFormData = next(coroutine)  # same as .send(None)\nuserResponse = serveWebForm(webFormData)\n\n# ...at some point later on web form submit...\n\nsuccessStatus = coroutine.send(userResponse)\ndef interactiveProcedure():\n    userResponse = yield makeQuestionWebpage()\n    print('user response:', userResponse)\n    yield 'success'\n\ncoroutine = interactiveProcedure()\nwebFormData = next(coroutine)  # same as .send(None)\nuserResponse = serveWebForm(webFormData)\n\n# ...at some point later on web form submit...\n\nsuccessStatus = coroutine.send(userResponse)\nA coroutine (generators which generally accept input via the yield keyword e.g. nextInput = yield nextOutput, as a form of two-way communication) is basically a computation which is allowed to pause itself and request input (e.g. to what it should do next). When the coroutine pauses itself (when the running coroutine eventually hits a yield keyword), the computation is paused and control is inverted (yielded) back to the 'calling' function (the frame which requested the next value of the computation). The paused generator/coroutine remains paused until another invoking function (possibly a different function/context) requests the next value to unpause it (usually passing input data to direct the paused logic interior to the coroutine's code).yieldnextInput = yield nextOutputyieldnextYou can think of python coroutines as lazy incrementally-pending lists, where the next element doesn't just depend on the previous computation, but also on input you may opt to inject during the generation process.You can think of python coroutines as lazy incrementally-pending lists, where the next element doesn't just depend on the previous computation, but also on input you may opt to inject during the generation process.MinutiaeNormally, most people would not care about the following distinctions and probably want to stop reading here.In Python-speak, an iterable is any object which \"understands the concept of a for-loop\" like a list [1,2,3], and an iterator is a specific instance of the requested for-loop like [1,2,3].__iter__(). A generator is exactly the same as any iterator, except for the way it was written (with function syntax).iterable[1,2,3]iterator[1,2,3].__iter__()generatorWhen you request an iterator from a list, it creates a new iterator. However, when you request an iterator from an iterator (which you would rarely do), it just gives you a copy of itself.Thus, in the unlikely event that you are failing to do something like this...> x = myRange(5)\n> list(x)\n[0, 1, 2, 3, 4]\n> list(x)\n[]\n> x = myRange(5)\n> list(x)\n[0, 1, 2, 3, 4]\n> list(x)\n[]\n... then remember that a generator is an iterator; that is, it is one-time-use. If you want to reuse it, you should call myRange(...) again. If you need to use the result twice, convert the result to a list and store it in a variable x = list(myRange(5)). Those who absolutely need to clone a generator (for example, who are doing terrifyingly hackish metaprogramming) can use itertools.tee (still works in Python 3) if absolutely necessary, since the copyable iterator Python PEP standards proposal has been deferred.iteratormyRange(...)x = list(myRange(5))itertools.teeitertools.teestill works in Python 3copyable iterator Python PEP standards proposal",
                "\nWhat does the yield keyword do in Python?\nWhat does the yield keyword do in Python?What does the yield keyword do in Python?yieldAnswer Outline/Summary\nA function with yield, when called, returns a Generator.\nGenerators are iterators because they implement the iterator protocol, so you can iterate over them.\nA generator can also be sent information, making it conceptually a coroutine.\nIn Python 3, you can delegate from one generator to another in both directions with yield from.\n(Appendix critiques a couple of answers, including the top one, and discusses the use of return in a generator.)\nA function with yield, when called, returns a Generator.yieldyieldyieldreturns a Generator.GeneratorGenerators are iterators because they implement the iterator protocol, so you can iterate over them.iterator protocoliterator protocolA generator can also be sent information, making it conceptually a coroutine.sent informationcoroutineIn Python 3, you can delegate from one generator to another in both directions with yield from.delegateyield fromyield from(Appendix critiques a couple of answers, including the top one, and discusses the use of return in a generator.)returnGenerators:yield is only legal inside of a function definition, and the inclusion of yield in a function definition makes it return a generator.yieldyieldthe inclusion of yield in a function definition makes it return a generator.yieldThe idea for generators comes from other languages (see footnote 1) with varying implementations. In Python's Generators, the execution of the code is frozen at the point of the yield. When the generator is called (methods are discussed below) execution resumes and then freezes at the next yield.frozenyield provides an\neasy way of implementing the iterator protocol, defined by the following two methods:\n__iter__ and __next__.  Both of those methods\nmake an object an iterator that you could type-check with the Iterator Abstract Base\nClass from the collections module.yieldimplementing the iterator protocol__iter____next__Iteratorcollectionsdef func():\n    yield 'I am'\n    yield 'a generator!'\ndef func():\n    yield 'I am'\n    yield 'a generator!'\nLet's do some introspection:>>> type(func)                 # A function with yield is still a function\n<type 'function'>\n>>> gen = func()\n>>> type(gen)                  # but it returns a generator\n<type 'generator'>\n>>> hasattr(gen, '__iter__')   # that's an iterable\nTrue\n>>> hasattr(gen, '__next__')   # and with .__next__\nTrue                           # implements the iterator protocol.\n>>> type(func)                 # A function with yield is still a function\n<type 'function'>\n>>> gen = func()\n>>> type(gen)                  # but it returns a generator\n<type 'generator'>\n>>> hasattr(gen, '__iter__')   # that's an iterable\nTrue\n>>> hasattr(gen, '__next__')   # and with .__next__\nTrue                           # implements the iterator protocol.\nThe generator type is a sub-type of iterator:from types import GeneratorType\nfrom collections.abc import Iterator\n\n>>> issubclass(GeneratorType, Iterator)\nTrue\nfrom types import GeneratorType\nfrom collections.abc import Iterator\n\n>>> issubclass(GeneratorType, Iterator)\nTrue\nAnd if necessary, we can type-check like this:>>> isinstance(gen, GeneratorType)\nTrue\n>>> isinstance(gen, Iterator)\nTrue\n>>> isinstance(gen, GeneratorType)\nTrue\n>>> isinstance(gen, Iterator)\nTrue\nA feature of an Iterator is that once exhausted, you can't reuse or reset it:Iteratoris that once exhausted>>> list(gen)\n['I am', 'a generator!']\n>>> list(gen)\n[]\n>>> list(gen)\n['I am', 'a generator!']\n>>> list(gen)\n[]\nYou'll have to make another if you want to use its functionality again (see footnote 2):>>> list(func())\n['I am', 'a generator!']\n>>> list(func())\n['I am', 'a generator!']\nOne can yield data programmatically, for example:def func(an_iterable):\n    for item in an_iterable:\n        yield item\ndef func(an_iterable):\n    for item in an_iterable:\n        yield item\nThe above simple generator is also equivalent to the below - as of Python 3.3 you can use yield from:yield fromyield fromdef func(an_iterable):\n    yield from an_iterable\ndef func(an_iterable):\n    yield from an_iterable\nHowever, yield from also allows for delegation to subgenerators,\nwhich will be explained in the following section on cooperative delegation with sub-coroutines.yield fromCoroutines:yield forms an expression that allows data to be sent into the generator (see footnote 3)yieldHere is an example, take note of the received variable, which will point to the data that is sent to the generator:receiveddef bank_account(deposited, interest_rate):\n    while True:\n        calculated_interest = interest_rate * deposited \n        received = yield calculated_interest\n        if received:\n            deposited += received\n\n\n>>> my_account = bank_account(1000, .05)\ndef bank_account(deposited, interest_rate):\n    while True:\n        calculated_interest = interest_rate * deposited \n        received = yield calculated_interest\n        if received:\n            deposited += received\n\n\n>>> my_account = bank_account(1000, .05)\nFirst, we must queue up the generator with the builtin function, next. It will\ncall the appropriate next or __next__ method, depending on the version of\nPython you are using:nextnextnext__next__>>> first_year_interest = next(my_account)\n>>> first_year_interest\n50.0\n>>> first_year_interest = next(my_account)\n>>> first_year_interest\n50.0\nAnd now we can send data into the generator. (Sending None is\nthe same as calling next.) :Sending None is\nthe same as calling nextNonenext>>> next_year_interest = my_account.send(first_year_interest + 1000)\n>>> next_year_interest\n102.5\n>>> next_year_interest = my_account.send(first_year_interest + 1000)\n>>> next_year_interest\n102.5\nCooperative Delegation to Sub-Coroutine with yield fromyield fromNow, recall that yield from is available in Python 3. This allows us to delegate coroutines to a subcoroutine:yield from\ndef money_manager(expected_rate):\n    # must receive deposited value from .send():\n    under_management = yield                   # yield None to start.\n    while True:\n        try:\n            additional_investment = yield expected_rate * under_management \n            if additional_investment:\n                under_management += additional_investment\n        except GeneratorExit:\n            '''TODO: write function to send unclaimed funds to state'''\n            raise\n        finally:\n            '''TODO: write function to mail tax info to client'''\n        \n\ndef investment_account(deposited, manager):\n    '''very simple model of an investment account that delegates to a manager'''\n    # must queue up manager:\n    next(manager)      # <- same as manager.send(None)\n    # This is where we send the initial deposit to the manager:\n    manager.send(deposited)\n    try:\n        yield from manager\n    except GeneratorExit:\n        return manager.close()  # delegate?\n\ndef money_manager(expected_rate):\n    # must receive deposited value from .send():\n    under_management = yield                   # yield None to start.\n    while True:\n        try:\n            additional_investment = yield expected_rate * under_management \n            if additional_investment:\n                under_management += additional_investment\n        except GeneratorExit:\n            '''TODO: write function to send unclaimed funds to state'''\n            raise\n        finally:\n            '''TODO: write function to mail tax info to client'''\n        \n\ndef investment_account(deposited, manager):\n    '''very simple model of an investment account that delegates to a manager'''\n    # must queue up manager:\n    next(manager)      # <- same as manager.send(None)\n    # This is where we send the initial deposit to the manager:\n    manager.send(deposited)\n    try:\n        yield from manager\n    except GeneratorExit:\n        return manager.close()  # delegate?\nAnd now we can delegate functionality to a sub-generator and it can be used\nby a generator just as above:my_manager = money_manager(.06)\nmy_account = investment_account(1000, my_manager)\nfirst_year_return = next(my_account) # -> 60.0\nmy_manager = money_manager(.06)\nmy_account = investment_account(1000, my_manager)\nfirst_year_return = next(my_account) # -> 60.0\nNow simulate adding another 1,000 to the account plus the return on the account (60.0):next_year_return = my_account.send(first_year_return + 1000)\nnext_year_return # 123.6\nnext_year_return = my_account.send(first_year_return + 1000)\nnext_year_return # 123.6\nYou can read more about the precise semantics of yield from in PEP 380.yield fromPEP 380.Other Methods: close and throwThe close method raises GeneratorExit at the point the function\nexecution was frozen. This will also be called by __del__ so you\ncan put any cleanup code where you handle the GeneratorExit:closeGeneratorExit__del__GeneratorExitmy_account.close()\nmy_account.close()\nYou can also throw an exception which can be handled in the generator\nor propagated back to the user:import sys\ntry:\n    raise ValueError\nexcept:\n    my_manager.throw(*sys.exc_info())\nimport sys\ntry:\n    raise ValueError\nexcept:\n    my_manager.throw(*sys.exc_info())\nRaises:Traceback (most recent call last):\n  File \"<stdin>\", line 4, in <module>\n  File \"<stdin>\", line 6, in money_manager\n  File \"<stdin>\", line 2, in <module>\nValueError\nTraceback (most recent call last):\n  File \"<stdin>\", line 4, in <module>\n  File \"<stdin>\", line 6, in money_manager\n  File \"<stdin>\", line 2, in <module>\nValueError\nConclusionI believe I have covered all aspects of the following question:\nWhat does the yield keyword do in Python?\nWhat does the yield keyword do in Python?What does the yield keyword do in Python?yieldIt turns out that yield does a lot. I'm sure I could add even more\nthorough examples to this. If you want more or have some constructive criticism, let me know by commenting\nbelow.yieldAppendix:Critique of the Top/Accepted Answer**\nIt is confused on what makes an iterable, just using a list as an example. See my references above, but in summary: an iterable has an __iter__ method returning an iterator. An iterator additionally provides a .__next__ method, which is implicitly called by for loops until it raises StopIteration, and once it does raise StopIteration, it will continue to do so.\nIt then uses a generator expression to describe what a generator is. Since a generator expression is simply a convenient way to create an iterator, it only confuses the matter, and we still have not yet gotten to the yield part.\nIn Controlling a generator exhaustion he calls the .next method (which only works in Python 2), when instead he should use the builtin function, next. Calling next(obj) would be an appropriate layer of indirection, because his code does not work in Python 3.\nItertools? This was not relevant to what yield does at all.\nNo discussion of the methods that yield provides along with the new functionality yield from in Python 3.\nIt is confused on what makes an iterable, just using a list as an example. See my references above, but in summary: an iterable has an __iter__ method returning an iterator. An iterator additionally provides a .__next__ method, which is implicitly called by for loops until it raises StopIteration, and once it does raise StopIteration, it will continue to do so.iterableiterable__iter__iteratoriterator.__next__forStopIterationStopIterationIt then uses a generator expression to describe what a generator is. Since a generator expression is simply a convenient way to create an iterator, it only confuses the matter, and we still have not yet gotten to the yield part.iteratoryieldIn Controlling a generator exhaustion he calls the .next method (which only works in Python 2), when instead he should use the builtin function, next. Calling next(obj) would be an appropriate layer of indirection, because his code does not work in Python 3.Controlling a generator exhaustion.nextnextnext(obj)Itertools? This was not relevant to what yield does at all.yieldNo discussion of the methods that yield provides along with the new functionality yield from in Python 3.yieldyield fromThe top/accepted answer is a very incomplete answer.The top/accepted answer is a very incomplete answer.Critique of answer suggesting yield in a generator expression or comprehension.yieldThe grammar currently allows any expression in a list comprehension.expr_stmt: testlist_star_expr (annassign | augassign (yield_expr|testlist) |\n                     ('=' (yield_expr|testlist_star_expr))*)\n...\nyield_expr: 'yield' [yield_arg]\nyield_arg: 'from' test | testlist\nexpr_stmt: testlist_star_expr (annassign | augassign (yield_expr|testlist) |\n                     ('=' (yield_expr|testlist_star_expr))*)\n...\nyield_expr: 'yield' [yield_arg]\nyield_arg: 'from' test | testlist\nSince yield is an expression, it has been touted by some as interesting to use it in comprehensions or generator expression - in spite of citing no particularly good use-case.The CPython core developers are discussing deprecating its allowance.\nHere's a relevant post from the mailing list:discussing deprecating its allowance\nOn 30 January 2017 at 19:05, Brett Cannon  wrote:\n\nOn Sun, 29 Jan 2017 at 16:39 Craig Rodrigues  wrote:\n\nI'm OK with either approach.  Leaving things the way they are in Python 3\nis no good, IMHO.\n\nMy vote is it be a SyntaxError since you're not getting what you expect from\nthe syntax.\n\nI'd agree that's a sensible place for us to end up, as any code\nrelying on the current behaviour is really too clever to be\nmaintainable.\nIn terms of getting there, we'll likely want:\n\nSyntaxWarning or DeprecationWarning in 3.7\nPy3k warning in 2.7.x\nSyntaxError in 3.8\n\nCheers, Nick.\n--  Nick Coghlan   |   ncoghlan at gmail.com   |   Brisbane, Australia\nOn 30 January 2017 at 19:05, Brett Cannon  wrote:\nOn Sun, 29 Jan 2017 at 16:39 Craig Rodrigues  wrote:\n\nI'm OK with either approach.  Leaving things the way they are in Python 3\nis no good, IMHO.\n\nMy vote is it be a SyntaxError since you're not getting what you expect from\nthe syntax.\nOn Sun, 29 Jan 2017 at 16:39 Craig Rodrigues  wrote:\nI'm OK with either approach.  Leaving things the way they are in Python 3\nis no good, IMHO.\nI'm OK with either approach.  Leaving things the way they are in Python 3\nis no good, IMHO.My vote is it be a SyntaxError since you're not getting what you expect from\nthe syntax.I'd agree that's a sensible place for us to end up, as any code\nrelying on the current behaviour is really too clever to be\nmaintainable.In terms of getting there, we'll likely want:\nSyntaxWarning or DeprecationWarning in 3.7\nPy3k warning in 2.7.x\nSyntaxError in 3.8\nSyntaxWarning or DeprecationWarning in 3.7Py3k warning in 2.7.xSyntaxError in 3.8Cheers, Nick.--  Nick Coghlan   |   ncoghlan at gmail.com   |   Brisbane, AustraliaFurther, there is an outstanding issue (10544) which seems to be pointing in the direction of this never being a good idea (PyPy, a Python implementation written in Python, is already raising syntax warnings.)outstanding issue (10544)neverBottom line, until the developers of CPython tell us otherwise: Don't put yield in a generator expression or comprehension.Don't put yield in a generator expression or comprehension.yieldThe return statement in a generatorreturnIn Python 3:Python 3\nIn a generator function, the return statement indicates that the generator is done and will cause StopIteration to be raised. The returned value (if any) is used as an argument to construct StopIteration and becomes the StopIteration.value attribute.\nIn a generator function, the return statement indicates that the generator is done and will cause StopIteration to be raised. The returned value (if any) is used as an argument to construct StopIteration and becomes the StopIteration.value attribute.returnStopIterationStopIterationStopIteration.valueHistorical note, in Python 2:\n\"In a generator function, the return statement is not allowed to include an expression_list. In that context, a bare return indicates that the generator is done and will cause StopIteration to be raised.\"\nAn expression_list is basically any number of expressions separated by commas - essentially, in Python 2, you can stop the generator with return, but you can't return a value.\nHistorical note, in Python 2:\n\"In a generator function, the return statement is not allowed to include an expression_list. In that context, a bare return indicates that the generator is done and will cause StopIteration to be raised.\"\nAn expression_list is basically any number of expressions separated by commas - essentially, in Python 2, you can stop the generator with return, but you can't return a value.\nPython 2returnexpression_listreturnStopIterationexpression_listreturnFootnotes\nThe languages CLU, Sather, and Icon were referenced in the proposal\nto introduce the concept of generators to Python. The general idea is\nthat a function can maintain internal state and yield intermediate\ndata points on demand by the user. This promised to be superior in performance\nto other approaches, including Python threading, which isn't even available on some systems.\n\n This means, for example, that range objects aren't Iterators, even though they are iterable, because they can be reused. Like lists, their __iter__ methods return iterator objects.\n\n yield was originally introduced as a statement, meaning that it\ncould only appear at the beginning of a line in a code block.\nNow yield creates a yield expression.\nhttps://docs.python.org/2/reference/simple_stmts.html#grammar-token-yield_stmt\nThis change was proposed to allow a user to send data into the generator just as\none might receive it. To send data, one must be able to assign it to something, and\nfor that, a statement just won't work.\n\nThe languages CLU, Sather, and Icon were referenced in the proposal\nto introduce the concept of generators to Python. The general idea is\nthat a function can maintain internal state and yield intermediate\ndata points on demand by the user. This promised to be superior in performance\nto other approaches, including Python threading, which isn't even available on some systems.\nThe languages CLU, Sather, and Icon were referenced in the proposal\nto introduce the concept of generators to Python. The general idea is\nthat a function can maintain internal state and yield intermediate\ndata points on demand by the user. This promised to be superior in performance\nto other approaches, including Python threading, which isn't even available on some systems.The languages CLU, Sather, and Icon were referenced in the proposal\nto introduce the concept of generators to Python. The general idea is\nthat a function can maintain internal state and yield intermediate\ndata points on demand by the user. This promised to be superior in performance\nto other approaches, including Python threading, which isn't even available on some systems.superior in performance\nto other approaches, including Python threading This means, for example, that range objects aren't Iterators, even though they are iterable, because they can be reused. Like lists, their __iter__ methods return iterator objects.\n This means, for example, that range objects aren't Iterators, even though they are iterable, because they can be reused. Like lists, their __iter__ methods return iterator objects. This means, for example, that range objects aren't Iterators, even though they are iterable, because they can be reused. Like lists, their __iter__ methods return iterator objects.rangeIterator__iter__ yield was originally introduced as a statement, meaning that it\ncould only appear at the beginning of a line in a code block.\nNow yield creates a yield expression.\nhttps://docs.python.org/2/reference/simple_stmts.html#grammar-token-yield_stmt\nThis change was proposed to allow a user to send data into the generator just as\none might receive it. To send data, one must be able to assign it to something, and\nfor that, a statement just won't work.\n yield was originally introduced as a statement, meaning that it\ncould only appear at the beginning of a line in a code block.\nNow yield creates a yield expression.\nhttps://docs.python.org/2/reference/simple_stmts.html#grammar-token-yield_stmt\nThis change was proposed to allow a user to send data into the generator just as\none might receive it. To send data, one must be able to assign it to something, and\nfor that, a statement just won't work. yield was originally introduced as a statement, meaning that it\ncould only appear at the beginning of a line in a code block.\nNow yield creates a yield expression.\nhttps://docs.python.org/2/reference/simple_stmts.html#grammar-token-yield_stmt\nThis change was proposed to allow a user to send data into the generator just as\none might receive it. To send data, one must be able to assign it to something, and\nfor that, a statement just won't work.yieldyieldhttps://docs.python.org/2/reference/simple_stmts.html#grammar-token-yield_stmtproposed",
                "yield is just like return - it returns whatever you tell it to (as a generator). The difference is that the next time you call the generator, execution starts from the last call to the yield statement. Unlike return, the stack frame is not cleaned up when a yield occurs, however control is transferred back to the caller, so its state will resume the next time the function is called.yieldreturnyieldthe stack frame is not cleaned up when a yield occurs, however control is transferred back to the caller, so its state will resume the next time the function is called.In the case of your code, the function get_child_candidates is acting like an iterator so that when you extend your list, it adds one element at a time to the new list.get_child_candidateslist.extend calls an iterator until it's exhausted. In the case of the code sample you posted, it would be much clearer to just return a tuple and append that to the list.list.extend",
                "There's one extra thing to mention: a function that yields doesn't actually have to terminate. I've written code like this:def fib():\n    last, cur = 0, 1\n    while True: \n        yield cur\n        last, cur = cur, last + cur\ndef fib():\n    last, cur = 0, 1\n    while True: \n        yield cur\n        last, cur = cur, last + cur\nThen I can use it in other code like this:for f in fib():\n    if some_condition: break\n    coolfuncs(f);\nfor f in fib():\n    if some_condition: break\n    coolfuncs(f);\nIt really helps simplify some problems, and makes some things easier to work with. ",
                "For those who prefer a minimal working example, meditate on this interactive Python session:>>> def f():\n...   yield 1\n...   yield 2\n...   yield 3\n... \n>>> g = f()\n>>> for i in g:\n...   print(i)\n... \n1\n2\n3\n>>> for i in g:\n...   print(i)\n... \n>>> # Note that this time nothing was printed\n>>> def f():\n...   yield 1\n...   yield 2\n...   yield 3\n... \n>>> g = f()\n>>> for i in g:\n...   print(i)\n... \n1\n2\n3\n>>> for i in g:\n...   print(i)\n... \n>>> # Note that this time nothing was printed\n",
                "TL;DRTL;DRInstead of this:def square_list(n):\n    the_list = []                         # Replace\n    for x in range(n):\n        y = x * x\n        the_list.append(y)                # these\n    return the_list                       # lines\ndef square_list(n):\n    the_list = []                         # Replace\n    for x in range(n):\n        y = x * x\n        the_list.append(y)                # these\n    return the_list                       # lines\ndo this:def square_yield(n):\n    for x in range(n):\n        y = x * x\n        yield y                           # with this one.\ndef square_yield(n):\n    for x in range(n):\n        y = x * x\n        yield y                           # with this one.\nWhenever you find yourself building a list from scratch, yield each piece instead. yieldThis was my first \"aha\" moment with yield.yield is a sugary way to say yieldsugary\nbuild a series of stuff\nbuild a series of stuffSame behavior:>>> for square in square_list(4):\n...     print(square)\n...\n0\n1\n4\n9\n>>> for square in square_yield(4):\n...     print(square)\n...\n0\n1\n4\n9\n>>> for square in square_list(4):\n...     print(square)\n...\n0\n1\n4\n9\n>>> for square in square_yield(4):\n...     print(square)\n...\n0\n1\n4\n9\nDifferent behavior:Yield is single-pass: you can only iterate through once. When a function has a yield in it we call it a generator function. And an iterator is what it returns. Those terms are revealing. We lose the convenience of a container, but gain the power of a series that's computed as needed, and arbitrarily long.single-passgenerator functioniteratorYield is lazy, it puts off computation. A function with a yield in it doesn't actually execute at all when you call it. It returns an iterator object that remembers where it left off. Each time you call next() on the iterator (this happens in a for-loop) execution inches forward to the next yield. return raises StopIteration and ends the series (this is the natural end of a for-loop).lazydoesn't actually execute at all when you call it.iterator objectnext()returnYield is versatile. Data doesn't have to be stored all together, it can be made available one at a time. It can be infinite.versatile>>> def squares_all_of_them():\n...     x = 0\n...     while True:\n...         yield x * x\n...         x += 1\n...\n>>> squares = squares_all_of_them()\n>>> for _ in range(4):\n...     print(next(squares))\n...\n0\n1\n4\n9\n>>> def squares_all_of_them():\n...     x = 0\n...     while True:\n...         yield x * x\n...         x += 1\n...\n>>> squares = squares_all_of_them()\n>>> for _ in range(4):\n...     print(next(squares))\n...\n0\n1\n4\n9\nIf you need multiple passes and the series isn't too long, just call list() on it:multiple passeslist()>>> list(square_yield(4))\n[0, 1, 4, 9]\n>>> list(square_yield(4))\n[0, 1, 4, 9]\nBrilliant choice of the word yield because both meanings apply:yieldboth meanings\nyield \u2014 produce or provide (as in agriculture)\nyield \u2014 produce or provide (as in agriculture)yield...provide the next data in the series.\nyield \u2014 give way or relinquish (as in political power)\nyield \u2014 give way or relinquish (as in political power)yield...relinquish CPU execution until the iterator advances.",
                "Yield gives you a generator. def get_odd_numbers(i):\n    return range(1, i, 2)\ndef yield_odd_numbers(i):\n    for x in range(1, i, 2):\n       yield x\nfoo = get_odd_numbers(10)\nbar = yield_odd_numbers(10)\nfoo\n[1, 3, 5, 7, 9]\nbar\n<generator object yield_odd_numbers at 0x1029c6f50>\nbar.next()\n1\nbar.next()\n3\nbar.next()\n5\ndef get_odd_numbers(i):\n    return range(1, i, 2)\ndef yield_odd_numbers(i):\n    for x in range(1, i, 2):\n       yield x\nfoo = get_odd_numbers(10)\nbar = yield_odd_numbers(10)\nfoo\n[1, 3, 5, 7, 9]\nbar\n<generator object yield_odd_numbers at 0x1029c6f50>\nbar.next()\n1\nbar.next()\n3\nbar.next()\n5\nAs you can see, in the first case foo holds the entire list in memory at once. It's not a big deal for a list with 5 elements, but what if you want a list of 5 million? Not only is this a huge memory eater, it also costs a lot of time to build at the time that the function is called.fooIn the second case, bar just gives you a generator. A generator is an iterable--which means you can use it in a for loop, etc, but each value can only be accessed once. All the values are also not stored in memory at the same time; the generator object \"remembers\" where it was in the looping the last time you called it--this way, if you're using an iterable to (say) count to 50 billion, you don't have to count to 50 billion all at once and store the 50 billion numbers to count through.barforAgain, this is a pretty contrived example, you probably would use itertools if you really wanted to count to 50 billion. :)This is the most simple use case of generators. As you said, it can be used to write efficient permutations, using yield to push things up through the call stack instead of using some sort of stack variable. Generators can also be used for specialized tree traversal, and all manner of other things.",
                "It's returning a generator. I'm not particularly familiar with Python, but I believe it's the same kind of thing as C#'s iterator blocks if you're familiar with those.C#'s iterator blocksThe key idea is that the compiler/interpreter/whatever does some trickery so that as far as the caller is concerned, they can keep calling next() and it will keep returning values - as if the generator method was paused. Now obviously you can't really \"pause\" a method, so the compiler builds a state machine for you to remember where you currently are and what the local variables etc look like. This is much easier than writing an iterator yourself.as if the generator method was paused",
                "There is one type of answer that I don't feel has been given yet, among the many great answers that describe how to use generators. Here is the programming language theory answer:The yield statement in Python returns a generator. A generator in Python is a function that returns continuations (and specifically a type of coroutine, but continuations represent the more general mechanism to understand what is going on).yieldcontinuationsContinuations in programming languages theory are a much more fundamental kind of computation, but they are not often used, because they are extremely hard to reason about and also very difficult to implement. But the idea of what a continuation is, is straightforward: it is the state of a computation that has not yet finished. In this state, the current values of variables, the operations that have yet to be performed, and so on, are saved. Then at some point later in the program the continuation can be invoked, such that the program's variables are reset to that state and the operations that were saved are carried out.Continuations, in this more general form, can be implemented in two ways. In the call/cc way, the program's stack is literally saved and then when the continuation is invoked, the stack is restored.call/ccIn continuation passing style (CPS), continuations are just normal functions (only in languages where functions are first class) which the programmer explicitly manages and passes around to subroutines. In this style, program state is represented by closures (and the variables that happen to be encoded in them) rather than variables that reside somewhere on the stack. Functions that manage control flow accept continuation as arguments (in some variations of CPS, functions may accept multiple continuations) and manipulate control flow by invoking them by simply calling them and returning afterwards. A very simple example of continuation passing style is as follows:def save_file(filename):\n  def write_file_continuation():\n    write_stuff_to_file(filename)\n\n  check_if_file_exists_and_user_wants_to_overwrite(write_file_continuation)\ndef save_file(filename):\n  def write_file_continuation():\n    write_stuff_to_file(filename)\n\n  check_if_file_exists_and_user_wants_to_overwrite(write_file_continuation)\nIn this (very simplistic) example, the programmer saves the operation of actually writing the file into a continuation (which can potentially be a very complex operation with many details to write out), and then passes that continuation (i.e, as a first-class closure) to another operator which does some more processing, and then calls it if necessary. (I use this design pattern a lot in actual GUI programming, either because it saves me lines of code or, more importantly, to manage control flow after GUI events trigger.)The rest of this post will, without loss of generality, conceptualize continuations as CPS, because it is a hell of a lot easier to understand and read.Now let's talk about generators in Python. Generators are a specific subtype of continuation. Whereas continuations are able in general to save the state of a computation (i.e., the program's call stack), generators are only able to save the state of iteration over an iterator. Although, this definition is slightly misleading for certain use cases of generators. For instance:continuations are able in general to save the state of a computationcomputationgenerators are only able to save the state of iteration over an iteratoriteratordef f():\n  while True:\n    yield 4\ndef f():\n  while True:\n    yield 4\nThis is clearly a reasonable iterable whose behavior is well defined -- each time the generator iterates over it, it returns 4 (and does so forever). But it isn't probably the prototypical type of iterable that comes to mind when thinking of iterators (i.e., for x in collection: do_something(x)). This example illustrates the power of generators: if anything is an iterator, a generator can save the state of its iteration.for x in collection: do_something(x)To reiterate: Continuations can save the state of a program's stack and generators can save the state of iteration. This means that continuations are more a lot powerful than generators, but also that generators are a lot, lot easier. They are easier for the language designer to implement, and they are easier for the programmer to use (if you have some time to burn, try to read and understand this page about continuations and call/cc).this page about continuations and call/ccBut you could easily implement (and conceptualize) generators as a simple, specific case of continuation passing style:Whenever yield is called, it tells the function to return a continuation.  When the function is called again, it starts from wherever it left off. So, in pseudo-pseudocode (i.e., not pseudocode, but not code) the generator's next method is basically as follows:yieldnextclass Generator():\n  def __init__(self,iterable,generatorfun):\n    self.next_continuation = lambda:generatorfun(iterable)\n\n  def next(self):\n    value, next_continuation = self.next_continuation()\n    self.next_continuation = next_continuation\n    return value\nclass Generator():\n  def __init__(self,iterable,generatorfun):\n    self.next_continuation = lambda:generatorfun(iterable)\n\n  def next(self):\n    value, next_continuation = self.next_continuation()\n    self.next_continuation = next_continuation\n    return value\nwhere the yield keyword is actually syntactic sugar for the real generator function, basically something like:yielddef generatorfun(iterable):\n  if len(iterable) == 0:\n    raise StopIteration\n  else:\n    return (iterable[0], lambda:generatorfun(iterable[1:]))\ndef generatorfun(iterable):\n  if len(iterable) == 0:\n    raise StopIteration\n  else:\n    return (iterable[0], lambda:generatorfun(iterable[1:]))\nRemember that this is just pseudocode and the actual implementation of generators in Python is more complex. But as an exercise to understand what is going on, try to use continuation passing style to implement generator objects without use of the yield keyword.yield",
                "Here is an example in plain language. I will provide a correspondence between high-level human concepts to low-level Python concepts.I want to operate on a sequence of numbers, but I don't want to bother my self with the creation of that sequence, I want only to focus on the operation I want to do. So, I do the following:\nI call you and tell you that I want a sequence of numbers which are calculated in a specific way, and I let you know what the algorithm is. \nThis step corresponds to defining the generator function, i.e. the function containing a yield.\nSometime later, I tell you, \"OK, get ready to tell me the sequence of numbers\". \nThis step corresponds to calling the generator function which returns a generator object. Note that you don't tell me any numbers yet; you just grab your paper and pencil.\nI ask you, \"tell me the next number\", and you tell me the first number; after that, you wait for me to ask you for the next number. It's your job to remember where you were, what numbers you have already said, and what is the next number. I don't care about the details. \nThis step corresponds to calling next(generator) on the generator object.\n(In Python 2, .next was a method of the generator object; in Python 3, it is named .__next__, but the proper way to call it is using the builtin next() function just like len() and .__len__)\n\u2026 repeat previous step, until\u2026\neventually, you might come to an end. You don't tell me a number; you just shout, \"hold your horses! I'm done! No more numbers!\" \nThis step corresponds to the generator object ending its job, and raising a StopIteration exception.\nThe generator function does not need to raise the exception. It's raised automatically when the function ends or issues a return.\nI call you and tell you that I want a sequence of numbers which are calculated in a specific way, and I let you know what the algorithm is. \nThis step corresponds to defining the generator function, i.e. the function containing a yield.\nThis step corresponds to defining the generator function, i.e. the function containing a yield.This step corresponds to defining the generator function, i.e. the function containing a yield.defyieldSometime later, I tell you, \"OK, get ready to tell me the sequence of numbers\". \nThis step corresponds to calling the generator function which returns a generator object. Note that you don't tell me any numbers yet; you just grab your paper and pencil.This step corresponds to calling the generator function which returns a generator object.I ask you, \"tell me the next number\", and you tell me the first number; after that, you wait for me to ask you for the next number. It's your job to remember where you were, what numbers you have already said, and what is the next number. I don't care about the details. \nThis step corresponds to calling next(generator) on the generator object.\n(In Python 2, .next was a method of the generator object; in Python 3, it is named .__next__, but the proper way to call it is using the builtin next() function just like len() and .__len__)This step corresponds to calling next(generator) on the generator object.next(generator).next.__next__next()len().__len__\u2026 repeat previous step, until\u2026eventually, you might come to an end. You don't tell me a number; you just shout, \"hold your horses! I'm done! No more numbers!\" \nThis step corresponds to the generator object ending its job, and raising a StopIteration exception.\nThe generator function does not need to raise the exception. It's raised automatically when the function ends or issues a return.This step corresponds to the generator object ending its job, and raising a StopIteration exception.StopIterationreturnThis is what a generator does (a function that contains a yield); it starts executing on the first next(), pauses whenever it does a yield, and when asked for the next() value it continues from the point it was last. It fits perfectly by design with the iterator protocol of Python, which describes how to sequentially request values.yieldnext()yieldnext()The most famous user of the iterator protocol is the for command in Python. So, whenever you do a:forfor item in sequence:\nfor item in sequence:\nit doesn't matter if sequence is a list, a string, a dictionary or a generator object like described above; the result is the same: you read items off a sequence one by one.sequenceobjectNote that defining a function which contains a yield keyword is not the only way to create a generator; it's just the easiest way to create one.defyieldFor more accurate information, read about iterator types, the yield statement and generators in the Python documentation.iterator typesyield statementgenerators",
                "While a lot of answers show why you'd use a yield to create a generator, there are more uses for yield.  It's quite easy to make a coroutine, which enables the passing of information between two blocks of code.  I won't repeat any of the fine examples that have already been given about using yield to create a generator.yieldyieldyieldTo help understand what a yield does in the following code, you can use your finger to trace the cycle through any code that has a yield.  Every time your finger hits the yield, you have to wait for a next or a send to be entered.  When a next is called, you trace through the code until you hit the yield\u2026 the code on the right of the yield is evaluated and returned to the caller\u2026 then you wait.  When next is called again, you perform another loop through the code.  However, you'll note that in a coroutine, yield can also be used with a send\u2026 which will send a value from the caller into the yielding function. If a send is given, then yield receives the value sent, and spits it out the left hand side\u2026 then the trace through the code progresses until you hit the yield again (returning the value at the end, as if next was called).yieldyieldyieldnextsendnextyieldyieldnextyieldsendintosendyieldyieldnextFor example:>>> def coroutine():\n...     i = -1\n...     while True:\n...         i += 1\n...         val = (yield i)\n...         print(\"Received %s\" % val)\n...\n>>> sequence = coroutine()\n>>> sequence.next()\n0\n>>> sequence.next()\nReceived None\n1\n>>> sequence.send('hello')\nReceived hello\n2\n>>> sequence.close()\n>>> def coroutine():\n...     i = -1\n...     while True:\n...         i += 1\n...         val = (yield i)\n...         print(\"Received %s\" % val)\n...\n>>> sequence = coroutine()\n>>> sequence.next()\n0\n>>> sequence.next()\nReceived None\n1\n>>> sequence.send('hello')\nReceived hello\n2\n>>> sequence.close()\n",
                "There is another yield use and meaning (since Python 3.3):yieldyield from <expr>\nyield from <expr>\nFrom PEP 380 -- Syntax for Delegating to a Subgenerator:PEP 380 -- Syntax for Delegating to a SubgeneratorPEP 380 -- Syntax for Delegating to a Subgenerator\nA syntax is proposed for a generator to delegate part of its operations to another generator. This allows a section of code containing 'yield' to be factored out and placed in another generator. Additionally, the subgenerator is allowed to return with a value, and the value is made available to the delegating generator.\nThe new syntax also opens up some opportunities for optimisation when one generator re-yields values produced by another.\nA syntax is proposed for a generator to delegate part of its operations to another generator. This allows a section of code containing 'yield' to be factored out and placed in another generator. Additionally, the subgenerator is allowed to return with a value, and the value is made available to the delegating generator.The new syntax also opens up some opportunities for optimisation when one generator re-yields values produced by another.Moreover this will introduce (since Python 3.5):thisasync def new_coroutine(data):\n   ...\n   await blocking_action()\nasync def new_coroutine(data):\n   ...\n   await blocking_action()\nto avoid coroutines being confused with a regular generator (today yield is used in both).yield",
                "All great answers, however a bit difficult for newbies.I assume you have learned the return statement.returnAs an analogy, return and yield are twins. return means 'return and stop' whereas 'yield` means 'return, but continue'returnyieldreturn\n\nTry to get a num_list with return.\n\n\nTry to get a num_list with return.\nTry to get a num_list with return.returndef num_list(n):\n    for i in range(n):\n        return i\ndef num_list(n):\n    for i in range(n):\n        return i\nRun it:In [5]: num_list(3)\nOut[5]: 0\nIn [5]: num_list(3)\nOut[5]: 0\nSee, you get only a single number rather than a list of them. return never allows you prevail happily, just implements once and quit.return\n\nThere comes yield\n\n\nThere comes yield\nThere comes yieldyieldReplace return with yield:returnyieldIn [10]: def num_list(n):\n    ...:     for i in range(n):\n    ...:         yield i\n    ...:\n\nIn [11]: num_list(3)\nOut[11]: <generator object num_list at 0x10327c990>\n\nIn [12]: list(num_list(3))\nOut[12]: [0, 1, 2]\nIn [10]: def num_list(n):\n    ...:     for i in range(n):\n    ...:         yield i\n    ...:\n\nIn [11]: num_list(3)\nOut[11]: <generator object num_list at 0x10327c990>\n\nIn [12]: list(num_list(3))\nOut[12]: [0, 1, 2]\nNow, you win to get all the numbers.Comparing to return which runs once and stops, yield runs times you planed.\nYou can interpret return as return one of them, and yield as return all of them. This is called iterable.returnyieldreturnreturn one of themyieldreturn all of themiterable\n\nOne more step we can rewrite yield statement with return\n\n\nOne more step we can rewrite yield statement with return\nOne more step we can rewrite yield statement with returnyieldreturnIn [15]: def num_list(n):\n    ...:     result = []\n    ...:     for i in range(n):\n    ...:         result.append(i)\n    ...:     return result\n\nIn [16]: num_list(3)\nOut[16]: [0, 1, 2]\nIn [15]: def num_list(n):\n    ...:     result = []\n    ...:     for i in range(n):\n    ...:         result.append(i)\n    ...:     return result\n\nIn [16]: num_list(3)\nOut[16]: [0, 1, 2]\nIt's the core about yield.yieldThe difference between a list return outputs and the object yield output is:returnyieldYou will always get [0, 1, 2] from a list object but only could retrieve them from 'the object yield output' once. So, it has a new name generator object as displayed in Out[11]: <generator object num_list at 0x10327c990>.yieldgeneratorOut[11]: <generator object num_list at 0x10327c990>In conclusion, as a metaphor to grok it:\nreturn and yield are twins\nlist and generator are twins\nreturn and yield are twinsreturnyieldlist and generator are twinslistgenerator",
                "From a programming viewpoint, the iterators are implemented as thunks.thunksTo implement iterators, generators, and thread pools for concurrent execution, etc. as thunks, one uses messages sent to a closure object, which has a dispatcher, and the dispatcher answers to \"messages\".messages sent to a closure objectdispatcher answers to \"messages\"\"next\" is a message sent to a closure, created by the \"iter\" call.\"next\"nextiterThere are lots of ways to implement this computation. I used mutation, but it is possible to do this kind of computation without mutation, by returning the current value and the next yielder (making it referential transparent).  Racket uses a sequence of transformations of the initial program in some intermediary languages, one of such rewriting making the yield operator to be transformed in some language with simpler operators.referential transparentHere is a demonstration of how yield could be rewritten, which uses the structure of R6RS, but the semantics is identical to Python's. It's the same model of computation, and only a change in syntax is required to rewrite it using yield of Python.\nWelcome to Racket v6.5.0.3.\n\n-> (define gen\n     (lambda (l)\n       (define yield\n         (lambda ()\n           (if (null? l)\n               'END\n               (let ((v (car l)))\n                 (set! l (cdr l))\n                 v))))\n       (lambda(m)\n         (case m\n           ('yield (yield))\n           ('init  (lambda (data)\n                     (set! l data)\n                     'OK))))))\n-> (define stream (gen '(1 2 3)))\n-> (stream 'yield)\n1\n-> (stream 'yield)\n2\n-> (stream 'yield)\n3\n-> (stream 'yield)\n'END\n-> ((stream 'init) '(a b))\n'OK\n-> (stream 'yield)\n'a\n-> (stream 'yield)\n'b\n-> (stream 'yield)\n'END\n-> (stream 'yield)\n'END\n->\n\nWelcome to Racket v6.5.0.3.\n\n-> (define gen\n     (lambda (l)\n       (define yield\n         (lambda ()\n           (if (null? l)\n               'END\n               (let ((v (car l)))\n                 (set! l (cdr l))\n                 v))))\n       (lambda(m)\n         (case m\n           ('yield (yield))\n           ('init  (lambda (data)\n                     (set! l data)\n                     'OK))))))\n-> (define stream (gen '(1 2 3)))\n-> (stream 'yield)\n1\n-> (stream 'yield)\n2\n-> (stream 'yield)\n3\n-> (stream 'yield)\n'END\n-> ((stream 'init) '(a b))\n'OK\n-> (stream 'yield)\n'a\n-> (stream 'yield)\n'b\n-> (stream 'yield)\n'END\n-> (stream 'yield)\n'END\n->\nWelcome to Racket v6.5.0.3.\n\n-> (define gen\n     (lambda (l)\n       (define yield\n         (lambda ()\n           (if (null? l)\n               'END\n               (let ((v (car l)))\n                 (set! l (cdr l))\n                 v))))\n       (lambda(m)\n         (case m\n           ('yield (yield))\n           ('init  (lambda (data)\n                     (set! l data)\n                     'OK))))))\n-> (define stream (gen '(1 2 3)))\n-> (stream 'yield)\n1\n-> (stream 'yield)\n2\n-> (stream 'yield)\n3\n-> (stream 'yield)\n'END\n-> ((stream 'init) '(a b))\n'OK\n-> (stream 'yield)\n'a\n-> (stream 'yield)\n'b\n-> (stream 'yield)\n'END\n-> (stream 'yield)\n'END\n->\n",
                "Here are some Python examples of how to actually implement generators as if Python did not provide syntactic sugar for them:As a Python generator:As a Python generator:from itertools import islice\n\ndef fib_gen():\n    a, b = 1, 1\n    while True:\n        yield a\n        a, b = b, a + b\n\nassert [1, 1, 2, 3, 5] == list(islice(fib_gen(), 5))\nfrom itertools import islice\n\ndef fib_gen():\n    a, b = 1, 1\n    while True:\n        yield a\n        a, b = b, a + b\n\nassert [1, 1, 2, 3, 5] == list(islice(fib_gen(), 5))\nUsing lexical closures instead of generatorsUsing lexical closures instead of generatorsdef ftake(fnext, last):\n    return [fnext() for _ in xrange(last)]\n\ndef fib_gen2():\n    #funky scope due to python2.x workaround\n    #for python 3.x use nonlocal\n    def _():\n        _.a, _.b = _.b, _.a + _.b\n        return _.a\n    _.a, _.b = 0, 1\n    return _\n\nassert [1,1,2,3,5] == ftake(fib_gen2(), 5)\ndef ftake(fnext, last):\n    return [fnext() for _ in xrange(last)]\n\ndef fib_gen2():\n    #funky scope due to python2.x workaround\n    #for python 3.x use nonlocal\n    def _():\n        _.a, _.b = _.b, _.a + _.b\n        return _.a\n    _.a, _.b = 0, 1\n    return _\n\nassert [1,1,2,3,5] == ftake(fib_gen2(), 5)\nUsing object closures instead of generators (because ClosuresAndObjectsAreEquivalent)Using object closures instead of generatorsClosuresAndObjectsAreEquivalentclass fib_gen3:\n    def __init__(self):\n        self.a, self.b = 1, 1\n\n    def __call__(self):\n        r = self.a\n        self.a, self.b = self.b, self.a + self.b\n        return r\n\nassert [1,1,2,3,5] == ftake(fib_gen3(), 5)\nclass fib_gen3:\n    def __init__(self):\n        self.a, self.b = 1, 1\n\n    def __call__(self):\n        r = self.a\n        self.a, self.b = self.b, self.a + self.b\n        return r\n\nassert [1,1,2,3,5] == ftake(fib_gen3(), 5)\n",
                "I was going to post \"read page 19 of Beazley's 'Python: Essential Reference' for a quick description of generators\", but so many others have posted good descriptions already.Also, note that yield can be used in coroutines as the dual of their use in generator functions.  Although it isn't the same use as your code snippet, (yield) can be used as an expression in a function.  When a caller sends a value to the method using the send() method, then the coroutine will execute until the next (yield) statement is encountered.yield(yield)send()(yield)Generators and coroutines are a cool way to set up data-flow type applications.  I thought it would be worthwhile knowing about the other use of the yield statement in functions.yield",
                "Here is a simple example:def isPrimeNumber(n):\n    print \"isPrimeNumber({}) call\".format(n)\n    if n==1:\n        return False\n    for x in range(2,n):\n        if n % x == 0:\n            return False\n    return True\n\ndef primes (n=1):\n    while(True):\n        print \"loop step ---------------- {}\".format(n)\n        if isPrimeNumber(n): yield n\n        n += 1\n\nfor n in primes():\n    if n> 10:break\n    print \"wiriting result {}\".format(n)\ndef isPrimeNumber(n):\n    print \"isPrimeNumber({}) call\".format(n)\n    if n==1:\n        return False\n    for x in range(2,n):\n        if n % x == 0:\n            return False\n    return True\n\ndef primes (n=1):\n    while(True):\n        print \"loop step ---------------- {}\".format(n)\n        if isPrimeNumber(n): yield n\n        n += 1\n\nfor n in primes():\n    if n> 10:break\n    print \"wiriting result {}\".format(n)\nOutput:loop step ---------------- 1\nisPrimeNumber(1) call\nloop step ---------------- 2\nisPrimeNumber(2) call\nloop step ---------------- 3\nisPrimeNumber(3) call\nwiriting result 3\nloop step ---------------- 4\nisPrimeNumber(4) call\nloop step ---------------- 5\nisPrimeNumber(5) call\nwiriting result 5\nloop step ---------------- 6\nisPrimeNumber(6) call\nloop step ---------------- 7\nisPrimeNumber(7) call\nwiriting result 7\nloop step ---------------- 8\nisPrimeNumber(8) call\nloop step ---------------- 9\nisPrimeNumber(9) call\nloop step ---------------- 10\nisPrimeNumber(10) call\nloop step ---------------- 11\nisPrimeNumber(11) call\nloop step ---------------- 1\nisPrimeNumber(1) call\nloop step ---------------- 2\nisPrimeNumber(2) call\nloop step ---------------- 3\nisPrimeNumber(3) call\nwiriting result 3\nloop step ---------------- 4\nisPrimeNumber(4) call\nloop step ---------------- 5\nisPrimeNumber(5) call\nwiriting result 5\nloop step ---------------- 6\nisPrimeNumber(6) call\nloop step ---------------- 7\nisPrimeNumber(7) call\nwiriting result 7\nloop step ---------------- 8\nisPrimeNumber(8) call\nloop step ---------------- 9\nisPrimeNumber(9) call\nloop step ---------------- 10\nisPrimeNumber(10) call\nloop step ---------------- 11\nisPrimeNumber(11) call\nI am not a Python developer, but it looks to me yield holds the position of program flow and the next loop start from \"yield\" position. It seems like it is waiting at that position, and just before that, returning a value outside, and next time continues to work.yieldIt seems to be an interesting and nice ability :D",
                "Here is a mental image of what yield does.yieldI like to think of a thread as having a stack (even when it's not implemented that way).When a normal function is called, it puts its local variables on the stack, does some computation, then clears the stack and returns. The values of its local variables are never seen again.With a yield function, when its code begins to run (i.e. after the function is called, returning a generator object, whose next() method is then invoked), it similarly puts its local variables onto the stack and computes for a while. But then, when it hits the yield statement, before clearing its part of the stack and returning, it takes a snapshot of its local variables and stores them in the generator object. It also writes down the place where it's currently up to in its code (i.e. the particular yield statement).yieldnext()yieldyieldSo it's a kind of a frozen function that the generator is hanging onto.When next() is called subsequently, it retrieves the function's belongings onto the stack and re-animates it. The function continues to compute from where it left off, oblivious to the fact that it had just spent an eternity in cold storage.next()Compare the following examples:def normalFunction():\n    return\n    if False:\n        pass\n\ndef yielderFunction():\n    return\n    if False:\n        yield 12\ndef normalFunction():\n    return\n    if False:\n        pass\n\ndef yielderFunction():\n    return\n    if False:\n        yield 12\nWhen we call the second function, it behaves very differently to the first. The yield statement might be unreachable, but if it's present anywhere, it changes the nature of what we're dealing with.yield>>> yielderFunction()\n<generator object yielderFunction at 0x07742D28>\n>>> yielderFunction()\n<generator object yielderFunction at 0x07742D28>\nCalling yielderFunction() doesn't run its code, but makes a generator out of the code. (Maybe it's a good idea to name such things with the yielder prefix for readability.)yielderFunction()yielder>>> gen = yielderFunction()\n>>> dir(gen)\n['__class__',\n ...\n '__iter__',    #Returns gen itself, to make it work uniformly with containers\n ...            #when given to a for loop. (Containers return an iterator instead.)\n 'close',\n 'gi_code',\n 'gi_frame',\n 'gi_running',\n 'next',        #The method that runs the function's body.\n 'send',\n 'throw']\n>>> gen = yielderFunction()\n>>> dir(gen)\n['__class__',\n ...\n '__iter__',    #Returns gen itself, to make it work uniformly with containers\n ...            #when given to a for loop. (Containers return an iterator instead.)\n 'close',\n 'gi_code',\n 'gi_frame',\n 'gi_running',\n 'next',        #The method that runs the function's body.\n 'send',\n 'throw']\nThe gi_code and gi_frame fields are where the frozen state is stored. Exploring them with dir(..), we can confirm that our mental model above is credible.gi_codegi_framedir(..)",
                "\nImagine that you have created a remarkable machine that is capable of generating thousands and thousands of lightbulbs per day. The machine generates these lightbulbs in boxes with a unique serial number. You don't have enough space to store all of these lightbulbs at the same time, so you would like to adjust it to generate lightbulbs on-demand.\nPython generators don't differ much from this concept. Imagine that you have a function called barcode_generator that generates unique serial numbers for the boxes. Obviously, you can have a huge number of such barcodes returned by the function, subject to the hardware (RAM) limitations. A wiser, and space efficient, option is to generate those serial numbers on-demand.\nMachine's code:\ndef barcode_generator():\n    serial_number = 10000  # Initial barcode\n    while True:\n        yield serial_number\n        serial_number += 1\n\n\nbarcode = barcode_generator()\nwhile True:\n    number_of_lightbulbs_to_generate = int(input(\"How many lightbulbs to generate? \"))\n    barcodes = [next(barcode) for _ in range(number_of_lightbulbs_to_generate)]\n    print(barcodes)\n\n    # function_to_create_the_next_batch_of_lightbulbs(barcodes)\n\n    produce_more = input(\"Produce more? [Y/n]: \")\n    if produce_more == \"n\":\n        break\n\nNote the next(barcode) bit.\nAs you can see, we have a self-contained \u201cfunction\u201d to generate the next unique serial number each time. This function returns a generator! As you can see, we are not calling the function each time we need a new serial number, but instead we are using next() given the generator to obtain the next serial number.\nLazy Iterators\nTo be more precise, this generator is a lazy iterator! An iterator is an object that helps us traverse a sequence of objects. It's called lazy because it does not load all the items of the sequence in memory until they are needed. The use of next in the previous example is the explicit way to obtain the next item from the iterator. The implicit way is using for loops:\nfor barcode in barcode_generator():\n    print(barcode)\n\nThis will print barcodes infinitely, yet you will not run out of memory.\nIn other words, a generator looks like a function but behaves like an iterator.\nReal-world application?\nFinally, real-world applications? They are usually useful when you work with big sequences. Imagine reading a huge file from disk with billions of records. Reading the entire file in memory, before you can work with its content, will probably be infeasible (i.e., you will run out of memory).\nImagine that you have created a remarkable machine that is capable of generating thousands and thousands of lightbulbs per day. The machine generates these lightbulbs in boxes with a unique serial number. You don't have enough space to store all of these lightbulbs at the same time, so you would like to adjust it to generate lightbulbs on-demand.Python generators don't differ much from this concept. Imagine that you have a function called barcode_generator that generates unique serial numbers for the boxes. Obviously, you can have a huge number of such barcodes returned by the function, subject to the hardware (RAM) limitations. A wiser, and space efficient, option is to generate those serial numbers on-demand.barcode_generatorMachine's code:def barcode_generator():\n    serial_number = 10000  # Initial barcode\n    while True:\n        yield serial_number\n        serial_number += 1\n\n\nbarcode = barcode_generator()\nwhile True:\n    number_of_lightbulbs_to_generate = int(input(\"How many lightbulbs to generate? \"))\n    barcodes = [next(barcode) for _ in range(number_of_lightbulbs_to_generate)]\n    print(barcodes)\n\n    # function_to_create_the_next_batch_of_lightbulbs(barcodes)\n\n    produce_more = input(\"Produce more? [Y/n]: \")\n    if produce_more == \"n\":\n        break\ndef barcode_generator():\n    serial_number = 10000  # Initial barcode\n    while True:\n        yield serial_number\n        serial_number += 1\n\n\nbarcode = barcode_generator()\nwhile True:\n    number_of_lightbulbs_to_generate = int(input(\"How many lightbulbs to generate? \"))\n    barcodes = [next(barcode) for _ in range(number_of_lightbulbs_to_generate)]\n    print(barcodes)\n\n    # function_to_create_the_next_batch_of_lightbulbs(barcodes)\n\n    produce_more = input(\"Produce more? [Y/n]: \")\n    if produce_more == \"n\":\n        break\nNote the next(barcode) bit.next(barcode)As you can see, we have a self-contained \u201cfunction\u201d to generate the next unique serial number each time. This function returns a generator! As you can see, we are not calling the function each time we need a new serial number, but instead we are using next() given the generator to obtain the next serial number.generatornext()Lazy IteratorsTo be more precise, this generator is a lazy iterator! An iterator is an object that helps us traverse a sequence of objects. It's called lazy because it does not load all the items of the sequence in memory until they are needed. The use of next in the previous example is the explicit way to obtain the next item from the iterator. The implicit way is using for loops:lazy iteratorlazynextexplicitimplicitfor barcode in barcode_generator():\n    print(barcode)\nfor barcode in barcode_generator():\n    print(barcode)\nThis will print barcodes infinitely, yet you will not run out of memory.In other words, a generator looks like a function but behaves like an iterator.In other words, a generator looks like a function but behaves like an iterator.looks likebehaves likeReal-world application?Finally, real-world applications? They are usually useful when you work with big sequences. Imagine reading a huge file from disk with billions of records. Reading the entire file in memory, before you can work with its content, will probably be infeasible (i.e., you will run out of memory).huge",
                "An easy example to understand what it is: yieldyielddef f123():\n    for _ in range(4):\n        yield 1\n        yield 2\n\n\nfor i in f123():\n    print (i)\ndef f123():\n    for _ in range(4):\n        yield 1\n        yield 2\n\n\nfor i in f123():\n    print (i)\nThe output is: 1 2 1 2 1 2 1 2\n1 2 1 2 1 2 1 2\n",
                "Like every answer suggests, yield is used for creating a sequence generator. It's used for generating some sequence dynamically. For example, while reading a file line by line on a network, you can use the yield function as follows:yieldyielddef getNextLines():\n   while con.isOpen():\n       yield con.read()\ndef getNextLines():\n   while con.isOpen():\n       yield con.read()\nYou can use it in your code as follows:for line in getNextLines():\n    doSomeThing(line)\nfor line in getNextLines():\n    doSomeThing(line)\nExecution Control Transfer gotchaExecution Control Transfer gotchaExecution Control Transfer gotchaThe execution control will be transferred from getNextLines() to the for loop when yield is executed. Thus, every time getNextLines() is invoked, execution begins from the point where it was paused last time.forThus in short, a function with the following codedef simpleYield():\n    yield \"first time\"\n    yield \"second time\"\n    yield \"third time\"\n    yield \"Now some useful value {}\".format(12)\n\nfor i in simpleYield():\n    print i\ndef simpleYield():\n    yield \"first time\"\n    yield \"second time\"\n    yield \"third time\"\n    yield \"Now some useful value {}\".format(12)\n\nfor i in simpleYield():\n    print i\nwill print\"first time\"\n\"second time\"\n\"third time\"\n\"Now some useful value 12\"\n\"first time\"\n\"second time\"\n\"third time\"\n\"Now some useful value 12\"\n",
                "(My below answer only speaks from the perspective of using Python generator, not the underlying implementation of generator mechanism, which involves some tricks of stack and heap manipulation.)underlying implementation of generator mechanismWhen yield is used instead of a return in a python function, that function is turned into something special called generator function. That function will return an object of generator type. The yield keyword is a flag to notify the python compiler to treat such function specially. Normal functions will terminate once some value is returned from it. But with the help of the compiler, the generator function can be thought of as resumable. That is, the execution context will be restored and the execution will continue from last run. Until you explicitly call return, which will raise a StopIteration exception (which is also part of the iterator protocol), or reach the end of the function. I found a lot of references about generator but this one from the functional programming perspective is the most digestable.yieldreturngenerator functiongeneratorThe yield keyword is a flag to notify the python compiler to treat such function specially.yieldcan be thought ofStopIterationgeneratoronefunctional programming perspective(Now I want to talk about the rationale behind generator, and the iterator based on my own understanding. I hope this can help you grasp the essential motivation of iterator and generator. Such concept shows up in other languages as well such as C#.)generatoriteratoressential motivationessential motivationAs I understand, when we want to process a bunch of data, we usually first store the data somewhere and then process it one by one. But this naive approach is problematic. If the data volume is huge, it's expensive to store them as a whole beforehand. So instead of storing the data itself directly, why not store some kind of metadata indirectly, i.e. the logic how the data is computed. naiveSo instead of storing the data itself directly, why not store some kind of metadata indirectly, i.e. the logic how the data is computeddatametadatathe logic how the data is computedThere are 2 approaches to wrap such metadata.\nThe OO approach, we wrap the metadata as a class. This is the so-called iterator who implements the iterator protocol (i.e. the __next__(), and __iter__() methods). This is also the commonly seen iterator design pattern.\nThe functional approach, we wrap the metadata as a function. This is\nthe so-called generator function. But under the hood, the returned generator object still IS-A iterator because it also implements the iterator protocol.\nThe OO approach, we wrap the metadata as a class. This is the so-called iterator who implements the iterator protocol (i.e. the __next__(), and __iter__() methods). This is also the commonly seen iterator design pattern.as a classiterator__next__()__iter__()iterator design patternThe functional approach, we wrap the metadata as a function. This is\nthe so-called generator function. But under the hood, the returned generator object still IS-A iterator because it also implements the iterator protocol.as a functiongenerator functiongenerator objectIS-AEither way, an iterator is created, i.e. some object that can give you the data you want. The OO approach may be a bit complex. Anyway, which one to use is up to you.",
                "In summary, the yield statement transforms your function into a factory that produces a special object called a generator which wraps around the body of your original function. When the generator is iterated, it executes your function  until it reaches the next yield then suspends execution and evaluates to the value passed to yield. It repeats this process on each iteration until the path of execution exits the function. For instance,yieldgeneratorgeneratoryieldyielddef simple_generator():\n    yield 'one'\n    yield 'two'\n    yield 'three'\n\nfor i in simple_generator():\n    print i\ndef simple_generator():\n    yield 'one'\n    yield 'two'\n    yield 'three'\n\nfor i in simple_generator():\n    print i\nsimply outputsone\ntwo\nthree\none\ntwo\nthree\nThe power comes from using the generator with a loop that calculates a sequence, the generator executes the loop stopping each time to 'yield' the next result of the calculation, in this way it calculates a list on the fly, the benefit being the memory saved for especially large calculationsSay you wanted to create a your own range function that produces an iterable range of numbers, you could do it like so,rangedef myRangeNaive(i):\n    n = 0\n    range = []\n    while n < i:\n        range.append(n)\n        n = n + 1\n    return range\ndef myRangeNaive(i):\n    n = 0\n    range = []\n    while n < i:\n        range.append(n)\n        n = n + 1\n    return range\nand use it like this;for i in myRangeNaive(10):\n    print i\nfor i in myRangeNaive(10):\n    print i\nBut this is inefficient because\nYou create an array that you only use once (this wastes memory)\nThis code actually loops over that array twice! :(\nYou create an array that you only use once (this wastes memory)This code actually loops over that array twice! :(Luckily Guido and his team were generous enough to develop generators so we could just do this;def myRangeSmart(i):\n    n = 0\n    while n < i:\n       yield n\n       n = n + 1\n    return\n\nfor i in myRangeSmart(10):\n    print i\ndef myRangeSmart(i):\n    n = 0\n    while n < i:\n       yield n\n       n = n + 1\n    return\n\nfor i in myRangeSmart(10):\n    print i\nNow upon each iteration a function on the generator called next() executes the function until it either reaches a 'yield' statement in which it stops and  'yields' the value or reaches the end of the function. In this case on the first call, next() executes up to the yield statement and yield 'n', on the next call it will execute the  increment statement, jump back to the 'while', evaluate it, and if true, it will stop and yield 'n' again, it will continue that way until the while condition returns false and the generator jumps to the end of the function.next()next()",
                "Yield is an objectYield is an objectA return in a function will return a single value.returnIf you want a function to return a huge set of values, use yield.a function to return a huge set of valuesyieldMore importantly, yield is a barrier.yieldbarrier\nlike barrier in the CUDA language, it will not transfer control until it gets\n  completed.\nlike barrier in the CUDA language, it will not transfer control until it gets\n  completed.That is, it will run the code in your function from the beginning until it hits yield. Then, it\u2019ll return the first value of the loop.yieldThen, every other call will run the loop you have written in the function one more time, returning the next value until there isn't any value to return.",
                "Many people use return rather than yield, but in some cases yield can be more efficient and easier to work with.returnyieldyieldHere is an example which yield is definitely best for:yield\nreturn (in function)\nreturn (in function)returnimport random\n\ndef return_dates():\n    dates = [] # With 'return' you need to create a list then return it\n    for i in range(5):\n        date = random.choice([\"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\", \"6th\", \"7th\", \"8th\", \"9th\", \"10th\"])\n        dates.append(date)\n    return dates\nimport random\n\ndef return_dates():\n    dates = [] # With 'return' you need to create a list then return it\n    for i in range(5):\n        date = random.choice([\"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\", \"6th\", \"7th\", \"8th\", \"9th\", \"10th\"])\n        dates.append(date)\n    return dates\n\nyield (in function)\nyield (in function)yielddef yield_dates():\n    for i in range(5):\n        date = random.choice([\"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\", \"6th\", \"7th\", \"8th\", \"9th\", \"10th\"])\n        yield date # 'yield' makes a generator automatically which works\n                   # in a similar way. This is much more efficient.\ndef yield_dates():\n    for i in range(5):\n        date = random.choice([\"1st\", \"2nd\", \"3rd\", \"4th\", \"5th\", \"6th\", \"7th\", \"8th\", \"9th\", \"10th\"])\n        yield date # 'yield' makes a generator automatically which works\n                   # in a similar way. This is much more efficient.\n\nCalling functions\nCalling functionsCalling functionsdates_list = return_dates()\nprint(dates_list)\nfor i in dates_list:\n    print(i)\n\ndates_generator = yield_dates()\nprint(dates_generator)\nfor i in dates_generator:\n    print(i)\ndates_list = return_dates()\nprint(dates_list)\nfor i in dates_list:\n    print(i)\n\ndates_generator = yield_dates()\nprint(dates_generator)\nfor i in dates_generator:\n    print(i)\nBoth functions do the same thing, but yield uses three lines instead of five and has one less variable to worry about.yield\n\nThis is the result from the code:\n\n\nThis is the result from the code:\nThis is the result from the code:This is the result from the code:As you can see both functions do the same thing. The only difference is return_dates() gives a list and yield_dates() gives a generator.return_dates()yield_dates()A real life example would be something like reading a file line by line or if you just want to make a generator.",
                "The yield keyword simply collects returning results. Think of yield like return +=yieldyieldreturn +=",
                "yield is like a return element for a function. The difference is, that the yield element turns a function into a generator. A generator behaves just like a function until something is 'yielded'. The generator stops until it is next called, and continues from exactly the same point as it started. You can get a sequence of all the 'yielded' values in one, by calling list(generator()).yieldyieldlist(generator())"
            ]
        },
        {
            "tag": "__main__",
            "patterns": [
                "What does if __name__ == \"__main__\": do?",
                "What does if __name__ == \"__main__\": do?"
            ],
            "responses": [
                "Short AnswerIt's boilerplate code that protects users from accidentally invoking the script when they didn't intend to. Here are some common problems when the guard is omitted from a script:\nIf you import the guardless script in another script (e.g. import my_script_without_a_name_eq_main_guard), then the latter script will trigger the former to run at import time and using the second script's command line arguments. This is almost always a mistake.\n\nIf you have a custom class in the guardless script and save it to a pickle file, then unpickling it in another script will trigger an import of the guardless script, with the same problems outlined in the previous bullet.\n\nIf you import the guardless script in another script (e.g. import my_script_without_a_name_eq_main_guard), then the latter script will trigger the former to run at import time and using the second script's command line arguments. This is almost always a mistake.\nIf you import the guardless script in another script (e.g. import my_script_without_a_name_eq_main_guard), then the latter script will trigger the former to run at import time and using the second script's command line arguments. This is almost always a mistake.import my_script_without_a_name_eq_main_guardat import timeusing the second script's command line argumentsIf you have a custom class in the guardless script and save it to a pickle file, then unpickling it in another script will trigger an import of the guardless script, with the same problems outlined in the previous bullet.\nIf you have a custom class in the guardless script and save it to a pickle file, then unpickling it in another script will trigger an import of the guardless script, with the same problems outlined in the previous bullet.Long AnswerTo better understand why and how this matters, we need to take a step back to understand how Python initializes scripts and how this interacts with its module import mechanism.Whenever the Python interpreter reads a source file, it does two things:\nit sets a few special variables like __name__, and then\n\nit executes all of the code found in the file.\n\nit sets a few special variables like __name__, and then\nit sets a few special variables like __name__, and then__name__it executes all of the code found in the file.\nit executes all of the code found in the file.Let's see how this works and how it relates to your question about the __name__ checks we always see in Python scripts.__name__Code SampleLet's use a slightly different code sample to explore how imports and scripts work.  Suppose the following is in a file called foo.py.foo.py# Suppose this is foo.py.\n\nprint(\"before import\")\nimport math\n\nprint(\"before function_a\")\ndef function_a():\n    print(\"Function A\")\n\nprint(\"before function_b\")\ndef function_b():\n    print(\"Function B {}\".format(math.sqrt(100)))\n\nprint(\"before __name__ guard\")\nif __name__ == '__main__':\n    function_a()\n    function_b()\nprint(\"after __name__ guard\")\n# Suppose this is foo.py.\n\nprint(\"before import\")\nimport math\n\nprint(\"before function_a\")\ndef function_a():\n    print(\"Function A\")\n\nprint(\"before function_b\")\ndef function_b():\n    print(\"Function B {}\".format(math.sqrt(100)))\n\nprint(\"before __name__ guard\")\nif __name__ == '__main__':\n    function_a()\n    function_b()\nprint(\"after __name__ guard\")\nSpecial VariablesWhen the Python interpreter reads a source file, it first defines a few special variables. In this case, we care about the __name__ variable.__name__When Your Module Is the Main ProgramWhen Your Module Is the Main ProgramIf you are running your module (the source file) as the main program, e.g.python foo.py\npython foo.py\nthe interpreter will assign the hard-coded string \"__main__\" to the __name__ variable, i.e.\"__main__\"__name__# It's as if the interpreter inserts this at the top\n# of your module when run as the main program.\n__name__ = \"__main__\" \n# It's as if the interpreter inserts this at the top\n# of your module when run as the main program.\n__name__ = \"__main__\" \nWhen Your Module Is Imported By AnotherWhen Your Module Is Imported By AnotherOn the other hand, suppose some other module is the main program and it imports your module. This means there's a statement like this in the main program, or in some other module the main program imports:# Suppose this is in some other main program.\nimport foo\n# Suppose this is in some other main program.\nimport foo\nThe interpreter will search for your foo.py file (along with searching for a few other variants), and prior to executing that module, it will assign the name \"foo\" from the import statement to the __name__ variable, i.e.foo.py\"foo\"__name__# It's as if the interpreter inserts this at the top\n# of your module when it's imported from another module.\n__name__ = \"foo\"\n# It's as if the interpreter inserts this at the top\n# of your module when it's imported from another module.\n__name__ = \"foo\"\nExecuting the Module's CodeAfter the special variables are set up, the interpreter executes all the code in the module, one statement at a time. You may want to open another window on the side with the code sample so you can follow along with this explanation.AlwaysAlways\nIt prints the string \"before import\" (without quotes).\n\nIt loads the math module and assigns it to a variable called math. This is equivalent to replacing import math with the following (note that __import__ is a low-level function in Python that takes a string and triggers the actual import):\n\nIt prints the string \"before import\" (without quotes).\nIt prints the string \"before import\" (without quotes).\"before import\"It loads the math module and assigns it to a variable called math. This is equivalent to replacing import math with the following (note that __import__ is a low-level function in Python that takes a string and triggers the actual import):\nIt loads the math module and assigns it to a variable called math. This is equivalent to replacing import math with the following (note that __import__ is a low-level function in Python that takes a string and triggers the actual import):mathmathimport math__import__# Find and load a module given its string name, \"math\",\n# then assign it to a local variable called math.\nmath = __import__(\"math\")\n# Find and load a module given its string name, \"math\",\n# then assign it to a local variable called math.\nmath = __import__(\"math\")\n\nIt prints the string \"before function_a\".\n\nIt executes the def block, creating a function object, then assigning that function object to a variable called function_a.\n\nIt prints the string \"before function_b\".\n\nIt executes the second def block, creating another function object, then assigning it to a variable called function_b.\n\nIt prints the string \"before __name__ guard\".\n\nIt prints the string \"before function_a\".\nIt prints the string \"before function_a\".\"before function_a\"It executes the def block, creating a function object, then assigning that function object to a variable called function_a.\nIt executes the def block, creating a function object, then assigning that function object to a variable called function_a.deffunction_aIt prints the string \"before function_b\".\nIt prints the string \"before function_b\".\"before function_b\"It executes the second def block, creating another function object, then assigning it to a variable called function_b.\nIt executes the second def block, creating another function object, then assigning it to a variable called function_b.deffunction_bIt prints the string \"before __name__ guard\".\nIt prints the string \"before __name__ guard\".\"before __name__ guard\"Only When Your Module Is the Main ProgramOnly When Your Module Is the Main Program\nIf your module is the main program, then it will see that __name__ was indeed set to \"__main__\" and it calls the two functions, printing the strings \"Function A\" and \"Function B 10.0\".\nIf your module is the main program, then it will see that __name__ was indeed set to \"__main__\" and it calls the two functions, printing the strings \"Function A\" and \"Function B 10.0\".__name__\"__main__\"\"Function A\"\"Function B 10.0\"Only When Your Module Is Imported by AnotherOnly When Your Module Is Imported by Another\n(instead) If your module is not the main program but was imported by another one, then __name__ will be \"foo\", not \"__main__\", and it'll skip the body of the if statement.\n(instead) If your module is not the main program but was imported by another one, then __name__ will be \"foo\", not \"__main__\", and it'll skip the body of the if statement.instead__name__\"foo\"\"__main__\"ifAlwaysAlways\nIt will print the string \"after __name__ guard\" in both situations.\nIt will print the string \"after __name__ guard\" in both situations.\"after __name__ guard\"SummarySummarySummaryIn summary, here's what'd be printed in the two cases:# What gets printed if foo is the main program\nbefore import\nbefore function_a\nbefore function_b\nbefore __name__ guard\nFunction A\nFunction B 10.0\nafter __name__ guard\n# What gets printed if foo is the main program\nbefore import\nbefore function_a\nbefore function_b\nbefore __name__ guard\nFunction A\nFunction B 10.0\nafter __name__ guard\n# What gets printed if foo is imported as a regular module\nbefore import\nbefore function_a\nbefore function_b\nbefore __name__ guard\nafter __name__ guard\n# What gets printed if foo is imported as a regular module\nbefore import\nbefore function_a\nbefore function_b\nbefore __name__ guard\nafter __name__ guard\nWhy Does It Work This Way?You might naturally wonder why anybody would want this.  Well, sometimes you want to write a .py file that can be both used by other programs and/or modules as a module, and can also be run as the main program itself.  Examples:.py\nYour module is a library, but you want to have a script mode where it runs some unit tests or a demo.\n\nYour module is only used as a main program, but it has some unit tests, and the testing framework works by importing .py files like your script and running special test functions. You don't want it to try running the script just because it's importing the module.\n\nYour module is mostly used as a main program, but it also provides a programmer-friendly API for advanced users.\n\nYour module is a library, but you want to have a script mode where it runs some unit tests or a demo.\nYour module is a library, but you want to have a script mode where it runs some unit tests or a demo.Your module is only used as a main program, but it has some unit tests, and the testing framework works by importing .py files like your script and running special test functions. You don't want it to try running the script just because it's importing the module.\nYour module is only used as a main program, but it has some unit tests, and the testing framework works by importing .py files like your script and running special test functions. You don't want it to try running the script just because it's importing the module..pyYour module is mostly used as a main program, but it also provides a programmer-friendly API for advanced users.\nYour module is mostly used as a main program, but it also provides a programmer-friendly API for advanced users.Beyond those examples, it's elegant that running a script in Python is just setting up a few magic variables and importing the script. \"Running\" the script is a side effect of importing the script's module.Food for Thought\nQuestion: Can I have multiple __name__ checking blocks?  Answer: it's strange to do so, but the language won't stop you.\n\nSuppose the following is in foo2.py.  What happens if you say python foo2.py on the command-line? Why?\n\nQuestion: Can I have multiple __name__ checking blocks?  Answer: it's strange to do so, but the language won't stop you.\nQuestion: Can I have multiple __name__ checking blocks?  Answer: it's strange to do so, but the language won't stop you.__name__Suppose the following is in foo2.py.  What happens if you say python foo2.py on the command-line? Why?\nSuppose the following is in foo2.py.  What happens if you say python foo2.py on the command-line? Why?foo2.pypython foo2.py# Suppose this is foo2.py.\nimport os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters\n\ndef function_a():\n    print(\"a1\")\n    from foo2 import function_b\n    print(\"a2\")\n    function_b()\n    print(\"a3\")\n\ndef function_b():\n    print(\"b\")\n\nprint(\"t1\")\nif __name__ == \"__main__\":\n    print(\"m1\")\n    function_a()\n    print(\"m2\")\nprint(\"t2\")\n      \n# Suppose this is foo2.py.\nimport os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters\n\ndef function_a():\n    print(\"a1\")\n    from foo2 import function_b\n    print(\"a2\")\n    function_b()\n    print(\"a3\")\n\ndef function_b():\n    print(\"b\")\n\nprint(\"t1\")\nif __name__ == \"__main__\":\n    print(\"m1\")\n    function_a()\n    print(\"m2\")\nprint(\"t2\")\n      \n\nNow, figure out what will happen if you remove the __name__ check in foo3.py:\nNow, figure out what will happen if you remove the __name__ check in foo3.py:__name__foo3.py# Suppose this is foo3.py.\nimport os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters\n\ndef function_a():\n    print(\"a1\")\n    from foo3 import function_b\n    print(\"a2\")\n    function_b()\n    print(\"a3\")\n\ndef function_b():\n    print(\"b\")\n\nprint(\"t1\")\nprint(\"m1\")\nfunction_a()\nprint(\"m2\")\nprint(\"t2\")\n# Suppose this is foo3.py.\nimport os, sys; sys.path.insert(0, os.path.dirname(__file__)) # needed for some interpreters\n\ndef function_a():\n    print(\"a1\")\n    from foo3 import function_b\n    print(\"a2\")\n    function_b()\n    print(\"a3\")\n\ndef function_b():\n    print(\"b\")\n\nprint(\"t1\")\nprint(\"m1\")\nfunction_a()\nprint(\"m2\")\nprint(\"t2\")\n\nWhat will this do when used as a script?  When imported as a module?\nWhat will this do when used as a script?  When imported as a module?# Suppose this is in foo4.py\n__name__ = \"__main__\"\n\ndef bar():\n    print(\"bar\")\n    \nprint(\"before __name__ guard\")\nif __name__ == \"__main__\":\n    bar()\nprint(\"after __name__ guard\")\n# Suppose this is in foo4.py\n__name__ = \"__main__\"\n\ndef bar():\n    print(\"bar\")\n    \nprint(\"before __name__ guard\")\nif __name__ == \"__main__\":\n    bar()\nprint(\"after __name__ guard\")\n",
                "When your script is run by passing it as a command to the Python interpreter,python myscript.py\npython myscript.py\nall of the code that is at indentation level 0 gets executed.  Functions and classes that are defined are, well, defined, but none of their code gets run.  Unlike other languages, there's no main() function that gets run automatically - the main() function is implicitly all the code at the top level.main()main()In this case, the top-level code is an if block.  __name__ is a built-in variable which evaluates to the name of the current module.  However, if a module is being run directly (as in myscript.py above), then __name__ instead is set to the string \"__main__\".  Thus, you can test whether your script is being run directly or being imported by something else by testingif__name__myscript.py__name__\"__main__\"if __name__ == \"__main__\":\n    ...\nif __name__ == \"__main__\":\n    ...\nIf your script is being imported into another module, its various function and class definitions will be imported and its top-level code will be executed, but the code in the then-body of the if clause above won't get run as the condition is not met. As a basic example, consider the following two scripts:if# file one.py\ndef func():\n    print(\"func() in one.py\")\n\nprint(\"top-level in one.py\")\n\nif __name__ == \"__main__\":\n    print(\"one.py is being run directly\")\nelse:\n    print(\"one.py is being imported into another module\")\n# file one.py\ndef func():\n    print(\"func() in one.py\")\n\nprint(\"top-level in one.py\")\n\nif __name__ == \"__main__\":\n    print(\"one.py is being run directly\")\nelse:\n    print(\"one.py is being imported into another module\")\n# file two.py\nimport one\n\nprint(\"top-level in two.py\")\none.func()\n\nif __name__ == \"__main__\":\n    print(\"two.py is being run directly\")\nelse:\n    print(\"two.py is being imported into another module\")\n# file two.py\nimport one\n\nprint(\"top-level in two.py\")\none.func()\n\nif __name__ == \"__main__\":\n    print(\"two.py is being run directly\")\nelse:\n    print(\"two.py is being imported into another module\")\nNow, if you invoke the interpreter aspython one.py\npython one.py\nThe output will betop-level in one.py\none.py is being run directly\ntop-level in one.py\none.py is being run directly\nIf you run two.py instead:two.pypython two.py\npython two.py\nYou gettop-level in one.py\none.py is being imported into another module\ntop-level in two.py\nfunc() in one.py\ntwo.py is being run directly\ntop-level in one.py\none.py is being imported into another module\ntop-level in two.py\nfunc() in one.py\ntwo.py is being run directly\nThus, when module one gets loaded, its __name__ equals \"one\" instead of \"__main__\".one__name__\"one\"\"__main__\"",
                "Create the following two files:# a.py\n\nimport b\n# a.py\n\nimport b\n# b.py\n\nprint(\"__name__ equals \" + __name__)\n\nif __name__ == '__main__':\n    print(\"if-statement was executed\")\n# b.py\n\nprint(\"__name__ equals \" + __name__)\n\nif __name__ == '__main__':\n    print(\"if-statement was executed\")\nNow run each file individually.Running python a.py:Running python a.py:python a.py$ python a.py\n__name__ equals b\n$ python a.py\n__name__ equals b\nWhen a.py is executed, it imports the module b. This causes all the code inside b to run. Python sets globals()['__name__'] in the b module to the module's name, b.a.pybbglobals()['__name__']bbRunning python b.py:Running python b.py:python b.py$ python b.py\n__name__ equals __main__\nif-statement was executed\n$ python b.py\n__name__ equals __main__\nif-statement was executed\nWhen only the file b.py is executed, Python sets globals()['__name__'] in this file to \"__main__\". Therefore, the if statement evaluates to True this time.b.pyglobals()['__name__']\"__main__\"ifTrue",
                "\nWhat does the if __name__ == \"__main__\": do?\nWhat does the if __name__ == \"__main__\": do?if __name__ == \"__main__\":To outline the basics:\nThe global variable, __name__, in the module that is the entry point to your program, is '__main__'. Otherwise, it's the name you import the module by.\nSo, code under the if block will only run if the module is the entry point to your program.\nIt allows the code in the module to be importable by other modules, without executing the code block beneath on import.\nThe global variable, __name__, in the module that is the entry point to your program, is '__main__'. Otherwise, it's the name you import the module by.The global variable, __name__, in the module that is the entry point to your program, is '__main__'. Otherwise, it's the name you import the module by.__name__'__main__'So, code under the if block will only run if the module is the entry point to your program.So, code under the if block will only run if the module is the entry point to your program.ifIt allows the code in the module to be importable by other modules, without executing the code block beneath on import.It allows the code in the module to be importable by other modules, without executing the code block beneath on import.Why do we need this?Developing and Testing Your CodeSay you're writing a Python script designed to be used as a module:def do_important():\n    \"\"\"This function does something very important\"\"\"\ndef do_important():\n    \"\"\"This function does something very important\"\"\"\nYou could test the module by adding this call of the function to the bottom:coulddo_important()\ndo_important()\nand running it (on a command prompt) with something like:~$ python important.py\n~$ python important.py\nThe ProblemHowever, if you want to import the module to another script:import important\nimport important\nOn import, the do_important function would be called, so you'd probably comment out your function call, do_important(), at the bottom. do_importantdo_important()# do_important() # I must remember to uncomment to execute this!\n# do_important() # I must remember to uncomment to execute this!\nAnd then you'll have to remember whether or not you've commented out your test function call. And this extra complexity would mean you're likely to forget, making your development process more troublesome.A Better WayThe __name__ variable points to the namespace wherever the Python interpreter happens to be at the moment. __name__Inside an imported module, it's the name of that module. But inside the primary module (or an interactive Python session, i.e. the interpreter's Read, Eval, Print Loop, or REPL) you are running everything from its \"__main__\".\"__main__\"So if you check before executing:if __name__ == \"__main__\":\n    do_important()\nif __name__ == \"__main__\":\n    do_important()\nWith the above, your code will only execute when you're running it as the primary module (or intentionally call it from another script). An Even Better WayThere's a Pythonic way to improve on this, though. What if we want to run this business process from outside the module?If we put the code we want to exercise as we develop and test in a function like this and then do our check for '__main__' immediately after:'__main__'def main():\n    \"\"\"business logic for when running this module as the primary one!\"\"\"\n    setup()\n    foo = do_important()\n    bar = do_even_more_important(foo)\n    for baz in bar:\n        do_super_important(baz)\n    teardown()\n\n# Here's our payoff idiom!\nif __name__ == '__main__':\n    main()\ndef main():\n    \"\"\"business logic for when running this module as the primary one!\"\"\"\n    setup()\n    foo = do_important()\n    bar = do_even_more_important(foo)\n    for baz in bar:\n        do_super_important(baz)\n    teardown()\n\n# Here's our payoff idiom!\nif __name__ == '__main__':\n    main()\nWe now have a final function for the end of our module that will run if we run the module as the primary module. It will allow the module and its functions and classes to be imported into other scripts without running the main function, and will also allow the module (and its functions and classes) to be called when running from a different '__main__' module, i.e.main'__main__'import important\nimportant.main()\nimport important\nimportant.main()\nThis idiom can also be found in the Python documentation in an explanation of the __main__ module. That text states:This idiom can also be found in the Python documentation in an explanation of the __main__ module.__main__\nThis module represents the (otherwise anonymous) scope in which the\n  interpreter\u2019s main program executes \u2014 commands read either from\n  standard input, from a script file, or from an interactive prompt. It\n  is this environment in which the idiomatic \u201cconditional script\u201d stanza\n  causes a script to run:\nif __name__ == '__main__':\n    main()\n\nThis module represents the (otherwise anonymous) scope in which the\n  interpreter\u2019s main program executes \u2014 commands read either from\n  standard input, from a script file, or from an interactive prompt. It\n  is this environment in which the idiomatic \u201cconditional script\u201d stanza\n  causes a script to run:if __name__ == '__main__':\n    main()\nif __name__ == '__main__':\n    main()\n",
                "if __name__ == \"__main__\" is the part that runs when the script is run from (say) the command line using a command like python myscript.py.if __name__ == \"__main__\"python myscript.py",
                "\nWhat does if __name__ == \"__main__\": do?\nWhat does if __name__ == \"__main__\": do?if __name__ == \"__main__\":__name__ is a global variable (in Python, global actually means on the module level) that exists in all namespaces. It is typically the module's name (as a str type).__name__module levelstrAs the only special case, however, in whatever Python process you run, as in mycode.py:python mycode.py\npython mycode.py\nthe otherwise anonymous global namespace is assigned the value of '__main__' to its __name__.'__main__'__name__Thus, including the final linesthe final linesif __name__ == '__main__':\n    main()\nif __name__ == '__main__':\n    main()\n\nat the end of your mycode.py script,\nwhen it is the primary, entry-point module that is run by a Python process,\nat the end of your mycode.py script,when it is the primary, entry-point module that is run by a Python process,will cause your script's uniquely defined main function to run.mainAnother benefit of using this construct: you can also import your code as a module in another script and then run the main function if and when your program decides:import mycode\n# ... any amount of other code\nmycode.main()\nimport mycode\n# ... any amount of other code\nmycode.main()\n",
                "There are lots of different takes here on the mechanics of the code in question, the \"How\", but for me none of it made sense until I understood the \"Why\". This should be especially helpful for new programmers.Take file \"ab.py\":def a():\n    print('A function in ab file');\na()\ndef a():\n    print('A function in ab file');\na()\nAnd a second file \"xy.py\":import ab\ndef main():\n    print('main function: this is where the action is')\ndef x():\n    print ('peripheral task: might be useful in other projects')\nx()\nif __name__ == \"__main__\":\n    main()\nimport ab\ndef main():\n    print('main function: this is where the action is')\ndef x():\n    print ('peripheral task: might be useful in other projects')\nx()\nif __name__ == \"__main__\":\n    main()\n\nWhat is this code actually doing?\nWhat is this code actually doing?When you execute xy.py, you import ab. The import statement runs the module immediately on import, so ab's operations get executed before the remainder of xy's. Once finished with ab, it continues with xy.xy.pyimport ababxyabxyThe interpreter keeps track of which scripts are running with __name__. When you run a script - no matter what you've named it - the interpreter calls it \"__main__\", making it the master or 'home' script that gets returned to after running an external script.__name__\"__main__\"Any other script that's called from this \"__main__\" script is assigned its filename as its __name__ (e.g., __name__ == \"ab.py\"). Hence, the line if __name__ == \"__main__\": is the interpreter's test to determine if it's interpreting/parsing the 'home' script that was initially executed, or if it's temporarily peeking into another (external) script. This gives the programmer flexibility to have the script behave differently if it's executed directly vs. called externally.\"__main__\"__name____name__ == \"ab.py\"if __name__ == \"__main__\":Let's step through the above code to understand what's happening, focusing first on the unindented lines and the order they appear in the scripts. Remember that function - or def - blocks don't do anything by themselves until they're called. What the interpreter might say if mumbled to itself:def\nOpen xy.py as the 'home' file; call it \"__main__\" in the __name__ variable.\nImport and open file with the __name__ == \"ab.py\".\nOh, a function. I'll remember that.\nOk, function a(); I just learned that. Printing 'A function in ab file'.\nEnd of file; back to \"__main__\"!\nOh, a function. I'll remember that.\nAnother one.\nFunction x(); ok, printing 'peripheral task: might be useful in other projects'.\nWhat's this? An if statement. Well, the condition has been met (the variable __name__ has been set to \"__main__\"), so I'll enter the main() function and print 'main function: this is where the action is'.\nOpen xy.py as the 'home' file; call it \"__main__\" in the __name__ variable.\"__main__\"__name__Import and open file with the __name__ == \"ab.py\".__name__ == \"ab.py\"Oh, a function. I'll remember that.Ok, function a(); I just learned that. Printing 'A function in ab file'.a()A function in ab fileEnd of file; back to \"__main__\"!\"__main__\"Oh, a function. I'll remember that.Another one.Function x(); ok, printing 'peripheral task: might be useful in other projects'.x()peripheral task: might be useful in other projectsWhat's this? An if statement. Well, the condition has been met (the variable __name__ has been set to \"__main__\"), so I'll enter the main() function and print 'main function: this is where the action is'.if__name__\"__main__\"main()main function: this is where the action isThe bottom two lines mean: \"If this is the \"__main__\" or 'home' script, execute the function called main()\". That's why you'll see a def main(): block up top, which contains the main flow of the script's functionality.\"__main__\"main()def main():\nWhy implement this?\nWhy implement this?Remember what I said earlier about import statements? When you import a module it doesn't just 'recognize' it and wait for further instructions - it actually runs all the executable operations contained within the script. So, putting the meat of your script into the main() function effectively quarantines it, putting it in isolation so that it won't immediately run when imported by another script.main()Again, there will be exceptions, but common practice is that main() doesn't usually get called externally. So you may be wondering one more thing: if we're not calling main(), why are we calling the script at all? It's because many people structure their scripts with standalone functions that are built to be run independent of the rest of the code in the file. They're then later called somewhere else in the body of the script. Which brings me to this:main()main()\nBut the code works without it\nBut the code works without itYes, that's right. These separate functions can be called from an in-line script that's not contained inside a main() function. If you're accustomed (as I am, in my early learning stages of programming) to building in-line scripts that do exactly what you need, and you'll try to figure it out again if you ever need that operation again ... well, you're not used to this kind of internal structure to your code, because it's more complicated to build and it's not as intuitive to read.canmain()But that's a script that probably can't have its functions called externally, because if it did it would immediately start calculating and assigning variables. And chances are if you're trying to re-use a function, your new script is related closely enough to the old one that there will be conflicting variables.In splitting out independent functions, you gain the ability to re-use your previous work by calling them into another script. For example, \"example.py\" might import \"xy.py\" and call x(), making use of the 'x' function from \"xy.py\". (Maybe it's capitalizing the third word of a given text string; creating a NumPy array from a list of numbers and squaring them; or detrending a 3D surface. The possibilities are limitless.)x()(As an aside, this question contains an answer by @kindall that finally helped me to understand - the why, not the how. Unfortunately it's been marked as a duplicate of this one, which I think is a mistake.)this questionthis one",
                "The code under if __name__ == '__main__': will only be executed if the module is invoked as a script.if __name__ == '__main__':onlyAs an example, consider the following module my_test_module.py:my_test_module.py# my_test_module.py\n\nprint('This is going to be printed out, no matter what')\n\nif __name__ == '__main__':\n    print('This is going to be printed out, only if user invokes the module as a script')\n# my_test_module.py\n\nprint('This is going to be printed out, no matter what')\n\nif __name__ == '__main__':\n    print('This is going to be printed out, only if user invokes the module as a script')\n\nFirst possibility: Import my_test_module.py in another module\n# main.py\n\nimport my_test_module\n\nif __name__ == '__main__':\n    print('Hello from main.py')\n\nNow if you invoke main.py:\npython main.py\n\n>> 'This is going to be printed out, no matter what'\n>> 'Hello from main.py'\n\nNote that only the top-level print() statement in my_test_module is executed.\n\nSecond possibility: Invoke my_test_module.py as a script\nNow if you run my_test_module.py as a Python script, both print() statements will be executed:\npython my_test_module.py\n\n>>> 'This is going to be printed out, no matter what'\n>>> 'This is going to be printed out, only if user invokes the module as a script'\n\n\nFor a more comprehensive explanation, you can read What does if __name__ == '__main__' do in Python.\nFirst possibility: Import my_test_module.py in another moduleFirst possibility: Import my_test_module.py in another modulemy_test_module.py# main.py\n\nimport my_test_module\n\nif __name__ == '__main__':\n    print('Hello from main.py')\n# main.py\n\nimport my_test_module\n\nif __name__ == '__main__':\n    print('Hello from main.py')\nNow if you invoke main.py:main.pypython main.py\n\n>> 'This is going to be printed out, no matter what'\n>> 'Hello from main.py'\npython main.py\n\n>> 'This is going to be printed out, no matter what'\n>> 'Hello from main.py'\nNote that only the top-level print() statement in my_test_module is executed.print()my_test_moduleSecond possibility: Invoke my_test_module.py as a scriptSecond possibility: Invoke my_test_module.py as a scriptmy_test_module.pyNow if you run my_test_module.py as a Python script, both print() statements will be executed:my_test_module.pyprint()python my_test_module.py\n\n>>> 'This is going to be printed out, no matter what'\n>>> 'This is going to be printed out, only if user invokes the module as a script'\npython my_test_module.py\n\n>>> 'This is going to be printed out, no matter what'\n>>> 'This is going to be printed out, only if user invokes the module as a script'\nFor a more comprehensive explanation, you can read What does if __name__ == '__main__' do in Python.What does if __name__ == '__main__' do in PythonWhat does if __name__ == '__main__' do in Pythonif __name__ == '__main__'",
                "When there are certain statements in our module (M.py) we want to be executed when it'll be running as main (not imported), we can place those statements (test-cases, print statements) under this if block.M.pyifAs by default (when module running as main, not imported) the __name__ variable is set to \"__main__\", and when it'll be imported the __name__ variable will get a different value, most probably the name of the module ('M').\nThis is helpful in running different variants of a modules together, and separating their specific input & output statements and also if there are any test-cases.__name__\"__main__\"__name__'M'In short, use this 'if __name__ == \"main\" ' block to prevent (certain) code from being run when the module is imported.In shortif __name__ == \"main\"",
                "Put simply, __name__ is a variable defined for each script that defines whether the script is being run as the main module or it is being run as an imported module.__name__So if we have two scripts;#script1.py\nprint \"Script 1's name: {}\".format(__name__)\n#script1.py\nprint \"Script 1's name: {}\".format(__name__)\nand#script2.py\nimport script1\nprint \"Script 2's name: {}\".format(__name__)\n#script2.py\nimport script1\nprint \"Script 2's name: {}\".format(__name__)\nThe output from executing script1 isScript 1's name: __main__\nScript 1's name: __main__\nAnd the output from executing script2 is:Script1's name is script1\nScript 2's name: __main__\nScript1's name is script1\nScript 2's name: __main__\nAs you can see, __name__ tells us which code is the 'main' module.\nThis is great, because you can just write code and not have to worry about structural issues like in C/C++, where, if a file does not implement a 'main' function then it cannot be compiled as an executable and if it does, it cannot then be used as a library.__name__Say you write a Python script that does something great and you implement a boatload of functions that are useful for other purposes. If I want to use them I can just import your script and use them without executing your program (given that your code only executes within the  if __name__ == \"__main__\": context). Whereas in C/C++ you would have to portion out those pieces into a separate module that then includes the file. Picture the situation below;if __name__ == \"__main__\":The arrows are import links. For three modules each trying to include the previous modules code there are six files (nine, counting the implementation files) and five links. This makes it difficult to include other code into a C project unless it is compiled specifically as a library. Now picture it for Python:You write a module, and if someone wants to use your code they just import it and the __name__ variable can help to separate the executable portion of the program from the library part.__name__",
                "To be short, you need to know several points:\nimport a action actually runs all that can be run in a.py, meaning each line in a.py\n\nBecause of point 1, you may not want everything to be run in a.py when importing it\n\nTo solve the problem in point 2, Python allows you to use a condition check\n\n__name__ is an implicit variable in all .py modules:\n\nimport a action actually runs all that can be run in a.py, meaning each line in a.py\nimport a action actually runs all that can be run in a.py, meaning each line in a.pyimport aa.pya.pyBecause of point 1, you may not want everything to be run in a.py when importing it\nBecause of point 1, you may not want everything to be run in a.py when importing ita.pyTo solve the problem in point 2, Python allows you to use a condition check\nTo solve the problem in point 2, Python allows you to use a condition check__name__ is an implicit variable in all .py modules:\n__name__ is an implicit variable in all .py modules:__name__.py\nwhen a.py is imported, the value of __name__ of a.py module is set to its file name \"a\"\nwhen a.py is run directly using \"python a.py\", the value of __name__ is set to a string __main__\nwhen a.py is imported, the value of __name__ of a.py module is set to its file name \"a\"a.pyimport__name__a.pyawhen a.py is run directly using \"python a.py\", the value of __name__ is set to a string __main__a.pypython a.py__name____main__\nBased on the mechanism how Python sets the variable __name__ for each module, do you know how to achieve point 3? The answer is fairly easy, right? Use an if condition: if __name__ == \"__main__\": // do A\nBased on the mechanism how Python sets the variable __name__ for each module, do you know how to achieve point 3? The answer is fairly easy, right? Use an if condition: if __name__ == \"__main__\": // do A__name__ifif __name__ == \"__main__\": // do A\nthen python a.py will run the part // do A\nand import a will skip the part // do A\nthen python a.py will run the part // do Apython a.py// do Aand import a will skip the part // do Aimport a// do A\nYou can even put if __name__ == \"a\" depending on your functional need, but rarely do\nYou can even put if __name__ == \"a\" depending on your functional need, but rarely do__name__ == \"a\"The important thing that Python is special at is point 4! The rest is just basic logic.I've been reading so much throughout the answers on this page. I would say, if you know the thing, for sure you will understand those answers, otherwise, you are still confused.I've been reading so much throughout the answers on this page. I would say, if you know the thing, for sure you will understand those answers, otherwise, you are still confused.",
                "Let's look at the answer in a more abstract way:Suppose we have this code in x.py:x.py...\n<Block A>\nif __name__ == '__main__':\n    <Block B>\n...\n...\n<Block A>\nif __name__ == '__main__':\n    <Block B>\n...\nBlocks A and B are run when we are running x.py.x.pyBut just block A (and not B) is run when we are running another module, y.py for example, in which x.py is imported and the code is run from there (like when a function in x.py is called from y.py).y.pyx.pyx.pyy.py",
                "When you run Python interactively the local __name__ variable is assigned a value of __main__. Likewise, when you execute a Python module from the command line, rather than importing it into another module, its __name__ attribute is assigned a value of __main__, rather than the actual name of the module. In this way, modules can look at their own __name__ value to determine for themselves how they are being used, whether as support for another program or as the main application executed from the command line. Thus, the following idiom is quite common in Python modules:__name____main____name____main____name__if __name__ == '__main__':\n    # Do something appropriate here, like calling a\n    # main() function defined elsewhere in this module.\n    main()\nelse:\n    # Do nothing. This module has been imported by another\n    # module that wants to make use of the functions,\n    # classes and other useful bits it has defined.\nif __name__ == '__main__':\n    # Do something appropriate here, like calling a\n    # main() function defined elsewhere in this module.\n    main()\nelse:\n    # Do nothing. This module has been imported by another\n    # module that wants to make use of the functions,\n    # classes and other useful bits it has defined.\n",
                "Consider:if __name__ == \"__main__\":\n    main()\nif __name__ == \"__main__\":\n    main()\nIt checks if the __name__ attribute of the Python script is \"__main__\". In other words, if the program itself is executed, the attribute will be __main__, so the program will be executed (in this case the main() function).__name__\"__main__\"__main__main()However, if your Python script is used by a module, any code outside of the if statement will be executed, so if __name__ == \"__main__\" is used just to check if the program is used as a module or not, and therefore decides whether to run the code.ifif __name__ == \"__main__\"",
                "Before explaining anything about if __name__ == '__main__' it is important to understand what __name__ is and what it does.Before explaining anything about if __name__ == '__main__' it is important to understand what __name__ is and what it does.if __name__ == '__main__'__name__What is __name__?__name____name__ is a DunderAlias - can be thought of as a global variable (accessible from modules) and works in a similar way to global.__name__DunderAliasglobalglobalIt is a string (global as mentioned above) as indicated by type(__name__) (yielding <class 'str'>), and is an inbuilt standard for both Python 3 and Python 2 versions.type(__name__)<class 'str'>Python 3Python 2WhereIt can not only be used in scripts but can also be found in both the interpreter and modules/packages.Interpreter:>>> print(__name__)\n__main__\n>>>\n>>> print(__name__)\n__main__\n>>>\nScript:test_file.py:test_file.pyprint(__name__)\nprint(__name__)\nResulting in __main____main__Module or package:somefile.py:somefile.py:def somefunction():\n    print(__name__)\ndef somefunction():\n    print(__name__)\ntest_file.py:test_file.py:import somefile\nsomefile.somefunction()\nimport somefile\nsomefile.somefunction()\nResulting in somefilesomefileNotice that when used in a package or module, __name__ takes the name of the file.  The path of the actual module or package path is not given, but has its own DunderAlias __file__, that allows for this.__name____file__You should see that, where __name__, where it is the main file (or program) will always return __main__, and if it is a module/package, or anything that is running off some other Python script, will return the name of the file where it has originated from.__name__always__main__PracticeBeing a variable means that it's value can be overwritten (\"can\" does not mean \"should\"), overwriting the value of __name__ will result in a lack of readability.  So do not do it, for any reason.  If you need a variable define a new variable.can__name__It is always assumed that the value of __name__ to be __main__ or the name of the file.  Once again changing this default value will cause more confusion that it will do good, causing problems further down the line.__name____main__Example:>>> __name__ = 'Horrify' # Change default from __main__\n>>> if __name__ == 'Horrify': print(__name__)\n...\n>>> else: print('Not Horrify')\n...\nHorrify\n>>>\n>>> __name__ = 'Horrify' # Change default from __main__\n>>> if __name__ == 'Horrify': print(__name__)\n...\n>>> else: print('Not Horrify')\n...\nHorrify\n>>>\nIt is considered good practice in general to include the if __name__ == '__main__' in scripts.if __name__ == '__main__'Now to answer if __name__ == '__main__':if __name__ == '__main__'Now we know the behaviour of __name__ things become clearer:Now we know the behaviour of __name__ things become clearer:__name__An if is a flow control statement that contains the block of code will execute if the value given is true. We have seen that __name__ can take either\n__main__ or the file name it has been imported from.ifif__name____main__This means that if __name__ is equal to __main__ then the file must be the main file and must actually be running (or it is the interpreter), not a module or package imported into the script.__name____main__If indeed __name__ does take the value of __main__ then whatever is in that block of code will execute.__name____main__This tells us that if the file running is the main file (or you are running from the interpreter directly) then that condition must execute.  If it is a package then it should not, and the value will not be __main__.__main__Modules__name__ can also be used in modules to define the name of a module__name__VariantsIt is also possible to do other, less common but useful things with __name__, some I will show here:__name__Executing only if the file is a module or packageif __name__ != '__main__':\n    # Do some useful things \nif __name__ != '__main__':\n    # Do some useful things \nRunning one condition if the file is the main one and another if it is notif __name__ == '__main__':\n    # Execute something\nelse:\n    # Do some useful things\nif __name__ == '__main__':\n    # Execute something\nelse:\n    # Do some useful things\nYou can also use it to provide runnable help functions/utilities on packages and modules without the elaborate use of libraries.It also allows modules to be run from the command line as main scripts, which can be also very useful.",
                "I think it's best to break the answer in depth and in simple words:__name__: Every module in Python has a special attribute called __name__.\nIt is a built-in variable that returns the name of the module.__name____name____main__: Like other programming languages, Python too has an execution entry point, i.e., main. '__main__' is the name of the scope in which top-level code executes. Basically you have two ways of using a Python module: Run it directly as a script, or import it. When a module is run as a script, its __name__ is set to __main__.__main__'__main__'is the name of the scope in which top-level code executes__name____main__Thus, the value of the __name__ attribute is set to __main__ when the module is run as the main program. Otherwise the value of __name__  is set to contain the name of the module.__name____main____name__",
                "It is a special for when a Python file is called from the command line. This is typically used to call a \"main()\" function or execute other appropriate startup code, like commandline arguments handling for instance.It could be written in several ways. Another is:def some_function_for_instance_main():\n    dosomething()\n\n\n__name__ == '__main__' and some_function_for_instance_main()\ndef some_function_for_instance_main():\n    dosomething()\n\n\n__name__ == '__main__' and some_function_for_instance_main()\nI am not saying you should use this in production code, but it serves to illustrate that there is nothing \"magical\" about if __name__ == '__main__'.if __name__ == '__main__'It just a convention for invoking a main function in Python files.",
                "There are a number of variables that the system (Python interpreter) provides for source files (modules).  You can get their values anytime you want, so, let us focus on the __name__ variable/attribute:__name__When Python loads a source code file, it executes all of the code found in it. (Note that it doesn't call all of the methods and functions defined in the file, but it does define them.)Before the interpreter executes the source code file though, it defines a few special variables for that file; __name__ is one of those special variables that Python automatically defines for each source code file.__name__If Python is loading this source code file as the main program (i.e. the file you run), then it sets the special __name__ variable for this file to have a value \"__main__\".__name__\"__main__\"If this is being imported from another module, __name__ will be set to that module's name.__name__So, in your example in part:if __name__ == \"__main__\":\n   lock = thread.allocate_lock()\n   thread.start_new_thread(myfunction, (\"Thread #: 1\", 2, lock))\n   thread.start_new_thread(myfunction, (\"Thread #: 2\", 2, lock))\nif __name__ == \"__main__\":\n   lock = thread.allocate_lock()\n   thread.start_new_thread(myfunction, (\"Thread #: 1\", 2, lock))\n   thread.start_new_thread(myfunction, (\"Thread #: 2\", 2, lock))\nmeans that the code block:lock = thread.allocate_lock()\nthread.start_new_thread(myfunction, (\"Thread #: 1\", 2, lock))\nthread.start_new_thread(myfunction, (\"Thread #: 2\", 2, lock))\nlock = thread.allocate_lock()\nthread.start_new_thread(myfunction, (\"Thread #: 1\", 2, lock))\nthread.start_new_thread(myfunction, (\"Thread #: 2\", 2, lock))\nwill be executed only when you run the module directly; the code block will not execute if another module is calling/importing it because the value of __name__ will not equal to \"main\" in that particular instance.__name__mainHope this helps out.",
                "if __name__ == \"__main__\": is basically the top-level script environment, and it specifies the interpreter that ('I have the highest priority to be executed first').if __name__ == \"__main__\":'__main__' is the name of the scope in which top-level code executes. A module\u2019s __name__ is set equal to '__main__' when read from standard input, a script, or from an interactive prompt.'__main__'__name__'__main__'if __name__ == \"__main__\":\n    # Execute only if run as a script\n    main()\nif __name__ == \"__main__\":\n    # Execute only if run as a script\n    main()\n",
                "Consider:print __name__\nprint __name__\nThe output for the above is __main__.__main__if __name__ == \"__main__\":\n  print \"direct method\"\nif __name__ == \"__main__\":\n  print \"direct method\"\nThe above statement is true and prints \"direct method\". Suppose if they imported this class in another class it doesn't print \"direct method\" because, while importing, it will set __name__ equal to \"first model name\".\"direct method\"\"direct method\"__name__ equal to \"first model name\"",
                "In simple words:The code you see under if __name__ == \"__main__\": will only get called upon when your Python file is executed as python example1.pyif __name__ == \"__main__\":python example1.pyHowever, if you wish to import your Python file example1.py as a module to work with another Python file, say example2.py, the code under if __name__ == \"__main__\": will not run or take any effect.example1.pyexample2.pyif __name__ == \"__main__\":",
                "\nYou can make the file usable as a script as well as an importable module.\nYou can make the file usable as a script as well as an importable module.scriptimportable modulefibo.py (a module named fibo)fibo.py (a module named fibo)fibo# Other modules can IMPORT this MODULE to use the function fib\ndef fib(n):    # write Fibonacci series up to n\n    a, b = 0, 1\n    while b < n:\n        print(b, end=' ')\n        a, b = b, a+b\n    print()\n\n# This allows the file to be used as a SCRIPT\nif __name__ == \"__main__\":\n    import sys\n    fib(int(sys.argv[1]))\n# Other modules can IMPORT this MODULE to use the function fib\ndef fib(n):    # write Fibonacci series up to n\n    a, b = 0, 1\n    while b < n:\n        print(b, end=' ')\n        a, b = b, a+b\n    print()\n\n# This allows the file to be used as a SCRIPT\nif __name__ == \"__main__\":\n    import sys\n    fib(int(sys.argv[1]))\nReference: https://docs.python.org/3.5/tutorial/modules.htmlhttps://docs.python.org/3.5/tutorial/modules.html",
                "The reason forif __name__ == \"__main__\":\n    main()\nif __name__ == \"__main__\":\n    main()\nis primarily to avoid the import lock problems that would arise from having code directly imported. You want main() to run if your file was directly invoked (that's the __name__ == \"__main__\" case), but if your code was imported then the importer has to enter your code from the true main module to avoid import lock problems.import lockhaving code directly importedmain()__name__ == \"__main__\"A side-effect is that you automatically sign on to a methodology that supports multiple entry points. You can run your program using main() as the entry point, but you don't have to. While setup.py expects main(), other tools use alternate entry points. For example, to run your file as a gunicorn process, you define an app() function instead of a main(). Just as with setup.py, gunicorn imports your code so you don't want it do do anything while it's being imported (because of the import lock issue).main()but you don't have tosetup.pymain()gunicornapp()main()setup.pygunicorn",
                "If you are a beginner, probably the only answer you need right now is that this code is unnecessary for a simple script. It is only useful if you want to be able to import your script (or unpickle etc; see the other answers here for some other non-beginner scenarios).this code is unnecessaryimportunpickleIn slightly different words, the if __name__ guard is a mechanism for hiding code from other code. If you don't have a specific reason to hide something, don't: If you don't need to hide some code from import, don't put it behind this guard, and if you do, hide as little as possible.if __name__importIn slightly more detail, let's say you have a simple script fib.py (adapted from this answer):fib.pythis answer# XXX FIXME: useless (see below)\nif __name__ == \"__main__\":\n    n = int(input('Write a number: '))\n    a, b = 0, 1\n    while b < n:\n        a, b = b, a+b\n    print('Fibonacci number %i: %i' % (n, b))\n# XXX FIXME: useless (see below)\nif __name__ == \"__main__\":\n    n = int(input('Write a number: '))\n    a, b = 0, 1\n    while b < n:\n        a, b = b, a+b\n    print('Fibonacci number %i: %i' % (n, b))\nNow, if you simply run python fib.py it works fine. But __name__ will always be \"__main__\" in this scenario, so the condition is actually unnecessary. The script could be simplified to justpython fib.py__name__\"__main__\"n = int(input('Write a number: '))\na, b = 0, 1\nwhile b < n:\n    a, b = b, a+b\nprint('Fibonacci number %i: %i' % (n, b))\nn = int(input('Write a number: '))\na, b = 0, 1\nwhile b < n:\n    a, b = b, a+b\nprint('Fibonacci number %i: %i' % (n, b))\nNow, you can't import fib with the new version, but if you didn't plan to do that in the first place, this version is actually better, because it's simpler and clearer.import fibIf you do want to be able to import fib, the first version is useless, too, because the useful code is in a section which will not run when you import this file (in which case __name__ will not be \"__main__\"). The proper design in that case would be to refactor the code so that the useful parts are in a function you can run when you want to after you have imported it.doimport fibimport__name__\"__main__\"importdef main():\n    n = int(input('Write a number: '))\n    a, b = 0, 1\n    while b < n:\n        a, b = b, a+b\n    print('Fibonacci number %i: %i' % (n, b))\n\nif __name__ == \"__main__\":\n    main()\ndef main():\n    n = int(input('Write a number: '))\n    a, b = 0, 1\n    while b < n:\n        a, b = b, a+b\n    print('Fibonacci number %i: %i' % (n, b))\n\nif __name__ == \"__main__\":\n    main()\nNow, if you import fib, the call to main() will not be executed; but when you run python fib.py, it will.import fibmain()python fib.pyActually, a better design still would be to isolate the reusable part (the actual calculation) from the user-visible input/output:def fibn(n: int) -> int:\n    a, b = 0, 1\n    while b < n:\n        a, b = b, a+b\n    return b\n\ndef main() -> None:\n    n = int(input('Write a number: '))\n    print('Fibonacci number %i: %i' % (n, fibn(n)))\n\nif __name__ == \"__main__\":\n    main()\ndef fibn(n: int) -> int:\n    a, b = 0, 1\n    while b < n:\n        a, b = b, a+b\n    return b\n\ndef main() -> None:\n    n = int(input('Write a number: '))\n    print('Fibonacci number %i: %i' % (n, fibn(n)))\n\nif __name__ == \"__main__\":\n    main()\nNow, you can from fib import fibn and call the fibn() function from the code which performs this import.from fib import fibnfibn()import(I called the function fibn() just to make it clearer what is what in this example. In real life, you might call it fib() and do from fib import fib.)fibn()fib()from fib import fibSimilarly, you could import and call the main function if you wanted to reuse it.importmainReturning to the code in the question, I would similarly move the code from the if into a function as well, so that callers can invoke that function if they want to.ifdef main():\n    lock = thread.allocate_lock()\n    thread.start_new_thread(myfunction, (\"Thread #: 1\", 2, lock))\n    thread.start_new_thread(myfunction, (\"Thread #: 2\", 2, lock))\n\nif __name__ == \"__main__\":\n    main()\ndef main():\n    lock = thread.allocate_lock()\n    thread.start_new_thread(myfunction, (\"Thread #: 1\", 2, lock))\n    thread.start_new_thread(myfunction, (\"Thread #: 2\", 2, lock))\n\nif __name__ == \"__main__\":\n    main()\nThis changes the scope of the lock variable; if the surrounding code needs access to it, you will need to make it global (or, perhaps, better, refactor main to return lock, and have the caller capture the value in a local variable of its own).lockglobalmainreturn lock(Unlike in languages like C, the name main has no specific meaning to Python; but it's a common convention to use it as the name of the thing which will be run. You still have to actually explicitly call it, like main(), unlike in C.)mainmain()",
                "Every module in Python has an attribute called __name__. The value of __name__  attribute is  __main__ when the module is run directly, like python my_module.py. Otherwise (like when you say import my_module) the value of __name__  is the name of the module.__name____name____main__python my_module.pyimport my_module__name__Small example to explain in short.Script test.pytest.pyapple = 42\n\ndef hello_world():\n    print(\"I am inside hello_world\")\n\nif __name__ == \"__main__\":\n    print(\"Value of __name__ is: \", __name__)\n    print(\"Going to call hello_world\")\n    hello_world()\napple = 42\n\ndef hello_world():\n    print(\"I am inside hello_world\")\n\nif __name__ == \"__main__\":\n    print(\"Value of __name__ is: \", __name__)\n    print(\"Going to call hello_world\")\n    hello_world()\nWe can execute this directly aspython test.py\npython test.py\nOutputValue of __name__ is: __main__\nGoing to call hello_world\nI am inside hello_world\nValue of __name__ is: __main__\nGoing to call hello_world\nI am inside hello_world\nNow suppose we call the above script from another script:Script external_calling.pyexternal_calling.pyimport test\n\nprint(test.apple)\ntest.hello_world()\n\nprint(test.__name__)\nimport test\n\nprint(test.apple)\ntest.hello_world()\n\nprint(test.__name__)\nWhen you execute this,python external_calling.py\npython external_calling.py\nOutput42\nI am inside hello_world\ntest\n42\nI am inside hello_world\ntest\nSo, the above is self-explanatory that when you call test from another script, if loop __name__ in test.py will not execute.test__name__test.py",
                "This answer is for Java programmers learning Python.\nEvery Java file typically contains one public class. You can use that class in two ways: \nCall the class from other files. You just have to import it in the calling program.\nRun the class stand alone, for testing purposes. \nCall the class from other files. You just have to import it in the calling program.Call the class from other files. You just have to import it in the calling program.Run the class stand alone, for testing purposes. Run the class stand alone, for testing purposes. For the latter case, the class should contain a public static void main() method. In Python this purpose is served by the globally defined label '__main__'.'__main__'",
                "If this .py file are imported by other .py files, the code under the if statement will not be executed.ifIf this .py are run by python this_py.py under shell, or double clicked in Windows. the code under the if statement will be executed.python this_py.pyifIt is usually written for testing.",
                "We see if __name__ == '__main__': quite often.__name__ == '__main__':It checks if a module is being imported or not.In other words, the code within the if block will be executed only when the code runs directly. Here directly means not imported.ifdirectlynot importedLet's see what it does using a simple code that prints the name of the module:# test.py\ndef test():\n   print('test module name=%s' %(__name__))\n\nif __name__ == '__main__':\n   print('call test()')\n   test()\n# test.py\ndef test():\n   print('test module name=%s' %(__name__))\n\nif __name__ == '__main__':\n   print('call test()')\n   test()\nIf we run the code directly via python test.py, the module name is __main__:python test.py__main__call test()\ntest module name=__main__\ncall test()\ntest module name=__main__\n",
                "If the Python interpreter is running a particular module then the __name__ global variable will have the value \"__main__\":__name__\"__main__\"def a():\n    print(\"a\")\n\ndef b():\n    print(\"b\")\n\nif __name__ == \"__main__\":\n\n        print (\"you can see me\")\n        a()\nelse:\n\n        print (\"You can't see me\")\n        b()\ndef a():\n    print(\"a\")\n\ndef b():\n    print(\"b\")\n\nif __name__ == \"__main__\":\n\n        print (\"you can see me\")\n        a()\nelse:\n\n        print (\"You can't see me\")\n        b()\nWhen you run this script, it prints:you can see me\na\nyou can see me\na\nIf you import this file, say A to file B, and execute the file B then if __name__ == \"__main__\" in file A becomes False, so it prints:AABBBBif __name__ == \"__main__\"AAFalseYou can't see me\nb\nYou can't see me\nb\n",
                "All the answers have pretty much explained the functionality. But I will provide one example of its usage which might help clearing out the concept further.Assume that you have two Python files, a.py and b.py. Now, a.py imports b.py. We run the a.py file, where the import b.py code is executed first. Before the rest of the a.py code runs, the code in the file b.py must run completely.a.pyb.pya.pyimport b.pya.pyb.pyIn the b.py code, there is some code that is exclusive to that file b.py and we don't want any other file (other than the b.py file), that has imported the b.py file, to run it.b.pyb.pySo that is what this line of code checks. If it is the main file (i.e., b.py) running the code, which in this case it is not (a.py is the main file running), then only the code gets executed.b.pya.py"
            ]
        },
        {
            "tag": "ternary",
            "patterns": [
                "Does Python have a ternary conditional operator?",
                "does python have a ternary conditional operator?"
            ],
            "responses": [
                "Yes, it was added in version 2.5. The expression syntax is:addeda if condition else b\na if condition else b\nFirst condition is evaluated, then exactly one of either a or b is evaluated and returned based on the Boolean value of condition. If condition evaluates to True, then a is evaluated and returned but b is ignored, or else when b is evaluated and returned but a is ignored.conditionabBooleanconditionconditionTrueabbaThis allows short-circuiting because when condition is true only a is evaluated and b is not evaluated at all, but when condition is false only b is evaluated and a is not evaluated at all.conditionabconditionbaFor example:>>> 'true' if True else 'false'\n'true'\n>>> 'true' if False else 'false'\n'false'\n>>> 'true' if True else 'false'\n'true'\n>>> 'true' if False else 'false'\n'false'\nNote that conditionals are an expression, not a statement. This means you can't use statements such as pass, or assignments with = (or \"augmented\" assignments like +=), within a conditional expression:expressionstatementstatementspass=+=expression>>> pass if False else pass\n  File \"<stdin>\", line 1\n    pass if False else pass\n         ^\nSyntaxError: invalid syntax\n\n>>> # Python parses this as `x = (1 if False else y) = 2`\n>>> # The `(1 if False else x)` part is actually valid, but\n>>> # it can't be on the left-hand side of `=`.\n>>> x = 1 if False else y = 2\n  File \"<stdin>\", line 1\nSyntaxError: cannot assign to conditional expression\n\n>>> # If we parenthesize it instead...\n>>> (x = 1) if False else (y = 2)\n  File \"<stdin>\", line 1\n    (x = 1) if False else (y = 2)\n       ^\nSyntaxError: invalid syntax\n>>> pass if False else pass\n  File \"<stdin>\", line 1\n    pass if False else pass\n         ^\nSyntaxError: invalid syntax\n\n>>> # Python parses this as `x = (1 if False else y) = 2`\n>>> # The `(1 if False else x)` part is actually valid, but\n>>> # it can't be on the left-hand side of `=`.\n>>> x = 1 if False else y = 2\n  File \"<stdin>\", line 1\nSyntaxError: cannot assign to conditional expression\n\n>>> # If we parenthesize it instead...\n>>> (x = 1) if False else (y = 2)\n  File \"<stdin>\", line 1\n    (x = 1) if False else (y = 2)\n       ^\nSyntaxError: invalid syntax\n(In 3.8 and above, the := \"walrus\" operator allows simple assignment of values as an expression, which is then compatible with this syntax. But please don't write code like that; it will quickly become very difficult to understand.):=as an expressionSimilarly, because it is an expression, the else part is mandatory:elsemandatory# Invalid syntax: we didn't specify what the value should be if the \n# condition isn't met. It doesn't matter if we can verify that\n# ahead of time.\na if True\n# Invalid syntax: we didn't specify what the value should be if the \n# condition isn't met. It doesn't matter if we can verify that\n# ahead of time.\na if True\nYou can, however, use conditional expressions to assign a variable like so:x = a if True else b\nx = a if True else b\nOr for example to return a value:# Of course we should just use the standard library `max`;\n# this is just for demonstration purposes.\ndef my_max(a, b):\n    return a if a > b else b\n# Of course we should just use the standard library `max`;\n# this is just for demonstration purposes.\ndef my_max(a, b):\n    return a if a > b else b\nThink of the conditional expression as switching between two values. We can use it when we are in a 'one value or another' situation, where we will do the same thing with the result, regardless of whether the condition is met. We use the expression to compute the value, and then do something with it. If you need to do something different depending on the condition, then use a normal if statement instead.do the same thingdo something differentifstatementKeep in mind that it's frowned upon by some Pythonistas for several reasons:\nThe order of the arguments is different from those of the classic condition ? a : b ternary operator from many other languages (such as C, C++, Go, Perl, Ruby, Java, JavaScript, etc.), which may lead to bugs when people unfamiliar with Python's \"surprising\" behaviour use it (they may reverse the argument order).\nSome find it \"unwieldy\", since it goes contrary to the normal flow of thought (thinking of the condition first and then the effects).\nStylistic reasons. (Although the 'inline if' can be really useful, and make your script more concise, it really does complicate your code)\nThe order of the arguments is different from those of the classic condition ? a : b ternary operator from many other languages (such as C, C++, Go, Perl, Ruby, Java, JavaScript, etc.), which may lead to bugs when people unfamiliar with Python's \"surprising\" behaviour use it (they may reverse the argument order).condition ? a : bCC++GoPerlRubyJavaJavaScriptSome find it \"unwieldy\", since it goes contrary to the normal flow of thought (thinking of the condition first and then the effects).Stylistic reasons. (Although the 'inline if' can be really useful, and make your script more concise, it really does complicate your code)ifreallyIf you're having trouble remembering the order, then remember that when read aloud, you (almost) say what you mean. For example, x = 4 if b > 8 else 9 is read aloud as x will be 4 if b is greater than 8 otherwise 9.x = 4 if b > 8 else 9x will be 4 if b is greater than 8 otherwise 9Official documentation:\nConditional expressions\nIs there an equivalent of C\u2019s \u201d?:\u201d ternary operator?\nConditional expressionsConditional expressionsIs there an equivalent of C\u2019s \u201d?:\u201d ternary operator?Is there an equivalent of C\u2019s \u201d?:\u201d ternary operator?",
                "You can index into a tuple:(falseValue, trueValue)[test]\n(falseValue, trueValue)[test]\ntest needs to return True or False.\nIt might be safer to always implement it as:testTrueFalse(falseValue, trueValue)[test == True]\n(falseValue, trueValue)[test == True]\nor you can use the built-in bool() to assure a Boolean value:bool()bool()Boolean(falseValue, trueValue)[bool(<expression>)]\n(falseValue, trueValue)[bool(<expression>)]\n",
                "For versions prior to 2.5, there's the trick:[expression] and [on_true] or [on_false]\n[expression] and [on_true] or [on_false]\nIt can give wrong results when on_true has a false Boolean value.1on_true1Although it does have the benefit of evaluating expressions left to right, which is clearer in my opinion.1. Is there an equivalent of C\u2019s \u201d?:\u201d ternary operator?1. Is there an equivalent of C\u2019s \u201d?:\u201d ternary operator?Is there an equivalent of C\u2019s \u201d?:\u201d ternary operator?",
                " <expression 1> if <condition> else <expression 2>  <expression 1> if <condition> else <expression 2> <expression 1>if<condition>else<expression 2>a = 1\nb = 2\n\n1 if a > b else -1 \n# Output is -1\n\n1 if a > b else -1 if a < b else 0\n# Output is -1\na = 1\nb = 2\n\n1 if a > b else -1 \n# Output is -1\n\n1 if a > b else -1 if a < b else 0\n# Output is -1\n",
                "From the documentation:the documentation\nConditional expressions (sometimes called a \u201cternary operator\u201d) have the lowest priority of all Python operations.\nThe expression x if C else y first evaluates the condition, C (not x); if C is true, x is evaluated and its value is returned; otherwise, y is evaluated and its value is returned.\nSee PEP 308 for more details about conditional expressions.\nConditional expressions (sometimes called a \u201cternary operator\u201d) have the lowest priority of all Python operations.The expression x if C else y first evaluates the condition, C (not x); if C is true, x is evaluated and its value is returned; otherwise, y is evaluated and its value is returned.x if C else yCnot xCxySee PEP 308 for more details about conditional expressions.PEP 308New since version 2.5.",
                "An operator for a conditional expression in Python was added in 2006 as part of Python Enhancement Proposal 308. Its form differ from common ?: operator and it's:Python Enhancement Proposal 308?:<expression1> if <condition> else <expression2>\n<expression1> if <condition> else <expression2>\nwhich is equivalent to:if <condition>: <expression1> else: <expression2>\nif <condition>: <expression1> else: <expression2>\nHere is an example:result = x if a > b else y\nresult = x if a > b else y\nAnother syntax which can be used (compatible with versions before 2.5):result = (lambda:y, lambda:x)[a > b]()\nresult = (lambda:y, lambda:x)[a > b]()\nwhere operands are lazily evaluated.lazily evaluatedAnother way is by indexing a tuple (which isn't consistent with the conditional operator of most other languages):result = (y, x)[a > b]\nresult = (y, x)[a > b]\nor explicitly constructed dictionary:result = {True: x, False: y}[a > b]\nresult = {True: x, False: y}[a > b]\nAnother (less reliable), but simpler method is to use and and or operators:andorresult = (a > b) and x or y\nresult = (a > b) and x or y\nhowever this won't work if x would be False.xFalseA possible workaround is to make x and y lists or tuples as in the following:xyresult = ((a > b) and [x] or [y])[0]\nresult = ((a > b) and [x] or [y])[0]\nor:result = ((a > b) and (x,) or (y,))[0]\nresult = ((a > b) and (x,) or (y,))[0]\nIf you're working with dictionaries, instead of using a ternary conditional, you can take advantage of get(key, default), for example:get(key, default)get(key, default)shell = os.environ.get('SHELL', \"/bin/sh\")\nshell = os.environ.get('SHELL', \"/bin/sh\")\nSource: ?: in Python at WikipediaSource: ?: in Python at Wikipedia?: in Python at Wikipedia",
                "Unfortunately, the(falseValue, trueValue)[test]\n(falseValue, trueValue)[test]\nsolution doesn't have short-circuit behaviour; thus both falseValue and trueValue are evaluated regardless of the condition. This could be suboptimal or even buggy (i.e. both trueValue and falseValue could be methods and have side effects).falseValuetrueValuetrueValuefalseValueOne solution to this would be(lambda: falseValue, lambda: trueValue)[test]()\n(lambda: falseValue, lambda: trueValue)[test]()\n(execution delayed until the winner is known ;)), but it introduces inconsistency between callable and non-callable objects. In addition, it doesn't solve the case when using properties.And so the story goes - choosing between three mentioned solutions is a trade-off between having the short-circuit feature, using at least Python 2.5 (IMHO, not a problem anymore) and not being prone to \"trueValue-evaluates-to-false\" errors.trueValue",
                "Ternary operator in different programming languagesTernary operator in different programming languagesHere I just try to show some important differences in the ternary operator between a couple of programming languages.ternary operatorTernary operator in JavaScriptTernary operator in JavaScriptJavaScriptvar a = true ? 1 : 0;\n# 1\nvar b = false ? 1 : 0;\n# 0\nvar a = true ? 1 : 0;\n# 1\nvar b = false ? 1 : 0;\n# 0\nTernary operator in RubyTernary operator in RubyRubya = true ? 1 : 0\n# 1\nb = false ? 1 : 0\n# 0\na = true ? 1 : 0\n# 1\nb = false ? 1 : 0\n# 0\nTernary operator in ScalaTernary operator in ScalaScalaval a = true ? 1 | 0\n# 1\nval b = false ? 1 | 0\n# 0\nval a = true ? 1 | 0\n# 1\nval b = false ? 1 | 0\n# 0\nTernary operator in R programmingTernary operator in R programmingRa <- if (TRUE) 1 else 0\n# 1\nb <- if (FALSE) 1 else 0\n# 0\na <- if (TRUE) 1 else 0\n# 1\nb <- if (FALSE) 1 else 0\n# 0\nTernary operator in PythonTernary operator in Pythona = 1 if True else 0\n# 1\nb = 1 if False else 0\n# 0\na = 1 if True else 0\n# 1\nb = 1 if False else 0\n# 0\n",
                "For Python 2.5 and newer there is a specific syntax:[on_true] if [cond] else [on_false]\n[on_true] if [cond] else [on_false]\nIn older Pythons a ternary operator is not implemented but it's possible to simulate it.cond and on_true or on_false\ncond and on_true or on_false\nThough, there is a potential problem, which if cond evaluates to True and on_true evaluates to False then on_false is returned instead of on_true. If you want this behavior the method is OK, otherwise use this:condTrueon_trueFalseon_falseon_true{True: on_true, False: on_false}[cond is True] # is True, not == True\n{True: on_true, False: on_false}[cond is True] # is True, not == True\nwhich can be wrapped by:def q(cond, on_true, on_false)\n    return {True: on_true, False: on_false}[cond is True]\ndef q(cond, on_true, on_false)\n    return {True: on_true, False: on_false}[cond is True]\nand used this way:q(cond, on_true, on_false)\nq(cond, on_true, on_false)\nIt is compatible with all Python versions.",
                "You might often findcond and on_true or on_false\ncond and on_true or on_false\nbut this leads to a problem when on_true == 0>>> x = 0\n>>> print x == 0 and 0 or 1\n1\n>>> x = 1\n>>> print x == 0 and 0 or 1\n1\n>>> x = 0\n>>> print x == 0 and 0 or 1\n1\n>>> x = 1\n>>> print x == 0 and 0 or 1\n1\nWhere you would expect this result for a normal ternary operator:>>> x = 0\n>>> print 0 if x == 0 else 1\n0\n>>> x = 1\n>>> print 0 if x == 0 else 1\n1\n>>> x = 0\n>>> print 0 if x == 0 else 1\n0\n>>> x = 1\n>>> print 0 if x == 0 else 1\n1\n",
                "\nDoes Python have a ternary conditional operator?\nDoes Python have a ternary conditional operator?Yes. From the grammar file:grammar filetest: or_test ['if' or_test 'else' test] | lambdef\ntest: or_test ['if' or_test 'else' test] | lambdef\nThe part of interest is:or_test ['if' or_test 'else' test]\nor_test ['if' or_test 'else' test]\nSo, a ternary conditional operation is of the form:expression1 if expression2 else expression3\nexpression1 if expression2 else expression3\nexpression3 will be lazily evaluated (that is, evaluated only if expression2 is false in a boolean context). And because of the recursive definition, you can chain them indefinitely (though it may considered bad style.)expression3expression2expression1 if expression2 else expression3 if expression4 else expression5 # and so on\nexpression1 if expression2 else expression3 if expression4 else expression5 # and so on\nA note on usage:Note that every if must be followed with an else. People learning list comprehensions and generator expressions may find this to be a difficult lesson to learn - the following will not work, as Python expects a third expression for an else:ifelse[expression1 if expression2 for element in iterable]\n#                          ^-- need an else here\n[expression1 if expression2 for element in iterable]\n#                          ^-- need an else here\nwhich raises a SyntaxError: invalid syntax.\nSo the above is either an incomplete piece of logic (perhaps the user expects a no-op in the false condition) or what may be intended is to use expression2 as a filter - notes that the following is legal Python:SyntaxError: invalid syntaxexpression2[expression1 for element in iterable if expression2]\n[expression1 for element in iterable if expression2]\nexpression2 works as a filter for the list comprehension, and is not a ternary conditional operator.expression2notAlternative syntax for a more narrow case:You may find it somewhat painful to write the following:expression1 if expression1 else expression2\nexpression1 if expression1 else expression2\nexpression1 will have to be evaluated twice with the above usage. It can limit redundancy if it is simply a local variable. However, a common and performant Pythonic idiom for this use-case is to use or's shortcutting behavior:expression1orexpression1 or expression2\nexpression1 or expression2\nwhich is equivalent in semantics. Note that some style-guides may limit this usage on the grounds of clarity - it does pack a lot of meaning into very little syntax.",
                "One of the alternatives to Python's conditional expressionconditional expression\"yes\" if boolean else \"no\"\n\"yes\" if boolean else \"no\"\nis the following:{True: \"yes\", False: \"no\"}[boolean]\n{True: \"yes\", False: \"no\"}[boolean]\nwhich has the following nice extension:{True: \"yes\", False: \"no\", None: \"maybe\"}[boolean_or_none]\n{True: \"yes\", False: \"no\", None: \"maybe\"}[boolean_or_none]\nThe shortest alternative remains(\"no\", \"yes\")[boolean]\n(\"no\", \"yes\")[boolean]\nwhich works because issubclass(bool, int).issubclass(bool, int)Careful, though: the alternative toyes() if boolean else no()\nyes() if boolean else no()\nis notnot(no(), yes())[boolean]  # bad: BOTH no() and yes() are called\n(no(), yes())[boolean]  # bad: BOTH no() and yes() are called\nbut(no, yes)[boolean]()\n(no, yes)[boolean]()\nThis works fine as long as no and yes are to be called with exactly the same parameters. If they are not, like innoyesyes(\"ok\") if boolean else no()  # (1)\nyes(\"ok\") if boolean else no()  # (1)\nor inyes(\"ok\") if boolean else no(\"sorry\")  # (2)\nyes(\"ok\") if boolean else no(\"sorry\")  # (2)\nthen a similar alternative either does not exist (1) or is hardly viable (2). (In rare cases, depending on the context, something likemsg = (\"sorry\", \"ok\")[boolean]\n(no, yes)[boolean](msg)\nmsg = (\"sorry\", \"ok\")[boolean]\n(no, yes)[boolean](msg)\ncould make sense.)Thanks to Radek Roj\u00edk for his commentThanks to Radek Roj\u00edk for his comment",
                "As already answered, yes, there is a ternary operator in Python:<expression 1> if <condition> else <expression 2>\n<expression 1> if <condition> else <expression 2>\nIn many cases <expression 1> is also used as Boolean evaluated <condition>. Then you can use short-circuit evaluation.<expression 1><condition>short-circuit evaluationa = 0\nb = 1\n\n# Instead of this:\nx = a if a else b\n# Evaluates as 'a if bool(a) else b'\n\n# You could use short-circuit evaluation:\nx = a or b\na = 0\nb = 1\n\n# Instead of this:\nx = a if a else b\n# Evaluates as 'a if bool(a) else b'\n\n# You could use short-circuit evaluation:\nx = a or b\nOne big pro of short-circuit evaluation is the possibility of chaining more than two expressions:x = a or b or c or d or e\nx = a or b or c or d or e\nWhen working with functions it is more different in detail:# Evaluating functions:\ndef foo(x):\n    print('foo executed')\n    return x\n\n\ndef bar(y):\n    print('bar executed')\n    return y\n\n\ndef blubb(z):\n    print('blubb executed')\n    return z\n\n\n# Ternary Operator expression 1 equals to False\nprint(foo(0) if foo(0) else bar(1))\n''' foo and bar are executed once\nfoo executed\nbar executed\n1\n'''\n\n# Ternary Operator expression 1 equals to True\nprint(foo(2) if foo(2) else bar(3))\n''' foo is executed twice!\nfoo executed\nfoo executed\n2\n'''\n\n# Short-circuit evaluation second equals to True\nprint(foo(0) or bar(1) or blubb(2))\n''' blubb is not executed\nfoo executed\nbar executed\n1\n'''\n\n# Short-circuit evaluation third equals to True\nprint(foo(0) or bar(0) or blubb(2))\n'''\nfoo executed\nbar executed\nblubb executed\n2\n'''\n\n# Short-circuit evaluation all equal to False\nprint(foo(0) or bar(0) or blubb(0))\n''' Result is 0 (from blubb(0)) because no value equals to True\nfoo executed\nbar executed\nblubb executed\n0\n'''\n# Evaluating functions:\ndef foo(x):\n    print('foo executed')\n    return x\n\n\ndef bar(y):\n    print('bar executed')\n    return y\n\n\ndef blubb(z):\n    print('blubb executed')\n    return z\n\n\n# Ternary Operator expression 1 equals to False\nprint(foo(0) if foo(0) else bar(1))\n''' foo and bar are executed once\nfoo executed\nbar executed\n1\n'''\n\n# Ternary Operator expression 1 equals to True\nprint(foo(2) if foo(2) else bar(3))\n''' foo is executed twice!\nfoo executed\nfoo executed\n2\n'''\n\n# Short-circuit evaluation second equals to True\nprint(foo(0) or bar(1) or blubb(2))\n''' blubb is not executed\nfoo executed\nbar executed\n1\n'''\n\n# Short-circuit evaluation third equals to True\nprint(foo(0) or bar(0) or blubb(2))\n'''\nfoo executed\nbar executed\nblubb executed\n2\n'''\n\n# Short-circuit evaluation all equal to False\nprint(foo(0) or bar(0) or blubb(0))\n''' Result is 0 (from blubb(0)) because no value equals to True\nfoo executed\nbar executed\nblubb executed\n0\n'''\nPS: Of course, a short-circuit evaluation is not a ternary operator, but often the ternary is used in cases where the short circuit would be enough. It has a better readability and can be chained.",
                "Simulating the Python ternary operator.For examplea, b, x, y = 1, 2, 'a greather than b', 'b greater than a'\nresult = (lambda:y, lambda:x)[a > b]()\na, b, x, y = 1, 2, 'a greather than b', 'b greater than a'\nresult = (lambda:y, lambda:x)[a > b]()\nOutput:'b greater than a'\n'b greater than a'\n",
                "a if condition else b\na if condition else b\nJust memorize this pyramid if you have trouble remembering:     condition\n  if           else\na                   b \n     condition\n  if           else\na                   b \n",
                "The ternary conditional operator simply allows testing a condition in a single line replacing the multiline if-else making the code compact.Syntax:\n[on_true] if [expression] else [on_false]\n[on_true] if [expression] else [on_false]1- Simple Method to use ternary operator:# Program to demonstrate conditional operator\na, b = 10, 20\n# Copy value of a in min if a < b else copy b\nmin = a if a < b else b\nprint(min)  # Output: 10\n# Program to demonstrate conditional operator\na, b = 10, 20\n# Copy value of a in min if a < b else copy b\nmin = a if a < b else b\nprint(min)  # Output: 10\n2- Direct Method of using tuples, Dictionary, and lambda:# Python program to demonstrate ternary operator\na, b = 10, 20\n# Use tuple for selecting an item\nprint( (b, a) [a < b] )\n# Use Dictionary for selecting an item\nprint({True: a, False: b} [a < b])\n# lambda is more efficient than above two methods\n# because in lambda  we are assure that\n# only one expression will be evaluated unlike in\n# tuple and Dictionary\nprint((lambda: b, lambda: a)[a < b]()) # in output you should see three 10\n# Python program to demonstrate ternary operator\na, b = 10, 20\n# Use tuple for selecting an item\nprint( (b, a) [a < b] )\n# Use Dictionary for selecting an item\nprint({True: a, False: b} [a < b])\n# lambda is more efficient than above two methods\n# because in lambda  we are assure that\n# only one expression will be evaluated unlike in\n# tuple and Dictionary\nprint((lambda: b, lambda: a)[a < b]()) # in output you should see three 10\n3- Ternary operator can be written as nested if-else:# Python program to demonstrate nested ternary operator\na, b = 10, 20\nprint (\"Both a and b are equal\" if a == b else \"a is greater than b\"\n        if a > b else \"b is greater than a\")\n# Python program to demonstrate nested ternary operator\na, b = 10, 20\nprint (\"Both a and b are equal\" if a == b else \"a is greater than b\"\n        if a > b else \"b is greater than a\")\nAbove approach can be written as:# Python program to demonstrate nested ternary operator\na, b = 10, 20\nif a != b:\n    if a > b:\n        print(\"a is greater than b\")\n    else:\n        print(\"b is greater than a\")\nelse:\n    print(\"Both a and b are equal\")\n# Output: b is greater than a\n# Python program to demonstrate nested ternary operator\na, b = 10, 20\nif a != b:\n    if a > b:\n        print(\"a is greater than b\")\n    else:\n        print(\"b is greater than a\")\nelse:\n    print(\"Both a and b are equal\")\n# Output: b is greater than a\n",
                "Vinko Vrsalovic's answer is good enough. There is only one more thing:Vinko Vrsalovic's answer\nNote that conditionals are an expression, not a statement. This means you can't use assignment statements or pass or other statements within a conditional expression\nNote that conditionals are an expression, not a statement. This means you can't use assignment statements or pass or other statements within a conditional expressionexpressionstatementpassstatementsexpressionWalrus operator in Python 3.8After the walrus operator was introduced in Python 3.8, something changed.walrus operator(a := 3) if True else (b := 5)\n(a := 3) if True else (b := 5)\ngives a = 3 and b is not defined,a = 3b is not defined(a := 3) if False else (b := 5)\n(a := 3) if False else (b := 5)\ngives a is not defined and b = 5, anda is not definedb = 5c = (a := 3) if False else (b := 5)\nc = (a := 3) if False else (b := 5)\ngives c = 5, a is not defined and b = 5.c = 5a is not definedb = 5Even if this may be ugly, assignments can be done inside conditional expressions after Python 3.8. Anyway, it is still better to use normal if statement instead in this case.assignmentsinsideifstatement",
                "More a tip than an answer (I don't need to repeat the obvious for the hundredth time), but I sometimes use it as a one-liner shortcut in such constructs:if conditionX:\n    print('yes')\nelse:\n    print('nah')\nif conditionX:\n    print('yes')\nelse:\n    print('nah')\n, becomes:print('yes') if conditionX else print('nah')\nprint('yes') if conditionX else print('nah')\nSome (many :) may frown upon it as unpythonic (even, Ruby-ish :), but I personally find it more natural - i.e., how you'd express it normally, plus a bit more visually appealing in large blocks of code.",
                "You can do this:[condition] and [expression_1] or [expression_2];\n[condition] and [expression_1] or [expression_2];\nExample:print(number%2 and \"odd\" or \"even\")\nprint(number%2 and \"odd\" or \"even\")\nThis would print \"odd\" if the number is odd or \"even\" if the number is even.The result: If condition is true, exp_1 is executed, else exp_2 is executed.The result:Note: 0, None, False, emptylist, and emptyString evaluates as False.Note:And any data other than 0 evaluates to True.Here's how it works:If the condition [condition] becomes \"True\", then expression_1 will be evaluated, but not expression_2.If we \"and\" something with 0 (zero), the result will always to be false. So in the below statement,0 and exp\n0 and exp\nThe expression exp won't be evaluated at all since \"and\" with 0 will always evaluate to zero and there is no need to evaluate the expression. This is how the compiler itself works, in all languages.expIn1 or exp\n1 or exp\nthe expression exp won't be evaluated at all since \"or\" with 1 will always be 1. So it won't bother to evaluate the expression exp since the result will be 1 anyway (compiler optimization methods).expBut in case ofTrue and exp1 or exp2\nTrue and exp1 or exp2\nThe second expression exp2 won't be evaluated since True and exp1 would be True when exp1 isn't false.True and exp1Similarly inFalse and exp1 or exp2\nFalse and exp1 or exp2\nThe expression exp1 won't be evaluated since False is equivalent to writing 0 and doing \"and\" with 0 would be 0 itself, but after exp1 since \"or\" is used, it will evaluate the expression exp2 after \"or\".exp1Note:- This kind of branching using \"or\" and \"and\" can only be used when the expression_1 doesn't have a Truth value of False (or 0 or None or emptylist [ ] or emptystring ' '.) since if expression_1 becomes False, then the expression_2 will be evaluated because of the presence \"or\" between exp_1 and exp_2.Note:-In case you still want to make it work for all the cases regardless of what exp_1 and exp_2 truth values are, do this:In case you still want to make it work for all the cases regardless of what exp_1 and exp_2 truth values are, do this:[condition] and ([expression_1] or 1) or [expression_2];\n[condition] and ([expression_1] or 1) or [expression_2];\n",
                "Many programming languages derived from C usually have the following syntax of the ternary conditional operator:C<condition> ? <expression1> : <expression2>\n<condition> ? <expression1> : <expression2>\nAt first, the Python's benevolent dictator for life (I mean Guido van Rossum, of course) rejected it (as non-Pythonic style), since it's quite hard to understand for people not used to C language. Also, the colon sign : already has many uses in Python. After PEP 308 was approved, Python finally received its own shortcut conditional expression (what we use now):benevolent dictator for lifeGuido van Rossum::PEP 308<expression1> if <condition> else <expression2>\n<expression1> if <condition> else <expression2>\nSo, firstly it evaluates the condition. If it returns True, expression1 will be evaluated to give the result, otherwise expression2 will be evaluated. Due to lazy evaluation mechanics \u2013 only one expression will be executed.Trueexpression1expression2lazy evaluationHere are some examples (conditions will be evaluated from left to right):pressure = 10\nprint('High' if pressure < 20 else 'Critical')\n\n# Result is 'High'\npressure = 10\nprint('High' if pressure < 20 else 'Critical')\n\n# Result is 'High'\nTernary operators can be chained in series:pressure = 5\nprint('Normal' if pressure < 10 else 'High' if pressure < 20 else 'Critical')\n\n# Result is 'Normal'\npressure = 5\nprint('Normal' if pressure < 10 else 'High' if pressure < 20 else 'Critical')\n\n# Result is 'Normal'\nThe following one is the same as previous one:pressure = 5\n\nif pressure < 20:\n    if pressure < 10:\n        print('Normal')\n    else:\n        print('High')\nelse:\n    print('Critical')\n\n# Result is 'Normal'\npressure = 5\n\nif pressure < 20:\n    if pressure < 10:\n        print('Normal')\n    else:\n        print('High')\nelse:\n    print('Critical')\n\n# Result is 'Normal'\n",
                "Yes, Python have a ternary operator, here is the syntax and an example code to demonstrate the same :)Yes#[On true] if [expression] else[On false]\n# if the expression evaluates to true then it will pass On true otherwise On false\n\na = input(\"Enter the First Number \")\nb = input(\"Enter the Second Number \")\n\nprint(\"A is Bigger\") if a>b else print(\"B is Bigger\")\n#[On true] if [expression] else[On false]\n# if the expression evaluates to true then it will pass On true otherwise On false\n\na = input(\"Enter the First Number \")\nb = input(\"Enter the Second Number \")\n\nprint(\"A is Bigger\") if a>b else print(\"B is Bigger\")\n",
                "Other answers correctly talk about the Python ternary operator. I would like to complement by mentioning a scenario for which the ternary operator is often used, but for which there is a better idiom. This is the scenario of using a default value.Suppose we want to use option_value with a default value if it is not set:option_valuerun_algorithm(option_value if option_value is not None else 10)\nrun_algorithm(option_value if option_value is not None else 10)\nor, if option_value is never set to a falsy value (0, \"\", etc.), simplyoption_value0\"\"run_algorithm(option_value if option_value else 10)\nrun_algorithm(option_value if option_value else 10)\nHowever, in this case an ever better solution is simply to writerun_algorithm(option_value or 10)\nrun_algorithm(option_value or 10)\n",
                "The syntax for the ternary operator in Python is:[on_true] if [expression] else [on_false][on_true] if [expression] else [on_false]Using that syntax, here is how we would rewrite the code above using Python\u2019s ternary operator:game_type = 'home'\nshirt = 'white' if game_type == 'home' else 'green'\n\ngame_type = 'home'\nshirt = 'white' if game_type == 'home' else 'green'\n\nIt's still pretty clear, but much shorter. Note that the expression could be any type of expression, including a function call, that returns a value that evaluates to True or False.",
                "Python has a ternary form for assignments; however there may be even a shorter form that people should be aware of.It's very common to need to assign to a variable one value or another depending on a condition.>>> li1 = None\n>>> li2 = [1, 2, 3]\n>>>\n>>> if li1:\n...     a = li1\n... else:\n...     a = li2\n...\n>>> a\n[1, 2, 3]\n>>> li1 = None\n>>> li2 = [1, 2, 3]\n>>>\n>>> if li1:\n...     a = li1\n... else:\n...     a = li2\n...\n>>> a\n[1, 2, 3]\n^ This is the long form for doing such assignments.Below is the ternary form. But this isn't the most succinct way - see the last example.>>> a = li1 if li1 else li2\n>>>\n>>> a\n[1, 2, 3]\n>>>\n>>> a = li1 if li1 else li2\n>>>\n>>> a\n[1, 2, 3]\n>>>\nWith Python, you can simply use or for alternative assignments.or>>> a = li1 or li2\n>>>\n>>> a\n[1, 2, 3]\n>>>\n>>> a = li1 or li2\n>>>\n>>> a\n[1, 2, 3]\n>>>\nThe above works since li1 is None and the interpreter treats that as False in logic expressions. The interpreter then moves on and evaluates the second expression, which is not None and it's not an empty list - so it gets assigned to a.li1NoneNoneaThis also works with empty lists. For instance, if you want to assign a whichever list has items.a>>> li1 = []\n>>> li2 = [1, 2, 3]\n>>>\n>>> a = li1 or li2\n>>>\n>>> a\n[1, 2, 3]\n>>>\n>>> li1 = []\n>>> li2 = [1, 2, 3]\n>>>\n>>> a = li1 or li2\n>>>\n>>> a\n[1, 2, 3]\n>>>\nKnowing this, you can simply such assignments whenever you encounter them. This also works with strings and other iterables. You could assign a whichever string isn't empty.a>>> s1 = ''\n>>> s2 = 'hello world'\n>>>\n>>> a = s1 or s2\n>>>\n>>> a\n'hello world'\n>>>\n>>> s1 = ''\n>>> s2 = 'hello world'\n>>>\n>>> a = s1 or s2\n>>>\n>>> a\n'hello world'\n>>>\nI always liked the C ternary syntax, but Python takes it a step further!I understand that some may say this isn't a good stylistic choice, because it relies on mechanics that aren't immediately apparent to all developers. I personally disagree with that viewpoint. Python is a syntax-rich language with lots of idiomatic tricks that aren't immediately apparent to the dabbler. But the more you learn and understand the mechanics of the underlying system, the more you appreciate it.",
                "Pythonic way of doing the things:\"true\" if var else \"false\"\n\"true\" if var else \"false\"\nBut there always exists a different way of doing a ternary condition too:\"true\" and var or \"false\"\n\"true\" and var or \"false\"\n",
                "There are multiple ways. The simplest one is to use the condition inside the \"print\" method.You can useprint(\"Twenty\" if number == 20 else \"Not twenty\")\nprint(\"Twenty\" if number == 20 else \"Not twenty\")\nWhich is equivalent to:if number == 20:\n    print(\"Twenty\")\nelse:\n    print(\"Not twenty\")\nif number == 20:\n    print(\"Twenty\")\nelse:\n    print(\"Not twenty\")\nIn this way, more than two statements are also possible to print. For example:if number == 20:\n    print(\"Twenty\")\nelif number < 20:\n    print(\"Lesser\")\nelif 30 > number > 20:\n    print(\"Between\")\nelse:\n    print(\"Greater\")\nif number == 20:\n    print(\"Twenty\")\nelif number < 20:\n    print(\"Lesser\")\nelif 30 > number > 20:\n    print(\"Between\")\nelse:\n    print(\"Greater\")\ncan be written as:print(\"Twenty\" if number == 20 else \"Lesser\" if number < 20 else \"Between\" if 30 > number > 20 else \"Greater\")\nprint(\"Twenty\" if number == 20 else \"Lesser\" if number < 20 else \"Between\" if 30 > number > 20 else \"Greater\")\n",
                "The if else-if version can be written as:if else-ifsample_set=\"train\" if \"Train\" in full_path else (\"test\" if \"Test\" in full_path else \"validation\")\nsample_set=\"train\" if \"Train\" in full_path else (\"test\" if \"Test\" in full_path else \"validation\")\n",
                "Yes, it has, but it's different from C-syntax-like programming languages (which is condition ? value_if_true : value_if_falsecondition ? value_if_true : value_if_falseIn Python, it goes like this: value_if_true if condition else value_if_falsevalue_if_true if condition else value_if_falseExample: even_or_odd = \"even\" if x % 2 == 0 else \"odd\"even_or_odd = \"even\" if x % 2 == 0 else \"odd\"",
                "A neat way to chain multiple operators:A neat way to chain multiple operators:f = lambda x,y: 'greater' if x > y else 'less' if y > x else 'equal'\n\narray = [(0,0),(0,1),(1,0),(1,1)]\n\nfor a in array:\n  x, y = a[0], a[1]\n  print(f(x,y))\n\n# Output is:\n#   equal,\n#   less,\n#   greater,\n#   equal\n\nf = lambda x,y: 'greater' if x > y else 'less' if y > x else 'equal'\n\narray = [(0,0),(0,1),(1,0),(1,1)]\n\nfor a in array:\n  x, y = a[0], a[1]\n  print(f(x,y))\n\n# Output is:\n#   equal,\n#   less,\n#   greater,\n#   equal\n\n",
                "I find the default Python syntax val = a if cond else b cumbersome, so sometimes I do this:val = a if cond else biif = lambda (cond, a, b): a if cond else b\n# So I can then use it like:\nval = iif(cond, a, b)\niif = lambda (cond, a, b): a if cond else b\n# So I can then use it like:\nval = iif(cond, a, b)\nOf course, it has the downside of always evaluating both sides (a and b), but the syntax is way clearer to me.ab"
            ]
        },
        {
            "tag": "metaclasses",
            "patterns": [
                "What are metaclasses in Python?",
                "how are metaclasses used in python?",
                "tell me the metaclass in python?",
                "what is metaclass in python?",
                "what is metaclasses in python?",
                "what are metaclasses in python?"
            ],
            "responses": [
                "Classes as objectsBefore understanding metaclasses, you need to master classes in Python. And Python has a very peculiar idea of what classes are, borrowed from the Smalltalk language.In most languages, classes are just pieces of code that describe how to produce an object. That's kinda true in Python too:>>> class ObjectCreator(object):\n...       pass\n...\n\n>>> my_object = ObjectCreator()\n>>> print(my_object)\n<__main__.ObjectCreator object at 0x8974f2c>\n>>> class ObjectCreator(object):\n...       pass\n...\n\n>>> my_object = ObjectCreator()\n>>> print(my_object)\n<__main__.ObjectCreator object at 0x8974f2c>\nBut classes are more than that in Python. Classes are objects too.Yes, objects.As soon as you use the keyword class, Python executes it and creates\nan object. The instructionclassobject>>> class ObjectCreator(object):\n...       pass\n...\n>>> class ObjectCreator(object):\n...       pass\n...\ncreates in memory an object with the name ObjectCreator.ObjectCreatorThis object (the class) is itself capable of creating objects (the instances),\nand this is why it's a class.This object (the class) is itself capable of creating objects (the instances),\nand this is why it's a classBut still, it's an object, and therefore:\nyou can assign it to a variable\nyou can copy it\nyou can add attributes to it\nyou can pass it as a function parameter\nyou can assign it to a variableyou can copy ityou can add attributes to ityou can pass it as a function parametere.g.:>>> print(ObjectCreator) # you can print a class because it's an object\n<class '__main__.ObjectCreator'>\n>>> def echo(o):\n...       print(o)\n...\n>>> echo(ObjectCreator) # you can pass a class as a parameter\n<class '__main__.ObjectCreator'>\n>>> print(hasattr(ObjectCreator, 'new_attribute'))\nFalse\n>>> ObjectCreator.new_attribute = 'foo' # you can add attributes to a class\n>>> print(hasattr(ObjectCreator, 'new_attribute'))\nTrue\n>>> print(ObjectCreator.new_attribute)\nfoo\n>>> ObjectCreatorMirror = ObjectCreator # you can assign a class to a variable\n>>> print(ObjectCreatorMirror.new_attribute)\nfoo\n>>> print(ObjectCreatorMirror())\n<__main__.ObjectCreator object at 0x8997b4c>\n>>> print(ObjectCreator) # you can print a class because it's an object\n<class '__main__.ObjectCreator'>\n>>> def echo(o):\n...       print(o)\n...\n>>> echo(ObjectCreator) # you can pass a class as a parameter\n<class '__main__.ObjectCreator'>\n>>> print(hasattr(ObjectCreator, 'new_attribute'))\nFalse\n>>> ObjectCreator.new_attribute = 'foo' # you can add attributes to a class\n>>> print(hasattr(ObjectCreator, 'new_attribute'))\nTrue\n>>> print(ObjectCreator.new_attribute)\nfoo\n>>> ObjectCreatorMirror = ObjectCreator # you can assign a class to a variable\n>>> print(ObjectCreatorMirror.new_attribute)\nfoo\n>>> print(ObjectCreatorMirror())\n<__main__.ObjectCreator object at 0x8997b4c>\nCreating classes dynamicallySince classes are objects, you can create them on the fly, like any object.First, you can create a class in a function using class:class>>> def choose_class(name):\n...     if name == 'foo':\n...         class Foo(object):\n...             pass\n...         return Foo # return the class, not an instance\n...     else:\n...         class Bar(object):\n...             pass\n...         return Bar\n...\n>>> MyClass = choose_class('foo')\n>>> print(MyClass) # the function returns a class, not an instance\n<class '__main__.Foo'>\n>>> print(MyClass()) # you can create an object from this class\n<__main__.Foo object at 0x89c6d4c>\n>>> def choose_class(name):\n...     if name == 'foo':\n...         class Foo(object):\n...             pass\n...         return Foo # return the class, not an instance\n...     else:\n...         class Bar(object):\n...             pass\n...         return Bar\n...\n>>> MyClass = choose_class('foo')\n>>> print(MyClass) # the function returns a class, not an instance\n<class '__main__.Foo'>\n>>> print(MyClass()) # you can create an object from this class\n<__main__.Foo object at 0x89c6d4c>\nBut it's not so dynamic, since you still have to write the whole class yourself.Since classes are objects, they must be generated by something.When you use the class keyword, Python creates this object automatically. But as\nwith most things in Python, it gives you a way to do it manually.classRemember the function type? The good old function that lets you know what\ntype an object is:type>>> print(type(1))\n<type 'int'>\n>>> print(type(\"1\"))\n<type 'str'>\n>>> print(type(ObjectCreator))\n<type 'type'>\n>>> print(type(ObjectCreator()))\n<class '__main__.ObjectCreator'>\n>>> print(type(1))\n<type 'int'>\n>>> print(type(\"1\"))\n<type 'str'>\n>>> print(type(ObjectCreator))\n<type 'type'>\n>>> print(type(ObjectCreator()))\n<class '__main__.ObjectCreator'>\nWell, type has also a completely different ability: it can create classes on the fly. type can take the description of a class as parameters,\nand return a class.typetypetype(I  know, it's silly that the same function can have two completely different uses according to the parameters you pass to it. It's an issue due to backward\ncompatibility in Python)type works this way:typetype(name, bases, attrs)\ntype(name, bases, attrs)\nWhere:\nname: name of the class\nbases: tuple of the parent class (for inheritance, can be empty)\nattrs: dictionary containing attributes names and values\nname: name of the classnamenamebases: tuple of the parent class (for inheritance, can be empty)basesbasesattrs: dictionary containing attributes names and valuesattrsattrse.g.:>>> class MyShinyClass(object):\n...       pass\n>>> class MyShinyClass(object):\n...       pass\ncan be created manually this way:>>> MyShinyClass = type('MyShinyClass', (), {}) # returns a class object\n>>> print(MyShinyClass)\n<class '__main__.MyShinyClass'>\n>>> print(MyShinyClass()) # create an instance with the class\n<__main__.MyShinyClass object at 0x8997cec>\n>>> MyShinyClass = type('MyShinyClass', (), {}) # returns a class object\n>>> print(MyShinyClass)\n<class '__main__.MyShinyClass'>\n>>> print(MyShinyClass()) # create an instance with the class\n<__main__.MyShinyClass object at 0x8997cec>\nYou'll notice that we use MyShinyClass as the name of the class\nand as the variable to hold the class reference. They can be different,\nbut there is no reason to complicate things.MyShinyClasstype accepts a dictionary to define the attributes of the class. So:type>>> class Foo(object):\n...       bar = True\n>>> class Foo(object):\n...       bar = True\nCan be translated to:>>> Foo = type('Foo', (), {'bar':True})\n>>> Foo = type('Foo', (), {'bar':True})\nAnd used as a normal class:>>> print(Foo)\n<class '__main__.Foo'>\n>>> print(Foo.bar)\nTrue\n>>> f = Foo()\n>>> print(f)\n<__main__.Foo object at 0x8a9b84c>\n>>> print(f.bar)\nTrue\n>>> print(Foo)\n<class '__main__.Foo'>\n>>> print(Foo.bar)\nTrue\n>>> f = Foo()\n>>> print(f)\n<__main__.Foo object at 0x8a9b84c>\n>>> print(f.bar)\nTrue\nAnd of course, you can inherit from it, so:>>>   class FooChild(Foo):\n...         pass\n>>>   class FooChild(Foo):\n...         pass\nwould be:>>> FooChild = type('FooChild', (Foo,), {})\n>>> print(FooChild)\n<class '__main__.FooChild'>\n>>> print(FooChild.bar) # bar is inherited from Foo\nTrue\n>>> FooChild = type('FooChild', (Foo,), {})\n>>> print(FooChild)\n<class '__main__.FooChild'>\n>>> print(FooChild.bar) # bar is inherited from Foo\nTrue\nEventually, you'll want to add methods to your class. Just define a function\nwith the proper signature and assign it as an attribute.>>> def echo_bar(self):\n...       print(self.bar)\n...\n>>> FooChild = type('FooChild', (Foo,), {'echo_bar': echo_bar})\n>>> hasattr(Foo, 'echo_bar')\nFalse\n>>> hasattr(FooChild, 'echo_bar')\nTrue\n>>> my_foo = FooChild()\n>>> my_foo.echo_bar()\nTrue\n>>> def echo_bar(self):\n...       print(self.bar)\n...\n>>> FooChild = type('FooChild', (Foo,), {'echo_bar': echo_bar})\n>>> hasattr(Foo, 'echo_bar')\nFalse\n>>> hasattr(FooChild, 'echo_bar')\nTrue\n>>> my_foo = FooChild()\n>>> my_foo.echo_bar()\nTrue\nAnd you can add even more methods after you dynamically create the class, just like adding methods to a normally created class object.>>> def echo_bar_more(self):\n...       print('yet another method')\n...\n>>> FooChild.echo_bar_more = echo_bar_more\n>>> hasattr(FooChild, 'echo_bar_more')\nTrue\n>>> def echo_bar_more(self):\n...       print('yet another method')\n...\n>>> FooChild.echo_bar_more = echo_bar_more\n>>> hasattr(FooChild, 'echo_bar_more')\nTrue\nYou see where we are going: in Python, classes are objects, and you can create a class on the fly, dynamically.This is what Python does when you use the keyword class, and it does so by using a metaclass.classWhat are metaclasses (finally)Metaclasses are the 'stuff' that creates classes.You define classes in order to create objects, right?But we learned that Python classes are objects.Well, metaclasses are what create these objects. They are the classes' classes,\nyou can picture them this way:MyClass = MetaClass()\nmy_object = MyClass()\nMyClass = MetaClass()\nmy_object = MyClass()\nYou've seen that type lets you do something like this:typeMyClass = type('MyClass', (), {})\nMyClass = type('MyClass', (), {})\nIt's because the function type is in fact a metaclass. type is the\nmetaclass Python uses to create all classes behind the scenes.typetypeNow you wonder \"why the heck is it written in lowercase, and not Type?\"TypeWell, I guess it's a matter of consistency with str, the class that creates\nstrings objects, and int the class that creates integer objects. type is\njust the class that creates class objects.strinttypeYou see that by checking the __class__ attribute.__class__Everything, and I mean everything, is an object in Python. That includes integers,\nstrings, functions and classes. All of them are objects. And all of them have\nbeen created from a class:>>> age = 35\n>>> age.__class__\n<type 'int'>\n>>> name = 'bob'\n>>> name.__class__\n<type 'str'>\n>>> def foo(): pass\n>>> foo.__class__\n<type 'function'>\n>>> class Bar(object): pass\n>>> b = Bar()\n>>> b.__class__\n<class '__main__.Bar'>\n>>> age = 35\n>>> age.__class__\n<type 'int'>\n>>> name = 'bob'\n>>> name.__class__\n<type 'str'>\n>>> def foo(): pass\n>>> foo.__class__\n<type 'function'>\n>>> class Bar(object): pass\n>>> b = Bar()\n>>> b.__class__\n<class '__main__.Bar'>\nNow, what is the __class__ of any __class__ ?__class____class__>>> age.__class__.__class__\n<type 'type'>\n>>> name.__class__.__class__\n<type 'type'>\n>>> foo.__class__.__class__\n<type 'type'>\n>>> b.__class__.__class__\n<type 'type'>\n>>> age.__class__.__class__\n<type 'type'>\n>>> name.__class__.__class__\n<type 'type'>\n>>> foo.__class__.__class__\n<type 'type'>\n>>> b.__class__.__class__\n<type 'type'>\nSo, a metaclass is just the stuff that creates class objects.You can call it a 'class factory' if you wish.type is the built-in metaclass Python uses, but of course, you can create your\nown metaclass.typeThe __metaclass__ attribute__metaclass____metaclass__In Python 2, you can add a __metaclass__ attribute when you write a class (see next section for the Python 3 syntax):__metaclass__class Foo(object):\n    __metaclass__ = something...\n    [...]\nclass Foo(object):\n    __metaclass__ = something...\n    [...]\nIf you do so, Python will use the metaclass to create the class Foo.FooCareful, it's tricky.You write class Foo(object) first, but the class object Foo is not created\nin memory yet.class Foo(object)FooPython will look for __metaclass__ in the class definition. If it finds it,\nit will use it to create the object class Foo. If it doesn't, it will use\ntype to create the class.__metaclass__FootypeRead that several times.When you do:class Foo(Bar):\n    pass\nclass Foo(Bar):\n    pass\nPython does the following:Is there a __metaclass__ attribute in Foo?__metaclass__FooIf yes, create in-memory a class object (I said a class object, stay with me here), with the name Foo by using what is in __metaclass__.Foo__metaclass__If Python can't find __metaclass__, it will look for a __metaclass__ at the MODULE level, and try to do the same (but only for classes that don't inherit anything, basically old-style classes).__metaclass____metaclass__Then if it can't find any __metaclass__ at all, it will use the Bar's (the first parent) own metaclass (which might be the default type) to create the class object.__metaclass__BartypeBe careful here that the __metaclass__ attribute will not be inherited, the metaclass of the parent (Bar.__class__) will be. If Bar used a __metaclass__ attribute that created Bar with type() (and not type.__new__()), the subclasses will not inherit that behavior.__metaclass__Bar.__class__Bar__metaclass__Bartype()type.__new__()Now the big question is, what can you put in __metaclass__?__metaclass__The answer is something that can create a class.And what can create a class? type, or anything that subclasses or uses it.typeMetaclasses in Python 3The syntax to set the metaclass has been changed in Python 3:class Foo(object, metaclass=something):\n    ...\nclass Foo(object, metaclass=something):\n    ...\ni.e. the __metaclass__ attribute is no longer used, in favor of a keyword argument in the list of base classes.__metaclass__The behavior of metaclasses however stays largely the same.largely the sameOne thing added to metaclasses in Python 3 is that you can also pass attributes as keyword-arguments into a metaclass, like so:class Foo(object, metaclass=something, kwarg1=value1, kwarg2=value2):\n    ...\nclass Foo(object, metaclass=something, kwarg1=value1, kwarg2=value2):\n    ...\nRead the section below for how Python handles this.Custom metaclassesThe main purpose of a metaclass is to change the class automatically,\nwhen it's created.You usually do this for APIs, where you want to create classes matching the\ncurrent context.Imagine a stupid example, where you decide that all classes in your module\nshould have their attributes written in uppercase. There are several ways to\ndo this, but one way is to set __metaclass__ at the module level.__metaclass__This way, all classes of this module will be created using this metaclass,\nand we just have to tell the metaclass to turn all attributes to uppercase.Luckily, __metaclass__ can actually be any callable, it doesn't need to be a\nformal class (I know, something with 'class' in its name doesn't need to be\na class, go figure... but it's helpful).__metaclass__So we will start with a simple example, by using a function.# the metaclass will automatically get passed the same argument\n# that you usually pass to `type`\ndef upper_attr(future_class_name, future_class_parents, future_class_attrs):\n    \"\"\"\n      Return a class object, with the list of its attribute turned\n      into uppercase.\n    \"\"\"\n    # pick up any attribute that doesn't start with '__' and uppercase it\n    uppercase_attrs = {\n        attr if attr.startswith(\"__\") else attr.upper(): v\n        for attr, v in future_class_attrs.items()\n    }\n\n    # let `type` do the class creation\n    return type(future_class_name, future_class_parents, uppercase_attrs)\n\n__metaclass__ = upper_attr # this will affect all classes in the module\n\nclass Foo(): # global __metaclass__ won't work with \"object\" though\n    # but we can define __metaclass__ here instead to affect only this class\n    # and this will work with \"object\" children\n    bar = 'bip'\n# the metaclass will automatically get passed the same argument\n# that you usually pass to `type`\ndef upper_attr(future_class_name, future_class_parents, future_class_attrs):\n    \"\"\"\n      Return a class object, with the list of its attribute turned\n      into uppercase.\n    \"\"\"\n    # pick up any attribute that doesn't start with '__' and uppercase it\n    uppercase_attrs = {\n        attr if attr.startswith(\"__\") else attr.upper(): v\n        for attr, v in future_class_attrs.items()\n    }\n\n    # let `type` do the class creation\n    return type(future_class_name, future_class_parents, uppercase_attrs)\n\n__metaclass__ = upper_attr # this will affect all classes in the module\n\nclass Foo(): # global __metaclass__ won't work with \"object\" though\n    # but we can define __metaclass__ here instead to affect only this class\n    # and this will work with \"object\" children\n    bar = 'bip'\nLet's check:>>> hasattr(Foo, 'bar')\nFalse\n>>> hasattr(Foo, 'BAR')\nTrue\n>>> Foo.BAR\n'bip'\n>>> hasattr(Foo, 'bar')\nFalse\n>>> hasattr(Foo, 'BAR')\nTrue\n>>> Foo.BAR\n'bip'\nNow, let's do exactly the same, but using a real class for a metaclass:# remember that `type` is actually a class like `str` and `int`\n# so you can inherit from it\nclass UpperAttrMetaclass(type):\n    # __new__ is the method called before __init__\n    # it's the method that creates the object and returns it\n    # while __init__ just initializes the object passed as parameter\n    # you rarely use __new__, except when you want to control how the object\n    # is created.\n    # here the created object is the class, and we want to customize it\n    # so we override __new__\n    # you can do some stuff in __init__ too if you wish\n    # some advanced use involves overriding __call__ as well, but we won't\n    # see this\n    def __new__(upperattr_metaclass, future_class_name,\n                future_class_parents, future_class_attrs):\n        uppercase_attrs = {\n            attr if attr.startswith(\"__\") else attr.upper(): v\n            for attr, v in future_class_attrs.items()\n        }\n        return type(future_class_name, future_class_parents, uppercase_attrs)\n# remember that `type` is actually a class like `str` and `int`\n# so you can inherit from it\nclass UpperAttrMetaclass(type):\n    # __new__ is the method called before __init__\n    # it's the method that creates the object and returns it\n    # while __init__ just initializes the object passed as parameter\n    # you rarely use __new__, except when you want to control how the object\n    # is created.\n    # here the created object is the class, and we want to customize it\n    # so we override __new__\n    # you can do some stuff in __init__ too if you wish\n    # some advanced use involves overriding __call__ as well, but we won't\n    # see this\n    def __new__(upperattr_metaclass, future_class_name,\n                future_class_parents, future_class_attrs):\n        uppercase_attrs = {\n            attr if attr.startswith(\"__\") else attr.upper(): v\n            for attr, v in future_class_attrs.items()\n        }\n        return type(future_class_name, future_class_parents, uppercase_attrs)\nLet's rewrite the above, but with shorter and more realistic variable names now that we know what they mean:class UpperAttrMetaclass(type):\n    def __new__(cls, clsname, bases, attrs):\n        uppercase_attrs = {\n            attr if attr.startswith(\"__\") else attr.upper(): v\n            for attr, v in attrs.items()\n        }\n        return type(clsname, bases, uppercase_attrs)\nclass UpperAttrMetaclass(type):\n    def __new__(cls, clsname, bases, attrs):\n        uppercase_attrs = {\n            attr if attr.startswith(\"__\") else attr.upper(): v\n            for attr, v in attrs.items()\n        }\n        return type(clsname, bases, uppercase_attrs)\nYou may have noticed the extra argument cls. There is\nnothing special about it: __new__ always receives the class it's defined in, as the first parameter. Just like you have self for ordinary methods which receive the instance as the first parameter, or the defining class for class methods.cls__new__selfBut this is not proper OOP. We are calling type directly and we aren't overriding or calling the parent's __new__. Let's do that instead:type__new__class UpperAttrMetaclass(type):\n    def __new__(cls, clsname, bases, attrs):\n        uppercase_attrs = {\n            attr if attr.startswith(\"__\") else attr.upper(): v\n            for attr, v in attrs.items()\n        }\n        return type.__new__(cls, clsname, bases, uppercase_attrs)\nclass UpperAttrMetaclass(type):\n    def __new__(cls, clsname, bases, attrs):\n        uppercase_attrs = {\n            attr if attr.startswith(\"__\") else attr.upper(): v\n            for attr, v in attrs.items()\n        }\n        return type.__new__(cls, clsname, bases, uppercase_attrs)\nWe can make it even cleaner by using super, which will ease inheritance (because yes, you can have metaclasses, inheriting from metaclasses, inheriting from type):superclass UpperAttrMetaclass(type):\n    def __new__(cls, clsname, bases, attrs):\n        uppercase_attrs = {\n            attr if attr.startswith(\"__\") else attr.upper(): v\n            for attr, v in attrs.items()\n        }\n\n        # Python 2 requires passing arguments to super:\n        return super(UpperAttrMetaclass, cls).__new__(\n            cls, clsname, bases, uppercase_attrs)\n\n        # Python 3 can use no-arg super() which infers them:\n        return super().__new__(cls, clsname, bases, uppercase_attrs)\nclass UpperAttrMetaclass(type):\n    def __new__(cls, clsname, bases, attrs):\n        uppercase_attrs = {\n            attr if attr.startswith(\"__\") else attr.upper(): v\n            for attr, v in attrs.items()\n        }\n\n        # Python 2 requires passing arguments to super:\n        return super(UpperAttrMetaclass, cls).__new__(\n            cls, clsname, bases, uppercase_attrs)\n\n        # Python 3 can use no-arg super() which infers them:\n        return super().__new__(cls, clsname, bases, uppercase_attrs)\nOh, and in Python 3 if you do this call with keyword arguments, like this:class Foo(object, metaclass=MyMetaclass, kwarg1=value1):\n    ...\nclass Foo(object, metaclass=MyMetaclass, kwarg1=value1):\n    ...\nIt translates to this in the metaclass to use it:class MyMetaclass(type):\n    def __new__(cls, clsname, bases, dct, kwargs1=default):\n        ...\nclass MyMetaclass(type):\n    def __new__(cls, clsname, bases, dct, kwargs1=default):\n        ...\nThat's it. There is really nothing more about metaclasses.The reason behind the complexity of the code using metaclasses is not because\nof metaclasses, it's because you usually use metaclasses to do twisted stuff\nrelying on introspection, manipulating inheritance, vars such as __dict__, etc.__dict__Indeed, metaclasses are especially useful to do black magic, and therefore\ncomplicated stuff. But by themselves, they are simple:\nintercept a class creation\nmodify the class\nreturn the modified class\nintercept a class creationmodify the classreturn the modified classWhy would you use metaclasses classes instead of functions?Since __metaclass__ can accept any callable, why would you use a class\nsince it's obviously more complicated?__metaclass__There are several reasons to do so:\nThe intention is clear. When you read UpperAttrMetaclass(type), you know\nwhat's going to follow\nYou can use OOP. Metaclass can inherit from metaclass, override parent methods. Metaclasses can even use metaclasses.\nSubclasses of a class will be instances of its metaclass if you specified a metaclass-class, but not with a metaclass-function.\nYou can structure your code better. You never use metaclasses for something as trivial as the above example. It's usually for something complicated. Having the ability to make several methods and group them in one class is very useful to make the code easier to read.\nYou can hook on __new__, __init__ and __call__. Which will allow you to do different stuff, Even if usually you can do it all in __new__,\nsome people are just more comfortable using __init__.\nThese are called metaclasses, damn it! It must mean something!\nThe intention is clear. When you read UpperAttrMetaclass(type), you know\nwhat's going to followUpperAttrMetaclass(type)You can use OOP. Metaclass can inherit from metaclass, override parent methods. Metaclasses can even use metaclasses.Subclasses of a class will be instances of its metaclass if you specified a metaclass-class, but not with a metaclass-function.You can structure your code better. You never use metaclasses for something as trivial as the above example. It's usually for something complicated. Having the ability to make several methods and group them in one class is very useful to make the code easier to read.You can hook on __new__, __init__ and __call__. Which will allow you to do different stuff, Even if usually you can do it all in __new__,\nsome people are just more comfortable using __init__.__new____init____call____new____init__These are called metaclasses, damn it! It must mean something!Why would you use metaclasses?Now the big question. Why would you use some obscure error-prone feature?Well, usually you don't:\nMetaclasses are deeper magic that\n99% of users should never worry about it.\nIf you wonder whether you need them,\nyou don't (the people who actually\nneed them know with certainty that\nthey need them, and don't need an\nexplanation about why).\nMetaclasses are deeper magic that\n99% of users should never worry about it.\nIf you wonder whether you need them,\nyou don't (the people who actually\nneed them know with certainty that\nthey need them, and don't need an\nexplanation about why).Python Guru Tim PetersPython Guru Tim PetersThe main use case for a metaclass is creating an API. A typical example of this is the Django ORM. It allows you to define something like this:class Person(models.Model):\n    name = models.CharField(max_length=30)\n    age = models.IntegerField()\nclass Person(models.Model):\n    name = models.CharField(max_length=30)\n    age = models.IntegerField()\nBut if you do this:person = Person(name='bob', age='35')\nprint(person.age)\nperson = Person(name='bob', age='35')\nprint(person.age)\nIt won't return an IntegerField object. It will return an int, and can even take it directly from the database.IntegerFieldintThis is possible because models.Model defines __metaclass__ and\nit uses some magic that will turn the Person you just defined with simple statements\ninto a complex hook to a database field.models.Model__metaclass__PersonDjango makes something complex look simple by exposing a simple API\nand using metaclasses, recreating code from this API to do the real job\nbehind the scenes.The last wordFirst, you know that classes are objects that can create instances.Well, in fact, classes are themselves instances. Of metaclasses.>>> class Foo(object): pass\n>>> id(Foo)\n142630324\n>>> class Foo(object): pass\n>>> id(Foo)\n142630324\nEverything is an object in Python, and they are all either instance of classes\nor instances of metaclasses.Except for type.typetype is actually its own metaclass. This is not something you could\nreproduce in pure Python, and is done by cheating a little bit at the implementation\nlevel.typeSecondly, metaclasses are complicated. You may not want to use them for\nvery simple class alterations. You can change classes by using two different techniques:\nmonkey patching\nclass decorators\nmonkey patchingmonkey patchingclass decorators99% of the time you need class alteration, you are better off using these.But 98% of the time, you don't need class alteration at all.",
                "A metaclass is the class of a class. A class defines how an instance of the class (i.e. an object) behaves while a metaclass defines how a class behaves. A class is an instance of a metaclass.While in Python you can use arbitrary callables for metaclasses (like Jerub shows), the better approach is to make it an actual class itself. type is the usual metaclass in Python. type is itself a class, and it is its own type. You won't be able to recreate something like type purely in Python, but Python cheats a little. To create your own metaclass in Python you really just want to subclass type.JerubtypetypetypetypeA metaclass is most commonly used as a class-factory. When you create an object by calling the class, Python creates a new class (when it executes the 'class' statement) by calling the metaclass. Combined with the normal __init__ and __new__ methods, metaclasses therefore allow you to do 'extra things' when creating a class, like registering the new class with some registry or replace the class with something else entirely.__init____new__When the class statement is executed, Python first executes the body of the class statement as a normal block of code. The resulting namespace (a dict) holds the attributes of the class-to-be. The metaclass is determined by looking at the baseclasses of the class-to-be (metaclasses are inherited), at the __metaclass__ attribute of the class-to-be (if any) or the __metaclass__ global variable. The metaclass is then called with the name, bases and attributes of the class to instantiate it.classclass__metaclass____metaclass__However, metaclasses actually define the type of a class, not just a factory for it, so you can do much more with them. You can, for instance, define normal methods on the metaclass. These metaclass-methods are like classmethods in that they can be called on the class without an instance, but they are also not like classmethods in that they cannot be called on an instance of the class. type.__subclasses__() is an example of a method on the type metaclass. You can also define the normal 'magic' methods, like __add__, __iter__ and __getattr__, to implement or change how the class behaves.typetype.__subclasses__()type__add____iter____getattr__Here's an aggregated example of the bits and pieces:def make_hook(f):\n    \"\"\"Decorator to turn 'foo' method into '__foo__'\"\"\"\n    f.is_hook = 1\n    return f\n\nclass MyType(type):\n    def __new__(mcls, name, bases, attrs):\n\n        if name.startswith('None'):\n            return None\n\n        # Go over attributes and see if they should be renamed.\n        newattrs = {}\n        for attrname, attrvalue in attrs.iteritems():\n            if getattr(attrvalue, 'is_hook', 0):\n                newattrs['__%s__' % attrname] = attrvalue\n            else:\n                newattrs[attrname] = attrvalue\n\n        return super(MyType, mcls).__new__(mcls, name, bases, newattrs)\n\n    def __init__(self, name, bases, attrs):\n        super(MyType, self).__init__(name, bases, attrs)\n\n        # classregistry.register(self, self.interfaces)\n        print \"Would register class %s now.\" % self\n\n    def __add__(self, other):\n        class AutoClass(self, other):\n            pass\n        return AutoClass\n        # Alternatively, to autogenerate the classname as well as the class:\n        # return type(self.__name__ + other.__name__, (self, other), {})\n\n    def unregister(self):\n        # classregistry.unregister(self)\n        print \"Would unregister class %s now.\" % self\n\nclass MyObject:\n    __metaclass__ = MyType\n\n\nclass NoneSample(MyObject):\n    pass\n\n# Will print \"NoneType None\"\nprint type(NoneSample), repr(NoneSample)\n\nclass Example(MyObject):\n    def __init__(self, value):\n        self.value = value\n    @make_hook\n    def add(self, other):\n        return self.__class__(self.value + other.value)\n\n# Will unregister the class\nExample.unregister()\n\ninst = Example(10)\n# Will fail with an AttributeError\n#inst.unregister()\n\nprint inst + inst\nclass Sibling(MyObject):\n    pass\n\nExampleSibling = Example + Sibling\n# ExampleSibling is now a subclass of both Example and Sibling (with no\n# content of its own) although it will believe it's called 'AutoClass'\nprint ExampleSibling\nprint ExampleSibling.__mro__\ndef make_hook(f):\n    \"\"\"Decorator to turn 'foo' method into '__foo__'\"\"\"\n    f.is_hook = 1\n    return f\n\nclass MyType(type):\n    def __new__(mcls, name, bases, attrs):\n\n        if name.startswith('None'):\n            return None\n\n        # Go over attributes and see if they should be renamed.\n        newattrs = {}\n        for attrname, attrvalue in attrs.iteritems():\n            if getattr(attrvalue, 'is_hook', 0):\n                newattrs['__%s__' % attrname] = attrvalue\n            else:\n                newattrs[attrname] = attrvalue\n\n        return super(MyType, mcls).__new__(mcls, name, bases, newattrs)\n\n    def __init__(self, name, bases, attrs):\n        super(MyType, self).__init__(name, bases, attrs)\n\n        # classregistry.register(self, self.interfaces)\n        print \"Would register class %s now.\" % self\n\n    def __add__(self, other):\n        class AutoClass(self, other):\n            pass\n        return AutoClass\n        # Alternatively, to autogenerate the classname as well as the class:\n        # return type(self.__name__ + other.__name__, (self, other), {})\n\n    def unregister(self):\n        # classregistry.unregister(self)\n        print \"Would unregister class %s now.\" % self\n\nclass MyObject:\n    __metaclass__ = MyType\n\n\nclass NoneSample(MyObject):\n    pass\n\n# Will print \"NoneType None\"\nprint type(NoneSample), repr(NoneSample)\n\nclass Example(MyObject):\n    def __init__(self, value):\n        self.value = value\n    @make_hook\n    def add(self, other):\n        return self.__class__(self.value + other.value)\n\n# Will unregister the class\nExample.unregister()\n\ninst = Example(10)\n# Will fail with an AttributeError\n#inst.unregister()\n\nprint inst + inst\nclass Sibling(MyObject):\n    pass\n\nExampleSibling = Example + Sibling\n# ExampleSibling is now a subclass of both Example and Sibling (with no\n# content of its own) although it will believe it's called 'AutoClass'\nprint ExampleSibling\nprint ExampleSibling.__mro__\n",
                "Note, this answer is for Python 2.x as it was written in 2008, metaclasses are slightly different in 3.x.Note, this answer is for Python 2.x as it was written in 2008, metaclasses are slightly different in 3.x.Metaclasses are the secret sauce that make 'class' work. The default metaclass for a new style object is called 'type'.class type(object)\n  |  type(object) -> the object's type\n  |  type(name, bases, dict) -> a new type\nclass type(object)\n  |  type(object) -> the object's type\n  |  type(name, bases, dict) -> a new type\nMetaclasses take 3 args. 'name', 'bases' and 'dict'namebasesdictHere is where the secret starts. Look for where name, bases and the dict come from in this example class definition.class ThisIsTheName(Bases, Are, Here):\n    All_the_code_here\n    def doesIs(create, a):\n        dict\nclass ThisIsTheName(Bases, Are, Here):\n    All_the_code_here\n    def doesIs(create, a):\n        dict\nLets define a metaclass that will demonstrate how 'class:' calls it.class:def test_metaclass(name, bases, dict):\n    print 'The Class Name is', name\n    print 'The Class Bases are', bases\n    print 'The dict has', len(dict), 'elems, the keys are', dict.keys()\n\n    return \"yellow\"\n\nclass TestName(object, None, int, 1):\n    __metaclass__ = test_metaclass\n    foo = 1\n    def baz(self, arr):\n        pass\n\nprint 'TestName = ', repr(TestName)\n\n# output => \nThe Class Name is TestName\nThe Class Bases are (<type 'object'>, None, <type 'int'>, 1)\nThe dict has 4 elems, the keys are ['baz', '__module__', 'foo', '__metaclass__']\nTestName =  'yellow'\ndef test_metaclass(name, bases, dict):\n    print 'The Class Name is', name\n    print 'The Class Bases are', bases\n    print 'The dict has', len(dict), 'elems, the keys are', dict.keys()\n\n    return \"yellow\"\n\nclass TestName(object, None, int, 1):\n    __metaclass__ = test_metaclass\n    foo = 1\n    def baz(self, arr):\n        pass\n\nprint 'TestName = ', repr(TestName)\n\n# output => \nThe Class Name is TestName\nThe Class Bases are (<type 'object'>, None, <type 'int'>, 1)\nThe dict has 4 elems, the keys are ['baz', '__module__', 'foo', '__metaclass__']\nTestName =  'yellow'\nAnd now, an example that actually means something, this will automatically make the variables in the list \"attributes\" set on the class, and set to None.def init_attributes(name, bases, dict):\n    if 'attributes' in dict:\n        for attr in dict['attributes']:\n            dict[attr] = None\n\n    return type(name, bases, dict)\n\nclass Initialised(object):\n    __metaclass__ = init_attributes\n    attributes = ['foo', 'bar', 'baz']\n\nprint 'foo =>', Initialised.foo\n# output=>\nfoo => None\ndef init_attributes(name, bases, dict):\n    if 'attributes' in dict:\n        for attr in dict['attributes']:\n            dict[attr] = None\n\n    return type(name, bases, dict)\n\nclass Initialised(object):\n    __metaclass__ = init_attributes\n    attributes = ['foo', 'bar', 'baz']\n\nprint 'foo =>', Initialised.foo\n# output=>\nfoo => None\nNote that the magic behaviour that Initialised gains by having the metaclass init_attributes is not passed onto a subclass of Initialised.Note that the magic behaviour that Initialised gains by having the metaclass init_attributes is not passed onto a subclass of Initialised.Initialisedinit_attributesInitialisedHere is an even more concrete example, showing how you can subclass 'type' to make a metaclass that performs an action when the class is created. This is quite tricky:class MetaSingleton(type):\n    instance = None\n    def __call__(cls, *args, **kw):\n        if cls.instance is None:\n            cls.instance = super(MetaSingleton, cls).__call__(*args, **kw)\n        return cls.instance\n\nclass Foo(object):\n    __metaclass__ = MetaSingleton\n\na = Foo()\nb = Foo()\nassert a is b\nclass MetaSingleton(type):\n    instance = None\n    def __call__(cls, *args, **kw):\n        if cls.instance is None:\n            cls.instance = super(MetaSingleton, cls).__call__(*args, **kw)\n        return cls.instance\n\nclass Foo(object):\n    __metaclass__ = MetaSingleton\n\na = Foo()\nb = Foo()\nassert a is b\n",
                "Others have explained how metaclasses work and how they fit into the Python type system. Here's an example of what they can be used for. In a testing framework I wrote, I wanted to keep track of the order in which classes were defined, so that I could later instantiate them in this order. I found it easiest to do this using a metaclass.class MyMeta(type):\n\n    counter = 0\n\n    def __init__(cls, name, bases, dic):\n        type.__init__(cls, name, bases, dic)\n        cls._order = MyMeta.counter\n        MyMeta.counter += 1\n\nclass MyType(object):              # Python 2\n    __metaclass__ = MyMeta\n\nclass MyType(metaclass=MyMeta):    # Python 3\n    pass\nclass MyMeta(type):\n\n    counter = 0\n\n    def __init__(cls, name, bases, dic):\n        type.__init__(cls, name, bases, dic)\n        cls._order = MyMeta.counter\n        MyMeta.counter += 1\n\nclass MyType(object):              # Python 2\n    __metaclass__ = MyMeta\n\nclass MyType(metaclass=MyMeta):    # Python 3\n    pass\nAnything that's a subclass of MyType then gets a class attribute _order that records the order in which the classes were defined.MyType_order",
                "One use for metaclasses is adding new properties and methods to an instance automatically.For example, if you look at Django models, their definition looks a bit confusing. It looks as if you are only defining class properties:Django modelsclass Person(models.Model):\n    first_name = models.CharField(max_length=30)\n    last_name = models.CharField(max_length=30)\nclass Person(models.Model):\n    first_name = models.CharField(max_length=30)\n    last_name = models.CharField(max_length=30)\nHowever, at runtime the Person objects are filled with all sorts of useful methods. See the source for some amazing metaclassery.source",
                "I think the ONLamp introduction to metaclass programming is well written and gives a really good introduction to the topic despite being several years old already.http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html (archived at https://web.archive.org/web/20080206005253/http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html)http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.htmlhttps://web.archive.org/web/20080206005253/http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.htmlIn short: A class is a blueprint for the creation of an instance, a metaclass is a blueprint for the creation of a class. It can be easily seen that in Python classes need to be first-class objects too to enable this behavior.I've never written one myself, but I think one of the nicest uses of metaclasses can be seen in the Django framework. The model classes use a metaclass approach to enable a declarative style of writing new models or form classes. While the metaclass is creating the class, all members get the possibility to customize the class itself.Django framework\nCreating a new model\nThe metaclass enabling this\nCreating a new modelCreating a new modelThe metaclass enabling thisThe metaclass enabling thisThe thing that's left to say is: If you don't know what metaclasses are, the probability that you will not need them is 99%.will not need them",
                "\nWhat are metaclasses? What do you use them for?\nWhat are metaclasses? What do you use them for?TLDR: A metaclass instantiates and defines behavior for a class just like a class instantiates and defines behavior for an instance. Pseudocode:>>> Class(...)\ninstance\n>>> Class(...)\ninstance\nThe above should look familiar. Well, where does Class come from? It's an instance of a metaclass (also pseudocode):Class>>> Metaclass(...)\nClass\n>>> Metaclass(...)\nClass\nIn real code, we can pass the default metaclass, type, everything we need to instantiate a class and we get a class:type>>> type('Foo', (object,), {}) # requires a name, bases, and a namespace\n<class '__main__.Foo'>\n>>> type('Foo', (object,), {}) # requires a name, bases, and a namespace\n<class '__main__.Foo'>\nPutting it differently\nA class is to an instance as a metaclass is to a class. \nWhen we instantiate an object, we get an instance:\n>>> object()                          # instantiation of class\n<object object at 0x7f9069b4e0b0>     # instance\n\nLikewise, when we define a class explicitly with the default metaclass, type, we instantiate it:\n>>> type('Object', (object,), {})     # instantiation of metaclass\n<class '__main__.Object'>             # instance\n\nPut another way, a class is an instance of a metaclass:\n>>> isinstance(object, type)\nTrue\n\nPut a third way, a metaclass is a class's class.\n>>> type(object) == type\nTrue\n>>> object.__class__\n<class 'type'>\n\nA class is to an instance as a metaclass is to a class. \nWhen we instantiate an object, we get an instance:\n>>> object()                          # instantiation of class\n<object object at 0x7f9069b4e0b0>     # instance\n\nLikewise, when we define a class explicitly with the default metaclass, type, we instantiate it:\n>>> type('Object', (object,), {})     # instantiation of metaclass\n<class '__main__.Object'>             # instance\nA class is to an instance as a metaclass is to a class. When we instantiate an object, we get an instance:>>> object()                          # instantiation of class\n<object object at 0x7f9069b4e0b0>     # instance\n>>> object()                          # instantiation of class\n<object object at 0x7f9069b4e0b0>     # instance\nLikewise, when we define a class explicitly with the default metaclass, type, we instantiate it:type>>> type('Object', (object,), {})     # instantiation of metaclass\n<class '__main__.Object'>             # instance\n>>> type('Object', (object,), {})     # instantiation of metaclass\n<class '__main__.Object'>             # instance\nPut another way, a class is an instance of a metaclass:\n>>> isinstance(object, type)\nTrue\nPut another way, a class is an instance of a metaclass:>>> isinstance(object, type)\nTrue\n>>> isinstance(object, type)\nTrue\nPut a third way, a metaclass is a class's class.\n>>> type(object) == type\nTrue\n>>> object.__class__\n<class 'type'>\nPut a third way, a metaclass is a class's class.>>> type(object) == type\nTrue\n>>> object.__class__\n<class 'type'>\n>>> type(object) == type\nTrue\n>>> object.__class__\n<class 'type'>\nWhen you write a class definition and Python executes it, it uses a metaclass to instantiate the class object (which will, in turn, be used to instantiate instances of that class).Just as we can use class definitions to change how custom object instances behave, we can use a metaclass class definition to change the way a class object behaves.What can they be used for? From the docs:docs\nThe potential uses for metaclasses are boundless. Some ideas that have been explored include logging, interface checking, automatic delegation, automatic property creation, proxies, frameworks, and automatic resource locking/synchronization.\nThe potential uses for metaclasses are boundless. Some ideas that have been explored include logging, interface checking, automatic delegation, automatic property creation, proxies, frameworks, and automatic resource locking/synchronization.Nevertheless, it is usually encouraged for users to avoid using metaclasses unless absolutely necessary.You use a metaclass every time you create a class:When you write a class definition, for example, like this,class Foo(object): \n    'demo'\nclass Foo(object): \n    'demo'\nYou instantiate a class object.>>> Foo\n<class '__main__.Foo'>\n>>> isinstance(Foo, type), isinstance(Foo, object)\n(True, True)\n>>> Foo\n<class '__main__.Foo'>\n>>> isinstance(Foo, type), isinstance(Foo, object)\n(True, True)\nIt is the same as functionally calling type with the appropriate arguments and assigning the result to a variable of that name:typename = 'Foo'\nbases = (object,)\nnamespace = {'__doc__': 'demo'}\nFoo = type(name, bases, namespace)\nname = 'Foo'\nbases = (object,)\nnamespace = {'__doc__': 'demo'}\nFoo = type(name, bases, namespace)\nNote, some things automatically get added to the __dict__, i.e., the namespace:__dict__>>> Foo.__dict__\ndict_proxy({'__dict__': <attribute '__dict__' of 'Foo' objects>, \n'__module__': '__main__', '__weakref__': <attribute '__weakref__' \nof 'Foo' objects>, '__doc__': 'demo'})\n>>> Foo.__dict__\ndict_proxy({'__dict__': <attribute '__dict__' of 'Foo' objects>, \n'__module__': '__main__', '__weakref__': <attribute '__weakref__' \nof 'Foo' objects>, '__doc__': 'demo'})\nThe metaclass of the object we created, in both cases, is type. metaclasstype(A side-note on the contents of the class __dict__: __module__ is there because classes must know where they are defined, and  __dict__ and __weakref__ are there because we don't define __slots__ - if we define __slots__ we'll save a bit of space in the instances, as we can disallow __dict__ and __weakref__ by excluding them. For example:__dict____module____dict____weakref____slots__define __slots____slots____dict____weakref__>>> Baz = type('Bar', (object,), {'__doc__': 'demo', '__slots__': ()})\n>>> Baz.__dict__\nmappingproxy({'__doc__': 'demo', '__slots__': (), '__module__': '__main__'})\n>>> Baz = type('Bar', (object,), {'__doc__': 'demo', '__slots__': ()})\n>>> Baz.__dict__\nmappingproxy({'__doc__': 'demo', '__slots__': (), '__module__': '__main__'})\n... but I digress.)We can extend type just like any other class definition:typeHere's the default __repr__ of classes:__repr__>>> Foo\n<class '__main__.Foo'>\n>>> Foo\n<class '__main__.Foo'>\nOne of the most valuable things we can do by default in writing a Python object is to provide it with a good __repr__. When we call help(repr) we learn that there's a good test for a __repr__ that also requires a test for equality - obj == eval(repr(obj)). The following simple implementation of __repr__ and __eq__ for class instances of our type class provides us with a demonstration that may improve on the default __repr__ of classes:__repr__help(repr)__repr__obj == eval(repr(obj))__repr____eq____repr__class Type(type):\n    def __repr__(cls):\n        \"\"\"\n        >>> Baz\n        Type('Baz', (Foo, Bar,), {'__module__': '__main__', '__doc__': None})\n        >>> eval(repr(Baz))\n        Type('Baz', (Foo, Bar,), {'__module__': '__main__', '__doc__': None})\n        \"\"\"\n        metaname = type(cls).__name__\n        name = cls.__name__\n        parents = ', '.join(b.__name__ for b in cls.__bases__)\n        if parents:\n            parents += ','\n        namespace = ', '.join(': '.join(\n          (repr(k), repr(v) if not isinstance(v, type) else v.__name__))\n               for k, v in cls.__dict__.items())\n        return '{0}(\\'{1}\\', ({2}), {{{3}}})'.format(metaname, name, parents, namespace)\n    def __eq__(cls, other):\n        \"\"\"\n        >>> Baz == eval(repr(Baz))\n        True            \n        \"\"\"\n        return (cls.__name__, cls.__bases__, cls.__dict__) == (\n                other.__name__, other.__bases__, other.__dict__)\nclass Type(type):\n    def __repr__(cls):\n        \"\"\"\n        >>> Baz\n        Type('Baz', (Foo, Bar,), {'__module__': '__main__', '__doc__': None})\n        >>> eval(repr(Baz))\n        Type('Baz', (Foo, Bar,), {'__module__': '__main__', '__doc__': None})\n        \"\"\"\n        metaname = type(cls).__name__\n        name = cls.__name__\n        parents = ', '.join(b.__name__ for b in cls.__bases__)\n        if parents:\n            parents += ','\n        namespace = ', '.join(': '.join(\n          (repr(k), repr(v) if not isinstance(v, type) else v.__name__))\n               for k, v in cls.__dict__.items())\n        return '{0}(\\'{1}\\', ({2}), {{{3}}})'.format(metaname, name, parents, namespace)\n    def __eq__(cls, other):\n        \"\"\"\n        >>> Baz == eval(repr(Baz))\n        True            \n        \"\"\"\n        return (cls.__name__, cls.__bases__, cls.__dict__) == (\n                other.__name__, other.__bases__, other.__dict__)\nSo now when we create an object with this metaclass, the __repr__ echoed on the command line provides a much less ugly sight than the default:__repr__>>> class Bar(object): pass\n>>> Baz = Type('Baz', (Foo, Bar,), {'__module__': '__main__', '__doc__': None})\n>>> Baz\nType('Baz', (Foo, Bar,), {'__module__': '__main__', '__doc__': None})\n>>> class Bar(object): pass\n>>> Baz = Type('Baz', (Foo, Bar,), {'__module__': '__main__', '__doc__': None})\n>>> Baz\nType('Baz', (Foo, Bar,), {'__module__': '__main__', '__doc__': None})\nWith a nice __repr__ defined for the class instance, we have a stronger ability to debug our code. However, much further checking with eval(repr(Class)) is unlikely (as functions would be rather impossible to eval from their default __repr__'s).__repr__eval(repr(Class))__repr__An expected usage: __prepare__ a namespace__prepare__If, for example, we want to know in what order a class's methods are created in, we could provide an ordered dict as the namespace of the class. We would do this with __prepare__ which returns the namespace dict for the class if it is implemented in Python 3: __prepare__returns the namespace dict for the class if it is implemented in Python 3from collections import OrderedDict\n\nclass OrderedType(Type):\n    @classmethod\n    def __prepare__(metacls, name, bases, **kwargs):\n        return OrderedDict()\n    def __new__(cls, name, bases, namespace, **kwargs):\n        result = Type.__new__(cls, name, bases, dict(namespace))\n        result.members = tuple(namespace)\n        return result\nfrom collections import OrderedDict\n\nclass OrderedType(Type):\n    @classmethod\n    def __prepare__(metacls, name, bases, **kwargs):\n        return OrderedDict()\n    def __new__(cls, name, bases, namespace, **kwargs):\n        result = Type.__new__(cls, name, bases, dict(namespace))\n        result.members = tuple(namespace)\n        return result\nAnd usage:class OrderedMethodsObject(object, metaclass=OrderedType):\n    def method1(self): pass\n    def method2(self): pass\n    def method3(self): pass\n    def method4(self): pass\nclass OrderedMethodsObject(object, metaclass=OrderedType):\n    def method1(self): pass\n    def method2(self): pass\n    def method3(self): pass\n    def method4(self): pass\nAnd now we have a record of the order in which these methods (and other class attributes) were created:>>> OrderedMethodsObject.members\n('__module__', '__qualname__', 'method1', 'method2', 'method3', 'method4')\n>>> OrderedMethodsObject.members\n('__module__', '__qualname__', 'method1', 'method2', 'method3', 'method4')\nNote, this example was adapted from the documentation - the new enum in the standard library does this.documentationenum in the standard librarySo what we did was instantiate a metaclass by creating a class. We can also treat the metaclass as we would any other class. It has a method resolution order:>>> inspect.getmro(OrderedType)\n(<class '__main__.OrderedType'>, <class '__main__.Type'>, <class 'type'>, <class 'object'>)\n>>> inspect.getmro(OrderedType)\n(<class '__main__.OrderedType'>, <class '__main__.Type'>, <class 'type'>, <class 'object'>)\nAnd it has approximately the correct repr (which we can no longer eval unless we can find a way to represent our functions.):repr>>> OrderedMethodsObject\nOrderedType('OrderedMethodsObject', (object,), {'method1': <function OrderedMethodsObject.method1 at 0x0000000002DB01E0>, 'members': ('__module__', '__qualname__', 'method1', 'method2', 'method3', 'method4'), 'method3': <function OrderedMet\nhodsObject.method3 at 0x0000000002DB02F0>, 'method2': <function OrderedMethodsObject.method2 at 0x0000000002DB0268>, '__module__': '__main__', '__weakref__': <attribute '__weakref__' of 'OrderedMethodsObject' objects>, '__doc__': None, '__d\nict__': <attribute '__dict__' of 'OrderedMethodsObject' objects>, 'method4': <function OrderedMethodsObject.method4 at 0x0000000002DB0378>})\n>>> OrderedMethodsObject\nOrderedType('OrderedMethodsObject', (object,), {'method1': <function OrderedMethodsObject.method1 at 0x0000000002DB01E0>, 'members': ('__module__', '__qualname__', 'method1', 'method2', 'method3', 'method4'), 'method3': <function OrderedMet\nhodsObject.method3 at 0x0000000002DB02F0>, 'method2': <function OrderedMethodsObject.method2 at 0x0000000002DB0268>, '__module__': '__main__', '__weakref__': <attribute '__weakref__' of 'OrderedMethodsObject' objects>, '__doc__': None, '__d\nict__': <attribute '__dict__' of 'OrderedMethodsObject' objects>, 'method4': <function OrderedMethodsObject.method4 at 0x0000000002DB0378>})\n",
                "Python 3 updatePython 3 updateThere are (at this point) two key methods in a metaclass:\n__prepare__, and\n__new__\n__prepare__, and__prepare____new____new____prepare__ lets you supply a custom mapping (such as an OrderedDict) to be used as the namespace while the class is being created.  You must return an instance of whatever namespace you choose.  If you don't implement __prepare__ a normal dict is used.__prepare__OrderedDict__prepare__dict__new__ is responsible for the actual creation/modification of the final class.__new__A bare-bones, do-nothing-extra metaclass would like:class Meta(type):\n\n    def __prepare__(metaclass, cls, bases):\n        return dict()\n\n    def __new__(metacls, cls, bases, clsdict):\n        return super().__new__(metacls, cls, bases, clsdict)\nclass Meta(type):\n\n    def __prepare__(metaclass, cls, bases):\n        return dict()\n\n    def __new__(metacls, cls, bases, clsdict):\n        return super().__new__(metacls, cls, bases, clsdict)\nA simple example:Say you want some simple validation code to run on your attributes -- like it must always be an int or a str.  Without a metaclass, your class would look something like:intstrclass Person:\n    weight = ValidateType('weight', int)\n    age = ValidateType('age', int)\n    name = ValidateType('name', str)\nclass Person:\n    weight = ValidateType('weight', int)\n    age = ValidateType('age', int)\n    name = ValidateType('name', str)\nAs you can see, you have to repeat the name of the attribute twice.  This makes typos possible along with irritating bugs.A simple metaclass can address that problem:class Person(metaclass=Validator):\n    weight = ValidateType(int)\n    age = ValidateType(int)\n    name = ValidateType(str)\nclass Person(metaclass=Validator):\n    weight = ValidateType(int)\n    age = ValidateType(int)\n    name = ValidateType(str)\nThis is what the metaclass would look like (not using __prepare__ since it is not needed):__prepare__class Validator(type):\n    def __new__(metacls, cls, bases, clsdict):\n        # search clsdict looking for ValidateType descriptors\n        for name, attr in clsdict.items():\n            if isinstance(attr, ValidateType):\n                attr.name = name\n                attr.attr = '_' + name\n        # create final class and return it\n        return super().__new__(metacls, cls, bases, clsdict)\nclass Validator(type):\n    def __new__(metacls, cls, bases, clsdict):\n        # search clsdict looking for ValidateType descriptors\n        for name, attr in clsdict.items():\n            if isinstance(attr, ValidateType):\n                attr.name = name\n                attr.attr = '_' + name\n        # create final class and return it\n        return super().__new__(metacls, cls, bases, clsdict)\nA sample run of:p = Person()\np.weight = 9\nprint(p.weight)\np.weight = '9'\np = Person()\np.weight = 9\nprint(p.weight)\np.weight = '9'\nproduces:9\nTraceback (most recent call last):\n  File \"simple_meta.py\", line 36, in <module>\n    p.weight = '9'\n  File \"simple_meta.py\", line 24, in __set__\n    (self.name, self.type, value))\nTypeError: weight must be of type(s) <class 'int'> (got '9')\n9\nTraceback (most recent call last):\n  File \"simple_meta.py\", line 36, in <module>\n    p.weight = '9'\n  File \"simple_meta.py\", line 24, in __set__\n    (self.name, self.type, value))\nTypeError: weight must be of type(s) <class 'int'> (got '9')\nNote:  This example is simple enough it could have also been accomplished with a class decorator, but presumably an actual metaclass would be doing much more.NoteThe 'ValidateType' class for reference:class ValidateType:\n    def __init__(self, type):\n        self.name = None  # will be set by metaclass\n        self.attr = None  # will be set by metaclass\n        self.type = type\n    def __get__(self, inst, cls):\n        if inst is None:\n            return self\n        else:\n            return inst.__dict__[self.attr]\n    def __set__(self, inst, value):\n        if not isinstance(value, self.type):\n            raise TypeError('%s must be of type(s) %s (got %r)' %\n                    (self.name, self.type, value))\n        else:\n            inst.__dict__[self.attr] = value\nclass ValidateType:\n    def __init__(self, type):\n        self.name = None  # will be set by metaclass\n        self.attr = None  # will be set by metaclass\n        self.type = type\n    def __get__(self, inst, cls):\n        if inst is None:\n            return self\n        else:\n            return inst.__dict__[self.attr]\n    def __set__(self, inst, value):\n        if not isinstance(value, self.type):\n            raise TypeError('%s must be of type(s) %s (got %r)' %\n                    (self.name, self.type, value))\n        else:\n            inst.__dict__[self.attr] = value\n",
                "Role of a metaclass' __call__() method when creating a class instance__call__()If you've done Python programming for more than a few months you'll eventually stumble upon code that looks like this:# define a class\nclass SomeClass(object):\n    # ...\n    # some definition here ...\n    # ...\n\n# create an instance of it\ninstance = SomeClass()\n\n# then call the object as if it's a function\nresult = instance('foo', 'bar')\n# define a class\nclass SomeClass(object):\n    # ...\n    # some definition here ...\n    # ...\n\n# create an instance of it\ninstance = SomeClass()\n\n# then call the object as if it's a function\nresult = instance('foo', 'bar')\nThe latter is possible when you implement the __call__() magic method on the class.__call__()class SomeClass(object):\n    # ...\n    # some definition here ...\n    # ...\n\n    def __call__(self, foo, bar):\n        return bar + foo\nclass SomeClass(object):\n    # ...\n    # some definition here ...\n    # ...\n\n    def __call__(self, foo, bar):\n        return bar + foo\nThe __call__() method is invoked when an instance of a class is used as a callable. But as we've seen from previous answers a class itself is an instance of a metaclass, so when we use the class as a callable (i.e. when we create an instance of it) we're actually calling its metaclass' __call__() method. At this point most Python programmers are a bit confused because they've been told that when creating an instance like this instance = SomeClass() you're calling its __init__() method. Some who've dug a bit deeper know that before __init__() there's __new__(). Well, today another layer of truth is being revealed, before __new__() there's the metaclass' __call__().__call__()__call__()instance = SomeClass()__init__()__init__()__new__()__new__()__call__()Let's study the method call chain from specifically the perspective of creating an instance of a class.This is a metaclass that logs exactly the moment before an instance is created and the moment it's about to return it.class Meta_1(type):\n    def __call__(cls):\n        print \"Meta_1.__call__() before creating an instance of \", cls\n        instance = super(Meta_1, cls).__call__()\n        print \"Meta_1.__call__() about to return instance.\"\n        return instance\nclass Meta_1(type):\n    def __call__(cls):\n        print \"Meta_1.__call__() before creating an instance of \", cls\n        instance = super(Meta_1, cls).__call__()\n        print \"Meta_1.__call__() about to return instance.\"\n        return instance\nThis is a class that uses that metaclassclass Class_1(object):\n\n    __metaclass__ = Meta_1\n\n    def __new__(cls):\n        print \"Class_1.__new__() before creating an instance.\"\n        instance = super(Class_1, cls).__new__(cls)\n        print \"Class_1.__new__() about to return instance.\"\n        return instance\n\n    def __init__(self):\n        print \"entering Class_1.__init__() for instance initialization.\"\n        super(Class_1,self).__init__()\n        print \"exiting Class_1.__init__().\"\nclass Class_1(object):\n\n    __metaclass__ = Meta_1\n\n    def __new__(cls):\n        print \"Class_1.__new__() before creating an instance.\"\n        instance = super(Class_1, cls).__new__(cls)\n        print \"Class_1.__new__() about to return instance.\"\n        return instance\n\n    def __init__(self):\n        print \"entering Class_1.__init__() for instance initialization.\"\n        super(Class_1,self).__init__()\n        print \"exiting Class_1.__init__().\"\nAnd now let's create an instance of Class_1Class_1instance = Class_1()\n# Meta_1.__call__() before creating an instance of <class '__main__.Class_1'>.\n# Class_1.__new__() before creating an instance.\n# Class_1.__new__() about to return instance.\n# entering Class_1.__init__() for instance initialization.\n# exiting Class_1.__init__().\n# Meta_1.__call__() about to return instance.\ninstance = Class_1()\n# Meta_1.__call__() before creating an instance of <class '__main__.Class_1'>.\n# Class_1.__new__() before creating an instance.\n# Class_1.__new__() about to return instance.\n# entering Class_1.__init__() for instance initialization.\n# exiting Class_1.__init__().\n# Meta_1.__call__() about to return instance.\nObserve that the code above doesn't actually do anything more than logging the tasks. Each method delegates the actual work to its parent's implementation, thus keeping the default behavior. Since type is Meta_1's parent class (type being the default parent metaclass) and considering the ordering sequence of the output above, we now have a clue as to what would be the pseudo implementation of type.__call__():typeMeta_1typetype.__call__()class type:\n    def __call__(cls, *args, **kwarg):\n\n        # ... maybe a few things done to cls here\n\n        # then we call __new__() on the class to create an instance\n        instance = cls.__new__(cls, *args, **kwargs)\n\n        # ... maybe a few things done to the instance here\n\n        # then we initialize the instance with its __init__() method\n        instance.__init__(*args, **kwargs)\n\n        # ... maybe a few more things done to instance here\n\n        # then we return it\n        return instance\nclass type:\n    def __call__(cls, *args, **kwarg):\n\n        # ... maybe a few things done to cls here\n\n        # then we call __new__() on the class to create an instance\n        instance = cls.__new__(cls, *args, **kwargs)\n\n        # ... maybe a few things done to the instance here\n\n        # then we initialize the instance with its __init__() method\n        instance.__init__(*args, **kwargs)\n\n        # ... maybe a few more things done to instance here\n\n        # then we return it\n        return instance\nWe can see that the metaclass' __call__() method is the one that's called first. It then delegates creation of the instance to the class's __new__() method and initialization to the instance's __init__(). It's also the one that ultimately returns the instance.__call__()__new__()__init__()From the above it stems that the metaclass' __call__() is also given the opportunity to decide whether or not a call to Class_1.__new__() or Class_1.__init__() will eventually be made. Over the course of its execution it could actually return an object that hasn't been touched by either of these methods. Take for example this approach to the singleton pattern:__call__()Class_1.__new__()Class_1.__init__()class Meta_2(type):\n    singletons = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls in Meta_2.singletons:\n            # we return the only instance and skip a call to __new__()\n            # and __init__()\n            print (\"{} singleton returning from Meta_2.__call__(), \"\n                   \"skipping creation of new instance.\".format(cls))\n            return Meta_2.singletons[cls]\n\n        # else if the singleton isn't present we proceed as usual\n        print \"Meta_2.__call__() before creating an instance.\"\n        instance = super(Meta_2, cls).__call__(*args, **kwargs)\n        Meta_2.singletons[cls] = instance\n        print \"Meta_2.__call__() returning new instance.\"\n        return instance\n\nclass Class_2(object):\n\n    __metaclass__ = Meta_2\n\n    def __new__(cls, *args, **kwargs):\n        print \"Class_2.__new__() before creating instance.\"\n        instance = super(Class_2, cls).__new__(cls)\n        print \"Class_2.__new__() returning instance.\"\n        return instance\n\n    def __init__(self, *args, **kwargs):\n        print \"entering Class_2.__init__() for initialization.\"\n        super(Class_2, self).__init__()\n        print \"exiting Class_2.__init__().\"\nclass Meta_2(type):\n    singletons = {}\n\n    def __call__(cls, *args, **kwargs):\n        if cls in Meta_2.singletons:\n            # we return the only instance and skip a call to __new__()\n            # and __init__()\n            print (\"{} singleton returning from Meta_2.__call__(), \"\n                   \"skipping creation of new instance.\".format(cls))\n            return Meta_2.singletons[cls]\n\n        # else if the singleton isn't present we proceed as usual\n        print \"Meta_2.__call__() before creating an instance.\"\n        instance = super(Meta_2, cls).__call__(*args, **kwargs)\n        Meta_2.singletons[cls] = instance\n        print \"Meta_2.__call__() returning new instance.\"\n        return instance\n\nclass Class_2(object):\n\n    __metaclass__ = Meta_2\n\n    def __new__(cls, *args, **kwargs):\n        print \"Class_2.__new__() before creating instance.\"\n        instance = super(Class_2, cls).__new__(cls)\n        print \"Class_2.__new__() returning instance.\"\n        return instance\n\n    def __init__(self, *args, **kwargs):\n        print \"entering Class_2.__init__() for initialization.\"\n        super(Class_2, self).__init__()\n        print \"exiting Class_2.__init__().\"\nLet's observe what happens when repeatedly trying to create an object of type Class_2Class_2a = Class_2()\n# Meta_2.__call__() before creating an instance.\n# Class_2.__new__() before creating instance.\n# Class_2.__new__() returning instance.\n# entering Class_2.__init__() for initialization.\n# exiting Class_2.__init__().\n# Meta_2.__call__() returning new instance.\n\nb = Class_2()\n# <class '__main__.Class_2'> singleton returning from Meta_2.__call__(), skipping creation of new instance.\n\nc = Class_2()\n# <class '__main__.Class_2'> singleton returning from Meta_2.__call__(), skipping creation of new instance.\n\na is b is c # True\na = Class_2()\n# Meta_2.__call__() before creating an instance.\n# Class_2.__new__() before creating instance.\n# Class_2.__new__() returning instance.\n# entering Class_2.__init__() for initialization.\n# exiting Class_2.__init__().\n# Meta_2.__call__() returning new instance.\n\nb = Class_2()\n# <class '__main__.Class_2'> singleton returning from Meta_2.__call__(), skipping creation of new instance.\n\nc = Class_2()\n# <class '__main__.Class_2'> singleton returning from Meta_2.__call__(), skipping creation of new instance.\n\na is b is c # True\n",
                "A metaclass is a class that tells how (some) other class should be created.metaclassThis is a case where I saw metaclass as a solution to my problem:\nI had a really complicated problem, that probably could have been solved differently, but I chose to solve it using a metaclass.  Because of the complexity, it is one of the few modules I have written where the comments in the module surpass the amount of code that has been written.  Here it is...metaclassmetaclass#!/usr/bin/env python\n\n# Copyright (C) 2013-2014 Craig Phillips.  All rights reserved.\n\n# This requires some explaining.  The point of this metaclass excercise is to\n# create a static abstract class that is in one way or another, dormant until\n# queried.  I experimented with creating a singlton on import, but that did\n# not quite behave how I wanted it to.  See now here, we are creating a class\n# called GsyncOptions, that on import, will do nothing except state that its\n# class creator is GsyncOptionsType.  This means, docopt doesn't parse any\n# of the help document, nor does it start processing command line options.\n# So importing this module becomes really efficient.  The complicated bit\n# comes from requiring the GsyncOptions class to be static.  By that, I mean\n# any property on it, may or may not exist, since they are not statically\n# defined; so I can't simply just define the class with a whole bunch of\n# properties that are @property @staticmethods.\n#\n# So here's how it works:\n#\n# Executing 'from libgsync.options import GsyncOptions' does nothing more\n# than load up this module, define the Type and the Class and import them\n# into the callers namespace.  Simple.\n#\n# Invoking 'GsyncOptions.debug' for the first time, or any other property\n# causes the __metaclass__ __getattr__ method to be called, since the class\n# is not instantiated as a class instance yet.  The __getattr__ method on\n# the type then initialises the class (GsyncOptions) via the __initialiseClass\n# method.  This is the first and only time the class will actually have its\n# dictionary statically populated.  The docopt module is invoked to parse the\n# usage document and generate command line options from it.  These are then\n# paired with their defaults and what's in sys.argv.  After all that, we\n# setup some dynamic properties that could not be defined by their name in\n# the usage, before everything is then transplanted onto the actual class\n# object (or static class GsyncOptions).\n#\n# Another piece of magic, is to allow command line options to be set in\n# in their native form and be translated into argparse style properties.\n#\n# Finally, the GsyncListOptions class is actually where the options are\n# stored.  This only acts as a mechanism for storing options as lists, to\n# allow aggregation of duplicate options or options that can be specified\n# multiple times.  The __getattr__ call hides this by default, returning the\n# last item in a property's list.  However, if the entire list is required,\n# calling the 'list()' method on the GsyncOptions class, returns a reference\n# to the GsyncListOptions class, which contains all of the same properties\n# but as lists and without the duplication of having them as both lists and\n# static singlton values.\n#\n# So this actually means that GsyncOptions is actually a static proxy class...\n#\n# ...And all this is neatly hidden within a closure for safe keeping.\ndef GetGsyncOptionsType():\n    class GsyncListOptions(object):\n        __initialised = False\n\n    class GsyncOptionsType(type):\n        def __initialiseClass(cls):\n            if GsyncListOptions._GsyncListOptions__initialised: return\n\n            from docopt import docopt\n            from libgsync.options import doc\n            from libgsync import __version__\n\n            options = docopt(\n                doc.__doc__ % __version__,\n                version = __version__,\n                options_first = True\n            )\n\n            paths = options.pop('<path>', None)\n            setattr(cls, \"destination_path\", paths.pop() if paths else None)\n            setattr(cls, \"source_paths\", paths)\n            setattr(cls, \"options\", options)\n\n            for k, v in options.iteritems():\n                setattr(cls, k, v)\n\n            GsyncListOptions._GsyncListOptions__initialised = True\n\n        def list(cls):\n            return GsyncListOptions\n\n        def __getattr__(cls, name):\n            cls.__initialiseClass()\n            return getattr(GsyncListOptions, name)[-1]\n\n        def __setattr__(cls, name, value):\n            # Substitut option names: --an-option-name for an_option_name\n            import re\n            name = re.sub(r'^__', \"\", re.sub(r'-', \"_\", name))\n            listvalue = []\n\n            # Ensure value is converted to a list type for GsyncListOptions\n            if isinstance(value, list):\n                if value:\n                    listvalue = [] + value\n                else:\n                    listvalue = [ None ]\n            else:\n                listvalue = [ value ]\n\n            type.__setattr__(GsyncListOptions, name, listvalue)\n\n    # Cleanup this module to prevent tinkering.\n    import sys\n    module = sys.modules[__name__]\n    del module.__dict__['GetGsyncOptionsType']\n\n    return GsyncOptionsType\n\n# Our singlton abstract proxy class.\nclass GsyncOptions(object):\n    __metaclass__ = GetGsyncOptionsType()\n#!/usr/bin/env python\n\n# Copyright (C) 2013-2014 Craig Phillips.  All rights reserved.\n\n# This requires some explaining.  The point of this metaclass excercise is to\n# create a static abstract class that is in one way or another, dormant until\n# queried.  I experimented with creating a singlton on import, but that did\n# not quite behave how I wanted it to.  See now here, we are creating a class\n# called GsyncOptions, that on import, will do nothing except state that its\n# class creator is GsyncOptionsType.  This means, docopt doesn't parse any\n# of the help document, nor does it start processing command line options.\n# So importing this module becomes really efficient.  The complicated bit\n# comes from requiring the GsyncOptions class to be static.  By that, I mean\n# any property on it, may or may not exist, since they are not statically\n# defined; so I can't simply just define the class with a whole bunch of\n# properties that are @property @staticmethods.\n#\n# So here's how it works:\n#\n# Executing 'from libgsync.options import GsyncOptions' does nothing more\n# than load up this module, define the Type and the Class and import them\n# into the callers namespace.  Simple.\n#\n# Invoking 'GsyncOptions.debug' for the first time, or any other property\n# causes the __metaclass__ __getattr__ method to be called, since the class\n# is not instantiated as a class instance yet.  The __getattr__ method on\n# the type then initialises the class (GsyncOptions) via the __initialiseClass\n# method.  This is the first and only time the class will actually have its\n# dictionary statically populated.  The docopt module is invoked to parse the\n# usage document and generate command line options from it.  These are then\n# paired with their defaults and what's in sys.argv.  After all that, we\n# setup some dynamic properties that could not be defined by their name in\n# the usage, before everything is then transplanted onto the actual class\n# object (or static class GsyncOptions).\n#\n# Another piece of magic, is to allow command line options to be set in\n# in their native form and be translated into argparse style properties.\n#\n# Finally, the GsyncListOptions class is actually where the options are\n# stored.  This only acts as a mechanism for storing options as lists, to\n# allow aggregation of duplicate options or options that can be specified\n# multiple times.  The __getattr__ call hides this by default, returning the\n# last item in a property's list.  However, if the entire list is required,\n# calling the 'list()' method on the GsyncOptions class, returns a reference\n# to the GsyncListOptions class, which contains all of the same properties\n# but as lists and without the duplication of having them as both lists and\n# static singlton values.\n#\n# So this actually means that GsyncOptions is actually a static proxy class...\n#\n# ...And all this is neatly hidden within a closure for safe keeping.\ndef GetGsyncOptionsType():\n    class GsyncListOptions(object):\n        __initialised = False\n\n    class GsyncOptionsType(type):\n        def __initialiseClass(cls):\n            if GsyncListOptions._GsyncListOptions__initialised: return\n\n            from docopt import docopt\n            from libgsync.options import doc\n            from libgsync import __version__\n\n            options = docopt(\n                doc.__doc__ % __version__,\n                version = __version__,\n                options_first = True\n            )\n\n            paths = options.pop('<path>', None)\n            setattr(cls, \"destination_path\", paths.pop() if paths else None)\n            setattr(cls, \"source_paths\", paths)\n            setattr(cls, \"options\", options)\n\n            for k, v in options.iteritems():\n                setattr(cls, k, v)\n\n            GsyncListOptions._GsyncListOptions__initialised = True\n\n        def list(cls):\n            return GsyncListOptions\n\n        def __getattr__(cls, name):\n            cls.__initialiseClass()\n            return getattr(GsyncListOptions, name)[-1]\n\n        def __setattr__(cls, name, value):\n            # Substitut option names: --an-option-name for an_option_name\n            import re\n            name = re.sub(r'^__', \"\", re.sub(r'-', \"_\", name))\n            listvalue = []\n\n            # Ensure value is converted to a list type for GsyncListOptions\n            if isinstance(value, list):\n                if value:\n                    listvalue = [] + value\n                else:\n                    listvalue = [ None ]\n            else:\n                listvalue = [ value ]\n\n            type.__setattr__(GsyncListOptions, name, listvalue)\n\n    # Cleanup this module to prevent tinkering.\n    import sys\n    module = sys.modules[__name__]\n    del module.__dict__['GetGsyncOptionsType']\n\n    return GsyncOptionsType\n\n# Our singlton abstract proxy class.\nclass GsyncOptions(object):\n    __metaclass__ = GetGsyncOptionsType()\n",
                "The tl;dr versionThe type(obj) function gets you the type of an object. type(obj)The type() of a class is its metaclass.The type() of a class is its metaclass.type()metaclassTo use a metaclass:class Foo(object):\n    __metaclass__ = MyMetaClass\nclass Foo(object):\n    __metaclass__ = MyMetaClass\ntype is its own metaclass. The class of a class is a metaclass-- the body of a class is the arguments passed to the metaclass that is used to construct the class.typeHere you can read about how to use metaclasses to customize class construction.Here",
                "type is actually a metaclass -- a class that creates another classes.\nMost metaclass are the subclasses of type. The metaclass receives the new class as its first argument and provide access to class object with details as mentioned below:typemetaclassmetaclasstypemetaclassnew>>> class MetaClass(type):\n...     def __init__(cls, name, bases, attrs):\n...         print ('class name: %s' %name )\n...         print ('Defining class %s' %cls)\n...         print('Bases %s: ' %bases)\n...         print('Attributes')\n...         for (name, value) in attrs.items():\n...             print ('%s :%r' %(name, value))\n... \n\n>>> class NewClass(object, metaclass=MetaClass):\n...    get_choch='dairy'\n... \nclass name: NewClass\nBases <class 'object'>: \nDefining class <class 'NewClass'>\nget_choch :'dairy'\n__module__ :'builtins'\n__qualname__ :'NewClass'\n>>> class MetaClass(type):\n...     def __init__(cls, name, bases, attrs):\n...         print ('class name: %s' %name )\n...         print ('Defining class %s' %cls)\n...         print('Bases %s: ' %bases)\n...         print('Attributes')\n...         for (name, value) in attrs.items():\n...             print ('%s :%r' %(name, value))\n... \n\n>>> class NewClass(object, metaclass=MetaClass):\n...    get_choch='dairy'\n... \nclass name: NewClass\nBases <class 'object'>: \nDefining class <class 'NewClass'>\nget_choch :'dairy'\n__module__ :'builtins'\n__qualname__ :'NewClass'\nNote:Note:Notice that the class was not instantiated at any time; the simple act of creating the class triggered execution of the metaclass.metaclass",
                "Python classes are themselves objects - as in instance - of their meta-class. The default metaclass, which is applied when when you determine classes as:class foo:\n    ...\nclass foo:\n    ...\nmeta class are used to apply some rule to an entire set of classes. For example, suppose you're building an ORM to access a database, and you want records from each table to be of a class mapped to that table (based on fields, business rules, etc..,), a possible use of metaclass is for instance, connection pool logic, which is share by all classes of record from all tables. Another use is logic to to support foreign keys, which involves multiple classes of records. when you define metaclass, you subclass type, and can overrided the following magic methods to insert your logic. class somemeta(type):\n    __new__(mcs, name, bases, clsdict):\n      \"\"\"\n  mcs: is the base metaclass, in this case type.\n  name: name of the new class, as provided by the user.\n  bases: tuple of base classes \n  clsdict: a dictionary containing all methods and attributes defined on class\n\n  you must return a class object by invoking the __new__ constructor on the base metaclass. \n ie: \n    return type.__call__(mcs, name, bases, clsdict).\n\n  in the following case:\n\n  class foo(baseclass):\n        __metaclass__ = somemeta\n\n  an_attr = 12\n\n  def bar(self):\n      ...\n\n  @classmethod\n  def foo(cls):\n      ...\n\n      arguments would be : ( somemeta, \"foo\", (baseclass, baseofbase,..., object), {\"an_attr\":12, \"bar\": <function>, \"foo\": <bound class method>}\n\n      you can modify any of these values before passing on to type\n      \"\"\"\n      return type.__call__(mcs, name, bases, clsdict)\n\n\n    def __init__(self, name, bases, clsdict):\n      \"\"\" \n      called after type has been created. unlike in standard classes, __init__ method cannot modify the instance (cls) - and should be used for class validaton.\n      \"\"\"\n      pass\n\n\n    def __prepare__():\n        \"\"\"\n        returns a dict or something that can be used as a namespace.\n        the type will then attach methods and attributes from class definition to it.\n\n        call order :\n\n        somemeta.__new__ ->  type.__new__ -> type.__init__ -> somemeta.__init__ \n        \"\"\"\n        return dict()\n\n    def mymethod(cls):\n        \"\"\" works like a classmethod, but for class objects. Also, my method will not be visible to instances of cls.\n        \"\"\"\n        pass\nclass somemeta(type):\n    __new__(mcs, name, bases, clsdict):\n      \"\"\"\n  mcs: is the base metaclass, in this case type.\n  name: name of the new class, as provided by the user.\n  bases: tuple of base classes \n  clsdict: a dictionary containing all methods and attributes defined on class\n\n  you must return a class object by invoking the __new__ constructor on the base metaclass. \n ie: \n    return type.__call__(mcs, name, bases, clsdict).\n\n  in the following case:\n\n  class foo(baseclass):\n        __metaclass__ = somemeta\n\n  an_attr = 12\n\n  def bar(self):\n      ...\n\n  @classmethod\n  def foo(cls):\n      ...\n\n      arguments would be : ( somemeta, \"foo\", (baseclass, baseofbase,..., object), {\"an_attr\":12, \"bar\": <function>, \"foo\": <bound class method>}\n\n      you can modify any of these values before passing on to type\n      \"\"\"\n      return type.__call__(mcs, name, bases, clsdict)\n\n\n    def __init__(self, name, bases, clsdict):\n      \"\"\" \n      called after type has been created. unlike in standard classes, __init__ method cannot modify the instance (cls) - and should be used for class validaton.\n      \"\"\"\n      pass\n\n\n    def __prepare__():\n        \"\"\"\n        returns a dict or something that can be used as a namespace.\n        the type will then attach methods and attributes from class definition to it.\n\n        call order :\n\n        somemeta.__new__ ->  type.__new__ -> type.__init__ -> somemeta.__init__ \n        \"\"\"\n        return dict()\n\n    def mymethod(cls):\n        \"\"\" works like a classmethod, but for class objects. Also, my method will not be visible to instances of cls.\n        \"\"\"\n        pass\nanyhow, those two are the most commonly used hooks. metaclassing is powerful, and above is nowhere near and exhaustive list of uses for metaclassing. ",
                "The type() function can return the type of an object or create a new type, for example, we can create a Hi class with the type() function and do not  need to use this way with class Hi(object):def func(self, name='mike'):\n    print('Hi, %s.' % name)\n\nHi = type('Hi', (object,), dict(hi=func))\nh = Hi()\nh.hi()\nHi, mike.\n\ntype(Hi)\ntype\n\ntype(h)\n__main__.Hi\ndef func(self, name='mike'):\n    print('Hi, %s.' % name)\n\nHi = type('Hi', (object,), dict(hi=func))\nh = Hi()\nh.hi()\nHi, mike.\n\ntype(Hi)\ntype\n\ntype(h)\n__main__.Hi\nIn addition to using type() to create classes dynamically, you can control creation behavior of class and use metaclass.According to the Python object model, the class is the object, so the class must be an instance of another certain class.\nBy default, a Python class is instance of the type class. That is, type is metaclass of most of the built-in classes and metaclass of user-defined classes.class ListMetaclass(type):\n    def __new__(cls, name, bases, attrs):\n        attrs['add'] = lambda self, value: self.append(value)\n        return type.__new__(cls, name, bases, attrs)\n\nclass CustomList(list, metaclass=ListMetaclass):\n    pass\n\nlst = CustomList()\nlst.add('custom_list_1')\nlst.add('custom_list_2')\n\nlst\n['custom_list_1', 'custom_list_2']\nclass ListMetaclass(type):\n    def __new__(cls, name, bases, attrs):\n        attrs['add'] = lambda self, value: self.append(value)\n        return type.__new__(cls, name, bases, attrs)\n\nclass CustomList(list, metaclass=ListMetaclass):\n    pass\n\nlst = CustomList()\nlst.add('custom_list_1')\nlst.add('custom_list_2')\n\nlst\n['custom_list_1', 'custom_list_2']\nMagic will take effect when we passed keyword arguments in metaclass, it indicates the Python interpreter to create the CustomList through ListMetaclass. new (), at this point, we can modify the class definition, for example, and add a new method and then return the revised definition.new",
                "In addition to the published answers I can say that a metaclass defines the behaviour for a class. So, you can explicitly set your metaclass. Whenever Python gets a keyword class then it starts searching for the metaclass. If it's not found \u2013 the default metaclass type is used to create the class's object. Using the __metaclass__ attribute, you can set metaclass of your class:metaclassclassmetaclass__metaclass__metaclassclass MyClass:\n   __metaclass__ = type\n   # write here other method\n   # write here one more method\n\nprint(MyClass.__metaclass__)\nclass MyClass:\n   __metaclass__ = type\n   # write here other method\n   # write here one more method\n\nprint(MyClass.__metaclass__)\nIt'll produce the output like this:class 'type'\nclass 'type'\nAnd, of course, you can create your own metaclass to define the behaviour of any class that are created using your class.metaclassFor doing that, your default metaclass type class must be inherited as this is the main metaclass:metaclassmetaclassclass MyMetaClass(type):\n   __metaclass__ = type\n   # you can write here any behaviour you want\n\nclass MyTestClass:\n   __metaclass__ = MyMetaClass\n\nObj = MyTestClass()\nprint(Obj.__metaclass__)\nprint(MyMetaClass.__metaclass__)\nclass MyMetaClass(type):\n   __metaclass__ = type\n   # you can write here any behaviour you want\n\nclass MyTestClass:\n   __metaclass__ = MyMetaClass\n\nObj = MyTestClass()\nprint(Obj.__metaclass__)\nprint(MyMetaClass.__metaclass__)\nThe output will be:class '__main__.MyMetaClass'\nclass 'type'\nclass '__main__.MyMetaClass'\nclass 'type'\n",
                "Note that in python 3.6 a new dunder method __init_subclass__(cls, **kwargs) was introduced to replace a lot of common use cases for metaclasses. Is is called when a subclass of the defining class is created. See python docs.__init_subclass__(cls, **kwargs)python docs",
                "Here's another example of what it can be used for:\nYou can use the metaclass to change the function of its instance (the class).\nYou can use the metaclass to change the function of its instance (the class).metaclassclass MetaMemberControl(type):\n    __slots__ = ()\n\n    @classmethod\n    def __prepare__(mcs, f_cls_name, f_cls_parents,  # f_cls means: future class\n                    meta_args=None, meta_options=None):  # meta_args and meta_options is not necessarily needed, just so you know.\n        f_cls_attr = dict()\n        if not \"do something or if you want to define your cool stuff of dict...\":\n            return dict(make_your_special_dict=None)\n        else:\n            return f_cls_attr\n\n    def __new__(mcs, f_cls_name, f_cls_parents, f_cls_attr,\n                meta_args=None, meta_options=None):\n\n        original_getattr = f_cls_attr.get('__getattribute__')\n        original_setattr = f_cls_attr.get('__setattr__')\n\n        def init_getattr(self, item):\n            if not item.startswith('_'):  # you can set break points at here\n                alias_name = '_' + item\n                if alias_name in f_cls_attr['__slots__']:\n                    item = alias_name\n            if original_getattr is not None:\n                return original_getattr(self, item)\n            else:\n                return super(eval(f_cls_name), self).__getattribute__(item)\n\n        def init_setattr(self, key, value):\n            if not key.startswith('_') and ('_' + key) in f_cls_attr['__slots__']:\n                raise AttributeError(f\"you can't modify private members:_{key}\")\n            if original_setattr is not None:\n                original_setattr(self, key, value)\n            else:\n                super(eval(f_cls_name), self).__setattr__(key, value)\n\n        f_cls_attr['__getattribute__'] = init_getattr\n        f_cls_attr['__setattr__'] = init_setattr\n\n        cls = super().__new__(mcs, f_cls_name, f_cls_parents, f_cls_attr)\n        return cls\n\n\nclass Human(metaclass=MetaMemberControl):\n    __slots__ = ('_age', '_name')\n\n    def __init__(self, name, age):\n        self._name = name\n        self._age = age\n\n    def __getattribute__(self, item):\n        \"\"\"\n        is just for IDE recognize.\n        \"\"\"\n        return super().__getattribute__(item)\n\n    \"\"\" with MetaMemberControl then you don't have to write as following\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def age(self):\n        return self._age\n    \"\"\"\n\n\ndef test_demo():\n    human = Human('Carson', 27)\n    # human.age = 18  # you can't modify private members:_age  <-- this is defined by yourself.\n    # human.k = 18  # 'Human' object has no attribute 'k'  <-- system error.\n    age1 = human._age  # It's OK, although the IDE will show some warnings. (Access to a protected member _age of a class)\n\n    age2 = human.age  # It's OK! see below:\n    \"\"\"\n    if you do not define `__getattribute__` at the class of Human,\n    the IDE will show you: Unresolved attribute reference 'age' for class 'Human'\n    but it's ok on running since the MetaMemberControl will help you.\n    \"\"\"\n\n\nif __name__ == '__main__':\n    test_demo()\n\nclass MetaMemberControl(type):\n    __slots__ = ()\n\n    @classmethod\n    def __prepare__(mcs, f_cls_name, f_cls_parents,  # f_cls means: future class\n                    meta_args=None, meta_options=None):  # meta_args and meta_options is not necessarily needed, just so you know.\n        f_cls_attr = dict()\n        if not \"do something or if you want to define your cool stuff of dict...\":\n            return dict(make_your_special_dict=None)\n        else:\n            return f_cls_attr\n\n    def __new__(mcs, f_cls_name, f_cls_parents, f_cls_attr,\n                meta_args=None, meta_options=None):\n\n        original_getattr = f_cls_attr.get('__getattribute__')\n        original_setattr = f_cls_attr.get('__setattr__')\n\n        def init_getattr(self, item):\n            if not item.startswith('_'):  # you can set break points at here\n                alias_name = '_' + item\n                if alias_name in f_cls_attr['__slots__']:\n                    item = alias_name\n            if original_getattr is not None:\n                return original_getattr(self, item)\n            else:\n                return super(eval(f_cls_name), self).__getattribute__(item)\n\n        def init_setattr(self, key, value):\n            if not key.startswith('_') and ('_' + key) in f_cls_attr['__slots__']:\n                raise AttributeError(f\"you can't modify private members:_{key}\")\n            if original_setattr is not None:\n                original_setattr(self, key, value)\n            else:\n                super(eval(f_cls_name), self).__setattr__(key, value)\n\n        f_cls_attr['__getattribute__'] = init_getattr\n        f_cls_attr['__setattr__'] = init_setattr\n\n        cls = super().__new__(mcs, f_cls_name, f_cls_parents, f_cls_attr)\n        return cls\n\n\nclass Human(metaclass=MetaMemberControl):\n    __slots__ = ('_age', '_name')\n\n    def __init__(self, name, age):\n        self._name = name\n        self._age = age\n\n    def __getattribute__(self, item):\n        \"\"\"\n        is just for IDE recognize.\n        \"\"\"\n        return super().__getattribute__(item)\n\n    \"\"\" with MetaMemberControl then you don't have to write as following\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def age(self):\n        return self._age\n    \"\"\"\n\n\ndef test_demo():\n    human = Human('Carson', 27)\n    # human.age = 18  # you can't modify private members:_age  <-- this is defined by yourself.\n    # human.k = 18  # 'Human' object has no attribute 'k'  <-- system error.\n    age1 = human._age  # It's OK, although the IDE will show some warnings. (Access to a protected member _age of a class)\n\n    age2 = human.age  # It's OK! see below:\n    \"\"\"\n    if you do not define `__getattribute__` at the class of Human,\n    the IDE will show you: Unresolved attribute reference 'age' for class 'Human'\n    but it's ok on running since the MetaMemberControl will help you.\n    \"\"\"\n\n\nif __name__ == '__main__':\n    test_demo()\n\nThe metaclass is powerful, there are many things (such as monkey magic) you can do with it, but be careful this may only be known to you.metaclass",
                "The top answer is correct.The top answer is correctBut readers may be coming here searching answers about similarly named inner classes. They are present in popular libraries, such as Django and WTForms.DjangoWTFormsAs DavidW points out in the comments beneath this answer, these are library-specific features and are not to be confused with the advanced, unrelated Python language feature with a similar name.these are library-specific features and are not to be confused with the advanced, unrelated Python language feature with a similar namelibrary-specificPython languageRather, these are namespaces within classes' dicts. They are constructed using inner classes for sake of readability.In this example special field, abstract is visibly separate from fields of Author model.abstractfrom django.db import models\n\nclass Author(models.Model):\n    name = models.CharField(max_length=50)\n    email = models.EmailField()\n\n    class Meta:\n        abstract = True\nfrom django.db import models\n\nclass Author(models.Model):\n    name = models.CharField(max_length=50)\n    email = models.EmailField()\n\n    class Meta:\n        abstract = True\nAnother example is from the documentation for WTForms:WTFormsfrom wtforms.form import Form\nfrom wtforms.csrf.session import SessionCSRF\nfrom wtforms.fields import StringField\n\nclass MyBaseForm(Form):\n    class Meta:\n        csrf = True\n        csrf_class = SessionCSRF\n\n    name = StringField(\"name\")\nfrom wtforms.form import Form\nfrom wtforms.csrf.session import SessionCSRF\nfrom wtforms.fields import StringField\n\nclass MyBaseForm(Form):\n    class Meta:\n        csrf = True\n        csrf_class = SessionCSRF\n\n    name = StringField(\"name\")\nThis syntax does not get special treatment in the python programming language. Meta is not a keyword here, and does not trigger metaclass behavior. Rather, third-party library code in packages like Django and WTForms reads this property in the constructors of certain classes, and elsewhere.MetaDjangoWTFormsThe presence of these declarations modifies the behavior of the classes that have these declarations. For example, WTForms reads self.Meta.csrf to determine if the form needs a csrf field.WTFormsself.Meta.csrfcsrf",
                "In object-oriented programming, a metaclass is a class whose instances are classes. Just as an ordinary class defines the behavior of certain objects, a metaclass defines the behavior of certain class and their instances\nThe term metaclass simply means something used to create classes. In other words, it is the class of a class. The metaclass is used to create the class so like the object being an instance of a class, a class is an instance of a metaclass. In python classes are also considered objects.",
                "A class, in Python, is an object, and just like any other object, it is an instance of \"something\". This \"something\" is what is termed as a Metaclass. This metaclass is a special type of class that creates other class's objects. Hence, metaclass is responsible for making new classes. This allows the programmer to customize the way classes are generated.To create a metaclass, overriding of new() and init() methods is usually done. new() can be overridden to change the way objects are created, while init() can be overridden to change the way of initializing the object. Metaclass can be created by a number of ways. One of the ways is to use type() function. type() function, when called with 3 parameters, creates a metaclass. The parameters are :-newinitnewinit\nClass Name\nTuple having base classes inherited by class\nA dictionary having all class methods and class variables\nClass NameTuple having base classes inherited by classA dictionary having all class methods and class variablesAnother way of creating a metaclass comprises of 'metaclass' keyword. Define the metaclass as a simple class. In the parameters of inherited class, pass metaclass=metaclass_nameMetaclass can be specifically used in the following situations :-\nwhen a particular effect has to be applied to all the subclasses\nAutomatic change of class (on creation) is required\nBy API developers\nwhen a particular effect has to be applied to all the subclassesAutomatic change of class (on creation) is requiredBy API developers",
                "I saw an interesting use case for metaclasses in a package called classutilities. It checks if all class variables are in upper case format (it is convenient to have unified logic for configuration classes), and checks if there are no instance level methods in class.\nAnother interesting example for metaclases was deactivation of unittests based on complex conditions (checking values of multiple environmental variables).classutilities",
                "In Python, a metaclass is a subclass of a subclass that determines how a subclass behaves. A class is an instance of another metaclass. In Python, a class specifies how the class's instance will behave.Since metaclasses are in charge of class generation, you can\u00a0write your own custom metaclasses to change how classes are created by performing additional actions or injecting code. Custom metaclasses aren't always important, but they can be.",
                "look this:Python 3.10.0rc2 (tags/v3.10.0rc2:839d789, Sep  7 2021, 18:51:45) [MSC v.1929 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> class Object:\n...     pass\n... \n>>> class Meta(type):\n...     test = 'Worked!!!'\n...     def __repr__(self):\n...             return 'This is \"Meta\" metaclass'\n... \n>>> class ObjectWithMetaClass(metaclass=Meta):\n...     pass\n... \n>>> Object or type(Object())\n<class '__main__.Object'>\n>>> ObjectWithMetaClass or type(ObjectWithMetaClass())\nThis is \"Meta\" metaclass\n>>> Object.test\nAttributeError: ...\n>>> ObjectWithMetaClass.test\n'Worked!!!'\n>>> type(Object)\n<class 'type'>\n>>> type(ObjectWithMetaClass)\n<class '__main__.Meta'>\n>>> type(type(ObjectWithMetaClass))\n<class 'type'>\n>>> Object.__bases__\n(<class 'object'>,)\n>>> ObjectWithMetaClass.__bases__\n(<class 'object'>,)\n>>> type(ObjectWithMetaClass).__bases__\n(<class 'type'>,)\n>>> Object.__mro__\n(<class '__main__.Object'>, <class 'object'>)\n>>> ObjectWithMetaClass.__mro__\n(This is \"Meta\" metaclass, <class 'object'>)\n>>> \nPython 3.10.0rc2 (tags/v3.10.0rc2:839d789, Sep  7 2021, 18:51:45) [MSC v.1929 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> class Object:\n...     pass\n... \n>>> class Meta(type):\n...     test = 'Worked!!!'\n...     def __repr__(self):\n...             return 'This is \"Meta\" metaclass'\n... \n>>> class ObjectWithMetaClass(metaclass=Meta):\n...     pass\n... \n>>> Object or type(Object())\n<class '__main__.Object'>\n>>> ObjectWithMetaClass or type(ObjectWithMetaClass())\nThis is \"Meta\" metaclass\n>>> Object.test\nAttributeError: ...\n>>> ObjectWithMetaClass.test\n'Worked!!!'\n>>> type(Object)\n<class 'type'>\n>>> type(ObjectWithMetaClass)\n<class '__main__.Meta'>\n>>> type(type(ObjectWithMetaClass))\n<class 'type'>\n>>> Object.__bases__\n(<class 'object'>,)\n>>> ObjectWithMetaClass.__bases__\n(<class 'object'>,)\n>>> type(ObjectWithMetaClass).__bases__\n(<class 'type'>,)\n>>> Object.__mro__\n(<class '__main__.Object'>, <class 'object'>)\n>>> ObjectWithMetaClass.__mro__\n(This is \"Meta\" metaclass, <class 'object'>)\n>>> \nIn other words, when an object was not created (type of object), we looking MetaClass."
            ]
        },
        {
            "tag": "check_for_file",
            "patterns": [
                "How do I check whether a file exists without exceptions?",
                "how do i know if a file exists?",
                "how do i check if a file exists?",
                "how do i know whether a file exists or not?",
                "is there any way to check if a file exists without exceptions?",
                "how can you check if a file exists without exceptions?",
                "how do i check if a file exists without exceptions?",
                "how can i check whether a file exists without exception?"
            ],
            "responses": [
                "If the reason you're checking is so you can do something like if file_exists: open_it(), it's safer to use a try around the attempt to open it. Checking and then opening risks the file being deleted or moved or something between when you check and when you try to open it.if file_exists: open_it()tryIf you're not planning to open the file immediately, you can use os.path.isfileos.path.isfileos.path.isfile\nReturn True if path is an existing regular file. This follows symbolic links, so both islink() and isfile() can be true for the same path.\nReturn True if path is an existing regular file. This follows symbolic links, so both islink() and isfile() can be true for the same path.Trueislink()isfile()import os.path\nos.path.isfile(fname) \nimport os.path\nos.path.isfile(fname) \nif you need to be sure it's a file.Starting with Python 3.4, the pathlib module offers an object-oriented approach (backported to pathlib2 in Python 2.7):pathlib modulepathlibpathlib2from pathlib import Path\n\nmy_file = Path(\"/path/to/file\")\nif my_file.is_file():\n    # file exists\nfrom pathlib import Path\n\nmy_file = Path(\"/path/to/file\")\nif my_file.is_file():\n    # file exists\nTo check a directory, do:if my_file.is_dir():\n    # directory exists\nif my_file.is_dir():\n    # directory exists\nTo check whether a Path object exists independently of whether is it a file or directory, use exists():Pathexists()if my_file.exists():\n    # path exists\nif my_file.exists():\n    # path exists\nYou can also use resolve(strict=True) in a try block:resolve(strict=True)trytry:\n    my_abs_path = my_file.resolve(strict=True)\nexcept FileNotFoundError:\n    # doesn't exist\nelse:\n    # exists\ntry:\n    my_abs_path = my_file.resolve(strict=True)\nexcept FileNotFoundError:\n    # doesn't exist\nelse:\n    # exists\n",
                "Use os.path.exists to check both files and directories:os.path.existsos.path.existsimport os.path\nos.path.exists(file_path)\nimport os.path\nos.path.exists(file_path)\nUse os.path.isfile to check only files (note: follows symbolic links):os.path.isfileos.path.isfilesymbolic linksos.path.isfile(file_path)\nos.path.isfile(file_path)\n",
                "Unlike isfile(), exists() will return True for directories. So depending on if you want only plain files or also directories, you'll use isfile() or exists(). Here is some simple REPL output:isfile()isfile()exists()exists()Trueisfile()exists()REPL>>> os.path.isfile(\"/etc/password.txt\")\nTrue\n>>> os.path.isfile(\"/etc\")\nFalse\n>>> os.path.isfile(\"/does/not/exist\")\nFalse\n>>> os.path.exists(\"/etc/password.txt\")\nTrue\n>>> os.path.exists(\"/etc\")\nTrue\n>>> os.path.exists(\"/does/not/exist\")\nFalse\n>>> os.path.isfile(\"/etc/password.txt\")\nTrue\n>>> os.path.isfile(\"/etc\")\nFalse\n>>> os.path.isfile(\"/does/not/exist\")\nFalse\n>>> os.path.exists(\"/etc/password.txt\")\nTrue\n>>> os.path.exists(\"/etc\")\nTrue\n>>> os.path.exists(\"/does/not/exist\")\nFalse\n",
                "import os\n\nif os.path.isfile(filepath):\n   print(\"File exists\")\nimport os\n\nif os.path.isfile(filepath):\n   print(\"File exists\")\n",
                "Use os.path.isfile() with os.access():os.path.isfile()os.path.isfile()os.access()os.access()import os\n\nPATH = './file.txt'\nif os.path.isfile(PATH) and os.access(PATH, os.R_OK):\n    print(\"File exists and is readable\")\nelse:\n    print(\"Either the file is missing or not readable\")\nimport os\n\nPATH = './file.txt'\nif os.path.isfile(PATH) and os.access(PATH, os.R_OK):\n    print(\"File exists and is readable\")\nelse:\n    print(\"Either the file is missing or not readable\")\n",
                "import os\nos.path.exists(path) # Returns whether the path (directory or file) exists or not\nos.path.isfile(path) # Returns whether the file exists or not\nimport os\nos.path.exists(path) # Returns whether the path (directory or file) exists or not\nos.path.isfile(path) # Returns whether the file exists or not\n",
                "Although almost every possible way has been listed in (at least one of) the existing answers (e.g. Python 3.4 specific stuff was added), I'll try to group everything together.Python 3.4Note: every piece of Python standard library code that I'm going to post, belongs to version 3.5.3.NotePython3.5.3Problem statement:Problem statement\nCheck file (arguable: also folder (\"special\" file) ?) existence\n\nDon't use try / except / else / finally blocks\n\nCheck file (arguable: also folder (\"special\" file) ?) existence\nCheck file (arguable: also folder (\"special\" file) ?) existenceDon't use try / except / else / finally blocks\nDon't use try / except / else / finally blockstrytryexceptexceptelseelsefinallyfinallyPossible solutions:Possible solutions1. [Python.Docs]: os.path.exists(path)[Python.Docs]: os.path.exists(path)Also check other function family members like os.path.isfile, os.path.isdir, os.path.lexists for slightly different behaviors:os.path.isfileos.path.isfileos.path.isdiros.path.lexists\nReturn True if path refers to an existing path or an open file descriptor. Returns False for broken symbolic links. On some platforms, this function may return False if permission is not granted to execute os.stat() on the requested file, even if the path physically exists.\nReturn True if path refers to an existing path or an open file descriptor. Returns False for broken symbolic links. On some platforms, this function may return False if permission is not granted to execute os.stat() on the requested file, even if the path physically exists.TruepathFalseFalseos.stat()pathAll good, but if following the import tree:\nos.path - posixpath.py (ntpath.py)\n\ngenericpath.py - line ~20+\ndef exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return True\n\n\n\n\nos.path - posixpath.py (ntpath.py)\n\ngenericpath.py - line ~20+\ndef exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return True\n\n\n\nos.path - posixpath.py (ntpath.py)os.pathposixpath.pyntpath.py\ngenericpath.py - line ~20+\ndef exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return True\n\n\ngenericpath.py - line ~20+\ndef exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return True\n\ngenericpath.py - line ~20+genericpath.py~20+def exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return True\ndef exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return True\nit's just a try / except block around [Python.Docs]: os.stat(path, *, dir_fd=None, follow_symlinks=True). So, your code is try / except free, but lower in the framestack there's (at least) one such block. This also applies to other functions (including os.path.isfile).trytryexceptexcept[Python.Docs]: os.stat(path, *, dir_fd=None, follow_symlinks=True)trytryexceptexceptoneincludingos.path.isfile1.1. [Python.Docs]: pathlib - Path.is_file()[Python.Docs]: pathlib - Path.is_file()\nIt's a fancier (and more [Wiktionary]: Pythonic) way of handling paths, but\n\nUnder the hood, it does exactly the same thing (pathlib.py - line ~1330):\ndef is_file(self):\n    \"\"\"\n    Whether this path is a regular file (also True for symlinks pointing\n        to regular files).\n    \"\"\"\n    try:\n        return S_ISREG(self.stat().st_mode)\n    except OSError as e:\n        if e.errno not in (ENOENT, ENOTDIR):\n            raise\n        # Path doesn't exist or is a broken symlink\n        # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n        return False\n\n\nIt's a fancier (and more [Wiktionary]: Pythonic) way of handling paths, but\nIt's a fancier (and more [Wiktionary]: Pythonic) way of handling paths, but[Wiktionary]: PythonicbutUnder the hood, it does exactly the same thing (pathlib.py - line ~1330):\ndef is_file(self):\n    \"\"\"\n    Whether this path is a regular file (also True for symlinks pointing\n        to regular files).\n    \"\"\"\n    try:\n        return S_ISREG(self.stat().st_mode)\n    except OSError as e:\n        if e.errno not in (ENOENT, ENOTDIR):\n            raise\n        # Path doesn't exist or is a broken symlink\n        # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n        return False\n\nUnder the hood, it does exactly the same thing (pathlib.py - line ~1330):exactlypathlib.py~1330def is_file(self):\n    \"\"\"\n    Whether this path is a regular file (also True for symlinks pointing\n        to regular files).\n    \"\"\"\n    try:\n        return S_ISREG(self.stat().st_mode)\n    except OSError as e:\n        if e.errno not in (ENOENT, ENOTDIR):\n            raise\n        # Path doesn't exist or is a broken symlink\n        # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n        return False\ndef is_file(self):\n    \"\"\"\n    Whether this path is a regular file (also True for symlinks pointing\n        to regular files).\n    \"\"\"\n    try:\n        return S_ISREG(self.stat().st_mode)\n    except OSError as e:\n        if e.errno not in (ENOENT, ENOTDIR):\n            raise\n        # Path doesn't exist or is a broken symlink\n        # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n        return False\n2. [Python.Docs]: With Statement Context Managers[Python.Docs]: With Statement Context ManagersEither:\nCreate one:\nclass Swallow:  # Dummy example\n    swallowed_exceptions = (FileNotFoundError,)\n\n    def __enter__(self):\n        print(\"Entering...\")\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        print(\"Exiting:\", exc_type, exc_value, exc_traceback)\n        # Only swallow FileNotFoundError (not e.g. TypeError - if the user passes a wrong argument like None or float or ...)\n        return exc_type in Swallow.swallowed_exceptions\n\n\nAnd its usage - I'll replicate the os.path.isfile behavior (note that this is just for demonstrating purposes, do not attempt to write such code for production):\nimport os\nimport stat\n\n\ndef isfile_seaman(path):  # Dummy func\n    result = False\n    with Swallow():\n        result = stat.S_ISREG(os.stat(path).st_mode)\n    return result\n\n\n\n\nUse [Python.Docs]: contextlib.suppress(*exceptions) - which was specifically designed for selectively suppressing exceptions\n\nCreate one:\nclass Swallow:  # Dummy example\n    swallowed_exceptions = (FileNotFoundError,)\n\n    def __enter__(self):\n        print(\"Entering...\")\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        print(\"Exiting:\", exc_type, exc_value, exc_traceback)\n        # Only swallow FileNotFoundError (not e.g. TypeError - if the user passes a wrong argument like None or float or ...)\n        return exc_type in Swallow.swallowed_exceptions\n\n\nAnd its usage - I'll replicate the os.path.isfile behavior (note that this is just for demonstrating purposes, do not attempt to write such code for production):\nimport os\nimport stat\n\n\ndef isfile_seaman(path):  # Dummy func\n    result = False\n    with Swallow():\n        result = stat.S_ISREG(os.stat(path).st_mode)\n    return result\n\n\n\nCreate one:class Swallow:  # Dummy example\n    swallowed_exceptions = (FileNotFoundError,)\n\n    def __enter__(self):\n        print(\"Entering...\")\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        print(\"Exiting:\", exc_type, exc_value, exc_traceback)\n        # Only swallow FileNotFoundError (not e.g. TypeError - if the user passes a wrong argument like None or float or ...)\n        return exc_type in Swallow.swallowed_exceptions\nclass Swallow:  # Dummy example\n    swallowed_exceptions = (FileNotFoundError,)\n\n    def __enter__(self):\n        print(\"Entering...\")\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        print(\"Exiting:\", exc_type, exc_value, exc_traceback)\n        # Only swallow FileNotFoundError (not e.g. TypeError - if the user passes a wrong argument like None or float or ...)\n        return exc_type in Swallow.swallowed_exceptions\n\nAnd its usage - I'll replicate the os.path.isfile behavior (note that this is just for demonstrating purposes, do not attempt to write such code for production):\nimport os\nimport stat\n\n\ndef isfile_seaman(path):  # Dummy func\n    result = False\n    with Swallow():\n        result = stat.S_ISREG(os.stat(path).st_mode)\n    return result\n\n\nAnd its usage - I'll replicate the os.path.isfile behavior (note that this is just for demonstrating purposes, do not attempt to write such code for production):\nimport os\nimport stat\n\n\ndef isfile_seaman(path):  # Dummy func\n    result = False\n    with Swallow():\n        result = stat.S_ISREG(os.stat(path).st_mode)\n    return result\n\nAnd its usage - I'll replicate the os.path.isfile behavior (note that this is just for demonstrating purposes, do not attempt to write such code for production):os.path.isfilenotproductionimport os\nimport stat\n\n\ndef isfile_seaman(path):  # Dummy func\n    result = False\n    with Swallow():\n        result = stat.S_ISREG(os.stat(path).st_mode)\n    return result\nimport os\nimport stat\n\n\ndef isfile_seaman(path):  # Dummy func\n    result = False\n    with Swallow():\n        result = stat.S_ISREG(os.stat(path).st_mode)\n    return result\nUse [Python.Docs]: contextlib.suppress(*exceptions) - which was specifically designed for selectively suppressing exceptions\nUse [Python.Docs]: contextlib.suppress(*exceptions) - which was specifically designed for selectively suppressing exceptions[Python.Docs]: contextlib.suppress(*exceptions)specificallyBut, they seem to be wrappers over try / except / else / finally blocks, as [Python.Docs]: Compound statements - The with statement states:trytryexceptexceptelseelsefinallyfinally[Python.Docs]: Compound statements - The with statement\nThis allows common try...except...finally usage patterns to be encapsulated for convenient reuse.\nThis allows common try...except...finally usage patterns to be encapsulated for convenient reuse.tryexceptfinally3. Filesystem traversal functionsSearch the results for matching item(s):\n[Python.Docs]: os.listdir(path='.') (or [Python.Docs]: os.scandir(path='.') on Python v3.5+, backport: [PyPI]: scandir)\n\nUnder the hood, both use:\n\nNix: [Man7]: OPENDIR(3) / [Man7]: READDIR(3) / [Man7]: CLOSEDIR(3)\n\nWin: [MS.Learn]: FindFirstFileW function (fileapi.h) / [MS.Learn]: FindNextFileW function (fileapi.h) / [MS.Learn]: FindClose function (fileapi.h)\n\n\nvia [GitHub]: python/cpython - (main) cpython/Modules/posixmodule.c\n\n\n\nUsing scandir() instead of listdir() can significantly increase the performance of code that also needs file type or file attribute information, because os.DirEntry objects expose this information if the operating system provides it when scanning a directory. All os.DirEntry methods may perform a system call, but is_dir() and is_file() usually only require a system call for symbolic links; os.DirEntry.stat() always requires a system call on Unix, but only requires one for symbolic links on Windows.\n\n\n[Python.Docs]: os.walk(top, topdown=True, onerror=None, followlinks=False)\n\nUses os.listdir (os.scandir when available)\n\n\n[Python.Docs]: glob.iglob(pathname, *, root_dir=None, dir_fd=None, recursive=False, include_hidden=False) (or its predecessor: glob.glob)\n\nDoesn't seem a traversing function per se (at least in some cases), but it still uses os.listdir\n\n\n[Python.Docs]: os.listdir(path='.') (or [Python.Docs]: os.scandir(path='.') on Python v3.5+, backport: [PyPI]: scandir)\n\nUnder the hood, both use:\n\nNix: [Man7]: OPENDIR(3) / [Man7]: READDIR(3) / [Man7]: CLOSEDIR(3)\n\nWin: [MS.Learn]: FindFirstFileW function (fileapi.h) / [MS.Learn]: FindNextFileW function (fileapi.h) / [MS.Learn]: FindClose function (fileapi.h)\n\n\nvia [GitHub]: python/cpython - (main) cpython/Modules/posixmodule.c\n\n\n\nUsing scandir() instead of listdir() can significantly increase the performance of code that also needs file type or file attribute information, because os.DirEntry objects expose this information if the operating system provides it when scanning a directory. All os.DirEntry methods may perform a system call, but is_dir() and is_file() usually only require a system call for symbolic links; os.DirEntry.stat() always requires a system call on Unix, but only requires one for symbolic links on Windows.\n\n[Python.Docs]: os.listdir(path='.') (or [Python.Docs]: os.scandir(path='.') on Python v3.5+, backport: [PyPI]: scandir)[Python.Docs]: os.listdir(path='.')[Python.Docs]: os.scandir(path='.')Python v3.53.5[PyPI]: scandir\nUnder the hood, both use:\n\nNix: [Man7]: OPENDIR(3) / [Man7]: READDIR(3) / [Man7]: CLOSEDIR(3)\n\nWin: [MS.Learn]: FindFirstFileW function (fileapi.h) / [MS.Learn]: FindNextFileW function (fileapi.h) / [MS.Learn]: FindClose function (fileapi.h)\n\n\nvia [GitHub]: python/cpython - (main) cpython/Modules/posixmodule.c\n\nUnder the hood, both use:\n\nNix: [Man7]: OPENDIR(3) / [Man7]: READDIR(3) / [Man7]: CLOSEDIR(3)\n\nWin: [MS.Learn]: FindFirstFileW function (fileapi.h) / [MS.Learn]: FindNextFileW function (fileapi.h) / [MS.Learn]: FindClose function (fileapi.h)\n\n\nvia [GitHub]: python/cpython - (main) cpython/Modules/posixmodule.c\nUnder the hood, both use:\nNix: [Man7]: OPENDIR(3) / [Man7]: READDIR(3) / [Man7]: CLOSEDIR(3)\n\nWin: [MS.Learn]: FindFirstFileW function (fileapi.h) / [MS.Learn]: FindNextFileW function (fileapi.h) / [MS.Learn]: FindClose function (fileapi.h)\n\nNix: [Man7]: OPENDIR(3) / [Man7]: READDIR(3) / [Man7]: CLOSEDIR(3)\nNix: [Man7]: OPENDIR(3) / [Man7]: READDIR(3) / [Man7]: CLOSEDIR(3)Nix[Man7]: OPENDIR(3)[Man7]: READDIR(3)[Man7]: CLOSEDIR(3)Win: [MS.Learn]: FindFirstFileW function (fileapi.h) / [MS.Learn]: FindNextFileW function (fileapi.h) / [MS.Learn]: FindClose function (fileapi.h)\nWin: [MS.Learn]: FindFirstFileW function (fileapi.h) / [MS.Learn]: FindNextFileW function (fileapi.h) / [MS.Learn]: FindClose function (fileapi.h)Win[MS.Learn]: FindFirstFileW function (fileapi.h)[MS.Learn]: FindNextFileW function (fileapi.h)[MS.Learn]: FindClose function (fileapi.h)via [GitHub]: python/cpython - (main) cpython/Modules/posixmodule.c[GitHub]: python/cpython - (main) cpython/Modules/posixmodule.c\nUsing scandir() instead of listdir() can significantly increase the performance of code that also needs file type or file attribute information, because os.DirEntry objects expose this information if the operating system provides it when scanning a directory. All os.DirEntry methods may perform a system call, but is_dir() and is_file() usually only require a system call for symbolic links; os.DirEntry.stat() always requires a system call on Unix, but only requires one for symbolic links on Windows.\nUsing scandir() instead of listdir() can significantly increase the performance of code that also needs file type or file attribute information, because os.DirEntry objects expose this information if the operating system provides it when scanning a directory. All os.DirEntry methods may perform a system call, but is_dir() and is_file() usually only require a system call for symbolic links; os.DirEntry.stat() always requires a system call on Unix, but only requires one for symbolic links on Windows.scandir()listdir()os.DirEntryos.DirEntryis_dir()is_file()os.DirEntry.stat()[Python.Docs]: os.walk(top, topdown=True, onerror=None, followlinks=False)\n\nUses os.listdir (os.scandir when available)\n\n[Python.Docs]: os.walk(top, topdown=True, onerror=None, followlinks=False)[Python.Docs]: os.walk(top, topdown=True, onerror=None, followlinks=False)\nUses os.listdir (os.scandir when available)\nUses os.listdir (os.scandir when available)os.listdiros.scandir[Python.Docs]: glob.iglob(pathname, *, root_dir=None, dir_fd=None, recursive=False, include_hidden=False) (or its predecessor: glob.glob)\n\nDoesn't seem a traversing function per se (at least in some cases), but it still uses os.listdir\n\n[Python.Docs]: glob.iglob(pathname, *, root_dir=None, dir_fd=None, recursive=False, include_hidden=False) (or its predecessor: glob.glob)[Python.Docs]: glob.iglob(pathname, *, root_dir=None, dir_fd=None, recursive=False, include_hidden=False)glob.glob\nDoesn't seem a traversing function per se (at least in some cases), but it still uses os.listdir\nDoesn't seem a traversing function per se (at least in some cases), but it still uses os.listdirper seos.listdirSince these iterate over folders, (in most of the cases) they are inefficient for our problem (there are exceptions, like non wildcarded globbing - as @ShadowRanger pointed out), so I'm not going to insist on them. Not to mention that in some cases, filename processing might be required.4. [Python.Docs]: os.access(path, mode, *, dir_fd=None, effective_ids=False, follow_symlinks=True)[Python.Docs]: os.access(path, mode, *, dir_fd=None, effective_ids=False, follow_symlinks=True)Its behavior is close to os.path.exists (actually it's wider, mainly because of the 2nd argument).os.path.existsnd\nUser permissions might restrict the file \"visibility\" as the doc states:\n\n... test if the invoking user has the specified access to path. mode should be F_OK to test the existence of path...\n\n\nSecurity considerations:\n\nUsing access() to check if a user is authorized to e.g. open a file before actually doing so using open() creates a security hole, because the user might exploit the short time interval between checking and opening the file to manipulate it.\n\n\nUser permissions might restrict the file \"visibility\" as the doc states:\n\n... test if the invoking user has the specified access to path. mode should be F_OK to test the existence of path...\n\nUser permissions might restrict the file \"visibility\" as the doc states:User permissionsdoc\n... test if the invoking user has the specified access to path. mode should be F_OK to test the existence of path...\n... test if the invoking user has the specified access to path. mode should be F_OK to test the existence of path...pathmodeF_OKSecurity considerations:\n\nUsing access() to check if a user is authorized to e.g. open a file before actually doing so using open() creates a security hole, because the user might exploit the short time interval between checking and opening the file to manipulate it.\n\nSecurity considerations:Security considerations\nUsing access() to check if a user is authorized to e.g. open a file before actually doing so using open() creates a security hole, because the user might exploit the short time interval between checking and opening the file to manipulate it.\nUsing access() to check if a user is authorized to e.g. open a file before actually doing so using open() creates a security hole, because the user might exploit the short time interval between checking and opening the file to manipulate it.access()open()os.access(\"/tmp\", os.F_OK)\nos.access(\"/tmp\", os.F_OK)\nSince I also work in C, I use this method as well because under the hood, it calls native APIs (again, via \"${PYTHON_SRC_DIR}/Modules/posixmodule.c\"), but it also opens a gate for possible user errors, and it's not as Pythonic as other variants. So, don't use it unless you know what you're doing:Cnative APIsAPI${PYTHON_SRC_DIR}/Modules/posixmodule.cuser errorsPython\nNix: [Man7]: ACCESS(2)\n\nWarning: Using these calls to check if a user is authorized to, for example, open a file before actually doing so using open(2) creates a security hole, because the user might exploit the short time interval between checking and opening the file to manipulate it. For this reason, the use of this system call should be avoided.\n\n\nWin: [MS.Learn]: GetFileAttributesW function (fileapi.h)\n\nNix: [Man7]: ACCESS(2)\n\nWarning: Using these calls to check if a user is authorized to, for example, open a file before actually doing so using open(2) creates a security hole, because the user might exploit the short time interval between checking and opening the file to manipulate it. For this reason, the use of this system call should be avoided.\n\nNix: [Man7]: ACCESS(2)Nix[Man7]: ACCESS(2)\nWarning: Using these calls to check if a user is authorized to, for example, open a file before actually doing so using open(2) creates a security hole, because the user might exploit the short time interval between checking and opening the file to manipulate it. For this reason, the use of this system call should be avoided.\nWarning: Using these calls to check if a user is authorized to, for example, open a file before actually doing so using open(2) creates a security hole, because the user might exploit the short time interval between checking and opening the file to manipulate it. For this reason, the use of this system call should be avoided.open(2)For this reason, the use of this system call should be avoidedWin: [MS.Learn]: GetFileAttributesW function (fileapi.h)\nWin: [MS.Learn]: GetFileAttributesW function (fileapi.h)Win[MS.Learn]: GetFileAttributesW function (fileapi.h)As seen, this approach is highly discouraged (especially on Nix).this approach is highly discouragedNixNote: calling native APIs is also possible via [Python.Docs]: ctypes - A foreign function library for Python, but in most cases it's more complicated. Before working with CTypes, check [SO]: C function called from Python via ctypes returns incorrect value (@CristiFati's answer) out.NoteAPI[Python.Docs]: ctypes - A foreign function library for PythonCTypes[SO]: C function called from Python via ctypes returns incorrect value (@CristiFati's answer)(Win specific): since vcruntime###.dll (msvcr###.dll for older VStudio versions - I'm going to refer to it as UCRT) exports a [MS.Learn]: _access, _waccess function family as well, here's an example (note that the recommended [Python.Docs]: msvcrt - Useful routines from the MS VC++ runtime doesn't export them):WinWinvcruntime###.dllmsvcr###.dllVStudioUCRTUCRT[MS.Learn]: _access, _waccess[Python.Docs]: msvcrt - Useful routines from the MS VC++ runtime\nPython 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import ctypes as cts, os\n>>> cts.CDLL(\"msvcrt\")._waccess(u\"C:\\\\Windows\\\\Temp\", os.F_OK)\n0\n>>> cts.CDLL(\"msvcrt\")._waccess(u\"C:\\\\Windows\\\\Temp.notexist\", os.F_OK)\n-1\n\nPython 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import ctypes as cts, os\n>>> cts.CDLL(\"msvcrt\")._waccess(u\"C:\\\\Windows\\\\Temp\", os.F_OK)\n0\n>>> cts.CDLL(\"msvcrt\")._waccess(u\"C:\\\\Windows\\\\Temp.notexist\", os.F_OK)\n-1\nPython 3.5.3 (v3.5.3:1880cb95a742, Jan 16 2017, 16:02:32) [MSC v.1900 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import ctypes as cts, os\n>>> cts.CDLL(\"msvcrt\")._waccess(u\"C:\\\\Windows\\\\Temp\", os.F_OK)\n0\n>>> cts.CDLL(\"msvcrt\")._waccess(u\"C:\\\\Windows\\\\Temp.notexist\", os.F_OK)\n-1\nNotes:Notes\nAlthough it's not a good practice, I'm using os.F_OK in the call, but that's just for clarity (its value is 0)\n\nI'm using _waccess so that the same code works on Python 3 and Python 2 (in spite of [Wikipedia]: Unicode related differences between them - [SO]: Passing utf-16 string to a Windows function (@CristiFati's answer))\n\nAlthough this targets a very specific area, it was not mentioned in any of the previous answers\n\nAlthough it's not a good practice, I'm using os.F_OK in the call, but that's just for clarity (its value is 0)\nAlthough it's not a good practice, I'm using os.F_OK in the call, but that's just for clarity (its value is 0)os.F_OK0I'm using _waccess so that the same code works on Python 3 and Python 2 (in spite of [Wikipedia]: Unicode related differences between them - [SO]: Passing utf-16 string to a Windows function (@CristiFati's answer))\nI'm using _waccess so that the same code works on Python 3 and Python 2 (in spite of [Wikipedia]: Unicode related differences between them - [SO]: Passing utf-16 string to a Windows function (@CristiFati's answer))_waccessPython 3Python 2[Wikipedia]: Unicode[SO]: Passing utf-16 string to a Windows function (@CristiFati's answer)Although this targets a very specific area, it was not mentioned in any of the previous answers\nAlthough this targets a very specific area, it was not mentioned in any of the previous answersit was not mentioned in any of the previous answersThe Linux (Ubuntu ([Wikipedia]: Ubuntu version history) 16 x86_64 (pc064)) counterpart as well:LinuxUbuntu[Wikipedia]: Ubuntu version history16 x86_64pc064\nPython 3.5.2 (default, Nov 17 2016, 17:05:23)\n[GCC 5.4.0 20160609] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import ctypes as cts, os\n>>> cts.CDLL(\"/lib/x86_64-linux-gnu/libc.so.6\").access(b\"/tmp\", os.F_OK)\n0\n>>> cts.CDLL(\"/lib/x86_64-linux-gnu/libc.so.6\").access(b\"/tmp.notexist\", os.F_OK)\n-1\n\nPython 3.5.2 (default, Nov 17 2016, 17:05:23)\n[GCC 5.4.0 20160609] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import ctypes as cts, os\n>>> cts.CDLL(\"/lib/x86_64-linux-gnu/libc.so.6\").access(b\"/tmp\", os.F_OK)\n0\n>>> cts.CDLL(\"/lib/x86_64-linux-gnu/libc.so.6\").access(b\"/tmp.notexist\", os.F_OK)\n-1\nPython 3.5.2 (default, Nov 17 2016, 17:05:23)\n[GCC 5.4.0 20160609] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import ctypes as cts, os\n>>> cts.CDLL(\"/lib/x86_64-linux-gnu/libc.so.6\").access(b\"/tmp\", os.F_OK)\n0\n>>> cts.CDLL(\"/lib/x86_64-linux-gnu/libc.so.6\").access(b\"/tmp.notexist\", os.F_OK)\n-1\nNotes:Notes\nInstead hardcoding libc.so (LibC)'s path (\"/lib/x86_64-linux-gnu/libc.so.6\") which may (and most likely, will) vary across systems, None (or the empty string) can be passed to CDLL constructor (ctypes.CDLL(None).access(b\"/tmp\", os.F_OK)). According to [Man7]: DLOPEN(3):\n\nIf filename is NULL, then the returned handle is for the main\nprogram.  When given to dlsym(3), this handle causes a search for a\nsymbol in the main program, followed by all shared objects loaded at\nprogram startup, and then all shared objects loaded by dlopen() with\nthe flag RTLD_GLOBAL.\n\n\nMain (current) program (python) is linked against LibC, so its symbols (including access) will be loaded\n\nThis has to be handled with care, since functions like main, Py_Main and (all the) others are available; calling them could have disastrous effects (on the current program)\n\nThis doesn't also apply to Windows (but that's not such a big deal, since UCRT is located in \"%SystemRoot%\\System32\" which is in %PATH% by default). I wanted to take things further and replicate this behavior on Windows (and submit a patch), but as it turns out, [MS.Learn]: GetProcAddress function (libloaderapi.h) only \"sees\" exported symbols, so unless someone declares the functions in the main executable as __declspec(dllexport) (why on Earth the common person would do that?), the main program is loadable, but it is pretty much unusable\n\nInstead hardcoding libc.so (LibC)'s path (\"/lib/x86_64-linux-gnu/libc.so.6\") which may (and most likely, will) vary across systems, None (or the empty string) can be passed to CDLL constructor (ctypes.CDLL(None).access(b\"/tmp\", os.F_OK)). According to [Man7]: DLOPEN(3):\n\nIf filename is NULL, then the returned handle is for the main\nprogram.  When given to dlsym(3), this handle causes a search for a\nsymbol in the main program, followed by all shared objects loaded at\nprogram startup, and then all shared objects loaded by dlopen() with\nthe flag RTLD_GLOBAL.\n\nInstead hardcoding libc.so (LibC)'s path (\"/lib/x86_64-linux-gnu/libc.so.6\") which may (and most likely, will) vary across systems, None (or the empty string) can be passed to CDLL constructor (ctypes.CDLL(None).access(b\"/tmp\", os.F_OK)). According to [Man7]: DLOPEN(3):libc.soLibC/lib/x86_64-linux-gnu/libc.so.6NoneCDLLctypes.CDLL(None).access(b\"/tmp\", os.F_OK)ctypes.CDLL(None).access(b\"/tmp\", os.F_OK)[Man7]: DLOPEN(3)\nIf filename is NULL, then the returned handle is for the main\nprogram.  When given to dlsym(3), this handle causes a search for a\nsymbol in the main program, followed by all shared objects loaded at\nprogram startup, and then all shared objects loaded by dlopen() with\nthe flag RTLD_GLOBAL.\nIf filename is NULL, then the returned handle is for the main\nprogram.  When given to dlsym(3), this handle causes a search for a\nsymbol in the main program, followed by all shared objects loaded at\nprogram startup, and then all shared objects loaded by dlopen() with\nthe flag RTLD_GLOBAL.filenamedlsym(3)dlopenRTLD_GLOBALMain (current) program (python) is linked against LibC, so its symbols (including access) will be loaded\nMain (current) program (python) is linked against LibC, so its symbols (including access) will be loadedpythonLibCaccessThis has to be handled with care, since functions like main, Py_Main and (all the) others are available; calling them could have disastrous effects (on the current program)\nThis has to be handled with care, since functions like main, Py_Main and (all the) others are available; calling them could have disastrous effects (on the current program)mainPy_MainThis doesn't also apply to Windows (but that's not such a big deal, since UCRT is located in \"%SystemRoot%\\System32\" which is in %PATH% by default). I wanted to take things further and replicate this behavior on Windows (and submit a patch), but as it turns out, [MS.Learn]: GetProcAddress function (libloaderapi.h) only \"sees\" exported symbols, so unless someone declares the functions in the main executable as __declspec(dllexport) (why on Earth the common person would do that?), the main program is loadable, but it is pretty much unusable\nThis doesn't also apply to Windows (but that's not such a big deal, since UCRT is located in \"%SystemRoot%\\System32\" which is in %PATH% by default). I wanted to take things further and replicate this behavior on Windows (and submit a patch), but as it turns out, [MS.Learn]: GetProcAddress function (libloaderapi.h) only \"sees\" exported symbols, so unless someone declares the functions in the main executable as __declspec(dllexport) (why on Earth the common person would do that?), the main program is loadable, but it is pretty much unusableWindowsUCRT%SystemRoot%\\System32%PATH%Windows[MS.Learn]: GetProcAddress function (libloaderapi.h)exported__declspec(dllexport)5. 3rd-party modules with filesystem capabilitiesrdMost likely, will rely on one of the ways above (maybe with slight customizations). One example would be (again, Win specific) [GitHub]: mhammond/pywin32 - Python for Windows (pywin32) Extensions, which is a Python wrapper over WinAPIs.Win[GitHub]: mhammond/pywin32 - Python for Windows (pywin32) ExtensionsPythonWinAPIBut, since this is more like a workaround, I'm stopping here.6. SysAdmin approachSysAdminI consider this a (lame) workaround (gainarie): use Python as a wrapper to execute shell commands:gainariePython\nWin:\n\n(py35x64_test) [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q000082831]> \"e:\\Work\\Dev\\VEnvs\\py35x64_test\\Scripts\\python.exe\" -c \"import os; print(os.system('dir /b \\\"C:\\\\Windows\\\\Temp\\\" > nul 2>&1'))\"\n0\n\n(py35x64_test) [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q000082831]> \"e:\\Work\\Dev\\VEnvs\\py35x64_test\\Scripts\\python.exe\" -c \"import os; print(os.system('dir /b \\\"C:\\\\Windows\\\\Temp.notexist\\\" > nul 2>&1'))\"\n1\n\n\n\nNix ([Wikipedia]: Unix-like) - Ubuntu:\n\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q000082831]> python3 -c \"import os; print(os.system('ls \\\"/tmp\\\" > /dev/null 2>&1'))\"\n0\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q000082831]> python3 -c \"import os; print(os.system('ls \\\"/tmp.notexist\\\" > /dev/null 2>&1'))\"\n512\n\n\n\nWin:\n\n(py35x64_test) [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q000082831]> \"e:\\Work\\Dev\\VEnvs\\py35x64_test\\Scripts\\python.exe\" -c \"import os; print(os.system('dir /b \\\"C:\\\\Windows\\\\Temp\\\" > nul 2>&1'))\"\n0\n\n(py35x64_test) [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q000082831]> \"e:\\Work\\Dev\\VEnvs\\py35x64_test\\Scripts\\python.exe\" -c \"import os; print(os.system('dir /b \\\"C:\\\\Windows\\\\Temp.notexist\\\" > nul 2>&1'))\"\n1\n\n\nWin:Win\n(py35x64_test) [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q000082831]> \"e:\\Work\\Dev\\VEnvs\\py35x64_test\\Scripts\\python.exe\" -c \"import os; print(os.system('dir /b \\\"C:\\\\Windows\\\\Temp\\\" > nul 2>&1'))\"\n0\n\n(py35x64_test) [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q000082831]> \"e:\\Work\\Dev\\VEnvs\\py35x64_test\\Scripts\\python.exe\" -c \"import os; print(os.system('dir /b \\\"C:\\\\Windows\\\\Temp.notexist\\\" > nul 2>&1'))\"\n1\n\n(py35x64_test) [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q000082831]> \"e:\\Work\\Dev\\VEnvs\\py35x64_test\\Scripts\\python.exe\" -c \"import os; print(os.system('dir /b \\\"C:\\\\Windows\\\\Temp\\\" > nul 2>&1'))\"\n0\n\n(py35x64_test) [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q000082831]> \"e:\\Work\\Dev\\VEnvs\\py35x64_test\\Scripts\\python.exe\" -c \"import os; print(os.system('dir /b \\\"C:\\\\Windows\\\\Temp.notexist\\\" > nul 2>&1'))\"\n1\n(py35x64_test) [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q000082831]> \"e:\\Work\\Dev\\VEnvs\\py35x64_test\\Scripts\\python.exe\" -c \"import os; print(os.system('dir /b \\\"C:\\\\Windows\\\\Temp\\\" > nul 2>&1'))\"\n0\n\n(py35x64_test) [cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q000082831]> \"e:\\Work\\Dev\\VEnvs\\py35x64_test\\Scripts\\python.exe\" -c \"import os; print(os.system('dir /b \\\"C:\\\\Windows\\\\Temp.notexist\\\" > nul 2>&1'))\"\n1\nNix ([Wikipedia]: Unix-like) - Ubuntu:\n\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q000082831]> python3 -c \"import os; print(os.system('ls \\\"/tmp\\\" > /dev/null 2>&1'))\"\n0\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q000082831]> python3 -c \"import os; print(os.system('ls \\\"/tmp.notexist\\\" > /dev/null 2>&1'))\"\n512\n\n\nNix ([Wikipedia]: Unix-like) - Ubuntu:Nix[Wikipedia]: Unix-likeUbuntu\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q000082831]> python3 -c \"import os; print(os.system('ls \\\"/tmp\\\" > /dev/null 2>&1'))\"\n0\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q000082831]> python3 -c \"import os; print(os.system('ls \\\"/tmp.notexist\\\" > /dev/null 2>&1'))\"\n512\n\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q000082831]> python3 -c \"import os; print(os.system('ls \\\"/tmp\\\" > /dev/null 2>&1'))\"\n0\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q000082831]> python3 -c \"import os; print(os.system('ls \\\"/tmp.notexist\\\" > /dev/null 2>&1'))\"\n512\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q000082831]> python3 -c \"import os; print(os.system('ls \\\"/tmp\\\" > /dev/null 2>&1'))\"\n0\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q000082831]> python3 -c \"import os; print(os.system('ls \\\"/tmp.notexist\\\" > /dev/null 2>&1'))\"\n512\nBottom line:\nDo use try / except / else / finally blocks, because they can prevent you running into a series of nasty problems\n\nA possible counterexample that I can think of, is performance: such blocks are costly, so try not to place them in code that it's supposed to run hundreds of thousands times per second (but since (in most cases) it involves disk access, it won't be the case)\n\nDo use try / except / else / finally blocks, because they can prevent you running into a series of nasty problems\nDo use try / except / else / finally blocks, because they can prevent you running into a series of nasty problemsDotrytryexceptexceptelseelsefinallyfinallyA possible counterexample that I can think of, is performance: such blocks are costly, so try not to place them in code that it's supposed to run hundreds of thousands times per second (but since (in most cases) it involves disk access, it won't be the case)\nA possible counterexample that I can think of, is performance: such blocks are costly, so try not to place them in code that it's supposed to run hundreds of thousands times per second (but since (in most cases) it involves disk access, it won't be the case)",
                "Python 3.4+ has an object-oriented path module: pathlib.  Using this new module, you can check whether a file exists like this:Python 3.4+pathlibpathlibimport pathlib\np = pathlib.Path('path/to/file')\nif p.is_file():  # or p.is_dir() to see if it is a directory\n    # do stuff\nimport pathlib\np = pathlib.Path('path/to/file')\nif p.is_file():  # or p.is_dir() to see if it is a directory\n    # do stuff\nYou can (and usually should) still use a try/except block when opening files:try/excepttry:\n    with p.open() as f:\n        # do awesome stuff\nexcept OSError:\n    print('Well darn.')\ntry:\n    with p.open() as f:\n        # do awesome stuff\nexcept OSError:\n    print('Well darn.')\nThe pathlib module has lots of cool stuff in it: convenient globbing, checking file's owner, easier path joining, etc.  It's worth checking out.  If you're on an older Python (version 2.6 or later), you can still install pathlib with pip:# installs pathlib2 on older Python versions\n# the original third-party module, pathlib, is no longer maintained.\npip install pathlib2\n# installs pathlib2 on older Python versions\n# the original third-party module, pathlib, is no longer maintained.\npip install pathlib2\nThen import it as follows:# Older Python versions\nimport pathlib2 as pathlib\n# Older Python versions\nimport pathlib2 as pathlib\n",
                "This is the simplest way to check if a file exists. Just because the file existed when you checked doesn't guarantee that it will be there when you need to open it.becauseguaranteeimport os\nfname = \"foo.txt\"\nif os.path.isfile(fname):\n    print(\"file does exist at this time\")\nelse:\n    print(\"no such file exists at this time\")\nimport os\nfname = \"foo.txt\"\nif os.path.isfile(fname):\n    print(\"file does exist at this time\")\nelse:\n    print(\"no such file exists at this time\")\n",
                "\nHow do I check whether a file exists, using Python, without using a try statement?\nHow do I check whether a file exists, using Python, without using a try statement?Now available since Python 3.4, import and instantiate a Path object with the file name, and check the is_file method (note that this returns True for symlinks pointing to regular files as well):Pathis_file>>> from pathlib import Path\n>>> Path('/').is_file()\nFalse\n>>> Path('/initrd.img').is_file()\nTrue\n>>> Path('/doesnotexist').is_file()\nFalse\n>>> from pathlib import Path\n>>> Path('/').is_file()\nFalse\n>>> Path('/initrd.img').is_file()\nTrue\n>>> Path('/doesnotexist').is_file()\nFalse\nIf you're on Python 2, you can backport the pathlib module from pypi, pathlib2, or otherwise check isfile from the os.path module:pathlib2pathlib2isfileos.path>>> import os\n>>> os.path.isfile('/')\nFalse\n>>> os.path.isfile('/initrd.img')\nTrue\n>>> os.path.isfile('/doesnotexist')\nFalse\n>>> import os\n>>> os.path.isfile('/')\nFalse\n>>> os.path.isfile('/initrd.img')\nTrue\n>>> os.path.isfile('/doesnotexist')\nFalse\nNow the above is probably the best pragmatic direct answer here, but there's the possibility of a race condition (depending on what you're trying to accomplish), and the fact that the underlying implementation uses a try, but Python uses try everywhere in its implementation. trytryBecause Python uses try everywhere, there's really no reason to avoid an implementation that uses it.tryBut the rest of this answer attempts to consider these caveats.Longer, much more pedantic answerAvailable since Python 3.4, use the new Path object in pathlib. Note that .exists is not quite right, because directories are not files (except in the unix sense that everything is a file).Pathpathlib.existseverything>>> from pathlib import Path\n>>> root = Path('/')\n>>> root.exists()\nTrue\n>>> from pathlib import Path\n>>> root = Path('/')\n>>> root.exists()\nTrue\nSo we need to use is_file:is_file>>> root.is_file()\nFalse\n>>> root.is_file()\nFalse\nHere's the help on is_file:is_fileis_file(self)\n    Whether this path is a regular file (also True for symlinks pointing\n    to regular files).\nis_file(self)\n    Whether this path is a regular file (also True for symlinks pointing\n    to regular files).\nSo let's get a file that we know is a file:>>> import tempfile\n>>> file = tempfile.NamedTemporaryFile()\n>>> filepathobj = Path(file.name)\n>>> filepathobj.is_file()\nTrue\n>>> filepathobj.exists()\nTrue\n>>> import tempfile\n>>> file = tempfile.NamedTemporaryFile()\n>>> filepathobj = Path(file.name)\n>>> filepathobj.is_file()\nTrue\n>>> filepathobj.exists()\nTrue\nBy default, NamedTemporaryFile deletes the file when closed (and will automatically close when no more references exist to it).NamedTemporaryFile>>> del file\n>>> filepathobj.exists()\nFalse\n>>> filepathobj.is_file()\nFalse\n>>> del file\n>>> filepathobj.exists()\nFalse\n>>> filepathobj.is_file()\nFalse\nIf you dig into the implementation, though, you'll see that is_file uses try:the implementationis_filetrydef is_file(self):\n    \"\"\"\n    Whether this path is a regular file (also True for symlinks pointing\n    to regular files).\n    \"\"\"\n    try:\n        return S_ISREG(self.stat().st_mode)\n    except OSError as e:\n        if e.errno not in (ENOENT, ENOTDIR):\n            raise\n        # Path doesn't exist or is a broken symlink\n        # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n        return False\ndef is_file(self):\n    \"\"\"\n    Whether this path is a regular file (also True for symlinks pointing\n    to regular files).\n    \"\"\"\n    try:\n        return S_ISREG(self.stat().st_mode)\n    except OSError as e:\n        if e.errno not in (ENOENT, ENOTDIR):\n            raise\n        # Path doesn't exist or is a broken symlink\n        # (see https://bitbucket.org/pitrou/pathlib/issue/12/)\n        return False\nRace Conditions: Why we like tryWe like try because it avoids race conditions. With try, you simply attempt to read your file, expecting it to be there, and if not, you catch the exception and perform whatever fallback behavior makes sense.trytryIf you want to check that a file exists before you attempt to read it, and you might be deleting it and then you might be using multiple threads or processes, or another program knows about that file and could delete it - you risk the chance of a race condition if you check it exists, because you are then racing to open it before its condition (its existence) changes. race conditionracingconditionRace conditions are very hard to debug because there's a very small window in which they can cause your program to fail.But if this is your motivation, you can get the value of a try statement by using the suppress context manager.cantrysuppressAvoiding race conditions without a try statement: suppresssuppressPython 3.4 gives us the suppress context manager (previously the ignore context manager), which does semantically exactly the same thing in fewer lines, while also (at least superficially) meeting the original ask to avoid a try statement:suppresssuppressignoreignoretryfrom contextlib import suppress\nfrom pathlib import Path\nfrom contextlib import suppress\nfrom pathlib import Path\nUsage:>>> with suppress(OSError), Path('doesnotexist').open() as f:\n...     for line in f:\n...         print(line)\n... \n>>>\n>>> with suppress(OSError):\n...     Path('doesnotexist').unlink()\n... \n>>> \n>>> with suppress(OSError), Path('doesnotexist').open() as f:\n...     for line in f:\n...         print(line)\n... \n>>>\n>>> with suppress(OSError):\n...     Path('doesnotexist').unlink()\n... \n>>> \nFor earlier Pythons, you could roll your own suppress, but without a try will be more verbose than with. I do believe this actually is the only answer that doesn't use try at any level in the Python that can be applied to prior to Python 3.4 because it uses a context manager instead:suppresstrythis actually is the only answer that doesn't use try at any level in the Pythontryclass suppress(object):\n    def __init__(self, *exceptions):\n        self.exceptions = exceptions\n    def __enter__(self):\n        return self\n    def __exit__(self, exc_type, exc_value, traceback):\n        if exc_type is not None:\n            return issubclass(exc_type, self.exceptions)\nclass suppress(object):\n    def __init__(self, *exceptions):\n        self.exceptions = exceptions\n    def __enter__(self):\n        return self\n    def __exit__(self, exc_type, exc_value, traceback):\n        if exc_type is not None:\n            return issubclass(exc_type, self.exceptions)\nPerhaps easier with a try:from contextlib import contextmanager\n\n@contextmanager\ndef suppress(*exceptions):\n    try:\n        yield\n    except exceptions:\n        pass\nfrom contextlib import contextmanager\n\n@contextmanager\ndef suppress(*exceptions):\n    try:\n        yield\n    except exceptions:\n        pass\nOther options that don't meet the ask for \"without try\":isfileisfileimport os\nos.path.isfile(path)\nimport os\nos.path.isfile(path)\nfrom the docs:docs\nos.path.isfile(path)\nReturn True if path is an existing regular file. This follows symbolic\n  links, so both islink() and isfile() can be true for the same path.\nos.path.isfile(path)os.path.isfile(path)Return True if path is an existing regular file. This follows symbolic\n  links, so both islink() and isfile() can be true for the same path.islink()isfile()But if you examine the source of this function, you'll see it actually does use a try statement:source\n# This follows symbolic links, so both islink() and isdir() can be true\n# for the same path on systems that support symlinks\ndef isfile(path):\n    \"\"\"Test whether a path is a regular file\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return stat.S_ISREG(st.st_mode)\n\n# This follows symbolic links, so both islink() and isdir() can be true\n# for the same path on systems that support symlinks\ndef isfile(path):\n    \"\"\"Test whether a path is a regular file\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return stat.S_ISREG(st.st_mode)\n# This follows symbolic links, so both islink() and isdir() can be true\n# for the same path on systems that support symlinks\ndef isfile(path):\n    \"\"\"Test whether a path is a regular file\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return stat.S_ISREG(st.st_mode)\n>>> OSError is os.error\nTrue\n>>> OSError is os.error\nTrue\nAll it's doing is using the given path to see if it can get stats on it,  catching OSError and then checking if it's a file if it didn't raise the exception.OSErrorIf you intend to do something with the file, I would suggest directly attempting it with a try-except to avoid a race condition:try:\n    with open(path) as f:\n        f.read()\nexcept OSError:\n    pass\ntry:\n    with open(path) as f:\n        f.read()\nexcept OSError:\n    pass\nos.accessos.accessAvailable for Unix and Windows is os.access, but to use you must pass flags, and it does not differentiate between files and directories. This is more used to test if the real invoking user has access in an elevated privilege environment:os.accessimport os\nos.access(path, os.F_OK)\nimport os\nos.access(path, os.F_OK)\nIt also suffers from the same race condition problems as isfile. From the docs:isfiledocs\nNote:\n  Using access() to check if a user is authorized to e.g. open a file\n  before actually doing so using open() creates a security hole, because\n  the user might exploit the short time interval between checking and\n  opening the file to manipulate it. It\u2019s preferable to use EAFP\n  techniques. For example:\nif os.access(\"myfile\", os.R_OK):\n    with open(\"myfile\") as fp:\n        return fp.read()\nreturn \"some default data\"\n\nis better written as:\ntry:\n    fp = open(\"myfile\")\nexcept IOError as e:\n    if e.errno == errno.EACCES:\n        return \"some default data\"\n    # Not a permission error.\n    raise\nelse:\n    with fp:\n        return fp.read()\n\nNote:\n  Using access() to check if a user is authorized to e.g. open a file\n  before actually doing so using open() creates a security hole, because\n  the user might exploit the short time interval between checking and\n  opening the file to manipulate it. It\u2019s preferable to use EAFP\n  techniques. For example:if os.access(\"myfile\", os.R_OK):\n    with open(\"myfile\") as fp:\n        return fp.read()\nreturn \"some default data\"\nif os.access(\"myfile\", os.R_OK):\n    with open(\"myfile\") as fp:\n        return fp.read()\nreturn \"some default data\"\nis better written as:try:\n    fp = open(\"myfile\")\nexcept IOError as e:\n    if e.errno == errno.EACCES:\n        return \"some default data\"\n    # Not a permission error.\n    raise\nelse:\n    with fp:\n        return fp.read()\ntry:\n    fp = open(\"myfile\")\nexcept IOError as e:\n    if e.errno == errno.EACCES:\n        return \"some default data\"\n    # Not a permission error.\n    raise\nelse:\n    with fp:\n        return fp.read()\nAvoid using os.access. It is a low level function that has more opportunities for user error than the higher level objects and functions discussed above.os.accessCriticism of another answer:Another answer says this about os.access:os.access\nPersonally, I prefer this one because under the hood, it calls native APIs (via \"${PYTHON_SRC_DIR}/Modules/posixmodule.c\"), but it also opens a gate for possible user errors, and it's not as Pythonic as other variants:\nPersonally, I prefer this one because under the hood, it calls native APIs (via \"${PYTHON_SRC_DIR}/Modules/posixmodule.c\"), but it also opens a gate for possible user errors, and it's not as Pythonic as other variants:This answer says it prefers a non-Pythonic, error-prone method, with no justification. It seems to encourage users to use low-level APIs without understanding them. It also creates a context manager which, by unconditionally returning True, allows all Exceptions (including KeyboardInterrupt and SystemExit!) to pass silently, which is a good way to hide bugs.TrueKeyboardInterruptSystemExitThis seems to encourage users to adopt poor practices.",
                "Prefer the try statement. It's considered better style and avoids race conditions.Don't take my word for it. There's plenty of support for this theory. Here's a couple:\nStyle: Section \"Handling unusual conditions\" of these course notes for Software Design (2007)\nAvoiding Race Conditions\nStyle: Section \"Handling unusual conditions\" of these course notes for Software Design (2007)these course notes for Software DesignSoftware DesignAvoiding Race ConditionsAvoiding Race Conditions",
                "Use:import os\n#Your path here e.g. \"C:\\Program Files\\text.txt\"\n#For access purposes: \"C:\\\\Program Files\\\\text.txt\"\nif os.path.exists(\"C:\\...\"):\n    print \"File found!\"\nelse:\n    print \"File not found!\"\nimport os\n#Your path here e.g. \"C:\\Program Files\\text.txt\"\n#For access purposes: \"C:\\\\Program Files\\\\text.txt\"\nif os.path.exists(\"C:\\...\"):\n    print \"File found!\"\nelse:\n    print \"File not found!\"\nImporting os makes it easier to navigate and perform standard actions with your operating system.osFor reference, also see How do I check whether a file exists without exceptions?.How do I check whether a file exists without exceptions?How do I check whether a file exists without exceptions?If you need high-level operations, use shutil.shutil",
                "Testing for files and folders with os.path.isfile(), os.path.isdir() and os.path.exists()os.path.isfile()os.path.isdir()os.path.exists()Assuming that the \"path\" is a valid path, this table shows what is returned by each function for files and folders:You can also test if a file is a certain type of file using os.path.splitext() to get the extension (if you don't already know it)os.path.splitext()>>> import os\n>>> path = \"path to a word document\"\n>>> os.path.isfile(path)\nTrue\n>>> os.path.splitext(path)[1] == \".docx\" # test if the extension is .docx\nTrue\n>>> import os\n>>> path = \"path to a word document\"\n>>> os.path.isfile(path)\nTrue\n>>> os.path.splitext(path)[1] == \".docx\" # test if the extension is .docx\nTrue\n",
                "TL;DR \nThe answer is: use the pathlib moduleTL;DRpathlibpathlibPathlib is probably the most modern and convenient way for almost all of the file operations. For the existence of a file or a folder a single line of code is enough. If file is not exists, it will not throw any exception.Pathlibfilefoldernotnotfrom pathlib import Path\n\nif Path(\"myfile.txt\").exists(): # works for both file and folders\n    # do your cool stuff...\nfrom pathlib import Path\n\nif Path(\"myfile.txt\").exists(): # works for both file and folders\n    # do your cool stuff...\nThe pathlib module was introduced in Python 3.4, so you need to have Python 3.4+. This library makes your life much easier while working with files and folders, and it is pretty to use. Here is more documentation about it: pathlib \u2014 Object-oriented filesystem paths.pathlibpathlib \u2014 Object-oriented filesystem pathspathlib \u2014 Object-oriented filesystem pathsBTW, if you are going to reuse the path, then it is better to assign it to a variable.So it will become:from pathlib import Path\n\np = Path(\"loc/of/myfile.txt\")\nif p.exists(): # works for both file and folders\n    # do stuffs...\n#reuse 'p' if needed.\nfrom pathlib import Path\n\np = Path(\"loc/of/myfile.txt\")\nif p.exists(): # works for both file and folders\n    # do stuffs...\n#reuse 'p' if needed.\n",
                "In 2016 the best way is still using os.path.isfile:os.path.isfile>>> os.path.isfile('/path/to/some/file.txt')\n>>> os.path.isfile('/path/to/some/file.txt')\nOr in Python 3 you can use pathlib:pathlibimport pathlib\npath = pathlib.Path('/path/to/some/file.txt')\nif path.is_file():\n    ...\nimport pathlib\npath = pathlib.Path('/path/to/some/file.txt')\nif path.is_file():\n    ...\n",
                "It doesn't seem like there's a meaningful functional difference between try/except and isfile(), so you should use which one makes sense.isfile()If you want to read a file, if it exists, dotry:\n    f = open(filepath)\nexcept IOError:\n    print 'Oh dear.'\ntry:\n    f = open(filepath)\nexcept IOError:\n    print 'Oh dear.'\nBut if you just wanted to rename a file if it exists, and therefore don't need to open it, doif os.path.isfile(filepath):\n    os.rename(filepath, filepath + '.old')\nif os.path.isfile(filepath):\n    os.rename(filepath, filepath + '.old')\nIf you want to write to a file, if it doesn't exist, do# Python 2\nif not os.path.isfile(filepath):\n    f = open(filepath, 'w')\n\n# Python 3: x opens for exclusive creation, failing if the file already exists\ntry:\n    f = open(filepath, 'wx')\nexcept IOError:\n    print 'file already exists'\n# Python 2\nif not os.path.isfile(filepath):\n    f = open(filepath, 'w')\n\n# Python 3: x opens for exclusive creation, failing if the file already exists\ntry:\n    f = open(filepath, 'wx')\nexcept IOError:\n    print 'file already exists'\nIf you need file locking, that's a different matter.",
                "You could try this (safer):try:\n    # http://effbot.org/zone/python-with-statement.htm\n    # 'with' is safer to open a file\n    with open('whatever.txt') as fh:\n        # Do something with 'fh'\nexcept IOError as e:\n    print(\"({})\".format(e))\ntry:\n    # http://effbot.org/zone/python-with-statement.htm\n    # 'with' is safer to open a file\n    with open('whatever.txt') as fh:\n        # Do something with 'fh'\nexcept IOError as e:\n    print(\"({})\".format(e))\nThe ouput would be:\n([Errno 2] No such file or directory:\n  'whatever.txt')\n([Errno 2] No such file or directory:\n  'whatever.txt')Then, depending on the result, your program can just keep running from there or you can code to stop it if you want.",
                "Date: 2017-12-04Every possible solution has been listed in other answers.An intuitive and arguable way to check if a file exists is the following:import os\n\nos.path.isfile('~/file.md')  # Returns True if exists, else False\n\n# Additionally, check a directory\nos.path.isdir('~/folder')  # Returns True if the folder exists, else False\n\n# Check either a directory or a file\nos.path.exists('~/file')\nimport os\n\nos.path.isfile('~/file.md')  # Returns True if exists, else False\n\n# Additionally, check a directory\nos.path.isdir('~/folder')  # Returns True if the folder exists, else False\n\n# Check either a directory or a file\nos.path.exists('~/file')\nI made an exhaustive cheat sheet for your reference:# os.path methods in exhaustive cheat sheet\n{'definition': ['dirname',\n               'basename',\n               'abspath',\n               'relpath',\n               'commonpath',\n               'normpath',\n               'realpath'],\n'operation': ['split', 'splitdrive', 'splitext',\n               'join', 'normcase'],\n'compare': ['samefile', 'sameopenfile', 'samestat'],\n'condition': ['isdir',\n              'isfile',\n              'exists',\n              'lexists'\n              'islink',\n              'isabs',\n              'ismount',],\n 'expand': ['expanduser',\n            'expandvars'],\n 'stat': ['getatime', 'getctime', 'getmtime',\n          'getsize']}\n# os.path methods in exhaustive cheat sheet\n{'definition': ['dirname',\n               'basename',\n               'abspath',\n               'relpath',\n               'commonpath',\n               'normpath',\n               'realpath'],\n'operation': ['split', 'splitdrive', 'splitext',\n               'join', 'normcase'],\n'compare': ['samefile', 'sameopenfile', 'samestat'],\n'condition': ['isdir',\n              'isfile',\n              'exists',\n              'lexists'\n              'islink',\n              'isabs',\n              'ismount',],\n 'expand': ['expanduser',\n            'expandvars'],\n 'stat': ['getatime', 'getctime', 'getmtime',\n          'getsize']}\n",
                "Although I always recommend using try and except statements, here are a few possibilities for you (my personal favourite is using os.access):tryexceptos.access\nTry opening the file:\nOpening the file will always verify the existence of the file. You can make a function just like so:\ndef File_Existence(filepath):\n    f = open(filepath)\n    return True\n\nIf it's False, it will stop execution with an unhanded IOError\nor OSError in later versions of Python. To catch the exception,\nyou have to use a try except clause. Of course, you can always\nuse a try except` statement like so (thanks to hsandt\nfor making me think):\ndef File_Existence(filepath):\n    try:\n        f = open(filepath)\n    except IOError, OSError: # Note OSError is for later versions of Python\n        return False\n\n    return True\n\nUse os.path.exists(path):\nThis will check the existence of what you specify. However, it checks for files and directories so beware about how you use it.\nimport os.path\n>>> os.path.exists(\"this/is/a/directory\")\nTrue\n>>> os.path.exists(\"this/is/a/file.txt\")\nTrue\n>>> os.path.exists(\"not/a/directory\")\nFalse\n\nUse os.access(path, mode):\nThis will check whether you have access to the file. It will check for permissions. Based on the os.py documentation, typing in os.F_OK, it will check the existence of the path. However, using this will create a security hole, as someone can attack your file using the time between checking the permissions and opening the file. You should instead go directly to opening the file instead of checking its permissions. (EAFP vs LBYP). If you're not going to open the file afterwards, and only checking its existence, then you can use this.\nAnyway, here:\n>>> import os\n>>> os.access(\"/is/a/file.txt\", os.F_OK)\nTrue\n\nTry opening the file:\nOpening the file will always verify the existence of the file. You can make a function just like so:\ndef File_Existence(filepath):\n    f = open(filepath)\n    return True\n\nIf it's False, it will stop execution with an unhanded IOError\nor OSError in later versions of Python. To catch the exception,\nyou have to use a try except clause. Of course, you can always\nuse a try except` statement like so (thanks to hsandt\nfor making me think):\ndef File_Existence(filepath):\n    try:\n        f = open(filepath)\n    except IOError, OSError: # Note OSError is for later versions of Python\n        return False\n\n    return True\nTry opening the file:Opening the file will always verify the existence of the file. You can make a function just like so:def File_Existence(filepath):\n    f = open(filepath)\n    return True\ndef File_Existence(filepath):\n    f = open(filepath)\n    return True\nIf it's False, it will stop execution with an unhanded IOError\nor OSError in later versions of Python. To catch the exception,\nyou have to use a try except clause. Of course, you can always\nuse a try except` statement like so (thanks to hsandt\nfor making me think):tryhsandtdef File_Existence(filepath):\n    try:\n        f = open(filepath)\n    except IOError, OSError: # Note OSError is for later versions of Python\n        return False\n\n    return True\ndef File_Existence(filepath):\n    try:\n        f = open(filepath)\n    except IOError, OSError: # Note OSError is for later versions of Python\n        return False\n\n    return True\nUse os.path.exists(path):\nThis will check the existence of what you specify. However, it checks for files and directories so beware about how you use it.\nimport os.path\n>>> os.path.exists(\"this/is/a/directory\")\nTrue\n>>> os.path.exists(\"this/is/a/file.txt\")\nTrue\n>>> os.path.exists(\"not/a/directory\")\nFalse\nUse os.path.exists(path):os.path.exists(path)This will check the existence of what you specify. However, it checks for files and directories so beware about how you use it.andimport os.path\n>>> os.path.exists(\"this/is/a/directory\")\nTrue\n>>> os.path.exists(\"this/is/a/file.txt\")\nTrue\n>>> os.path.exists(\"not/a/directory\")\nFalse\nimport os.path\n>>> os.path.exists(\"this/is/a/directory\")\nTrue\n>>> os.path.exists(\"this/is/a/file.txt\")\nTrue\n>>> os.path.exists(\"not/a/directory\")\nFalse\nUse os.access(path, mode):\nThis will check whether you have access to the file. It will check for permissions. Based on the os.py documentation, typing in os.F_OK, it will check the existence of the path. However, using this will create a security hole, as someone can attack your file using the time between checking the permissions and opening the file. You should instead go directly to opening the file instead of checking its permissions. (EAFP vs LBYP). If you're not going to open the file afterwards, and only checking its existence, then you can use this.\nAnyway, here:\n>>> import os\n>>> os.access(\"/is/a/file.txt\", os.F_OK)\nTrue\nUse os.access(path, mode):os.access(path, mode)This will check whether you have access to the file. It will check for permissions. Based on the os.py documentation, typing in os.F_OK, it will check the existence of the path. However, using this will create a security hole, as someone can attack your file using the time between checking the permissions and opening the file. You should instead go directly to opening the file instead of checking its permissions. (EAFP vs LBYP). If you're not going to open the file afterwards, and only checking its existence, then you can use this.os.F_OKEAFPLBYPAnyway, here:>>> import os\n>>> os.access(\"/is/a/file.txt\", os.F_OK)\nTrue\n>>> import os\n>>> os.access(\"/is/a/file.txt\", os.F_OK)\nTrue\nI should also mention that there are two ways that you will not be able to verify the existence of a file. Either the issue will be permission denied or no such file or directory. If you catch an IOError, set the IOError as e (like my first option), and then type in print(e.args) so that you can hopefully determine your issue. I hope it helps! :)permission deniedno such file or directoryIOErrorIOError as eprint(e.args)",
                "If the file is for opening you could use one of the following techniques:with open('somefile', 'xt') as f: # Using the x-flag, Python 3.3 and above\n    f.write('Hello\\n')\n\nif not os.path.exists('somefile'): \n    with open('somefile', 'wt') as f:\n        f.write(\"Hello\\n\")\nelse:\n    print('File already exists!')\nwith open('somefile', 'xt') as f: # Using the x-flag, Python 3.3 and above\n    f.write('Hello\\n')\n\nif not os.path.exists('somefile'): \n    with open('somefile', 'wt') as f:\n        f.write(\"Hello\\n\")\nelse:\n    print('File already exists!')\nNote: This finds either a file or a directory with the given name.or",
                "Additionally, os.access():os.access()if os.access(\"myfile\", os.R_OK):\n    with open(\"myfile\") as fp:\n        return fp.read()\nif os.access(\"myfile\", os.R_OK):\n    with open(\"myfile\") as fp:\n        return fp.read()\nBeing R_OK, W_OK, and X_OK the flags to test for permissions (doc).R_OKW_OKX_OKdoc",
                "if os.path.isfile(path_to_file):\n    try:\n        open(path_to_file)\n            pass\n    except IOError as e:\n        print \"Unable to open file\"\nif os.path.isfile(path_to_file):\n    try:\n        open(path_to_file)\n            pass\n    except IOError as e:\n        print \"Unable to open file\"\n\nRaising exceptions is considered to be an acceptable, and Pythonic,\napproach for flow control in your program. Consider handling missing\nfiles with IOErrors. In this situation, an IOError exception will be\nraised if the file exists but the user does not have read permissions.\nRaising exceptions is considered to be an acceptable, and Pythonic,\napproach for flow control in your program. Consider handling missing\nfiles with IOErrors. In this situation, an IOError exception will be\nraised if the file exists but the user does not have read permissions.Source: Using Python: How To Check If A File ExistsUsing Python: How To Check If A File ExistsUsing Python: How To Check If A File Exists",
                "If you imported NumPy already for other purposes then there is no need to import other libraries like pathlib, os, paths, etc.pathlibospathsimport numpy as np\nnp.DataSource().exists(\"path/to/your/file\")\nimport numpy as np\nnp.DataSource().exists(\"path/to/your/file\")\nThis will return true or false based on its existence.",
                "Check file or directory existsYou can follow these three ways:1. Using isfile()Note 1: The os.path.isfile used only for filesos.path.isfileimport os.path\nos.path.isfile(filename) # True if file exists\nos.path.isfile(dirname) # False if directory exists\nimport os.path\nos.path.isfile(filename) # True if file exists\nos.path.isfile(dirname) # False if directory exists\n2. Using existsexistsNote 2: The os.path.exists is used for both files and directoriesos.path.existsimport os.path\nos.path.exists(filename) # True if file exists\nos.path.exists(dirname) # True if directory exists\nimport os.path\nos.path.exists(filename) # True if file exists\nos.path.exists(dirname) # True if directory exists\n3. The pathlib.Path method (included in Python 3+, installable with pip for Python 2)pathlib.Pathfrom pathlib import Path\nPath(filename).exists()\nfrom pathlib import Path\nPath(filename).exists()\n",
                "You can write Brian's suggestion without the try:.try:from contextlib import suppress\n\nwith suppress(IOError), open('filename'):\n    process()\nfrom contextlib import suppress\n\nwith suppress(IOError), open('filename'):\n    process()\nsuppress is part of Python 3.4. In older releases you can quickly write your own suppress:suppressfrom contextlib import contextmanager\n\n@contextmanager\ndef suppress(*exceptions):\n    try:\n        yield\n    except exceptions:\n        pass\nfrom contextlib import contextmanager\n\n@contextmanager\ndef suppress(*exceptions):\n    try:\n        yield\n    except exceptions:\n        pass\n",
                "Adding one more slight variation which isn't exactly reflected in the other answers.This will handle the case of the file_path being None or empty string.file_pathNonedef file_exists(file_path):\n    if not file_path:\n        return False\n    elif not os.path.isfile(file_path):\n        return False\n    else:\n        return True\ndef file_exists(file_path):\n    if not file_path:\n        return False\n    elif not os.path.isfile(file_path):\n        return False\n    else:\n        return True\nAdding a variant based on suggestion from Shahbaz\ndef file_exists(file_path):\n    if not file_path:\n        return False\n    else:\n        return os.path.isfile(file_path)\ndef file_exists(file_path):\n    if not file_path:\n        return False\n    else:\n        return os.path.isfile(file_path)\nAdding a variant based on suggestion from Peter Wood\ndef file_exists(file_path):\n    return file_path and os.path.isfile(file_path):\ndef file_exists(file_path):\n    return file_path and os.path.isfile(file_path):\n",
                "I'm the author of a package that's been around for about 10 years, and it has a function that addresses this question directly. Basically, if you are on a non-Windows system, it uses Popen to access find.  However, if you are on Windows, it replicates find with an efficient filesystem walker.PopenfindfindThe code itself does not use a try block\u2026 except in determining the operating system and thus steering you to the \"Unix\"-style find or the hand-buillt find. Timing tests showed that the try was faster in determining the OS, so I did use one there (but nowhere else).tryfindfindtry>>> import pox\n>>> pox.find('*python*', type='file', root=pox.homedir(), recurse=False)\n['/Users/mmckerns/.python']\n>>> import pox\n>>> pox.find('*python*', type='file', root=pox.homedir(), recurse=False)\n['/Users/mmckerns/.python']\nAnd the doc\u2026>>> print pox.find.__doc__\nfind(patterns[,root,recurse,type]); Get path to a file or directory\n\n    patterns: name or partial name string of items to search for\n    root: path string of top-level directory to search\n    recurse: if True, recurse down from root directory\n    type: item filter; one of {None, file, dir, link, socket, block, char}\n    verbose: if True, be a little verbose about the search\n\n    On some OS, recursion can be specified by recursion depth (an integer).\n    patterns can be specified with basic pattern matching. Additionally,\n    multiple patterns can be specified by splitting patterns with a ';'\n    For example:\n        >>> find('pox*', root='..')\n        ['/Users/foo/pox/pox', '/Users/foo/pox/scripts/pox_launcher.py']\n\n        >>> find('*shutils*;*init*')\n        ['/Users/foo/pox/pox/shutils.py', '/Users/foo/pox/pox/__init__.py']\n\n>>>\n>>> print pox.find.__doc__\nfind(patterns[,root,recurse,type]); Get path to a file or directory\n\n    patterns: name or partial name string of items to search for\n    root: path string of top-level directory to search\n    recurse: if True, recurse down from root directory\n    type: item filter; one of {None, file, dir, link, socket, block, char}\n    verbose: if True, be a little verbose about the search\n\n    On some OS, recursion can be specified by recursion depth (an integer).\n    patterns can be specified with basic pattern matching. Additionally,\n    multiple patterns can be specified by splitting patterns with a ';'\n    For example:\n        >>> find('pox*', root='..')\n        ['/Users/foo/pox/pox', '/Users/foo/pox/scripts/pox_launcher.py']\n\n        >>> find('*shutils*;*init*')\n        ['/Users/foo/pox/pox/shutils.py', '/Users/foo/pox/pox/__init__.py']\n\n>>>\nThe implementation, if you care to look, is here:\nhttps://github.com/uqfoundation/pox/blob/89f90fb308f285ca7a62eabe2c38acb87e89dad9/pox/shutils.py#L190https://github.com/uqfoundation/pox/blob/89f90fb308f285ca7a62eabe2c38acb87e89dad9/pox/shutils.py#L190",
                "Here's a one-line Python command for the Linux command line environment. I find this very handy since I'm not such a hot Bash guy.very handypython -c \"import os.path; print os.path.isfile('/path_to/file.xxx')\"\npython -c \"import os.path; print os.path.isfile('/path_to/file.xxx')\"\n",
                "You can use the \"OS\" library of Python:>>> import os\n>>> os.path.exists(\"C:\\\\Users\\\\####\\\\Desktop\\\\test.txt\") \nTrue\n>>> os.path.exists(\"C:\\\\Users\\\\####\\\\Desktop\\\\test.tx\")\nFalse\n>>> import os\n>>> os.path.exists(\"C:\\\\Users\\\\####\\\\Desktop\\\\test.txt\") \nTrue\n>>> os.path.exists(\"C:\\\\Users\\\\####\\\\Desktop\\\\test.tx\")\nFalse\n",
                "\nHow do I check whether a file exists, without using the try statement?\nHow do I check whether a file exists, without using the try statement?How do I check whether a file exists, without using the try statement?In 2016, this is still arguably the easiest way to check if both a file exists and if it is a file:import os\nos.path.isfile('./file.txt')    # Returns True if exists, else False\nimport os\nos.path.isfile('./file.txt')    # Returns True if exists, else False\nisfile is actually just a helper method that internally uses os.stat and stat.S_ISREG(mode) underneath. This os.stat is a lower-level method that will provide you with detailed information about files, directories, sockets, buffers, and more. More about os.stat hereisfileos.statstat.S_ISREG(mode)os.statMore about os.stat hereNote: However, this approach will not lock the file in any way and therefore your code can become vulnerable to \"time of check to time of use\" (TOCTTOU) bugs.Note:time of check to time of useTOCTTOUSo raising exceptions is considered to be an acceptable, and Pythonic, approach for flow control in your program. And one should consider handling missing files with IOErrors, rather than if statements (just an advice).ifjust an advice"
            ]
        },
        {
            "tag": "merge_dict",
            "patterns": [
                "How do I merge two dictionaries in a single expression?",
                "how can you merge two dictionaries into the same word?",
                "how do i merge two dictionaries into one?",
                "how can i combine two dictionaries into one expression?",
                "how do i combine two dictionaries in one expression?",
                "tell me the best way to combine two dictionaries in a single expression?",
                "how do i combine two dictionaries into one expression?",
                "how do i merge two dictionaries into one expression?"
            ],
            "responses": [
                "How can I merge two Python dictionaries in a single expression?For dictionaries x and y, their shallowly-merged dictionary z takes values from y, replacing those from x.xyzyx\nIn Python 3.9.0 or greater (released 17 October 2020, PEP-584, discussed here):\nz = x | y\n\n\nIn Python 3.5 or greater:\nz = {**x, **y}\n\n\nIn Python 2, (or 3.4 or lower) write a function:\ndef merge_two_dicts(x, y):\n    z = x.copy()   # start with keys and values of x\n    z.update(y)    # modifies z with keys and values of y\n    return z\n\nand now:\nz = merge_two_dicts(x, y)\n\n\nIn Python 3.9.0 or greater (released 17 October 2020, PEP-584, discussed here):\nz = x | y\n\nIn Python 3.9.0 or greater (released 17 October 2020, PEP-584, discussed here):PEP-584PEP-584discussed herez = x | y\nz = x | y\nIn Python 3.5 or greater:\nz = {**x, **y}\n\nIn Python 3.5 or greater:z = {**x, **y}\nz = {**x, **y}\nIn Python 2, (or 3.4 or lower) write a function:\ndef merge_two_dicts(x, y):\n    z = x.copy()   # start with keys and values of x\n    z.update(y)    # modifies z with keys and values of y\n    return z\n\nand now:\nz = merge_two_dicts(x, y)\n\nIn Python 2, (or 3.4 or lower) write a function:def merge_two_dicts(x, y):\n    z = x.copy()   # start with keys and values of x\n    z.update(y)    # modifies z with keys and values of y\n    return z\ndef merge_two_dicts(x, y):\n    z = x.copy()   # start with keys and values of x\n    z.update(y)    # modifies z with keys and values of y\n    return z\nand now:z = merge_two_dicts(x, y)\nz = merge_two_dicts(x, y)\nExplanationSay you have two dictionaries and you want to merge them into a new dictionary without altering the original dictionaries:x = {'a': 1, 'b': 2}\ny = {'b': 3, 'c': 4}\nx = {'a': 1, 'b': 2}\ny = {'b': 3, 'c': 4}\nThe desired result is to get a new dictionary (z) with the values merged, and the second dictionary's values overwriting those from the first.z>>> z\n{'a': 1, 'b': 3, 'c': 4}\n>>> z\n{'a': 1, 'b': 3, 'c': 4}\nA new syntax for this, proposed in PEP 448 and available as of Python 3.5, isPEP 448available as of Python 3.5z = {**x, **y}\nz = {**x, **y}\nAnd it is indeed a single expression.Note that we can merge in with literal notation as well:z = {**x, 'foo': 1, 'bar': 2, **y}\nz = {**x, 'foo': 1, 'bar': 2, **y}\nand now:>>> z\n{'a': 1, 'b': 3, 'foo': 1, 'bar': 2, 'c': 4}\n>>> z\n{'a': 1, 'b': 3, 'foo': 1, 'bar': 2, 'c': 4}\nIt is now showing as implemented in the release schedule for 3.5, PEP 478, and it has now made its way into the What's New in Python 3.5 document.release schedule for 3.5, PEP 478What's New in Python 3.5However, since many organizations are still on Python 2, you may wish to do this in a backward-compatible way. The classically Pythonic way, available in Python 2 and Python 3.0-3.4, is to do this as a two-step process:z = x.copy()\nz.update(y) # which returns None since it mutates z\nz = x.copy()\nz.update(y) # which returns None since it mutates z\nIn both approaches, y will come second and its values will replace x's values, thus b will point to 3 in our final result.yxb3Not yet on Python 3.5, but want a single expressionsingle expressionIf you are not yet on Python 3.5 or need to write backward-compatible code, and you want this in a single expression, the most performant while the correct approach is to put it in a function:single expressiondef merge_two_dicts(x, y):\n    \"\"\"Given two dictionaries, merge them into a new dict as a shallow copy.\"\"\"\n    z = x.copy()\n    z.update(y)\n    return z\ndef merge_two_dicts(x, y):\n    \"\"\"Given two dictionaries, merge them into a new dict as a shallow copy.\"\"\"\n    z = x.copy()\n    z.update(y)\n    return z\nand then you have a single expression:z = merge_two_dicts(x, y)\nz = merge_two_dicts(x, y)\nYou can also make a function to merge an arbitrary number of dictionaries, from zero to a very large number:def merge_dicts(*dict_args):\n    \"\"\"\n    Given any number of dictionaries, shallow copy and merge into a new dict,\n    precedence goes to key-value pairs in latter dictionaries.\n    \"\"\"\n    result = {}\n    for dictionary in dict_args:\n        result.update(dictionary)\n    return result\ndef merge_dicts(*dict_args):\n    \"\"\"\n    Given any number of dictionaries, shallow copy and merge into a new dict,\n    precedence goes to key-value pairs in latter dictionaries.\n    \"\"\"\n    result = {}\n    for dictionary in dict_args:\n        result.update(dictionary)\n    return result\nThis function will work in Python 2 and 3 for all dictionaries. e.g. given dictionaries a to g:agz = merge_dicts(a, b, c, d, e, f, g) \nz = merge_dicts(a, b, c, d, e, f, g) \nand key-value pairs in g will take precedence over dictionaries a to f, and so on.gafCritiques of Other AnswersDon't use what you see in the formerly accepted answer:z = dict(x.items() + y.items())\nz = dict(x.items() + y.items())\nIn Python 2, you create two lists in memory for each dict, create a third list in memory with length equal to the length of the first two put together, and then discard all three lists to create the dict. In Python 3, this will fail because you're adding two dict_items objects together, not two lists -In Python 3, this will faildict_items>>> c = dict(a.items() + b.items())\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for +: 'dict_items' and 'dict_items'\n>>> c = dict(a.items() + b.items())\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unsupported operand type(s) for +: 'dict_items' and 'dict_items'\nand you would have to explicitly create them as lists, e.g. z = dict(list(x.items()) + list(y.items())). This is a waste of resources and computation power.z = dict(list(x.items()) + list(y.items()))Similarly, taking the union of items() in Python 3 (viewitems() in Python 2.7) will also fail when values are unhashable objects (like lists, for example). Even if your values are hashable, since sets are semantically unordered, the behavior is undefined in regards to precedence. So don't do this:items()viewitems()since sets are semantically unordered, the behavior is undefined in regards to precedence. So don't do this:>>> c = dict(a.items() | b.items())\n>>> c = dict(a.items() | b.items())\nThis example demonstrates what happens when values are unhashable:>>> x = {'a': []}\n>>> y = {'b': []}\n>>> dict(x.items() | y.items())\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unhashable type: 'list'\n>>> x = {'a': []}\n>>> y = {'b': []}\n>>> dict(x.items() | y.items())\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: unhashable type: 'list'\nHere's an example where y should have precedence, but instead the value from x is retained due to the arbitrary order of sets:yx>>> x = {'a': 2}\n>>> y = {'a': 1}\n>>> dict(x.items() | y.items())\n{'a': 2}\n>>> x = {'a': 2}\n>>> y = {'a': 1}\n>>> dict(x.items() | y.items())\n{'a': 2}\nAnother hack you should not use:z = dict(x, **y)\nz = dict(x, **y)\nThis uses the dict constructor and is very fast and memory-efficient (even slightly more so than our two-step process) but unless you know precisely what is happening here (that is, the second dict is being passed as keyword arguments to the dict constructor), it's difficult to read, it's not the intended usage, and so it is not Pythonic.dictHere's an example of the usage being remediated in django.remediated in djangoDictionaries are intended to take hashable keys (e.g. frozensets or tuples), but this method fails in Python 3 when keys are not strings.frozensetthis method fails in Python 3 when keys are not strings.>>> c = dict(a, **b)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: keyword arguments must be strings\n>>> c = dict(a, **b)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: keyword arguments must be strings\nFrom the mailing list, Guido van Rossum, the creator of the language, wrote:mailing list\nI am fine with\ndeclaring dict({}, **{1:3}) illegal, since after all it is abuse of\nthe ** mechanism.\nI am fine with\ndeclaring dict({}, **{1:3}) illegal, since after all it is abuse of\nthe ** mechanism.and\nApparently dict(x, **y) is going around as \"cool hack\" for \"call\nx.update(y) and return x\". Personally, I find it more despicable than\ncool.\nApparently dict(x, **y) is going around as \"cool hack\" for \"call\nx.update(y) and return x\". Personally, I find it more despicable than\ncool.It is my understanding (as well as the understanding of the creator of the language) that the intended usage for dict(**y) is for creating dictionaries for readability purposes, e.g.:creator of the languagedict(**y)dict(a=1, b=10, c=11)\ndict(a=1, b=10, c=11)\ninstead of{'a': 1, 'b': 10, 'c': 11}\n{'a': 1, 'b': 10, 'c': 11}\nResponse to comments\nDespite what Guido says, dict(x, **y) is in line with the dict specification, which btw. works for both Python 2 and 3. The fact that this only works for string keys is a direct consequence of how keyword parameters work and not a short-coming of dict. Nor is using the ** operator in this place an abuse of the mechanism, in fact, ** was designed precisely to pass dictionaries as keywords.\nDespite what Guido says, dict(x, **y) is in line with the dict specification, which btw. works for both Python 2 and 3. The fact that this only works for string keys is a direct consequence of how keyword parameters work and not a short-coming of dict. Nor is using the ** operator in this place an abuse of the mechanism, in fact, ** was designed precisely to pass dictionaries as keywords.dict(x, **y)Again, it doesn't work for 3 when keys are not strings. The implicit calling contract is that namespaces take ordinary dictionaries, while users must only pass keyword arguments that are strings. All other callables enforced it. dict broke this consistency in Python 2:dict>>> foo(**{('a', 'b'): None})\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: foo() keywords must be strings\n>>> dict(**{('a', 'b'): None})\n{('a', 'b'): None}\n>>> foo(**{('a', 'b'): None})\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: foo() keywords must be strings\n>>> dict(**{('a', 'b'): None})\n{('a', 'b'): None}\nThis inconsistency was bad given other implementations of Python (PyPy, Jython, IronPython). Thus it was fixed in Python 3, as this usage could be a breaking change.I submit to you that it is malicious incompetence to intentionally write code that only works in one version of a language or that only works given certain arbitrary constraints.More comments:\ndict(x.items() + y.items()) is still the most readable solution for Python 2. Readability counts.\ndict(x.items() + y.items()) is still the most readable solution for Python 2. Readability counts.dict(x.items() + y.items())My response: merge_two_dicts(x, y) actually seems much clearer to me, if we're actually concerned about readability. And it is not forward compatible, as Python 2 is increasingly deprecated.merge_two_dicts(x, y)\n{**x, **y} does not seem to handle nested dictionaries. the contents of nested keys are simply overwritten, not merged [...] I ended up being burnt by these answers that do not merge recursively and I was surprised no one mentioned it. In my interpretation of the word \"merging\" these answers describe \"updating one dict with another\", and not merging.\n{**x, **y} does not seem to handle nested dictionaries. the contents of nested keys are simply overwritten, not merged [...] I ended up being burnt by these answers that do not merge recursively and I was surprised no one mentioned it. In my interpretation of the word \"merging\" these answers describe \"updating one dict with another\", and not merging.{**x, **y}Yes. I must refer you back to the question, which is asking for a shallow merge of two dictionaries, with the first's values being overwritten by the second's - in a single expression.shallowtwotwoAssuming two dictionaries of dictionaries, one might recursively merge them in a single function, but you should be careful not to modify the dictionaries from either source, and the surest way to avoid that is to make a copy when assigning values. As keys must be hashable and are usually therefore immutable, it is pointless to copy them:from copy import deepcopy\n\ndef dict_of_dicts_merge(x, y):\n    z = {}\n    overlapping_keys = x.keys() & y.keys()\n    for key in overlapping_keys:\n        z[key] = dict_of_dicts_merge(x[key], y[key])\n    for key in x.keys() - overlapping_keys:\n        z[key] = deepcopy(x[key])\n    for key in y.keys() - overlapping_keys:\n        z[key] = deepcopy(y[key])\n    return z\nfrom copy import deepcopy\n\ndef dict_of_dicts_merge(x, y):\n    z = {}\n    overlapping_keys = x.keys() & y.keys()\n    for key in overlapping_keys:\n        z[key] = dict_of_dicts_merge(x[key], y[key])\n    for key in x.keys() - overlapping_keys:\n        z[key] = deepcopy(x[key])\n    for key in y.keys() - overlapping_keys:\n        z[key] = deepcopy(y[key])\n    return z\nUsage:>>> x = {'a':{1:{}}, 'b': {2:{}}}\n>>> y = {'b':{10:{}}, 'c': {11:{}}}\n>>> dict_of_dicts_merge(x, y)\n{'b': {2: {}, 10: {}}, 'a': {1: {}}, 'c': {11: {}}}\n>>> x = {'a':{1:{}}, 'b': {2:{}}}\n>>> y = {'b':{10:{}}, 'c': {11:{}}}\n>>> dict_of_dicts_merge(x, y)\n{'b': {2: {}, 10: {}}, 'a': {1: {}}, 'c': {11: {}}}\nComing up with contingencies for other value types is far beyond the scope of this question, so I will point you at my answer to the canonical question on a \"Dictionaries of dictionaries merge\".my answer to the canonical question on a \"Dictionaries of dictionaries merge\"Less Performant But Correct Ad-hocsThese approaches are less performant, but they will provide correct behavior.\nThey will be much less performant than copy and update or the new unpacking because they iterate through each key-value pair at a higher level of abstraction, but they do respect the order of precedence (latter dictionaries have precedence)much lesscopyupdatedoYou can also chain the dictionaries manually inside a dict comprehension:dict comprehension{k: v for d in dicts for k, v in d.items()} # iteritems in Python 2.7\n{k: v for d in dicts for k, v in d.items()} # iteritems in Python 2.7\nor in Python 2.6 (and perhaps as early as 2.4 when generator expressions were introduced):dict((k, v) for d in dicts for k, v in d.items()) # iteritems in Python 2\ndict((k, v) for d in dicts for k, v in d.items()) # iteritems in Python 2\nitertools.chain will chain the iterators over the key-value pairs in the correct order:itertools.chainfrom itertools import chain\nz = dict(chain(x.items(), y.items())) # iteritems in Python 2\nfrom itertools import chain\nz = dict(chain(x.items(), y.items())) # iteritems in Python 2\nPerformance AnalysisI'm only going to do the performance analysis of the usages known to behave correctly. (Self-contained so you can copy and paste yourself.)from timeit import repeat\nfrom itertools import chain\n\nx = dict.fromkeys('abcdefg')\ny = dict.fromkeys('efghijk')\n\ndef merge_two_dicts(x, y):\n    z = x.copy()\n    z.update(y)\n    return z\n\nmin(repeat(lambda: {**x, **y}))\nmin(repeat(lambda: merge_two_dicts(x, y)))\nmin(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()}))\nmin(repeat(lambda: dict(chain(x.items(), y.items()))))\nmin(repeat(lambda: dict(item for d in (x, y) for item in d.items())))\nfrom timeit import repeat\nfrom itertools import chain\n\nx = dict.fromkeys('abcdefg')\ny = dict.fromkeys('efghijk')\n\ndef merge_two_dicts(x, y):\n    z = x.copy()\n    z.update(y)\n    return z\n\nmin(repeat(lambda: {**x, **y}))\nmin(repeat(lambda: merge_two_dicts(x, y)))\nmin(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()}))\nmin(repeat(lambda: dict(chain(x.items(), y.items()))))\nmin(repeat(lambda: dict(item for d in (x, y) for item in d.items())))\nIn Python 3.8.1, NixOS:>>> min(repeat(lambda: {**x, **y}))\n1.0804965235292912\n>>> min(repeat(lambda: merge_two_dicts(x, y)))\n1.636518670246005\n>>> min(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()}))\n3.1779992282390594\n>>> min(repeat(lambda: dict(chain(x.items(), y.items()))))\n2.740647904574871\n>>> min(repeat(lambda: dict(item for d in (x, y) for item in d.items())))\n4.266070580109954\n>>> min(repeat(lambda: {**x, **y}))\n1.0804965235292912\n>>> min(repeat(lambda: merge_two_dicts(x, y)))\n1.636518670246005\n>>> min(repeat(lambda: {k: v for d in (x, y) for k, v in d.items()}))\n3.1779992282390594\n>>> min(repeat(lambda: dict(chain(x.items(), y.items()))))\n2.740647904574871\n>>> min(repeat(lambda: dict(item for d in (x, y) for item in d.items())))\n4.266070580109954\n$ uname -a\nLinux nixos 4.19.113 #1-NixOS SMP Wed Mar 25 07:06:15 UTC 2020 x86_64 GNU/Linux\n$ uname -a\nLinux nixos 4.19.113 #1-NixOS SMP Wed Mar 25 07:06:15 UTC 2020 x86_64 GNU/Linux\nResources on Dictionaries\nMy explanation of Python's dictionary implementation, updated for 3.6.\nAnswer on how to add new keys to a dictionary\nMapping two lists into a dictionary\nThe official Python docs on dictionaries\nThe Dictionary Even Mightier - talk by Brandon Rhodes at Pycon 2017\nModern Python Dictionaries, A Confluence of Great Ideas - talk by Raymond Hettinger at Pycon 2017\nMy explanation of Python's dictionary implementation, updated for 3.6.My explanation of Python's dictionary implementation, updated for 3.6.dictionary implementationAnswer on how to add new keys to a dictionaryAnswer on how to add new keys to a dictionaryMapping two lists into a dictionaryMapping two lists into a dictionaryThe official Python docs on dictionariesThe official Python docs on dictionariesThe Dictionary Even Mightier - talk by Brandon Rhodes at Pycon 2017The Dictionary Even MightierModern Python Dictionaries, A Confluence of Great Ideas - talk by Raymond Hettinger at Pycon 2017Modern Python Dictionaries, A Confluence of Great Ideas",
                "In your case, you can do:z = dict(list(x.items()) + list(y.items()))\nz = dict(list(x.items()) + list(y.items()))\nThis will, as you want it, put the final dict in z, and make the value for key b be properly overridden by the second (y) dict's value:zby>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> z = dict(list(x.items()) + list(y.items()))\n>>> z\n{'a': 1, 'c': 11, 'b': 10}\n\n>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> z = dict(list(x.items()) + list(y.items()))\n>>> z\n{'a': 1, 'c': 11, 'b': 10}\n\nIf you use Python 2, you can even remove the list() calls. To create z:list()>>> z = dict(x.items() + y.items())\n>>> z\n{'a': 1, 'c': 11, 'b': 10}\n>>> z = dict(x.items() + y.items())\n>>> z\n{'a': 1, 'c': 11, 'b': 10}\nIf you use Python version 3.9.0a4 or greater, then you can directly use:x = {'a':1, 'b': 2}\ny = {'b':10, 'c': 11}\nz = x | y\nprint(z)\nx = {'a':1, 'b': 2}\ny = {'b':10, 'c': 11}\nz = x | y\nprint(z)\n{'a': 1, 'c': 11, 'b': 10}\n{'a': 1, 'c': 11, 'b': 10}\n",
                "An alternative:z = x.copy()\nz.update(y)\nz = x.copy()\nz.update(y)\n",
                "Another, more concise, option:z = dict(x, **y)\nz = dict(x, **y)\nNote: this has become a popular answer, but it is important to point out that if y has any non-string keys, the fact that this works at all is an abuse of a CPython implementation detail, and it does not work in Python 3, or in PyPy, IronPython, or Jython. Also, Guido is not a fan. So I can't recommend this technique for forward-compatible or cross-implementation portable code, which really means it should be avoided entirely.NoteyGuido is not a fan",
                "This probably won't be a popular answer, but you almost certainly do not want to do this.  If you want a copy that's a merge, then use copy (or deepcopy, depending on what you want) and then update.  The two lines of code are much more readable - more Pythonic - than the single line creation with .items() + .items().  Explicit is better than implicit.deepcopyIn addition, when you use .items() (pre Python 3.0), you're creating a new list that contains the items from the dict.  If your dictionaries are large, then that is quite a lot of overhead (two large lists that will be thrown away as soon as the merged dict is created).  update() can work more efficiently, because it can run through the second dict item-by-item.In terms of time:time>>> timeit.Timer(\"dict(x, **y)\", \"x = dict(zip(range(1000), range(1000)))\\ny=dict(zip(range(1000,2000), range(1000,2000)))\").timeit(100000)\n15.52571702003479\n>>> timeit.Timer(\"temp = x.copy()\\ntemp.update(y)\", \"x = dict(zip(range(1000), range(1000)))\\ny=dict(zip(range(1000,2000), range(1000,2000)))\").timeit(100000)\n15.694622993469238\n>>> timeit.Timer(\"dict(x.items() + y.items())\", \"x = dict(zip(range(1000), range(1000)))\\ny=dict(zip(range(1000,2000), range(1000,2000)))\").timeit(100000)\n41.484580039978027\n>>> timeit.Timer(\"dict(x, **y)\", \"x = dict(zip(range(1000), range(1000)))\\ny=dict(zip(range(1000,2000), range(1000,2000)))\").timeit(100000)\n15.52571702003479\n>>> timeit.Timer(\"temp = x.copy()\\ntemp.update(y)\", \"x = dict(zip(range(1000), range(1000)))\\ny=dict(zip(range(1000,2000), range(1000,2000)))\").timeit(100000)\n15.694622993469238\n>>> timeit.Timer(\"dict(x.items() + y.items())\", \"x = dict(zip(range(1000), range(1000)))\\ny=dict(zip(range(1000,2000), range(1000,2000)))\").timeit(100000)\n41.484580039978027\nIMO the tiny slowdown between the first two is worth it for the readability.  In addition, keyword arguments for dictionary creation was only added in Python 2.3, whereas copy() and update() will work in older versions.",
                "In a follow-up answer, you asked about the relative performance of these two alternatives:z1 = dict(x.items() + y.items())\nz2 = dict(x, **y)\nz1 = dict(x.items() + y.items())\nz2 = dict(x, **y)\nOn my machine, at least (a fairly ordinary x86_64 running Python 2.5.2), alternative z2 is not only shorter and simpler but also significantly faster.  You can verify this for yourself using the timeit module that comes with Python.z2timeitExample 1: identical dictionaries mapping 20 consecutive integers to themselves:% python -m timeit -s 'x=y=dict((i,i) for i in range(20))' 'z1=dict(x.items() + y.items())'\n100000 loops, best of 3: 5.67 usec per loop\n% python -m timeit -s 'x=y=dict((i,i) for i in range(20))' 'z2=dict(x, **y)' \n100000 loops, best of 3: 1.53 usec per loop\n% python -m timeit -s 'x=y=dict((i,i) for i in range(20))' 'z1=dict(x.items() + y.items())'\n100000 loops, best of 3: 5.67 usec per loop\n% python -m timeit -s 'x=y=dict((i,i) for i in range(20))' 'z2=dict(x, **y)' \n100000 loops, best of 3: 1.53 usec per loop\nz2 wins by a factor of 3.5 or so.  Different dictionaries seem to yield quite different results, but z2 always seems to come out ahead.  (If you get inconsistent results for the same test, try passing in -r with a number larger than the default 3.)z2z2same-rExample 2: non-overlapping dictionaries mapping 252 short strings to integers and vice versa:% python -m timeit -s 'from htmlentitydefs import codepoint2name as x, name2codepoint as y' 'z1=dict(x.items() + y.items())'\n1000 loops, best of 3: 260 usec per loop\n% python -m timeit -s 'from htmlentitydefs import codepoint2name as x, name2codepoint as y' 'z2=dict(x, **y)'               \n10000 loops, best of 3: 26.9 usec per loop\n% python -m timeit -s 'from htmlentitydefs import codepoint2name as x, name2codepoint as y' 'z1=dict(x.items() + y.items())'\n1000 loops, best of 3: 260 usec per loop\n% python -m timeit -s 'from htmlentitydefs import codepoint2name as x, name2codepoint as y' 'z2=dict(x, **y)'               \n10000 loops, best of 3: 26.9 usec per loop\nz2 wins by about a factor of 10.  That's a pretty big win in my book!z2After comparing those two, I wondered if z1's poor performance could be attributed to the overhead of constructing the two item lists, which in turn led me to wonder if this variation might work better:z1from itertools import chain\nz3 = dict(chain(x.iteritems(), y.iteritems()))\nfrom itertools import chain\nz3 = dict(chain(x.iteritems(), y.iteritems()))\nA few quick tests, e.g.% python -m timeit -s 'from itertools import chain; from htmlentitydefs import codepoint2name as x, name2codepoint as y' 'z3=dict(chain(x.iteritems(), y.iteritems()))'\n10000 loops, best of 3: 66 usec per loop\n% python -m timeit -s 'from itertools import chain; from htmlentitydefs import codepoint2name as x, name2codepoint as y' 'z3=dict(chain(x.iteritems(), y.iteritems()))'\n10000 loops, best of 3: 66 usec per loop\nlead me to conclude that z3 is somewhat faster than z1, but not nearly as fast as z2.  Definitely not worth all the extra typing.z3z1z2This discussion is still missing something important, which is a performance comparison of these alternatives with the \"obvious\" way of merging two lists: using the update method.  To try to keep things on an equal footing with the expressions, none of which modify x or y, I'm going to make a copy of x instead of modifying it in-place, as follows:updatez0 = dict(x)\nz0.update(y)\nz0 = dict(x)\nz0.update(y)\nA typical result:% python -m timeit -s 'from htmlentitydefs import codepoint2name as x, name2codepoint as y' 'z0=dict(x); z0.update(y)'\n10000 loops, best of 3: 26.9 usec per loop\n% python -m timeit -s 'from htmlentitydefs import codepoint2name as x, name2codepoint as y' 'z0=dict(x); z0.update(y)'\n10000 loops, best of 3: 26.9 usec per loop\nIn other words, z0 and z2 seem to have essentially identical performance.  Do you think this might be a coincidence?  I don't....z0z2In fact, I'd go so far as to claim that it's impossible for pure Python code to do any better than this.  And if you can do significantly better in a C extension module, I imagine the Python folks might well be interested in incorporating your code (or a variation on your approach) into the Python core.  Python uses dict in lots of places; optimizing its operations is a big deal.dictYou could also write this asz0 = x.copy()\nz0.update(y)\nz0 = x.copy()\nz0.update(y)\nas Tony does, but (not surprisingly) the difference in notation turns out not to have any measurable effect on performance.  Use whichever looks right to you.  Of course, he's absolutely correct to point out that the two-statement version is much easier to understand.",
                "In Python 3.0 and later, you can use collections.ChainMap which groups multiple dicts or other mappings together to create a single, updateable view:In Python 3.0 and latercollections.ChainMapcollections.ChainMap>>> from collections import ChainMap\n>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> z = dict(ChainMap({}, y, x))\n>>> for k, v in z.items():\n        print(k, '-->', v)\n    \na --> 1\nb --> 10\nc --> 11\n>>> from collections import ChainMap\n>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> z = dict(ChainMap({}, y, x))\n>>> for k, v in z.items():\n        print(k, '-->', v)\n    \na --> 1\nb --> 10\nc --> 11\nUpdate for Python 3.5 and later: You can use PEP 448 extended dictionary packing and unpacking.  This is fast and easy:Update for Python 3.5 and laterPEP 448>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> {**x, **y}\n{'a': 1, 'b': 10, 'c': 11}\n>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> {**x, **y}\n{'a': 1, 'b': 10, 'c': 11}\nUpdate for Python 3.9 and later:  You can use the PEP 584 union operator:Update for Python 3.9 and laterPEP 584>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> x | y\n{'a': 1, 'b': 10, 'c': 11}\n>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> x | y\n{'a': 1, 'b': 10, 'c': 11}\n",
                "I wanted something similar, but with the ability to specify how the values on duplicate keys were merged, so I hacked this out (but did not heavily test it).  Obviously this is not a single expression, but it is a single function call.def merge(d1, d2, merge_fn=lambda x,y:y):\n    \"\"\"\n    Merges two dictionaries, non-destructively, combining \n    values on duplicate keys as defined by the optional merge\n    function.  The default behavior replaces the values in d1\n    with corresponding values in d2.  (There is no other generally\n    applicable merge strategy, but often you'll have homogeneous \n    types in your dicts, so specifying a merge technique can be \n    valuable.)\n\n    Examples:\n\n    >>> d1\n    {'a': 1, 'c': 3, 'b': 2}\n    >>> merge(d1, d1)\n    {'a': 1, 'c': 3, 'b': 2}\n    >>> merge(d1, d1, lambda x,y: x+y)\n    {'a': 2, 'c': 6, 'b': 4}\n\n    \"\"\"\n    result = dict(d1)\n    for k,v in d2.iteritems():\n        if k in result:\n            result[k] = merge_fn(result[k], v)\n        else:\n            result[k] = v\n    return result\ndef merge(d1, d2, merge_fn=lambda x,y:y):\n    \"\"\"\n    Merges two dictionaries, non-destructively, combining \n    values on duplicate keys as defined by the optional merge\n    function.  The default behavior replaces the values in d1\n    with corresponding values in d2.  (There is no other generally\n    applicable merge strategy, but often you'll have homogeneous \n    types in your dicts, so specifying a merge technique can be \n    valuable.)\n\n    Examples:\n\n    >>> d1\n    {'a': 1, 'c': 3, 'b': 2}\n    >>> merge(d1, d1)\n    {'a': 1, 'c': 3, 'b': 2}\n    >>> merge(d1, d1, lambda x,y: x+y)\n    {'a': 2, 'c': 6, 'b': 4}\n\n    \"\"\"\n    result = dict(d1)\n    for k,v in d2.iteritems():\n        if k in result:\n            result[k] = merge_fn(result[k], v)\n        else:\n            result[k] = v\n    return result\n",
                "Recursively/deep update a dictdef deepupdate(original, update):\n    \"\"\"\n    Recursively update a dict.\n    Subdict's won't be overwritten but also updated.\n    \"\"\"\n    for key, value in original.iteritems(): \n        if key not in update:\n            update[key] = value\n        elif isinstance(value, dict):\n            deepupdate(value, update[key]) \n    return updatedef deepupdate(original, update):\n    \"\"\"\n    Recursively update a dict.\n    Subdict's won't be overwritten but also updated.\n    \"\"\"\n    for key, value in original.iteritems(): \n        if key not in update:\n            update[key] = value\n        elif isinstance(value, dict):\n            deepupdate(value, update[key]) \n    return updateDemonstration:pluto_original = {\n    'name': 'Pluto',\n    'details': {\n        'tail': True,\n        'color': 'orange'\n    }\n}\n\npluto_update = {\n    'name': 'Pluutoo',\n    'details': {\n        'color': 'blue'\n    }\n}\n\nprint deepupdate(pluto_original, pluto_update)pluto_original = {\n    'name': 'Pluto',\n    'details': {\n        'tail': True,\n        'color': 'orange'\n    }\n}\n\npluto_update = {\n    'name': 'Pluutoo',\n    'details': {\n        'color': 'blue'\n    }\n}\n\nprint deepupdate(pluto_original, pluto_update)Outputs:{\n    'name': 'Pluutoo',\n    'details': {\n        'color': 'blue',\n        'tail': True\n    }\n}{\n    'name': 'Pluutoo',\n    'details': {\n        'color': 'blue',\n        'tail': True\n    }\n}Thanks rednaw for edits.",
                "Python 3.5 (PEP 448) allows a nicer syntax option:x = {'a': 1, 'b': 1}\ny = {'a': 2, 'c': 2}\nfinal = {**x, **y} \nfinal\n# {'a': 2, 'b': 1, 'c': 2}\nx = {'a': 1, 'b': 1}\ny = {'a': 2, 'c': 2}\nfinal = {**x, **y} \nfinal\n# {'a': 2, 'b': 1, 'c': 2}\nOr even final = {'a': 1, 'b': 1, **x, **y}\nfinal = {'a': 1, 'b': 1, **x, **y}\nIn Python 3.9 you also use | and |= with the below example from PEP 584d = {'spam': 1, 'eggs': 2, 'cheese': 3}\ne = {'cheese': 'cheddar', 'aardvark': 'Ethel'}\nd | e\n# {'spam': 1, 'eggs': 2, 'cheese': 'cheddar', 'aardvark': 'Ethel'}\nd = {'spam': 1, 'eggs': 2, 'cheese': 3}\ne = {'cheese': 'cheddar', 'aardvark': 'Ethel'}\nd | e\n# {'spam': 1, 'eggs': 2, 'cheese': 'cheddar', 'aardvark': 'Ethel'}\n",
                "x = {'a':1, 'b': 2}\ny = {'b':10, 'c': 11}\nz = dict(x.items() + y.items())\nprint z\nx = {'a':1, 'b': 2}\ny = {'b':10, 'c': 11}\nz = dict(x.items() + y.items())\nprint z\nFor items with keys in both dictionaries ('b'), you can control which one ends up in the output by putting that one last.",
                "The best version I could think while not using copy would be:from itertools import chain\nx = {'a':1, 'b': 2}\ny = {'b':10, 'c': 11}\ndict(chain(x.iteritems(), y.iteritems()))\nfrom itertools import chain\nx = {'a':1, 'b': 2}\ny = {'b':10, 'c': 11}\ndict(chain(x.iteritems(), y.iteritems()))\nIt's faster than dict(x.items() + y.items()) but not as fast as n = copy(a); n.update(b), at least on CPython. This version also works in Python 3 if you change iteritems() to items(), which is automatically done by the 2to3 tool.dict(x.items() + y.items())n = copy(a); n.update(b)iteritems()items()Personally I like this version best because it describes fairly good what I want in a single  functional syntax. The only minor problem is that it doesn't make completely obvious that values from y takes precedence over values from x, but I don't believe it's difficult to figure that out.",
                "While the question has already been answered several times,\nthis simple solution to the problem has not been listed yet.x = {'a':1, 'b': 2}\ny = {'b':10, 'c': 11}\nz4 = {}\nz4.update(x)\nz4.update(y)\nx = {'a':1, 'b': 2}\ny = {'b':10, 'c': 11}\nz4 = {}\nz4.update(x)\nz4.update(y)\nIt is as fast as z0 and the evil z2 mentioned above, but easy to understand and change.",
                "def dict_merge(a, b):\n  c = a.copy()\n  c.update(b)\n  return c\n\nnew = dict_merge(old, extras)\ndef dict_merge(a, b):\n  c = a.copy()\n  c.update(b)\n  return c\n\nnew = dict_merge(old, extras)\nAmong such shady and dubious answers, this shining example is the one and only good way to merge dicts in Python, endorsed by dictator for life Guido van Rossum himself!  Someone else suggested half of this, but did not put it in a function.Guido van Rossumprint dict_merge(\n      {'color':'red', 'model':'Mini'},\n      {'model':'Ferrari', 'owner':'Carl'})\nprint dict_merge(\n      {'color':'red', 'model':'Mini'},\n      {'model':'Ferrari', 'owner':'Carl'})\ngives:{'color': 'red', 'owner': 'Carl', 'model': 'Ferrari'}\n{'color': 'red', 'owner': 'Carl', 'model': 'Ferrari'}\n",
                "I benchmarked the suggested with perfplot and found that the good oldperfplotx | y\nx | y\nis the fastest solution together with the good old{**x, **y}\n{**x, **y}\nandtemp = x.copy()\ntemp.update(y)\ntemp = x.copy()\ntemp.update(y)\nCode to reproduce the plot:from collections import ChainMap\nfrom itertools import chain\nimport perfplot\n\n\ndef setup(n):\n    x = dict(zip(range(n), range(n)))\n    y = dict(zip(range(n, 2 * n), range(n, 2 * n)))\n    return x, y\n\n\ndef copy_update(x, y):\n    temp = x.copy()\n    temp.update(y)\n    return temp\n\n\ndef add_items(x, y):\n    return dict(list(x.items()) + list(y.items()))\n\n\ndef curly_star(x, y):\n    return {**x, **y}\n\n\ndef chain_map(x, y):\n    return dict(ChainMap({}, y, x))\n\n\ndef itertools_chain(x, y):\n    return dict(chain(x.items(), y.items()))\n\n\ndef python39_concat(x, y):\n    return x | y\n\n\nb = perfplot.bench(\n    setup=setup,\n    kernels=[\n        copy_update,\n        add_items,\n        curly_star,\n        chain_map,\n        itertools_chain,\n        python39_concat,\n    ],\n    labels=[\n        \"copy_update\",\n        \"dict(list(x.items()) + list(y.items()))\",\n        \"{**x, **y}\",\n        \"chain_map\",\n        \"itertools.chain\",\n        \"x | y\",\n    ],\n    n_range=[2 ** k for k in range(18)],\n    xlabel=\"len(x), len(y)\",\n    equality_check=None,\n)\nb.save(\"out.png\")\nb.show()\nfrom collections import ChainMap\nfrom itertools import chain\nimport perfplot\n\n\ndef setup(n):\n    x = dict(zip(range(n), range(n)))\n    y = dict(zip(range(n, 2 * n), range(n, 2 * n)))\n    return x, y\n\n\ndef copy_update(x, y):\n    temp = x.copy()\n    temp.update(y)\n    return temp\n\n\ndef add_items(x, y):\n    return dict(list(x.items()) + list(y.items()))\n\n\ndef curly_star(x, y):\n    return {**x, **y}\n\n\ndef chain_map(x, y):\n    return dict(ChainMap({}, y, x))\n\n\ndef itertools_chain(x, y):\n    return dict(chain(x.items(), y.items()))\n\n\ndef python39_concat(x, y):\n    return x | y\n\n\nb = perfplot.bench(\n    setup=setup,\n    kernels=[\n        copy_update,\n        add_items,\n        curly_star,\n        chain_map,\n        itertools_chain,\n        python39_concat,\n    ],\n    labels=[\n        \"copy_update\",\n        \"dict(list(x.items()) + list(y.items()))\",\n        \"{**x, **y}\",\n        \"chain_map\",\n        \"itertools.chain\",\n        \"x | y\",\n    ],\n    n_range=[2 ** k for k in range(18)],\n    xlabel=\"len(x), len(y)\",\n    equality_check=None,\n)\nb.save(\"out.png\")\nb.show()\n",
                "Be Pythonic. Use a comprehension:comprehensionz={k: v for d in [x,y] for k, v in d.items()}\n\n>>> print z\n{'a': 1, 'c': 11, 'b': 10}\nz={k: v for d in [x,y] for k, v in d.items()}\n\n>>> print z\n{'a': 1, 'c': 11, 'b': 10}\n",
                "If you think lambdas are evil then read no further.\nAs requested, you can write the fast and memory-efficient solution with one expression:x = {'a':1, 'b':2}\ny = {'b':10, 'c':11}\nz = (lambda a, b: (lambda a_copy: a_copy.update(b) or a_copy)(a.copy()))(x, y)\nprint z\n{'a': 1, 'c': 11, 'b': 10}\nprint x\n{'a': 1, 'b': 2}\nx = {'a':1, 'b':2}\ny = {'b':10, 'c':11}\nz = (lambda a, b: (lambda a_copy: a_copy.update(b) or a_copy)(a.copy()))(x, y)\nprint z\n{'a': 1, 'c': 11, 'b': 10}\nprint x\n{'a': 1, 'b': 2}\nAs suggested above, using two lines or writing a function is probably a better way to go.",
                "In python3, the items method no longer returns a list, but rather a view, which acts like a set. In this case you'll need to take the set union since concatenating with + won't work:itemsno longer returns a listview+dict(x.items() | y.items())\ndict(x.items() | y.items())\nFor python3-like behavior in version 2.7, the viewitems method should work in place of items:viewitemsitemsdict(x.viewitems() | y.viewitems())\ndict(x.viewitems() | y.viewitems())\nI prefer this notation anyways since it seems more natural to think of it as a set union operation rather than concatenation (as the title shows).Edit:Edit:A couple more points for python 3. First, note that the dict(x, **y) trick won't work in python 3 unless the keys in y are strings.dict(x, **y)yAlso, Raymond Hettinger's Chainmap answer is pretty elegant, since it can take an arbitrary number of dicts as arguments, but from the docs it looks like it sequentially looks through a list of all the dicts for each lookup:answerfrom the docs\nLookups search the underlying mappings successively until a key is found.\nLookups search the underlying mappings successively until a key is found.This can slow you down if you have a lot of lookups in your application:In [1]: from collections import ChainMap\nIn [2]: from string import ascii_uppercase as up, ascii_lowercase as lo; x = dict(zip(lo, up)); y = dict(zip(up, lo))\nIn [3]: chainmap_dict = ChainMap(y, x)\nIn [4]: union_dict = dict(x.items() | y.items())\nIn [5]: timeit for k in union_dict: union_dict[k]\n100000 loops, best of 3: 2.15 \u00b5s per loop\nIn [6]: timeit for k in chainmap_dict: chainmap_dict[k]\n10000 loops, best of 3: 27.1 \u00b5s per loop\nIn [1]: from collections import ChainMap\nIn [2]: from string import ascii_uppercase as up, ascii_lowercase as lo; x = dict(zip(lo, up)); y = dict(zip(up, lo))\nIn [3]: chainmap_dict = ChainMap(y, x)\nIn [4]: union_dict = dict(x.items() | y.items())\nIn [5]: timeit for k in union_dict: union_dict[k]\n100000 loops, best of 3: 2.15 \u00b5s per loop\nIn [6]: timeit for k in chainmap_dict: chainmap_dict[k]\n10000 loops, best of 3: 27.1 \u00b5s per loop\nSo about an order of magnitude slower for lookups. I'm a fan of Chainmap, but looks less practical where there may be many lookups.",
                "Two dictionariesTwo dictionariesdef union2(dict1, dict2):\n    return dict(list(dict1.items()) + list(dict2.items()))\ndef union2(dict1, dict2):\n    return dict(list(dict1.items()) + list(dict2.items()))\nn dictionariesn dictionariesndef union(*dicts):\n    return dict(itertools.chain.from_iterable(dct.items() for dct in dicts))\ndef union(*dicts):\n    return dict(itertools.chain.from_iterable(dct.items() for dct in dicts))\nsum has bad performance. See https://mathieularose.com/how-not-to-flatten-a-list-of-lists-in-python/sumhttps://mathieularose.com/how-not-to-flatten-a-list-of-lists-in-python/",
                "Simple solution using itertools that preserves order (latter dicts have precedence)# py2\nfrom itertools import chain, imap\nmerge = lambda *args: dict(chain.from_iterable(imap(dict.iteritems, args)))\n\n# py3\nfrom itertools import chain\nmerge = lambda *args: dict(chain.from_iterable(map(dict.items, args)))\n# py2\nfrom itertools import chain, imap\nmerge = lambda *args: dict(chain.from_iterable(imap(dict.iteritems, args)))\n\n# py3\nfrom itertools import chain\nmerge = lambda *args: dict(chain.from_iterable(map(dict.items, args)))\nAnd it's usage:>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> merge(x, y)\n{'a': 1, 'b': 10, 'c': 11}\n\n>>> z = {'c': 3, 'd': 4}\n>>> merge(x, y, z)\n{'a': 1, 'b': 10, 'c': 3, 'd': 4}\n>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> merge(x, y)\n{'a': 1, 'b': 10, 'c': 11}\n\n>>> z = {'c': 3, 'd': 4}\n>>> merge(x, y, z)\n{'a': 1, 'b': 10, 'c': 3, 'd': 4}\n",
                "Abuse leading to a one-expression solution for Matthew's answer:Matthew's answer>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> z = (lambda f=x.copy(): (f.update(y), f)[1])()\n>>> z\n{'a': 1, 'c': 11, 'b': 10}\n>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> z = (lambda f=x.copy(): (f.update(y), f)[1])()\n>>> z\n{'a': 1, 'c': 11, 'b': 10}\nYou said you wanted one expression, so I abused lambda to bind a name, and tuples to override lambda's one-expression limit. Feel free to cringe.lambdaYou could also do this of course if you don't care about copying it:>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> z = (x.update(y), x)[1]\n>>> z\n{'a': 1, 'b': 10, 'c': 11}\n>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> z = (x.update(y), x)[1]\n>>> z\n{'a': 1, 'b': 10, 'c': 11}\n",
                "If you don't mind mutating x,xx.update(y) or x\nx.update(y) or x\nSimple, readable, performant. You know update() always returns None, which is a false value. So the above expression will always evaluate to x, after updating it.knowupdate()NonexMost mutating methods in the standard library (like .update()) return None by convention, so this kind of pattern will work on those too. However, if you're using a dict subclass or some other method that doesn't follow this convention, then or may return its left operand, which may not be what you want. Instead, you can use a tuple display and index, which works regardless of what the first element evaluates to (although it's not quite as pretty):.update()Noneor(x.update(y), x)[-1]\n(x.update(y), x)[-1]\nIf you don't have x in a variable yet, you can use lambda to make a local without using an assignment statement. This amounts to using lambda as a let expression, which is a common technique in functional languages, but is maybe unpythonic.xlambdalambdalet expression(lambda x: x.update(y) or x)({'a': 1, 'b': 2})\n(lambda x: x.update(y) or x)({'a': 1, 'b': 2})\nAlthough it's not that different from the following use of the new walrus operator (Python 3.8+ only),(x := {'a': 1, 'b': 2}).update(y) or x\n(x := {'a': 1, 'b': 2}).update(y) or x\nespecially if you use a default argument:(lambda x={'a': 1, 'b': 2}: x.update(y) or x)()\n(lambda x={'a': 1, 'b': 2}: x.update(y) or x)()\nIf you do want a copy, PEP 584 style x | y is the most Pythonic on 3.9+. If you must support older versions, PEP 448 style {**x, **y} is easiest for 3.5+. But if that's not available in your (even older) Python version, the let expression pattern works here too.PEP 584x | yPEP 448{**x, **y}let expression(lambda z=x.copy(): z.update(y) or z)()\n(lambda z=x.copy(): z.update(y) or z)()\n(That is, of course, nearly equivalent to (z := x.copy()).update(y) or z, but if your Python version is new enough for that, then the PEP 448 style will be available.)(z := x.copy()).update(y) or z",
                "Drawing on ideas here and elsewhere I've comprehended a function:def merge(*dicts, **kv): \n      return { k:v for d in list(dicts) + [kv] for k,v in d.items() }\ndef merge(*dicts, **kv): \n      return { k:v for d in list(dicts) + [kv] for k,v in d.items() }\nUsage (tested in python 3):assert (merge({1:11,'a':'aaa'},{1:99, 'b':'bbb'},foo='bar')==\\\n    {1: 99, 'foo': 'bar', 'b': 'bbb', 'a': 'aaa'})\n\nassert (merge(foo='bar')=={'foo': 'bar'})\n\nassert (merge({1:11},{1:99},foo='bar',baz='quux')==\\\n    {1: 99, 'foo': 'bar', 'baz':'quux'})\n\nassert (merge({1:11},{1:99})=={1: 99})\nassert (merge({1:11,'a':'aaa'},{1:99, 'b':'bbb'},foo='bar')==\\\n    {1: 99, 'foo': 'bar', 'b': 'bbb', 'a': 'aaa'})\n\nassert (merge(foo='bar')=={'foo': 'bar'})\n\nassert (merge({1:11},{1:99},foo='bar',baz='quux')==\\\n    {1: 99, 'foo': 'bar', 'baz':'quux'})\n\nassert (merge({1:11},{1:99})=={1: 99})\nYou could use a lambda instead.",
                "New in Python 3.9: Use the union operator (|) to merge dicts similar to sets:New in Python 3.9:New|dictset>>> d = {'a': 1, 'b': 2}\n>>> e = {'a': 9, 'c': 3}\n>>> d | e\n{'a': 9, 'b': 2, 'c': 3}\n>>> d = {'a': 1, 'b': 2}\n>>> e = {'a': 9, 'c': 3}\n>>> d | e\n{'a': 9, 'b': 2, 'c': 3}\nFor matching keys, the right dict takes precedence.right dict takes precedencedictThis also works for |= to modify a dict in-place:|=dict>>> e |= d    # e = e | d\n>>> e\n{'a': 1, 'c': 3, 'b': 2}\n>>> e |= d    # e = e | d\n>>> e\n{'a': 1, 'c': 3, 'b': 2}\n",
                "It's so silly that .update returns nothing.\nI just use a simple helper function to solve the problem:.updatedef merge(dict1,*dicts):\n    for dict2 in dicts:\n        dict1.update(dict2)\n    return dict1\ndef merge(dict1,*dicts):\n    for dict2 in dicts:\n        dict1.update(dict2)\n    return dict1\nExamples:merge(dict1,dict2)\nmerge(dict1,dict2,dict3)\nmerge(dict1,dict2,dict3,dict4)\nmerge({},dict1,dict2)  # this one returns a new copy\nmerge(dict1,dict2)\nmerge(dict1,dict2,dict3)\nmerge(dict1,dict2,dict3,dict4)\nmerge({},dict1,dict2)  # this one returns a new copy\n",
                "(For Python\u00a02.7* only; there are simpler solutions for Python\u00a03*.)If you're not averse to importing a standard library module, you can dofrom functools import reduce\n\ndef merge_dicts(*dicts):\n    return reduce(lambda a, d: a.update(d) or a, dicts, {})\nfrom functools import reduce\n\ndef merge_dicts(*dicts):\n    return reduce(lambda a, d: a.update(d) or a, dicts, {})\n(The or a bit in the lambda is necessary because dict.update always returns None on success.)or alambdadict.updateNone",
                "The problem I have with solutions listed to date is that, in the merged dictionary, the value for key \"b\" is 10 but, to my way of thinking, it should be 12.\nIn that light, I present the following:import timeit\n\nn=100000\nsu = \"\"\"\nx = {'a':1, 'b': 2}\ny = {'b':10, 'c': 11}\n\"\"\"\n\ndef timeMerge(f,su,niter):\n    print \"{:4f} sec for: {:30s}\".format(timeit.Timer(f,setup=su).timeit(n),f)\n\ntimeMerge(\"dict(x, **y)\",su,n)\ntimeMerge(\"x.update(y)\",su,n)\ntimeMerge(\"dict(x.items() + y.items())\",su,n)\ntimeMerge(\"for k in y.keys(): x[k] = k in x and x[k]+y[k] or y[k] \",su,n)\n\n#confirm for loop adds b entries together\nx = {'a':1, 'b': 2}\ny = {'b':10, 'c': 11}\nfor k in y.keys(): x[k] = k in x and x[k]+y[k] or y[k]\nprint \"confirm b elements are added:\",x\nimport timeit\n\nn=100000\nsu = \"\"\"\nx = {'a':1, 'b': 2}\ny = {'b':10, 'c': 11}\n\"\"\"\n\ndef timeMerge(f,su,niter):\n    print \"{:4f} sec for: {:30s}\".format(timeit.Timer(f,setup=su).timeit(n),f)\n\ntimeMerge(\"dict(x, **y)\",su,n)\ntimeMerge(\"x.update(y)\",su,n)\ntimeMerge(\"dict(x.items() + y.items())\",su,n)\ntimeMerge(\"for k in y.keys(): x[k] = k in x and x[k]+y[k] or y[k] \",su,n)\n\n#confirm for loop adds b entries together\nx = {'a':1, 'b': 2}\ny = {'b':10, 'c': 11}\nfor k in y.keys(): x[k] = k in x and x[k]+y[k] or y[k]\nprint \"confirm b elements are added:\",x\nResults:0.049465 sec for: dict(x, **y)\n0.033729 sec for: x.update(y)                   \n0.150380 sec for: dict(x.items() + y.items())   \n0.083120 sec for: for k in y.keys(): x[k] = k in x and x[k]+y[k] or y[k]\n\nconfirm b elements are added: {'a': 1, 'c': 11, 'b': 12}\n0.049465 sec for: dict(x, **y)\n0.033729 sec for: x.update(y)                   \n0.150380 sec for: dict(x.items() + y.items())   \n0.083120 sec for: for k in y.keys(): x[k] = k in x and x[k]+y[k] or y[k]\n\nconfirm b elements are added: {'a': 1, 'c': 11, 'b': 12}\n",
                "There will be a new option when Python 3.8 releases (scheduled for 20 October, 2019), thanks to PEP 572: Assignment Expressions. The new assignment expression operator := allows you to assign the result of the copy and still use it to call update, leaving the combined code a single expression, rather than two statements, changing:scheduled for 20 October, 2019PEP 572: Assignment Expressions:=copyupdatenewdict = dict1.copy()\nnewdict.update(dict2)\nnewdict = dict1.copy()\nnewdict.update(dict2)\nto:(newdict := dict1.copy()).update(dict2)\n(newdict := dict1.copy()).update(dict2)\nwhile behaving identically in every way. If you must also return the resulting dict (you asked for an expression returning the dict; the above creates and assigns to newdict, but doesn't return it, so you couldn't use it to pass an argument to a function as is, a la myfunc((newdict := dict1.copy()).update(dict2))), then just add or newdict to the end (since update returns None, which is falsy, it will then evaluate and return newdict as the result of the expression):dictdictnewdictmyfunc((newdict := dict1.copy()).update(dict2))or newdictupdateNonenewdict(newdict := dict1.copy()).update(dict2) or newdict\n(newdict := dict1.copy()).update(dict2) or newdict\nImportant caveat: In general, I'd discourage this approach in favor of:Important caveat:newdict = {**dict1, **dict2}\nnewdict = {**dict1, **dict2}\nThe unpacking approach is clearer (to anyone who knows about generalized unpacking in the first place, which you should), doesn't require a name for the result at all (so it's much more concise when constructing a temporary that is immediately passed to a function or included in a list/tuple literal or the like), and is almost certainly faster as well, being (on CPython) roughly equivalent to:which you shouldlisttuplenewdict = {}\nnewdict.update(dict1)\nnewdict.update(dict2)\nnewdict = {}\nnewdict.update(dict1)\nnewdict.update(dict2)\nbut done at the C layer, using the concrete dict API, so no dynamic method lookup/binding or function call dispatch overhead is involved (where (newdict := dict1.copy()).update(dict2) is unavoidably identical to the original two-liner in behavior, performing the work in discrete steps, with dynamic lookup/binding/invocation of methods.dict(newdict := dict1.copy()).update(dict2)It's also more extensible, as merging three dicts is obvious:dict newdict = {**dict1, **dict2, **dict3}\n newdict = {**dict1, **dict2, **dict3}\nwhere using assignment expressions won't scale like that; the closest you could get would be: (newdict := dict1.copy()).update(dict2), newdict.update(dict3)\n (newdict := dict1.copy()).update(dict2), newdict.update(dict3)\nor without the temporary tuple of Nones, but with truthiness testing of each None result:NoneNone (newdict := dict1.copy()).update(dict2) or newdict.update(dict3)\n (newdict := dict1.copy()).update(dict2) or newdict.update(dict3)\neither of which is obviously much uglier, and includes further inefficiencies (either a wasted temporary tuple of Nones for comma separation, or pointless truthiness testing of each update's None return for or separation).tupleNoneupdateNoneorThe only real advantage to the assignment expression approach occurs if:The only real advantage to the assignment expression approach occurs if:\nYou have generic code that needs handle both sets and dicts (both of them support copy and update, so the code works roughly as you'd expect it to)\nYou expect to receive arbitrary dict-like objects, not just dict itself, and must preserve the type and semantics of the left hand side (rather than ending up with a plain dict). While myspecialdict({**speciala, **specialb}) might work, it would involve an extra temporary dict, and if myspecialdict has features plain dict can't preserve (e.g. regular dicts now preserve order based on the first appearance of a key, and value based on the last appearance of a key; you might want one that preserves order based on the last appearance of a key so updating a value also moves it to the end), then the semantics would be wrong. Since the assignment expression version uses the named methods (which are presumably overloaded to behave appropriately), it never creates a dict at all (unless dict1 was already a dict), preserving the original type (and original type's semantics), all while avoiding any temporaries.\nYou have generic code that needs handle both sets and dicts (both of them support copy and update, so the code works roughly as you'd expect it to)You have generic code that needs handle both sets and dictssetdictcopyupdateYou expect to receive arbitrary dict-like objects, not just dict itself, and must preserve the type and semantics of the left hand side (rather than ending up with a plain dict). While myspecialdict({**speciala, **specialb}) might work, it would involve an extra temporary dict, and if myspecialdict has features plain dict can't preserve (e.g. regular dicts now preserve order based on the first appearance of a key, and value based on the last appearance of a key; you might want one that preserves order based on the last appearance of a key so updating a value also moves it to the end), then the semantics would be wrong. Since the assignment expression version uses the named methods (which are presumably overloaded to behave appropriately), it never creates a dict at all (unless dict1 was already a dict), preserving the original type (and original type's semantics), all while avoiding any temporaries.You expect to receive arbitrary dict-like objectsdictand must preserve the type and semantics of the left hand sidedictmyspecialdict({**speciala, **specialb})dictmyspecialdictdictdictlastdictdict1dict",
                "from collections import Counter\ndict1 = {'a':1, 'b': 2}\ndict2 = {'b':10, 'c': 11}\nresult = dict(Counter(dict1) + Counter(dict2))\nfrom collections import Counter\ndict1 = {'a':1, 'b': 2}\ndict2 = {'b':10, 'c': 11}\nresult = dict(Counter(dict1) + Counter(dict2))\nThis should solve your problem.",
                "This can be done with a single dict comprehension:>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> { key: y[key] if key in y else x[key]\n      for key in set(x) + set(y)\n    }\n>>> x = {'a':1, 'b': 2}\n>>> y = {'b':10, 'c': 11}\n>>> { key: y[key] if key in y else x[key]\n      for key in set(x) + set(y)\n    }\nIn my view the best answer for the 'single expression' part as no extra functions are needed, and it is short."
            ]
        },
        {
            "tag": "system_command",
            "patterns": [
                "How do I execute a program or call a system command?",
                "what can i do to execute a program?",
                "how do you perform system command?",
                "how do i execute my program or call system commands?",
                "how do i execute a program or call system command?",
                "how do i execute a program or call a system command?"
            ],
            "responses": [
                "Use the subprocess module in the standard library:subprocesssubprocessimport subprocess\nsubprocess.run([\"ls\", \"-l\"])\nimport subprocess\nsubprocess.run([\"ls\", \"-l\"])\nThe advantage of subprocess.run over os.system is that it is more flexible (you can get the stdout, stderr, the \"real\" status code, better error handling, etc...).subprocess.runsubprocess.runos.systemos.systemstdoutstdoutstderrstderr\"real\" status codeerror handlingEven the documentation for os.system recommends using subprocess instead:the documentation for os.systemos.systemsubprocess\nThe subprocess module provides more powerful facilities for spawning new processes and retrieving their results; using that module is preferable to using this function. See the Replacing Older Functions with the subprocess Module section in the subprocess documentation for some helpful recipes.\nThe subprocess module provides more powerful facilities for spawning new processes and retrieving their results; using that module is preferable to using this function. See the Replacing Older Functions with the subprocess Module section in the subprocess documentation for some helpful recipes.subprocessReplacing Older Functions with the subprocess ModulesubprocesssubprocessOn Python 3.4 and earlier, use subprocess.call instead of .run:subprocess.call.runsubprocess.call([\"ls\", \"-l\"])\nsubprocess.call([\"ls\", \"-l\"])\n",
                "Here is a summary of ways to call external programs, including their advantages and disadvantages:\nos.system passes the command and arguments to your system's shell. This is nice because you can actually run multiple commands at once in this manner and set up pipes and input/output redirection. For example:\nos.system(\"some_command < input_file | another_command > output_file\")  \n\nHowever, while this is convenient, you have to manually handle the escaping of shell characters such as spaces, et cetera. On the other hand, this also lets you run commands which are simply shell commands and not actually external programs.\n\nos.popen will do the same thing as os.system except that it gives you a file-like object that you can use to access standard input/output for that process. There are 3 other variants of popen that all handle the i/o slightly differently. If you pass everything as a string, then your command is passed to the shell; if you pass them as a list then you don't need to worry about escaping anything. Example:\nprint(os.popen(\"ls -l\").read())\n\n\nsubprocess.Popen. This is intended as a replacement for os.popen, but has the downside of being slightly more complicated by virtue of being so comprehensive. For example, you'd say:\nprint subprocess.Popen(\"echo Hello World\", shell=True, stdout=subprocess.PIPE).stdout.read()\n\ninstead of\nprint os.popen(\"echo Hello World\").read()\n\nbut it is nice to have all of the options there in one unified class instead of 4 different popen functions. See the documentation.\n\nsubprocess.call. This is basically just like the Popen class and takes all of the same arguments, but it simply waits until the command completes and gives you the return code. For example:\nreturn_code = subprocess.call(\"echo Hello World\", shell=True)\n\n\nsubprocess.run. Python 3.5+ only. Similar to the above but even more flexible and returns a CompletedProcess object when the command finishes executing.\n\nos.fork, os.exec, os.spawn are similar to their C language counterparts, but I don't recommend using them directly.\n\nos.system passes the command and arguments to your system's shell. This is nice because you can actually run multiple commands at once in this manner and set up pipes and input/output redirection. For example:\nos.system(\"some_command < input_file | another_command > output_file\")  \n\nHowever, while this is convenient, you have to manually handle the escaping of shell characters such as spaces, et cetera. On the other hand, this also lets you run commands which are simply shell commands and not actually external programs.\nos.system passes the command and arguments to your system's shell. This is nice because you can actually run multiple commands at once in this manner and set up pipes and input/output redirection. For example:os.systemos.systemos.system(\"some_command < input_file | another_command > output_file\")  \nos.system(\"some_command < input_file | another_command > output_file\")  \nHowever, while this is convenient, you have to manually handle the escaping of shell characters such as spaces, et cetera. On the other hand, this also lets you run commands which are simply shell commands and not actually external programs.os.popen will do the same thing as os.system except that it gives you a file-like object that you can use to access standard input/output for that process. There are 3 other variants of popen that all handle the i/o slightly differently. If you pass everything as a string, then your command is passed to the shell; if you pass them as a list then you don't need to worry about escaping anything. Example:\nprint(os.popen(\"ls -l\").read())\n\nos.popen will do the same thing as os.system except that it gives you a file-like object that you can use to access standard input/output for that process. There are 3 other variants of popen that all handle the i/o slightly differently. If you pass everything as a string, then your command is passed to the shell; if you pass them as a list then you don't need to worry about escaping anything. Example:os.popenos.popenos.systemprint(os.popen(\"ls -l\").read())\nprint(os.popen(\"ls -l\").read())\nsubprocess.Popen. This is intended as a replacement for os.popen, but has the downside of being slightly more complicated by virtue of being so comprehensive. For example, you'd say:\nprint subprocess.Popen(\"echo Hello World\", shell=True, stdout=subprocess.PIPE).stdout.read()\n\ninstead of\nprint os.popen(\"echo Hello World\").read()\n\nbut it is nice to have all of the options there in one unified class instead of 4 different popen functions. See the documentation.\nsubprocess.Popen. This is intended as a replacement for os.popen, but has the downside of being slightly more complicated by virtue of being so comprehensive. For example, you'd say:subprocess.Popensubprocess.Popenos.popenprint subprocess.Popen(\"echo Hello World\", shell=True, stdout=subprocess.PIPE).stdout.read()\nprint subprocess.Popen(\"echo Hello World\", shell=True, stdout=subprocess.PIPE).stdout.read()\ninstead ofprint os.popen(\"echo Hello World\").read()\nprint os.popen(\"echo Hello World\").read()\nbut it is nice to have all of the options there in one unified class instead of 4 different popen functions. See the documentation.the documentationsubprocess.call. This is basically just like the Popen class and takes all of the same arguments, but it simply waits until the command completes and gives you the return code. For example:\nreturn_code = subprocess.call(\"echo Hello World\", shell=True)\n\nsubprocess.call. This is basically just like the Popen class and takes all of the same arguments, but it simply waits until the command completes and gives you the return code. For example:subprocess.callsubprocess.callPopenreturn_code = subprocess.call(\"echo Hello World\", shell=True)\nreturn_code = subprocess.call(\"echo Hello World\", shell=True)\nsubprocess.run. Python 3.5+ only. Similar to the above but even more flexible and returns a CompletedProcess object when the command finishes executing.\nsubprocess.run. Python 3.5+ only. Similar to the above but even more flexible and returns a CompletedProcess object when the command finishes executing.subprocess.runsubprocess.runCompletedProcessCompletedProcessos.fork, os.exec, os.spawn are similar to their C language counterparts, but I don't recommend using them directly.\nos.fork, os.exec, os.spawn are similar to their C language counterparts, but I don't recommend using them directly.os.forkos.execos.spawnThe subprocess module should probably be what you use.subprocessFinally, please be aware that for all methods where you pass the final command to be executed by the shell as a string and you are responsible for escaping it. There are serious security implications if any part of the string that you pass can not be fully trusted. For example, if a user is entering some/any part of the string. If you are unsure, only use these methods with constants. To give you a hint of the implications consider this code:There are serious security implicationsprint subprocess.Popen(\"echo %s \" % user_input, stdout=PIPE).stdout.read()\nprint subprocess.Popen(\"echo %s \" % user_input, stdout=PIPE).stdout.read()\nand imagine that the user enters something \"my mama didnt love me && rm -rf /\" which could erase the whole filesystem.my mama didnt love me && rm -rf /",
                "Typical implementation:import subprocess\n\np = subprocess.Popen('ls', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nfor line in p.stdout.readlines():\n    print line,\nretval = p.wait()\nimport subprocess\n\np = subprocess.Popen('ls', shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\nfor line in p.stdout.readlines():\n    print line,\nretval = p.wait()\nYou are free to do what you want with the stdout data in the pipe.  In fact, you can simply omit those parameters (stdout= and stderr=) and it'll behave like os.system().stdoutstdout=stderr=os.system()",
                "Some hints on detaching the child process from the calling one (starting the child process in background).Suppose you want to start a long task from a CGI script. That is, the child process should live longer than the CGI script execution process.The classical example from the subprocess module documentation is:import subprocess\nimport sys\n\n# Some code here\n\npid = subprocess.Popen([sys.executable, \"longtask.py\"]) # Call subprocess\n\n# Some more code here\nimport subprocess\nimport sys\n\n# Some code here\n\npid = subprocess.Popen([sys.executable, \"longtask.py\"]) # Call subprocess\n\n# Some more code here\nThe idea here is that you do not want to wait in the line 'call subprocess' until the longtask.py is finished. But it is not clear what happens after the line 'some more code here' from the example.My target platform was FreeBSD, but the development was on Windows, so I faced the problem on Windows first.On Windows (Windows\u00a0XP), the parent process will not finish until the longtask.py has finished its work. It is not what you want in a CGI script. The problem is not specific to Python; in the PHP community the problems are the same.The solution is to pass DETACHED_PROCESS Process Creation Flag to the underlying CreateProcess function in Windows API.\nIf you happen to have installed pywin32, you can import the flag from the win32process module, otherwise you should define it yourself:Process Creation FlagDETACHED_PROCESS = 0x00000008\n\npid = subprocess.Popen([sys.executable, \"longtask.py\"],\n                       creationflags=DETACHED_PROCESS).pid\nDETACHED_PROCESS = 0x00000008\n\npid = subprocess.Popen([sys.executable, \"longtask.py\"],\n                       creationflags=DETACHED_PROCESS).pid\n/* UPD 2015.10.27 @eryksun in a comment below notes, that the semantically correct flag is CREATE_NEW_CONSOLE (0x00000010) */UPD 2015.10.27On FreeBSD we have another problem: when the parent process is finished, it finishes the child processes as well. And that is not what you want in a CGI script either. Some experiments showed that the problem seemed to be in sharing sys.stdout. And the working solution was the following:pid = subprocess.Popen([sys.executable, \"longtask.py\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\npid = subprocess.Popen([sys.executable, \"longtask.py\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, stdin=subprocess.PIPE)\nI have not checked the code on other platforms and do not know the reasons of the behaviour on FreeBSD. If anyone knows, please share your ideas. Googling on starting background processes in Python does not shed any light yet.",
                "import os\nos.system(\"your command\")\nimport os\nos.system(\"your command\")\nNote that this is dangerous, since the command isn't cleaned. I leave it up to you to google for the relevant documentation on the 'os' and 'sys' modules. There are a bunch of functions (exec* and spawn*) that will do similar things.",
                "I'd recommend using the subprocess module instead of os.system because it does shell escaping for you and is therefore much safer.subprocesssubprocess.call(['ping', 'localhost'])\nsubprocess.call(['ping', 'localhost'])\n",
                "import os\ncmd = 'ls -al'\nos.system(cmd)\nimport os\ncmd = 'ls -al'\nos.system(cmd)\nIf you want to return the results of the command, you can use os.popen. However, this is deprecated since version 2.6 in favor of the subprocess module, which other answers have covered well.os.popenos.popensubprocess module",
                "There are lots of different libraries which allow you to call external commands with Python. For each library I've given a description and shown an example of calling an external command. The command I used as the example is ls -l (list all files). If you want to find out more about any of the libraries I've listed and linked the documentation for each of them.ls -lSources\nsubprocess: https://docs.python.org/3.5/library/subprocess.html\nshlex: https://docs.python.org/3/library/shlex.html\nos: https://docs.python.org/3.5/library/os.html\nsh: https://amoffat.github.io/sh/\nplumbum: https://plumbum.readthedocs.io/en/latest/\npexpect: https://pexpect.readthedocs.io/en/stable/\nfabric: http://www.fabfile.org/\nenvoy: https://github.com/kennethreitz/envoy\ncommands: https://docs.python.org/2/library/commands.html\nsubprocess: https://docs.python.org/3.5/library/subprocess.htmlhttps://docs.python.org/3.5/library/subprocess.htmlshlex: https://docs.python.org/3/library/shlex.htmlhttps://docs.python.org/3/library/shlex.htmlos: https://docs.python.org/3.5/library/os.htmlhttps://docs.python.org/3.5/library/os.htmlsh: https://amoffat.github.io/sh/https://amoffat.github.io/sh/plumbum: https://plumbum.readthedocs.io/en/latest/https://plumbum.readthedocs.io/en/latest/pexpect: https://pexpect.readthedocs.io/en/stable/https://pexpect.readthedocs.io/en/stable/fabric: http://www.fabfile.org/http://www.fabfile.org/envoy: https://github.com/kennethreitz/envoyhttps://github.com/kennethreitz/envoycommands: https://docs.python.org/2/library/commands.htmlhttps://docs.python.org/2/library/commands.htmlThese are all the librariesHopefully this will help you make a decision on which library to use :)subprocesssubprocessSubprocess allows you to call external commands and connect them to their input/output/error pipes (stdin, stdout, and stderr). Subprocess is the default choice for running commands, but sometimes other modules are better.subprocess.run([\"ls\", \"-l\"]) # Run command\nsubprocess.run([\"ls\", \"-l\"], stdout=subprocess.PIPE) # This will run the command and return any output\nsubprocess.run(shlex.split(\"ls -l\")) # You can also use the shlex library to split the command\nsubprocess.run([\"ls\", \"-l\"]) # Run command\nsubprocess.run([\"ls\", \"-l\"], stdout=subprocess.PIPE) # This will run the command and return any output\nsubprocess.run(shlex.split(\"ls -l\")) # You can also use the shlex library to split the command\nososos is used for \"operating system dependent functionality\". It can also be used to call external commands with os.system and os.popen (Note: There is also a subprocess.popen). os will always run the shell and is a simple alternative for people who don't need to, or don't know how to use subprocess.run.os.systemos.popensubprocess.runos.system(\"ls -l\") # Run command\nos.popen(\"ls -l\").read() # This will run the command and return any output\nos.system(\"ls -l\") # Run command\nos.popen(\"ls -l\").read() # This will run the command and return any output\nshshsh is a subprocess interface which lets you call programs as if they were functions. This is useful if you want to run a command multiple times.sh.ls(\"-l\") # Run command normally\nls_cmd = sh.Command(\"ls\") # Save command as a variable\nls_cmd() # Run command as if it were a function\nsh.ls(\"-l\") # Run command normally\nls_cmd = sh.Command(\"ls\") # Save command as a variable\nls_cmd() # Run command as if it were a function\nplumbumplumbumplumbum is a library for \"script-like\" Python programs. You can call programs like functions as in sh. Plumbum is useful if you want to run a pipeline without the shell.shls_cmd = plumbum.local(\"ls -l\") # Get command\nls_cmd() # Run command\nls_cmd = plumbum.local(\"ls -l\") # Get command\nls_cmd() # Run command\npexpectpexpectpexpect lets you spawn child applications, control them and find patterns in their output. This is a better alternative to subprocess for commands that expect a tty on Unix.pexpect.run(\"ls -l\") # Run command as normal\nchild = pexpect.spawn('scp foo user@example.com:.') # Spawns child application\nchild.expect('Password:') # When this is the output\nchild.sendline('mypassword')\npexpect.run(\"ls -l\") # Run command as normal\nchild = pexpect.spawn('scp foo user@example.com:.') # Spawns child application\nchild.expect('Password:') # When this is the output\nchild.sendline('mypassword')\nfabricfabricfabric is a Python 2.5 and 2.7 library. It allows you to execute local and remote shell commands. Fabric is simple alternative for running commands in a secure shell (SSH)fabric.operations.local('ls -l') # Run command as normal\nfabric.operations.local('ls -l', capture = True) # Run command and receive output\nfabric.operations.local('ls -l') # Run command as normal\nfabric.operations.local('ls -l', capture = True) # Run command and receive output\nenvoyenvoyenvoy is known as \"subprocess for humans\". It is used as a convenience wrapper around the subprocess module.subprocessr = envoy.run(\"ls -l\") # Run command\nr.std_out # Get output\nr = envoy.run(\"ls -l\") # Run command\nr.std_out # Get output\ncommandscommandscommands contains wrapper functions for os.popen, but it has been removed from Python 3 since subprocess is a better alternative.commandsos.popensubprocess",
                "With the standard libraryUse the subprocess module (Python 3):subprocess moduleimport subprocess\nsubprocess.run(['ls', '-l'])\nimport subprocess\nsubprocess.run(['ls', '-l'])\nIt is the recommended standard way. However, more complicated tasks (pipes, output, input, etc.) can be tedious to construct and write.Note on Python version: If you are still using Python 2, subprocess.call works in a similar way.Note on Python version: If you are still using Python 2, subprocess.call works in a similar way.subprocess.callProTip: shlex.split can help you to parse the command for run, call, and other subprocess functions in case you don't want (or you can't!) provide them in form of lists:ProTip: shlex.split can help you to parse the command for run, call, and other subprocess functions in case you don't want (or you can't!) provide them in form of lists:shlex.splitruncallsubprocessimport shlex\nimport subprocess\nsubprocess.run(shlex.split('ls -l'))\nimport shlex\nimport subprocess\nsubprocess.run(shlex.split('ls -l'))\nWith external dependenciesIf you do not mind external dependencies, use plumbum:plumbumfrom plumbum.cmd import ifconfig\nprint(ifconfig['wlan0']())\nfrom plumbum.cmd import ifconfig\nprint(ifconfig['wlan0']())\nIt is the best subprocess wrapper. It's cross-platform, i.e. it works on both Windows and Unix-like systems. Install by pip install plumbum.subprocesspip install plumbumAnother popular library is sh:shfrom sh import ifconfig\nprint(ifconfig('wlan0'))\nfrom sh import ifconfig\nprint(ifconfig('wlan0'))\nHowever, sh dropped Windows support, so it's not as awesome as it used to be. Install by pip install sh.shpip install sh",
                "I always use fabric for this things like:fabricfrom fabric.operations import local\nresult = local('ls', capture=True)\nprint \"Content:/n%s\" % (result, )\nfrom fabric.operations import local\nresult = local('ls', capture=True)\nprint \"Content:/n%s\" % (result, )\nBut this seem to be a good tool: sh (Python subprocess interface).sh (Python subprocess interface)shLook at an example:from sh import vgdisplay\nprint vgdisplay()\nprint vgdisplay('-v')\nprint vgdisplay(v=True)\nfrom sh import vgdisplay\nprint vgdisplay()\nprint vgdisplay('-v')\nprint vgdisplay(v=True)\n",
                "Check the \"pexpect\" Python library, too.It allows for interactive controlling of external programs/commands, even ssh, ftp, telnet, etc. You can just type something like:child = pexpect.spawn('ftp 192.168.0.24')\n\nchild.expect('(?i)name .*: ')\n\nchild.sendline('anonymous')\n\nchild.expect('(?i)password')\nchild = pexpect.spawn('ftp 192.168.0.24')\n\nchild.expect('(?i)name .*: ')\n\nchild.sendline('anonymous')\n\nchild.expect('(?i)password')\n",
                "If you need the output from the command you are calling,\nthen you can use subprocess.check_output (Python 2.7+).subprocess.check_output>>> subprocess.check_output([\"ls\", \"-l\", \"/dev/null\"])\n'crw-rw-rw- 1 root root 1, 3 Oct 18  2007 /dev/null\\n'\n>>> subprocess.check_output([\"ls\", \"-l\", \"/dev/null\"])\n'crw-rw-rw- 1 root root 1, 3 Oct 18  2007 /dev/null\\n'\nAlso note the shell parameter.shell\nIf shell is True, the specified command will be executed through the shell. This can be useful if you are using Python primarily for the enhanced control flow it offers over most system shells and still want convenient access to other shell features such as shell pipes, filename wildcards, environment variable expansion, and expansion of ~ to a user\u2019s home directory. However, note that Python itself offers implementations of many shell-like features (in particular, glob, fnmatch, os.walk(), os.path.expandvars(), os.path.expanduser(), and shutil).\nIf shell is True, the specified command will be executed through the shell. This can be useful if you are using Python primarily for the enhanced control flow it offers over most system shells and still want convenient access to other shell features such as shell pipes, filename wildcards, environment variable expansion, and expansion of ~ to a user\u2019s home directory. However, note that Python itself offers implementations of many shell-like features (in particular, glob, fnmatch, os.walk(), os.path.expandvars(), os.path.expanduser(), and shutil).Trueglobfnmatchos.walk()os.path.expandvars()os.path.expanduser()shutil",
                "Update:subprocess.run is the recommended approach as of Python 3.5 if your code does not need to maintain compatibility with earlier Python versions. It's more consistent and offers similar ease-of-use as Envoy. (Piping isn't as straightforward though. See this question for how.)subprocess.runas of Python 3.5this question for howHere's some examples from the documentation.the documentationRun a process:>>> subprocess.run([\"ls\", \"-l\"])  # Doesn't capture output\nCompletedProcess(args=['ls', '-l'], returncode=0)\n>>> subprocess.run([\"ls\", \"-l\"])  # Doesn't capture output\nCompletedProcess(args=['ls', '-l'], returncode=0)\nRaise on failed run:>>> subprocess.run(\"exit 1\", shell=True, check=True)\nTraceback (most recent call last):\n  ...\nsubprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1\n>>> subprocess.run(\"exit 1\", shell=True, check=True)\nTraceback (most recent call last):\n  ...\nsubprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1\nCapture output:>>> subprocess.run([\"ls\", \"-l\", \"/dev/null\"], stdout=subprocess.PIPE)\nCompletedProcess(args=['ls', '-l', '/dev/null'], returncode=0,\nstdout=b'crw-rw-rw- 1 root root 1, 3 Jan 23 16:23 /dev/null\\n')\n>>> subprocess.run([\"ls\", \"-l\", \"/dev/null\"], stdout=subprocess.PIPE)\nCompletedProcess(args=['ls', '-l', '/dev/null'], returncode=0,\nstdout=b'crw-rw-rw- 1 root root 1, 3 Jan 23 16:23 /dev/null\\n')\nOriginal answer:I recommend trying Envoy. It's a wrapper for subprocess, which in turn aims to replace the older modules and functions. Envoy is subprocess for humans.Envoyaims to replaceExample usage from the README:the README>>> r = envoy.run('git config', data='data to pipe in', timeout=2)\n\n>>> r.status_code\n129\n>>> r.std_out\n'usage: git config [options]'\n>>> r.std_err\n''\n>>> r = envoy.run('git config', data='data to pipe in', timeout=2)\n\n>>> r.status_code\n129\n>>> r.std_out\n'usage: git config [options]'\n>>> r.std_err\n''\nPipe stuff around too:>>> r = envoy.run('uptime | pbcopy')\n\n>>> r.command\n'pbcopy'\n>>> r.status_code\n0\n\n>>> r.history\n[<Response 'uptime'>]\n>>> r = envoy.run('uptime | pbcopy')\n\n>>> r.command\n'pbcopy'\n>>> r.status_code\n0\n\n>>> r.history\n[<Response 'uptime'>]\n",
                "This is how I run my commands. This code has everything you need pretty muchfrom subprocess import Popen, PIPE\ncmd = \"ls -l ~/\"\np = Popen(cmd , shell=True, stdout=PIPE, stderr=PIPE)\nout, err = p.communicate()\nprint \"Return code: \", p.returncode\nprint out.rstrip(), err.rstrip()\nfrom subprocess import Popen, PIPE\ncmd = \"ls -l ~/\"\np = Popen(cmd , shell=True, stdout=PIPE, stderr=PIPE)\nout, err = p.communicate()\nprint \"Return code: \", p.returncode\nprint out.rstrip(), err.rstrip()\n",
                "\nHow to execute a program or call a system command from Python\nHow to execute a program or call a system command from PythonSimple, use subprocess.run, which returns a CompletedProcess object:subprocess.runCompletedProcess>>> from subprocess import run\n>>> from shlex import split\n>>> completed_process = run(split('python --version'))\nPython 3.8.8\n>>> completed_process\nCompletedProcess(args=['python', '--version'], returncode=0)\n>>> from subprocess import run\n>>> from shlex import split\n>>> completed_process = run(split('python --version'))\nPython 3.8.8\n>>> completed_process\nCompletedProcess(args=['python', '--version'], returncode=0)\n(run wants a list of lexically parsed shell arguments - this is what you'd type in a shell, separated by spaces, but not where the spaces are quoted, so use a specialized function, split, to split up what you would literally type into your shell)runsplitWhy?As of Python 3.5, the documentation recommends subprocess.run:subprocess.run\nThe recommended approach to invoking subprocesses is to use the run() function for all use cases it can handle. For more advanced use cases, the underlying Popen interface can be used directly.\nThe recommended approach to invoking subprocesses is to use the run() function for all use cases it can handle. For more advanced use cases, the underlying Popen interface can be used directly.Here's an example of the simplest possible usage - and it does exactly as asked:>>> from subprocess import run\n>>> from shlex import split\n>>> completed_process = run(split('python --version'))\nPython 3.8.8\n>>> completed_process\nCompletedProcess(args=['python', '--version'], returncode=0)\n>>> from subprocess import run\n>>> from shlex import split\n>>> completed_process = run(split('python --version'))\nPython 3.8.8\n>>> completed_process\nCompletedProcess(args=['python', '--version'], returncode=0)\nrun waits for the command to successfully finish, then returns a CompletedProcess object. It may instead raise TimeoutExpired (if you give it a timeout= argument) or CalledProcessError (if it fails and you pass check=True).runCompletedProcessTimeoutExpiredtimeout=CalledProcessErrorcheck=TrueAs you might infer from the above example, stdout and stderr both get piped to your own stdout and stderr by default.We can inspect the returned object and see the command that was given and the returncode:>>> completed_process.args\n['python', '--version']\n>>> completed_process.returncode\n0\n>>> completed_process.args\n['python', '--version']\n>>> completed_process.returncode\n0\nCapturing outputIf you want to capture the output, you can pass subprocess.PIPE to the appropriate stderr or stdout:subprocess.PIPEstderrstdout>>> from subprocess import PIPE\n>>> completed_process = run(shlex.split('python --version'), stdout=PIPE, stderr=PIPE)\n>>> completed_process.stdout\nb'Python 3.8.8\\n'\n>>> completed_process.stderr\nb''\n>>> from subprocess import PIPE\n>>> completed_process = run(shlex.split('python --version'), stdout=PIPE, stderr=PIPE)\n>>> completed_process.stdout\nb'Python 3.8.8\\n'\n>>> completed_process.stderr\nb''\nAnd those respective attributes return bytes.Pass a command listOne might easily move from manually providing a command string (like the question suggests) to providing a string built programmatically. Don't build strings programmatically. This is a potential security issue. It's better to assume you don't trust the input.Don't build strings programmatically.>>> import textwrap\n>>> args = ['python', textwrap.__file__]\n>>> cp = run(args, stdout=subprocess.PIPE)\n>>> cp.stdout\nb'Hello there.\\n  This is indented.\\n'\n>>> import textwrap\n>>> args = ['python', textwrap.__file__]\n>>> cp = run(args, stdout=subprocess.PIPE)\n>>> cp.stdout\nb'Hello there.\\n  This is indented.\\n'\nNote, only args should be passed positionally.argsFull SignatureHere's the actual signature in the source and as shown by help(run):help(run)\ndef run(*popenargs, input=None, timeout=None, check=False, **kwargs):\n\ndef run(*popenargs, input=None, timeout=None, check=False, **kwargs):\ndef run(*popenargs, input=None, timeout=None, check=False, **kwargs):\nThe popenargs and kwargs are given to the Popen constructor. input can be a string of bytes (or unicode, if specify encoding or universal_newlines=True) that will be piped to the subprocess's stdin.popenargskwargsPopeninputuniversal_newlines=TrueThe documentation describes timeout= and check=True better than I could:timeout=check=True\nThe timeout argument is passed to Popen.communicate(). If the timeout\nexpires, the child process will be killed and waited for. The\nTimeoutExpired exception will be re-raised after the child process has\nterminated.\nIf check is true, and the process exits with a non-zero exit code, a\nCalledProcessError exception will be raised. Attributes of that\nexception hold the arguments, the exit code, and stdout and stderr if\nthey were captured.\nThe timeout argument is passed to Popen.communicate(). If the timeout\nexpires, the child process will be killed and waited for. The\nTimeoutExpired exception will be re-raised after the child process has\nterminated.If check is true, and the process exits with a non-zero exit code, a\nCalledProcessError exception will be raised. Attributes of that\nexception hold the arguments, the exit code, and stdout and stderr if\nthey were captured.and this example for check=True is better than one I could come up with:check=True\n>>> subprocess.run(\"exit 1\", shell=True, check=True)\nTraceback (most recent call last):\n  ...\nsubprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1\n\n>>> subprocess.run(\"exit 1\", shell=True, check=True)\nTraceback (most recent call last):\n  ...\nsubprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1\n>>> subprocess.run(\"exit 1\", shell=True, check=True)\nTraceback (most recent call last):\n  ...\nsubprocess.CalledProcessError: Command 'exit 1' returned non-zero exit status 1\nExpanded SignatureHere's an expanded signature, as given in the documentation:\nsubprocess.run(args, *, stdin=None, input=None, stdout=None, stderr=None, \nshell=False, cwd=None, timeout=None, check=False, encoding=None, \nerrors=None)\n\nsubprocess.run(args, *, stdin=None, input=None, stdout=None, stderr=None, \nshell=False, cwd=None, timeout=None, check=False, encoding=None, \nerrors=None)\nsubprocess.run(args, *, stdin=None, input=None, stdout=None, stderr=None, \nshell=False, cwd=None, timeout=None, check=False, encoding=None, \nerrors=None)\nNote that this indicates that only the args list should be passed positionally. So pass the remaining arguments as keyword arguments.PopenWhen use Popen instead? I would struggle to find use-case based on the arguments alone. Direct usage of Popen would, however, give you access to its methods, including poll, 'send_signal', 'terminate', and 'wait'.PopenPopenpollHere's the Popen signature as given in the source. I think this is the most precise encapsulation of the information (as opposed to help(Popen)):Popenthe sourcehelp(Popen)\ndef __init__(self, args, bufsize=-1, executable=None,\n             stdin=None, stdout=None, stderr=None,\n             preexec_fn=None, close_fds=True,\n             shell=False, cwd=None, env=None, universal_newlines=None,\n             startupinfo=None, creationflags=0,\n             restore_signals=True, start_new_session=False,\n             pass_fds=(), *, user=None, group=None, extra_groups=None,\n             encoding=None, errors=None, text=None, umask=-1, pipesize=-1):\n\n\ndef __init__(self, args, bufsize=-1, executable=None,\n             stdin=None, stdout=None, stderr=None,\n             preexec_fn=None, close_fds=True,\n             shell=False, cwd=None, env=None, universal_newlines=None,\n             startupinfo=None, creationflags=0,\n             restore_signals=True, start_new_session=False,\n             pass_fds=(), *, user=None, group=None, extra_groups=None,\n             encoding=None, errors=None, text=None, umask=-1, pipesize=-1):\n\nBut more informative is the Popen documentation:the Popen documentationPopen\nsubprocess.Popen(args, bufsize=-1, executable=None, stdin=None, stdout=None, \nstderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None,\nenv=None, universal_newlines=None, startupinfo=None, creationflags=0, \nrestore_signals=True, start_new_session=False, pass_fds=(), *, group=None, \nextra_groups=None, user=None, umask=-1, encoding=None, errors=None, \ntext=None)\n\nExecute a child program in a new process. On POSIX, the class uses\nos.execvp()-like behavior to execute the child program. On Windows,\nthe class uses the Windows CreateProcess() function. The arguments to\nPopen are as follows.\nsubprocess.Popen(args, bufsize=-1, executable=None, stdin=None, stdout=None, \nstderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None,\nenv=None, universal_newlines=None, startupinfo=None, creationflags=0, \nrestore_signals=True, start_new_session=False, pass_fds=(), *, group=None, \nextra_groups=None, user=None, umask=-1, encoding=None, errors=None, \ntext=None)\nsubprocess.Popen(args, bufsize=-1, executable=None, stdin=None, stdout=None, \nstderr=None, preexec_fn=None, close_fds=True, shell=False, cwd=None,\nenv=None, universal_newlines=None, startupinfo=None, creationflags=0, \nrestore_signals=True, start_new_session=False, pass_fds=(), *, group=None, \nextra_groups=None, user=None, umask=-1, encoding=None, errors=None, \ntext=None)\nExecute a child program in a new process. On POSIX, the class uses\nos.execvp()-like behavior to execute the child program. On Windows,\nthe class uses the Windows CreateProcess() function. The arguments to\nPopen are as follows.Understanding the remaining documentation on Popen will be left as an exercise for the reader.Popen",
                "Use subprocess.subprocesssubprocess...or for a very simple command:import os\nos.system('cat testfile')\nimport os\nos.system('cat testfile')\n",
                "As of Python 3.7.0 released on June 27th 2018 (https://docs.python.org/3/whatsnew/3.7.html), you can achieve your desired result in the most powerful while equally simple way. This answer intends to show you the essential summary of various options in a short manner. For in-depth answers, please see the other ones.As of Python 3.7.0 released on June 27th 2018 (https://docs.python.org/3/whatsnew/3.7.html), you can achieve your desired result in the most powerful while equally simple way. This answer intends to show you the essential summary of various options in a short manner. For in-depth answers, please see the other ones.Python 3.7.0 released on June 27th 2018 (https://docs.python.org/3/whatsnew/3.7.html)https://docs.python.org/3/whatsnew/3.7.htmlTL;DR in 2021The big advantage of os.system(...) was its simplicity. subprocess is better and still easy to use, especially as of Python 3.5.os.system(...)subprocessPython 3.5import subprocess\nsubprocess.run(\"ls -a\", shell=True)\nimport subprocess\nsubprocess.run(\"ls -a\", shell=True)\nNote: This is the exact answer to your question - running a commandNote:Note:\nlike in a shell\nlike in a shellPreferred WayIf possible, remove the shell overhead and run the command directly (requires a list).import subprocess\nsubprocess.run([\"help\"])\nsubprocess.run([\"ls\", \"-a\"])\nimport subprocess\nsubprocess.run([\"help\"])\nsubprocess.run([\"ls\", \"-a\"])\nPass program arguments in a list. Don't include \\\"-escaping for arguments containing spaces.Don't include \\\"-escaping for arguments containing spaces.\\\"Advanced Use CasesChecking The OutputThe following code speaks for itself:import subprocess\nresult = subprocess.run([\"ls\", \"-a\"], capture_output=True, text=True)\nif \"stackoverflow-logo.png\" in result.stdout:\n    print(\"You're a fan!\")\nelse:\n    print(\"You're not a fan?\")\nimport subprocess\nresult = subprocess.run([\"ls\", \"-a\"], capture_output=True, text=True)\nif \"stackoverflow-logo.png\" in result.stdout:\n    print(\"You're a fan!\")\nelse:\n    print(\"You're not a fan?\")\nresult.stdout is all normal program output excluding errors. Read result.stderr to get them.result.stdoutexcluding errorsresult.stderrcapture_output=True - turns capturing on. Otherwise result.stderr and result.stdout would be None. Available from Python 3.7.capture_output=Trueresult.stderrresult.stdoutNonePython 3.7text=True - a convenience argument added in Python 3.7 which converts the received binary data to Python strings you can easily work with.text=TruePython 3.7Checking the returncodeDoif result.returncode == 127: print(\"The program failed for some weird reason\")\nelif result.returncode == 0: print(\"The program succeeded\")\nelse: print(\"The program failed unexpectedly\")\nif result.returncode == 127: print(\"The program failed for some weird reason\")\nelif result.returncode == 0: print(\"The program succeeded\")\nelse: print(\"The program failed unexpectedly\")\nIf you just want to check if the program succeeded (returncode == 0) and otherwise throw an Exception, there is a more convenient function:result.check_returncode()\nresult.check_returncode()\nBut it's Python, so there's an even more convenient argument check which does the same thing automatically for you:checkresult = subprocess.run(..., check=True)\nresult = subprocess.run(..., check=True)\nstderr should be inside stdoutYou might want to have all program output inside stdout, even errors. To accomplish this, runresult = subprocess.run(..., stderr=subprocess.STDOUT)\nresult = subprocess.run(..., stderr=subprocess.STDOUT)\nresult.stderr will then be None and result.stdout will contain everything.result.stderrNoneresult.stdoutUsing shell=False with an argument stringshell=False expects a list of arguments. You might however, split an argument string on your own using shlex.shell=Falselistimport subprocess\nimport shlex\nsubprocess.run(shlex.split(\"ls -a\"))\nimport subprocess\nimport shlex\nsubprocess.run(shlex.split(\"ls -a\"))\nThat's it.Common ProblemsChances are high you just started using Python when you come across this question. Let's look at some common problems.\nFileNotFoundError: [Errno 2] No such file or directory: 'ls -a': 'ls -a'\nFileNotFoundError: [Errno 2] No such file or directory: 'ls -a': 'ls -a'You're running a subprocess without shell=True . Either use a list ([\"ls\", \"-a\"]) or set shell=True.You're running a subprocess without shell=True . Either use a list ([\"ls\", \"-a\"]) or set shell=True.shell=True[\"ls\", \"-a\"]shell=True\nTypeError: [...] NoneType [...]\nTypeError: [...] NoneType [...]Check that you've set capture_output=True.Check that you've set capture_output=True.capture_output=True\nTypeError: a bytes-like object is required, not [...]\nTypeError: a bytes-like object is required, not [...]You always receive byte results from your program. If you want to work with it like a normal string, set text=True.You always receive byte results from your program. If you want to work with it like a normal string, set text=True.text=True\nsubprocess.CalledProcessError: Command '[...]' returned non-zero exit status 1.\nsubprocess.CalledProcessError: Command '[...]' returned non-zero exit status 1.Your command didn't run successfully. You could disable returncode checking or check your actual program's validity.Your command didn't run successfully. You could disable returncode checking or check your actual program's validity.\nTypeError: init() got an unexpected keyword argument [...]\nTypeError: init() got an unexpected keyword argument [...]initYou're likely using a version of Python older than 3.7.0; update it to the most recent one available. Otherwise there are other answers in this Stack Overflow post showing you older alternative solutions.You're likely using a version of Python older than 3.7.0; update it to the most recent one available. Otherwise there are other answers in this Stack Overflow post showing you older alternative solutions.",
                "os.system is OK, but kind of dated.  It's also not very secure.  Instead, try subprocess.  subprocess does not call sh directly and is therefore more secure than os.system.os.systemsubprocesssubprocessos.systemGet more information here.here",
                "There is also PlumbumPlumbum>>> from plumbum import local\n>>> ls = local[\"ls\"]\n>>> ls\nLocalCommand(<LocalPath /bin/ls>)\n>>> ls()\nu'build.py\\ndist\\ndocs\\nLICENSE\\nplumbum\\nREADME.rst\\nsetup.py\\ntests\\ntodo.txt\\n'\n>>> notepad = local[\"c:\\\\windows\\\\notepad.exe\"]\n>>> notepad()                                   # Notepad window pops up\nu''                                             # Notepad window is closed by user, command returns\n>>> from plumbum import local\n>>> ls = local[\"ls\"]\n>>> ls\nLocalCommand(<LocalPath /bin/ls>)\n>>> ls()\nu'build.py\\ndist\\ndocs\\nLICENSE\\nplumbum\\nREADME.rst\\nsetup.py\\ntests\\ntodo.txt\\n'\n>>> notepad = local[\"c:\\\\windows\\\\notepad.exe\"]\n>>> notepad()                                   # Notepad window pops up\nu''                                             # Notepad window is closed by user, command returns\n",
                "Use:import os\n\ncmd = 'ls -al'\n\nos.system(cmd)\nimport os\n\ncmd = 'ls -al'\n\nos.system(cmd)\nos - This module provides a portable way of using operating system-dependent functionality.os - This module provides a portable way of using operating system-dependent functionality.For the more os functions, here is the documentation.oshere",
                "It can be this simple:import os\ncmd = \"your command\"\nos.system(cmd)\nimport os\ncmd = \"your command\"\nos.system(cmd)\n",
                "There is another difference here which is not mentioned previously.subprocess.Popen executes the <command> as a subprocess. In my case, I need to execute file <a> which needs to communicate with another program, <b>. subprocess.PopenI tried subprocess, and execution was successful. However <b> could not communicate with <a>.\nEverything is normal when I run both from the terminal.One more: \n(NOTE: kwrite behaves different from other applications. If you try the below with Firefox, the results will not be the same.)If you try os.system(\"kwrite\"), program flow freezes until the user closes kwrite. To overcome that I tried instead os.system(konsole -e kwrite). This time program continued to flow, but kwrite became the subprocess of the console.os.system(\"kwrite\")os.system(konsole -e kwrite)Anyone runs the kwrite not being a subprocess (i.e. in the system monitor it must appear at the leftmost edge of the tree).",
                "os.system does not allow you to store results, so if you want to store results in some list or something, a subprocess.call works.os.systemsubprocess.call",
                "subprocess.check_call is convenient if you don't want to test return values. It throws an exception on any error.subprocess.check_call",
                "I tend to use subprocess together with shlex (to handle escaping of quoted strings):subprocessshlex>>> import subprocess, shlex\n>>> command = 'ls -l \"/your/path/with spaces/\"'\n>>> call_params = shlex.split(command)\n>>> print call_params\n[\"ls\", \"-l\", \"/your/path/with spaces/\"]\n>>> subprocess.call(call_params)\n>>> import subprocess, shlex\n>>> command = 'ls -l \"/your/path/with spaces/\"'\n>>> call_params = shlex.split(command)\n>>> print call_params\n[\"ls\", \"-l\", \"/your/path/with spaces/\"]\n>>> subprocess.call(call_params)\n",
                "I wrote a library for this, shell.py.shell.pyshell.pyIt's basically a wrapper for popen and shlex for now. It also supports piping commands, so you can chain commands easier in Python. So you can do things like:ex('echo hello shell.py') | \"awk '{print $2}'\"\nex('echo hello shell.py') | \"awk '{print $2}'\"\n",
                "In Windows you can just import the subprocess module and run external commands by calling subprocess.Popen(), subprocess.Popen().communicate() and subprocess.Popen().wait() as below:subprocesssubprocess.Popen()subprocess.Popen().communicate()subprocess.Popen().wait()# Python script to run a command line\nimport subprocess\n\ndef execute(cmd):\n    \"\"\"\n        Purpose  : To execute a command and return exit status\n        Argument : cmd - command to execute\n        Return   : exit_code\n    \"\"\"\n    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    (result, error) = process.communicate()\n\n    rc = process.wait()\n\n    if rc != 0:\n        print \"Error: failed to execute command:\", cmd\n        print error\n    return result\n# def\n\ncommand = \"tasklist | grep python\"\nprint \"This process detail: \\n\", execute(command)\n# Python script to run a command line\nimport subprocess\n\ndef execute(cmd):\n    \"\"\"\n        Purpose  : To execute a command and return exit status\n        Argument : cmd - command to execute\n        Return   : exit_code\n    \"\"\"\n    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    (result, error) = process.communicate()\n\n    rc = process.wait()\n\n    if rc != 0:\n        print \"Error: failed to execute command:\", cmd\n        print error\n    return result\n# def\n\ncommand = \"tasklist | grep python\"\nprint \"This process detail: \\n\", execute(command)\nOutput:This process detail:\npython.exe                     604 RDP-Tcp#0                  4      5,660 K\nThis process detail:\npython.exe                     604 RDP-Tcp#0                  4      5,660 K\n",
                "Under Linux, in case you would like to call an external command that will execute independently (will keep running after the Python script terminates), you can use a simple queue as task spooler or the at command.task spooleratAn example with task spooler:import os\nos.system('ts <your-command>')\nimport os\nos.system('ts <your-command>')\nNotes about task spooler (ts):ts\nYou could set the number of concurrent processes to be run (\"slots\") with:\nts -S <number-of-slots>\n\nInstalling ts doesn't requires admin privileges. You can download and compile it from source with a simple make, add it to your path and you're done.\n\nYou could set the number of concurrent processes to be run (\"slots\") with:\nts -S <number-of-slots>\nYou could set the number of concurrent processes to be run (\"slots\") with:ts -S <number-of-slots>ts -S <number-of-slots>Installing ts doesn't requires admin privileges. You can download and compile it from source with a simple make, add it to your path and you're done.\nInstalling ts doesn't requires admin privileges. You can download and compile it from source with a simple make, add it to your path and you're done.tsmake",
                "Invoke is a Python (2.7 and 3.4+) task execution tool and library. It provides a clean, high-level API for running shell commands:InvokeInvoke>>> from invoke import run\n>>> cmd = \"pip install -r requirements.txt\"\n>>> result = run(cmd, hide=True, warn=True)\n>>> print(result.ok)\nTrue\n>>> print(result.stdout.splitlines()[-1])\nSuccessfully installed invocations-0.13.0 pep8-1.5.7 spec-1.3.1\n>>> from invoke import run\n>>> cmd = \"pip install -r requirements.txt\"\n>>> result = run(cmd, hide=True, warn=True)\n>>> print(result.ok)\nTrue\n>>> print(result.stdout.splitlines()[-1])\nSuccessfully installed invocations-0.13.0 pep8-1.5.7 spec-1.3.1\n",
                "You can use Popen, and then you can check the procedure's status:from subprocess import Popen\n\nproc = Popen(['ls', '-l'])\nif proc.poll() is None:\n    proc.kill()\nfrom subprocess import Popen\n\nproc = Popen(['ls', '-l'])\nif proc.poll() is None:\n    proc.kill()\nCheck out subprocess.Popen.subprocess.Popen"
            ]
        },
        {
            "tag": "nested_directory",
            "patterns": [
                "How can I safely create a nested directory?",
                "how do i create my own nested directory?",
                "what can i do to create a nested directory?"
            ],
            "responses": [
                "On Python \u2265 3.5, use pathlib.Path.mkdir:pathlib.Path.mkdirpathlib.Path.mkdirfrom pathlib import Path\nPath(\"/my/directory\").mkdir(parents=True, exist_ok=True)\nfrom pathlib import Path\nPath(\"/my/directory\").mkdir(parents=True, exist_ok=True)\nFor older versions of Python, I see two answers with good qualities, each with a small flaw, so I will give my take on it:Try os.path.exists, and consider os.makedirs for the creation.os.path.existsos.path.existsos.makedirsos.makedirsimport os\nif not os.path.exists(directory):\n    os.makedirs(directory)\nimport os\nif not os.path.exists(directory):\n    os.makedirs(directory)\nAs noted in comments and elsewhere, there's a race condition \u2013 if the directory is created between the os.path.exists and the os.makedirs calls, the os.makedirs will fail with an OSError. Unfortunately, blanket-catching OSError and continuing is not foolproof, as it will ignore a failure to create the directory due to other factors, such as insufficient permissions, full disk, etc.os.path.existsos.makedirsos.makedirsOSErrorOSErrorOne option would be to trap the OSError and examine the embedded error code (see Is there a cross-platform way of getting information from Python\u2019s OSError):OSErrorIs there a cross-platform way of getting information from Python\u2019s OSErrorimport os, errno\n\ntry:\n    os.makedirs(directory)\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise\nimport os, errno\n\ntry:\n    os.makedirs(directory)\nexcept OSError as e:\n    if e.errno != errno.EEXIST:\n        raise\nAlternatively, there could be a second os.path.exists, but suppose another created the directory after the first check, then removed it before the second one \u2013 we could still be fooled. os.path.existsDepending on the application, the danger of concurrent operations may be more or less than the danger posed by other factors such as file permissions. The developer would have to know more about the particular application being developed and its expected environment before choosing an implementation.Modern versions of Python improve this code quite a bit, both by exposing FileExistsError (in 3.3+)...FileExistsErrorFileExistsErrortry:\n    os.makedirs(\"path/to/directory\")\nexcept FileExistsError:\n    # directory already exists\n    pass\ntry:\n    os.makedirs(\"path/to/directory\")\nexcept FileExistsError:\n    # directory already exists\n    pass\n...and by allowing a keyword argument to os.makedirs called exist_ok (in 3.2+).a keyword argument to os.makedirs called exist_okos.makedirsexist_okos.makedirs(\"path/to/directory\", exist_ok=True)  # succeeds even if directory exists.\nos.makedirs(\"path/to/directory\", exist_ok=True)  # succeeds even if directory exists.\n",
                "Python 3.5+:import pathlib\npathlib.Path('/my/directory').mkdir(parents=True, exist_ok=True) \nimport pathlib\npathlib.Path('/my/directory').mkdir(parents=True, exist_ok=True) \npathlib.Path.mkdir as used above recursively creates the directory and does not raise an exception if the directory already exists. If you don't need or want the parents to be created, skip the parents argument.pathlib.Path.mkdirpathlib.Path.mkdirparentsPython 3.2+:Using pathlib:Using pathlib:pathlibIf you can, install the current pathlib backport named pathlib2. Do not install the older unmaintained backport named pathlib. Next, refer to the Python 3.5+ section above and use it the same.pathlibpathlib2pathlib2pathlibpathlibIf using Python 3.4, even though it comes with pathlib, it is missing the useful exist_ok option. The backport is intended to offer a newer and superior implementation of mkdir which includes this missing option.pathlibexist_okmkdirUsing os:Using os:osimport os\nos.makedirs(path, exist_ok=True)\nimport os\nos.makedirs(path, exist_ok=True)\nos.makedirs as used above recursively creates the directory and does not raise an exception if the directory already exists. It has the optional exist_ok argument only if using Python 3.2+, with a default value of False. This argument does not exist in Python 2.x up to 2.7. As such, there is no need for manual exception handling as with Python 2.7.os.makedirsos.makedirsexist_okFalsePython 2.7+:Using pathlib:Using pathlib:pathlibIf you can, install the current pathlib backport named pathlib2. Do not install the older unmaintained backport named pathlib. Next, refer to the Python 3.5+ section above and use it the same.pathlibpathlib2pathlib2pathlibpathlibUsing os:Using os:osimport os\ntry: \n    os.makedirs(path)\nexcept OSError:\n    if not os.path.isdir(path):\n        raise\nimport os\ntry: \n    os.makedirs(path)\nexcept OSError:\n    if not os.path.isdir(path):\n        raise\nWhile a naive solution may first use os.path.isdir followed by os.makedirs, the solution above reverses the order of the two operations. In doing so, it prevents a common race condition having to do with a duplicated attempt at creating the directory, and also disambiguates files from directories.os.path.isdiros.path.isdiros.makedirsos.makedirsNote that capturing the exception and using errno is of limited usefulness because OSError: [Errno 17] File exists, i.e. errno.EEXIST, is raised for both files and directories. It is more reliable simply to check if the directory exists.errnoOSError: [Errno 17] File existserrno.EEXISTAlternative:mkpath creates the nested directory, and does nothing if the directory already exists. This works in both Python 2 and 3.mkpathmkpathimport distutils.dir_util\ndistutils.dir_util.mkpath(path)\nimport distutils.dir_util\ndistutils.dir_util.mkpath(path)\nPer Bug 10948, a severe limitation of this alternative is that it works only once per python process for a given path. In other words, if you use it to create a directory, then delete the directory from inside or outside Python, then use mkpath again to recreate the same directory, mkpath will simply silently use its invalid cached info of having previously created the directory, and will not actually make the directory again. In contrast, os.makedirs doesn't rely on any such cache. This limitation may be okay for some applications.Bug 10948mkpathmkpathos.makedirsWith regard to the directory's mode, please refer to the documentation if you care about it.mode",
                "Using try except and the right error code from errno module gets rid of the race condition and is cross-platform:import os\nimport errno\n\ndef make_sure_path_exists(path):\n    try:\n        os.makedirs(path)\n    except OSError as exception:\n        if exception.errno != errno.EEXIST:\n            raise\nimport os\nimport errno\n\ndef make_sure_path_exists(path):\n    try:\n        os.makedirs(path)\n    except OSError as exception:\n        if exception.errno != errno.EEXIST:\n            raise\nIn other words, we try to create the directories, but if they already exist we ignore the error. On the other hand, any other error gets reported. For example, if you create dir 'a' beforehand and remove all permissions from it, you will get an OSError raised with errno.EACCES (Permission denied, error 13).OSErrorerrno.EACCES",
                "Starting from Python 3.5, pathlib.Path.mkdir has an exist_ok flag:pathlib.Path.mkdirpathlib.Path.mkdirexist_okfrom pathlib import Path\npath = Path('/my/directory/filename.txt')\npath.parent.mkdir(parents=True, exist_ok=True) \n# path.parent ~ os.path.dirname(path)\nfrom pathlib import Path\npath = Path('/my/directory/filename.txt')\npath.parent.mkdir(parents=True, exist_ok=True) \n# path.parent ~ os.path.dirname(path)\nThis recursively creates the directory and does not raise an exception if the directory already exists.(just as os.makedirs got an exist_ok flag starting from python 3.2 e.g os.makedirs(path, exist_ok=True))os.makedirsos.makedirsexist_okos.makedirs(path, exist_ok=True)\nNote: when i posted this answer none of the other answers mentioned exist_ok...\nNote: when i posted this answer none of the other answers mentioned exist_ok...exist_ok",
                "I would personally recommend that you use os.path.isdir() to test instead of os.path.exists().os.path.isdir()os.path.exists()>>> os.path.exists('/tmp/dirname')\nTrue\n>>> os.path.exists('/tmp/dirname/filename.etc')\nTrue\n>>> os.path.isdir('/tmp/dirname/filename.etc')\nFalse\n>>> os.path.isdir('/tmp/fakedirname')\nFalse\n>>> os.path.exists('/tmp/dirname')\nTrue\n>>> os.path.exists('/tmp/dirname/filename.etc')\nTrue\n>>> os.path.isdir('/tmp/dirname/filename.etc')\nFalse\n>>> os.path.isdir('/tmp/fakedirname')\nFalse\nIf you have:>>> directory = raw_input(\":: \")\n>>> directory = raw_input(\":: \")\nAnd a foolish user input::: /tmp/dirname/filename.etc\n:: /tmp/dirname/filename.etc\n... You're going to end up with a directory named filename.etc when you pass that argument to os.makedirs() if you test with os.path.exists().filename.etcos.makedirs()os.path.exists()",
                "Check os.makedirs:  (It makes sure the complete path exists.)\n To handle the fact the directory might exist, catch OSError.\n(If exist_ok is False (the default), an OSError is raised if the target directory already exists.)os.makedirsos.makedirsOSErrorexist_okFalseOSErrorimport os\ntry:\n    os.makedirs('./path/to/somewhere')\nexcept OSError:\n    pass\nimport os\ntry:\n    os.makedirs('./path/to/somewhere')\nexcept OSError:\n    pass\n",
                "Try the os.path.exists functionos.path.existsos.path.existsif not os.path.exists(dir):\n    os.mkdir(dir)\nif not os.path.exists(dir):\n    os.mkdir(dir)\n",
                "Insights on the specifics of this situationYou give a particular file at a certain path and you pull the directory from the file path. Then after making sure you have the directory, you attempt to open a file for reading. To comment on this code:\nfilename = \"/my/directory/filename.txt\"\ndir = os.path.dirname(filename)\n\nfilename = \"/my/directory/filename.txt\"\ndir = os.path.dirname(filename)\nfilename = \"/my/directory/filename.txt\"\ndir = os.path.dirname(filename)\nWe want to avoid overwriting the builtin function, dir. Also, filepath or perhaps fullfilepath is probably a better semantic name than filename so this would be better written:dirfilepathfullfilepathfilenameimport os\nfilepath = '/my/directory/filename.txt'\ndirectory = os.path.dirname(filepath)\nimport os\nfilepath = '/my/directory/filename.txt'\ndirectory = os.path.dirname(filepath)\nYour end goal is to open this file, you initially state, for writing, but you're essentially approaching this goal (based on your code) like this, which opens the file for reading:reading\nif not os.path.exists(directory):\n    os.makedirs(directory)\nf = file(filename)\n\nif not os.path.exists(directory):\n    os.makedirs(directory)\nf = file(filename)\nif not os.path.exists(directory):\n    os.makedirs(directory)\nf = file(filename)\nAssuming opening for readingWhy would you make a directory for a file that you expect to be there and be able to read? Just attempt to open the file.with open(filepath) as my_file:\n    do_stuff(my_file)\nwith open(filepath) as my_file:\n    do_stuff(my_file)\nIf the directory or file isn't there, you'll get an IOError with an associated error number: errno.ENOENT will point to the correct error number regardless of your platform. You can catch it if you want, for example:IOErrorerrno.ENOENTimport errno\ntry:\n    with open(filepath) as my_file:\n        do_stuff(my_file)\nexcept IOError as error:\n    if error.errno == errno.ENOENT:\n        print 'ignoring error because directory or file is not there'\n    else:\n        raise\nimport errno\ntry:\n    with open(filepath) as my_file:\n        do_stuff(my_file)\nexcept IOError as error:\n    if error.errno == errno.ENOENT:\n        print 'ignoring error because directory or file is not there'\n    else:\n        raise\nAssuming we're opening for writingThis is probably what you're wanting.probablyIn this case, we probably aren't facing any race conditions. So just do as you were, but note that for writing, you need to open with the w mode (or a to append). It's also a Python best practice to use the context manager for opening files.waimport os\nif not os.path.exists(directory):\n    os.makedirs(directory)\nwith open(filepath, 'w') as my_file:\n    do_stuff(my_file)\nimport os\nif not os.path.exists(directory):\n    os.makedirs(directory)\nwith open(filepath, 'w') as my_file:\n    do_stuff(my_file)\nHowever, say we have several Python processes that attempt to put all their data into the same directory. Then we may have contention over creation of the directory. In that case it's best to wrap the makedirs call in a try-except block.makedirsimport os\nimport errno\nif not os.path.exists(directory):\n    try:\n        os.makedirs(directory)\n    except OSError as error:\n        if error.errno != errno.EEXIST:\n            raise\nwith open(filepath, 'w') as my_file:\n    do_stuff(my_file)\nimport os\nimport errno\nif not os.path.exists(directory):\n    try:\n        os.makedirs(directory)\n    except OSError as error:\n        if error.errno != errno.EEXIST:\n            raise\nwith open(filepath, 'w') as my_file:\n    do_stuff(my_file)\n",
                "I have put the following down. It's not totally foolproof though.import os\n\ndirname = 'create/me'\n\ntry:\n    os.makedirs(dirname)\nexcept OSError:\n    if os.path.exists(dirname):\n        # We are nearly safe\n        pass\n    else:\n        # There was an error on creation, so make sure we know about it\n        raise\nimport os\n\ndirname = 'create/me'\n\ntry:\n    os.makedirs(dirname)\nexcept OSError:\n    if os.path.exists(dirname):\n        # We are nearly safe\n        pass\n    else:\n        # There was an error on creation, so make sure we know about it\n        raise\nNow as I say, this is not really foolproof, because we have the possiblity of failing to create the directory, and another process creating it during that period.",
                "\nCheck if a directory exists and create it if necessary?\nCheck if a directory exists and create it if necessary?Check if a directory exists and create it if necessary?The direct answer to this is, assuming a simple situation where you don't expect other users or processes to be messing with your directory:if not os.path.exists(d):\n    os.makedirs(d)\nif not os.path.exists(d):\n    os.makedirs(d)\nor if making the directory is subject to race conditions (i.e. if after checking the path exists, something else may have already made it) do this:orimport errno\ntry:\n    os.makedirs(d)\nexcept OSError as exception:\n    if exception.errno != errno.EEXIST:\n        raise\nimport errno\ntry:\n    os.makedirs(d)\nexcept OSError as exception:\n    if exception.errno != errno.EEXIST:\n        raise\nBut perhaps an even better approach is to sidestep the resource contention issue, by using temporary directories via tempfile:tempfiletempfileimport tempfile\n\nd = tempfile.mkdtemp()\nimport tempfile\n\nd = tempfile.mkdtemp()\nHere's the essentials from the online doc:\nmkdtemp(suffix='', prefix='tmp', dir=None)\n    User-callable function to create and return a unique temporary\n    directory.  The return value is the pathname of the directory.\n\n    The directory is readable, writable, and searchable only by the\n    creating user.\n\n    Caller is responsible for deleting the directory when done with it.\n\nmkdtemp(suffix='', prefix='tmp', dir=None)\n    User-callable function to create and return a unique temporary\n    directory.  The return value is the pathname of the directory.\n\n    The directory is readable, writable, and searchable only by the\n    creating user.\n\n    Caller is responsible for deleting the directory when done with it.\nmkdtemp(suffix='', prefix='tmp', dir=None)\n    User-callable function to create and return a unique temporary\n    directory.  The return value is the pathname of the directory.\n\n    The directory is readable, writable, and searchable only by the\n    creating user.\n\n    Caller is responsible for deleting the directory when done with it.\nNew in Python 3.5: pathlib.Path with exist_okpathlib.Pathexist_okThere's a new Path object (as of 3.4) with lots of methods one would want to use with paths - one of which is mkdir.Pathmkdir(For context, I'm tracking my weekly rep with a script. Here's the relevant parts of code from the script that allow me to avoid hitting Stack Overflow more than once a day for the same data.)First the relevant imports:from pathlib import Path\nimport tempfile\nfrom pathlib import Path\nimport tempfile\nWe don't have to deal with os.path.join now - just join path parts with a /:os.path.join/directory = Path(tempfile.gettempdir()) / 'sodata'\ndirectory = Path(tempfile.gettempdir()) / 'sodata'\nThen I idempotently ensure the directory exists - the exist_ok argument shows up in Python 3.5:exist_okdirectory.mkdir(exist_ok=True)\ndirectory.mkdir(exist_ok=True)\nHere's the relevant part of the documentation:documentation\nIf exist_ok is true, FileExistsError exceptions will be ignored (same behavior as the POSIX mkdir -p command), but only if the last path component is not an existing non-directory file.\nIf exist_ok is true, FileExistsError exceptions will be ignored (same behavior as the POSIX mkdir -p command), but only if the last path component is not an existing non-directory file.exist_okFileExistsErrorPOSIX mkdir -pHere's a little more of the script - in my case, I'm not subject to a race condition, I only have one process that expects the directory (or contained files) to be there, and I don't have anything trying to remove the directory. todays_file = directory / str(datetime.datetime.utcnow().date())\nif todays_file.exists():\n    logger.info(\"todays_file exists: \" + str(todays_file))\n    df = pd.read_json(str(todays_file))\ntodays_file = directory / str(datetime.datetime.utcnow().date())\nif todays_file.exists():\n    logger.info(\"todays_file exists: \" + str(todays_file))\n    df = pd.read_json(str(todays_file))\nPath objects have to be coerced to str before other APIs that expect str paths can use them.PathstrstrPerhaps Pandas should be updated to accept instances of the abstract base class, os.PathLike.os.PathLike",
                "Best way to do this in pythonBest way to do this in python#Devil\nimport os\ndirectory = \"./out_dir/subdir1/subdir2\"\nif not os.path.exists(directory):\n    os.makedirs(directory)\n#Devil\nimport os\ndirectory = \"./out_dir/subdir1/subdir2\"\nif not os.path.exists(directory):\n    os.makedirs(directory)\n",
                "fastest safest way to do it is:\nit will create if not exists and skip if exists:from pathlib import Path\nPath(\"path/with/childs/.../\").mkdir(parents=True, exist_ok=True)\nfrom pathlib import Path\nPath(\"path/with/childs/.../\").mkdir(parents=True, exist_ok=True)\n",
                "In Python 3.4 you can also use the brand new pathlib module:brand new pathlib modulepathlibfrom pathlib import Path\npath = Path(\"/my/directory/filename.txt\")\ntry:\n    if not path.parent.exists():\n        path.parent.mkdir(parents=True)\nexcept OSError:\n    # handle error; you can also catch specific errors like\n    # FileExistsError and so on.\nfrom pathlib import Path\npath = Path(\"/my/directory/filename.txt\")\ntry:\n    if not path.parent.exists():\n        path.parent.mkdir(parents=True)\nexcept OSError:\n    # handle error; you can also catch specific errors like\n    # FileExistsError and so on.\n",
                "For a one-liner solution, you can use IPython.utils.path.ensure_dir_exists():IPython.utils.path.ensure_dir_exists()IPython.utils.path.ensure_dir_exists()from IPython.utils.path import ensure_dir_exists\nensure_dir_exists(dir)\nfrom IPython.utils.path import ensure_dir_exists\nensure_dir_exists(dir)\nFrom the documentation: Ensure that a directory exists. If it doesn\u2019t exist, try to create it and protect against a race condition if another process is doing the same.documentationEnsure that a directory exists. If it doesn\u2019t exist, try to create it and protect against a race condition if another process is doing the same.IPython is an extension package, not part of the standard library.",
                "In Python3, os.makedirs supports setting exist_ok. The default setting is False, which means an OSError will be raised if the target directory already exists. By setting exist_ok to True, OSError (directory exists) will be ignored and the directory will not be created.Python3os.makedirsexist_okFalseOSErrorexist_okTrueOSErroros.makedirs(path,exist_ok=True)\nos.makedirs(path,exist_ok=True)\nIn Python2, os.makedirs doesn't support setting exist_ok. You can use the approach in heikki-toivonen's answer:Python2os.makedirsexist_okheikki-toivonen's answerimport os\nimport errno\n\ndef make_sure_path_exists(path):\n    try:\n        os.makedirs(path)\n    except OSError as exception:\n        if exception.errno != errno.EEXIST:\n            raise\nimport os\nimport errno\n\ndef make_sure_path_exists(path):\n    try:\n        os.makedirs(path)\n    except OSError as exception:\n        if exception.errno != errno.EEXIST:\n            raise\n",
                "The relevant Python documentation suggests the use of the EAFP coding style (Easier to Ask for Forgiveness than Permission). This means that the coderelevant Python documentationEAFP coding style (Easier to Ask for Forgiveness than Permission)try:\n    os.makedirs(path)\nexcept OSError as exception:\n    if exception.errno != errno.EEXIST:\n        raise\n    else:\n        print \"\\nBE CAREFUL! Directory %s already exists.\" % path\ntry:\n    os.makedirs(path)\nexcept OSError as exception:\n    if exception.errno != errno.EEXIST:\n        raise\n    else:\n        print \"\\nBE CAREFUL! Directory %s already exists.\" % path\nis better than the alternativeif not os.path.exists(path):\n    os.makedirs(path)\nelse:\n    print \"\\nBE CAREFUL! Directory %s already exists.\" % path\nif not os.path.exists(path):\n    os.makedirs(path)\nelse:\n    print \"\\nBE CAREFUL! Directory %s already exists.\" % path\nThe documentation suggests this exactly because of the race condition discussed in this question. In addition, as others mention here, there is a performance advantage in querying once instead of twice the OS. Finally, the argument placed forward, potentially, in favour of the second code in some cases --when the developer knows the environment the application is running-- can only be advocated in the special case that the program has set up a private environment for itself (and other instances of the same program).Even in that case, this is a bad practice and can lead to long useless debugging. For example, the fact we set the permissions for a directory should not leave us with the impression permissions are set appropriately for our purposes. A parent directory could be mounted with other permissions. In general, a program should always work correctly and the programmer should not expect one specific environment.",
                "I found this Q/A after I was puzzled by some of the failures and errors I was getting while working with directories in Python. I am working in Python 3 (v.3.5 in an Anaconda virtual environment on an Arch Linux x86_64 system).Consider this directory structure:\u2514\u2500\u2500 output/         ## dir\n   \u251c\u2500\u2500 corpus       ## file\n   \u251c\u2500\u2500 corpus2/     ## dir\n   \u2514\u2500\u2500 subdir/      ## dir\n\u2514\u2500\u2500 output/         ## dir\n   \u251c\u2500\u2500 corpus       ## file\n   \u251c\u2500\u2500 corpus2/     ## dir\n   \u2514\u2500\u2500 subdir/      ## dir\nHere are my experiments/notes, which provides clarification:# ----------------------------------------------------------------------------\n# [1] https://stackoverflow.com/questions/273192/how-can-i-create-a-directory-if-it-does-not-exist\n\nimport pathlib\n\n\"\"\" Notes:\n        1.  Include a trailing slash at the end of the directory path\n            (\"Method 1,\" below).\n        2.  If a subdirectory in your intended path matches an existing file\n            with same name, you will get the following error:\n            \"NotADirectoryError: [Errno 20] Not a directory:\" ...\n\"\"\"\n# Uncomment and try each of these \"out_dir\" paths, singly:\n\n# ----------------------------------------------------------------------------\n# METHOD 1:\n# Re-running does not overwrite existing directories and files; no errors.\n\n# out_dir = 'output/corpus3'                ## no error but no dir created (missing tailing /)\n# out_dir = 'output/corpus3/'               ## works\n# out_dir = 'output/corpus3/doc1'           ## no error but no dir created (missing tailing /)\n# out_dir = 'output/corpus3/doc1/'          ## works\n# out_dir = 'output/corpus3/doc1/doc.txt'   ## no error but no file created (os.makedirs creates dir, not files!  ;-)\n# out_dir = 'output/corpus2/tfidf/'         ## fails with \"Errno 20\" (existing file named \"corpus2\")\n# out_dir = 'output/corpus3/tfidf/'         ## works\n# out_dir = 'output/corpus3/a/b/c/d/'       ## works\n\n# [2] https://docs.python.org/3/library/os.html#os.makedirs\n\n# Uncomment these to run \"Method 1\":\n\n#directory = os.path.dirname(out_dir)\n#os.makedirs(directory, mode=0o777, exist_ok=True)\n\n# ----------------------------------------------------------------------------\n# METHOD 2:\n# Re-running does not overwrite existing directories and files; no errors.\n\n# out_dir = 'output/corpus3'                ## works\n# out_dir = 'output/corpus3/'               ## works\n# out_dir = 'output/corpus3/doc1'           ## works\n# out_dir = 'output/corpus3/doc1/'          ## works\n# out_dir = 'output/corpus3/doc1/doc.txt'   ## no error but creates a .../doc.txt./ dir\n# out_dir = 'output/corpus2/tfidf/'         ## fails with \"Errno 20\" (existing file named \"corpus2\")\n# out_dir = 'output/corpus3/tfidf/'         ## works\n# out_dir = 'output/corpus3/a/b/c/d/'       ## works\n\n# Uncomment these to run \"Method 2\":\n\n#import os, errno\n#try:\n#       os.makedirs(out_dir)\n#except OSError as e:\n#       if e.errno != errno.EEXIST:\n#               raise\n# ----------------------------------------------------------------------------\n# ----------------------------------------------------------------------------\n# [1] https://stackoverflow.com/questions/273192/how-can-i-create-a-directory-if-it-does-not-exist\n\nimport pathlib\n\n\"\"\" Notes:\n        1.  Include a trailing slash at the end of the directory path\n            (\"Method 1,\" below).\n        2.  If a subdirectory in your intended path matches an existing file\n            with same name, you will get the following error:\n            \"NotADirectoryError: [Errno 20] Not a directory:\" ...\n\"\"\"\n# Uncomment and try each of these \"out_dir\" paths, singly:\n\n# ----------------------------------------------------------------------------\n# METHOD 1:\n# Re-running does not overwrite existing directories and files; no errors.\n\n# out_dir = 'output/corpus3'                ## no error but no dir created (missing tailing /)\n# out_dir = 'output/corpus3/'               ## works\n# out_dir = 'output/corpus3/doc1'           ## no error but no dir created (missing tailing /)\n# out_dir = 'output/corpus3/doc1/'          ## works\n# out_dir = 'output/corpus3/doc1/doc.txt'   ## no error but no file created (os.makedirs creates dir, not files!  ;-)\n# out_dir = 'output/corpus2/tfidf/'         ## fails with \"Errno 20\" (existing file named \"corpus2\")\n# out_dir = 'output/corpus3/tfidf/'         ## works\n# out_dir = 'output/corpus3/a/b/c/d/'       ## works\n\n# [2] https://docs.python.org/3/library/os.html#os.makedirs\n\n# Uncomment these to run \"Method 1\":\n\n#directory = os.path.dirname(out_dir)\n#os.makedirs(directory, mode=0o777, exist_ok=True)\n\n# ----------------------------------------------------------------------------\n# METHOD 2:\n# Re-running does not overwrite existing directories and files; no errors.\n\n# out_dir = 'output/corpus3'                ## works\n# out_dir = 'output/corpus3/'               ## works\n# out_dir = 'output/corpus3/doc1'           ## works\n# out_dir = 'output/corpus3/doc1/'          ## works\n# out_dir = 'output/corpus3/doc1/doc.txt'   ## no error but creates a .../doc.txt./ dir\n# out_dir = 'output/corpus2/tfidf/'         ## fails with \"Errno 20\" (existing file named \"corpus2\")\n# out_dir = 'output/corpus3/tfidf/'         ## works\n# out_dir = 'output/corpus3/a/b/c/d/'       ## works\n\n# Uncomment these to run \"Method 2\":\n\n#import os, errno\n#try:\n#       os.makedirs(out_dir)\n#except OSError as e:\n#       if e.errno != errno.EEXIST:\n#               raise\n# ----------------------------------------------------------------------------\nConclusion: in my opinion, \"Method 2\" is more robust.[1] How can I safely create a nested directory?How can I safely create a nested directory?[2] https://docs.python.org/3/library/os.html#os.makedirshttps://docs.python.org/3/library/os.html#os.makedirs",
                "You can use mkpathmkpathmkpath# Create a directory and any missing ancestor directories. \n# If the directory already exists, do nothing.\n\nfrom distutils.dir_util import mkpath\nmkpath(\"test\")    \n# Create a directory and any missing ancestor directories. \n# If the directory already exists, do nothing.\n\nfrom distutils.dir_util import mkpath\nmkpath(\"test\")    \nNote that it will create the ancestor directories as well. It works for Python 2 and 3.",
                "In case you're writing a file to a variable path, you can use this on the file's path to make sure that the parent directories are created.from pathlib import Path\n\npath_to_file = Path(\"zero/or/more/directories/file.ext\")\nparent_directory_of_file = path_to_file.parent\nparent_directory_of_file.mkdir(parents=True, exist_ok=True)\nfrom pathlib import Path\n\npath_to_file = Path(\"zero/or/more/directories/file.ext\")\nparent_directory_of_file = path_to_file.parent\nparent_directory_of_file.mkdir(parents=True, exist_ok=True)\nWorks even if path_to_file is file.ext (zero directories deep).path_to_filefile.extSee pathlib.PurePath.parent and pathlib.Path.mkdir.pathlib.PurePath.parentpathlib.Path.mkdir",
                "Why not use subprocess module if running on a machine that supports command \nmkdir with -p option ? \nWorks on python 2.7 and python 3.6mkdir-pfrom subprocess import call\ncall(['mkdir', '-p', 'path1/path2/path3'])\nfrom subprocess import call\ncall(['mkdir', '-p', 'path1/path2/path3'])\nShould do the trick on most systems.In situations where portability doesn't matter (ex, using docker) the solution is a clean 2 lines. You also don't have to add logic to check if directories exist or not. Finally, it is safe to re-run without any side effectsIf you need error handling:from subprocess import check_call\ntry:\n    check_call(['mkdir', '-p', 'path1/path2/path3'])\nexcept:\n    handle...\nfrom subprocess import check_call\ntry:\n    check_call(['mkdir', '-p', 'path1/path2/path3'])\nexcept:\n    handle...\n",
                "You have to set the full path before creating the directory:import os,sys,inspect\nimport pathlib\n\ncurrentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\nyour_folder = currentdir + \"/\" + \"your_folder\"\n\nif not os.path.exists(your_folder):\n   pathlib.Path(your_folder).mkdir(parents=True, exist_ok=True)\nimport os,sys,inspect\nimport pathlib\n\ncurrentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\nyour_folder = currentdir + \"/\" + \"your_folder\"\n\nif not os.path.exists(your_folder):\n   pathlib.Path(your_folder).mkdir(parents=True, exist_ok=True)\nThis works for me and hopefully, it will works for you as well",
                "I saw Heikki Toivonen and A-B-B's answers and thought of this variation.Heikki ToivonenA-B-Bimport os\nimport errno\n\ndef make_sure_path_exists(path):\n    try:\n        os.makedirs(path)\n    except OSError as exception:\n        if exception.errno != errno.EEXIST or not os.path.isdir(path):\n            raise\nimport os\nimport errno\n\ndef make_sure_path_exists(path):\n    try:\n        os.makedirs(path)\n    except OSError as exception:\n        if exception.errno != errno.EEXIST or not os.path.isdir(path):\n            raise\n",
                "I use os.path.exists(), here is a Python 3 script that can be used to check if a directory exists, create one if it does not exist, and delete it if it does exist (if desired).os.path.exists()hereIt prompts users for input of the directory and can be easily modified.",
                "Use this command check and create dir if not os.path.isdir(test_img_dir):\n     os.mkdir(test_img_dir)\n if not os.path.isdir(test_img_dir):\n     os.mkdir(test_img_dir)\n",
                "Call the function create_dir() at the entry point of your program/project.create_dir()import os\n\ndef create_dir(directory):\n    if not os.path.exists(directory):\n        print('Creating Directory '+directory)\n        os.makedirs(directory)\n\ncreate_dir('Project directory')\nimport os\n\ndef create_dir(directory):\n    if not os.path.exists(directory):\n        print('Creating Directory '+directory)\n        os.makedirs(directory)\n\ncreate_dir('Project directory')\n",
                "If you consider the following: os.path.isdir('/tmp/dirname')\nos.path.isdir('/tmp/dirname')\nmeans a directory (path) exists AND is a directory. So for me this way does what I need. So I can make sure it is folder (not a file) and exists.",
                "You can use os.listdir for this:os.listdirimport os\nif 'dirName' in os.listdir('parentFolderPath')\n    print('Directory Exists')\nimport os\nif 'dirName' in os.listdir('parentFolderPath')\n    print('Directory Exists')\n",
                "This may not exactly answer the question. But I guess your real intention is to create a file and its parent directories, given its content all in 1 command.You can do that with fastcore extension to pathlib: path.mk_write(data)fastcorepath.mk_write(data)from fastcore.utils import Path\nPath('/dir/to/file.txt').mk_write('Hello World')\nfrom fastcore.utils import Path\nPath('/dir/to/file.txt').mk_write('Hello World')\nSee more in fastcore documentationfastcore documentation"
            ]
        },
        {
            "tag": "for_index",
            "patterns": [
                "Accessing the index in 'for' loops",
                "Accessing the index in 'for' loops"
            ],
            "responses": [
                "Use the built-in function enumerate():enumerate()enumerate()for idx, x in enumerate(xs):\n    print(idx, x)\nfor idx, x in enumerate(xs):\n    print(idx, x)\nIt is non-pythonic to manually index via for i in range(len(xs)): x = xs[i] or manually manage an additional state variable.non-pythonicnon-pythonicfor i in range(len(xs)): x = xs[i]Check out PEP 279 for more.PEP 279",
                "\nUsing a for loop, how do I access the loop index, from 1 to 5 in this case?\nUsing a for loop, how do I access the loop index, from 1 to 5 in this case?Use enumerate to get the index with the element as you iterate:enumeratefor index, item in enumerate(items):\n    print(index, item)\nfor index, item in enumerate(items):\n    print(index, item)\nAnd note that Python's indexes start at zero, so you would get 0 to 4 with the above. If you want the count, 1 to 5, do this:count = 0 # in case items is empty and you need it after the loop\nfor count, item in enumerate(items, start=1):\n    print(count, item)\ncount = 0 # in case items is empty and you need it after the loop\nfor count, item in enumerate(items, start=1):\n    print(count, item)\nUnidiomatic control flowWhat you are asking for is the Pythonic equivalent of the following, which is the algorithm most programmers of lower-level languages would use:\nindex = 0            # Python's indexing starts at zero\nfor item in items:   # Python's for loops are a \"for each\" loop \n    print(index, item)\n    index += 1\n\nindex = 0            # Python's indexing starts at zero\nfor item in items:   # Python's for loops are a \"for each\" loop \n    print(index, item)\n    index += 1\nindex = 0            # Python's indexing starts at zero\nfor item in items:   # Python's for loops are a \"for each\" loop \n    print(index, item)\n    index += 1\nOr in languages that do not have a for-each loop:\nindex = 0\nwhile index < len(items):\n    print(index, items[index])\n    index += 1\n\nindex = 0\nwhile index < len(items):\n    print(index, items[index])\n    index += 1\nindex = 0\nwhile index < len(items):\n    print(index, items[index])\n    index += 1\nor sometimes more commonly (but unidiomatically) found in Python:\nfor index in range(len(items)):\n    print(index, items[index])\n\nfor index in range(len(items)):\n    print(index, items[index])\nfor index in range(len(items)):\n    print(index, items[index])\nUse the Enumerate FunctionPython's enumerate function reduces the visual clutter by hiding the accounting for the indexes, and encapsulating the iterable into another iterable (an enumerate object) that yields a two-item tuple of the index and the item that the original iterable would provide. That looks like this:enumerate functionenumerateenumeratefor index, item in enumerate(items, start=0):   # default is zero\n    print(index, item)\nfor index, item in enumerate(items, start=0):   # default is zero\n    print(index, item)\nThis code sample is fairly well the canonical example of the difference between code that is idiomatic of Python and code that is not. Idiomatic code is sophisticated (but not complicated) Python, written in the way that it was intended to be used. Idiomatic code is expected by the designers of the language, which means that usually this code is not just more readable, but also more efficient.canonicalGetting a countEven if you don't need indexes as you go, but you need a count of the iterations (sometimes desirable) you can start with 1 and the final number will be your count.1count = 0 # in case items is empty\nfor count, item in enumerate(items, start=1):   # default is zero\n    print(item)\n\nprint('there were {0} items printed'.format(count))\ncount = 0 # in case items is empty\nfor count, item in enumerate(items, start=1):   # default is zero\n    print(item)\n\nprint('there were {0} items printed'.format(count))\nThe count seems to be more what you intend to ask for (as opposed to index) when you said you wanted from 1 to 5.Breaking it down - a step by step explanationTo break these examples down, say we have a list of items that we want to iterate over with an index:items = ['a', 'b', 'c', 'd', 'e']\nitems = ['a', 'b', 'c', 'd', 'e']\nNow we pass this iterable to enumerate, creating an enumerate object:enumerate_object = enumerate(items) # the enumerate object\nenumerate_object = enumerate(items) # the enumerate object\nWe can pull the first item out of this iterable that we would get in a loop with the next function:nextiteration = next(enumerate_object) # first iteration from enumerate\nprint(iteration)\niteration = next(enumerate_object) # first iteration from enumerate\nprint(iteration)\nAnd we see we get a tuple of 0, the first index, and 'a', the first item:0'a'(0, 'a')\n(0, 'a')\nwe can use what is referred to as \"sequence unpacking\" to extract the elements from this two-tuple:sequence unpackingindex, item = iteration\n#   0,  'a' = (0, 'a') # essentially this.\nindex, item = iteration\n#   0,  'a' = (0, 'a') # essentially this.\nand when we inspect index, we find it refers to the first index, 0, and item refers to the first item, 'a'.indexitem'a'>>> print(index)\n0\n>>> print(item)\na\n>>> print(index)\n0\n>>> print(item)\na\nConclusion\nPython indexes start at zero\nTo get these indexes from an iterable as you iterate over it, use the enumerate function\nUsing enumerate in the idiomatic way (along with tuple unpacking) creates code that is more readable and maintainable:\nPython indexes start at zeroTo get these indexes from an iterable as you iterate over it, use the enumerate functionUsing enumerate in the idiomatic way (along with tuple unpacking) creates code that is more readable and maintainable:So do this:for index, item in enumerate(items, start=0):   # Python indexes start at zero\n    print(index, item)\nfor index, item in enumerate(items, start=0):   # Python indexes start at zero\n    print(index, item)\n",
                "It's pretty simple to start it from 1 other than 0:10for index, item in enumerate(iterable, start=1):\n   print index, item  # Used to print in python<3.x\n   print(index, item) # Migrate to print() after 3.x+\n   \nfor index, item in enumerate(iterable, start=1):\n   print index, item  # Used to print in python<3.x\n   print(index, item) # Migrate to print() after 3.x+\n   \n",
                "for i in range(len(ints)):\n   print(i, ints[i]) # print updated to print() in Python 3.x+ \nfor i in range(len(ints)):\n   print(i, ints[i]) # print updated to print() in Python 3.x+ \n",
                "Here's how you can access the indices and array's elements using for-in loops.1. Looping elements with counter and += operator.+=items = [8, 23, 45, 12, 78]\ncounter = 0\n\nfor value in items:\n    print(counter, value)\n    counter += 1\nitems = [8, 23, 45, 12, 78]\ncounter = 0\n\nfor value in items:\n    print(counter, value)\n    counter += 1\nResult:#    0 8\n#    1 23\n#    2 45\n#    3 12\n#    4 78\n#    0 8\n#    1 23\n#    2 45\n#    3 12\n#    4 78\n2. Looping elements using enumerate() method.enumerate()items = [8, 23, 45, 12, 78]\n\nfor i in enumerate(items):\n    print(\"index/value\", i)\nitems = [8, 23, 45, 12, 78]\n\nfor i in enumerate(items):\n    print(\"index/value\", i)\nResult:#    index/value (0, 8)\n#    index/value (1, 23)\n#    index/value (2, 45)\n#    index/value (3, 12)\n#    index/value (4, 78)\n#    index/value (0, 8)\n#    index/value (1, 23)\n#    index/value (2, 45)\n#    index/value (3, 12)\n#    index/value (4, 78)\n3. Using index and value separately.indexvalueitems = [8, 23, 45, 12, 78]\n\nfor index, value in enumerate(items):\n    print(\"index\", index, \"for value\", value)\nitems = [8, 23, 45, 12, 78]\n\nfor index, value in enumerate(items):\n    print(\"index\", index, \"for value\", value)\nResult:#    index 0 for value 8\n#    index 1 for value 23\n#    index 2 for value 45\n#    index 3 for value 12\n#    index 4 for value 78\n#    index 0 for value 8\n#    index 1 for value 23\n#    index 2 for value 45\n#    index 3 for value 12\n#    index 4 for value 78\n4. You can change the index number to any increment.indexitems = [8, 23, 45, 12, 78]\n\nfor i, value in enumerate(items, start=1000):\n    print(i, value)\nitems = [8, 23, 45, 12, 78]\n\nfor i, value in enumerate(items, start=1000):\n    print(i, value)\nResult:#    1000 8\n#    1001 23\n#    1002 45\n#    1003 12\n#    1004 78\n#    1000 8\n#    1001 23\n#    1002 45\n#    1003 12\n#    1004 78\n5. Automatic counter incrementation with range(len(...)).range(len(...))items = [8, 23, 45, 12, 78]\n\nfor i in range(len(items)):\n    print(\"Index:\", i, \"Value:\", items[i])\nitems = [8, 23, 45, 12, 78]\n\nfor i in range(len(items)):\n    print(\"Index:\", i, \"Value:\", items[i])\nResult:#    ('Index:', 0, 'Value:', 8)\n#    ('Index:', 1, 'Value:', 23)\n#    ('Index:', 2, 'Value:', 45)\n#    ('Index:', 3, 'Value:', 12)\n#    ('Index:', 4, 'Value:', 78)\n#    ('Index:', 0, 'Value:', 8)\n#    ('Index:', 1, 'Value:', 23)\n#    ('Index:', 2, 'Value:', 45)\n#    ('Index:', 3, 'Value:', 12)\n#    ('Index:', 4, 'Value:', 78)\n6. Using for-in loop inside function.items = [8, 23, 45, 12, 78]\n\ndef enum(items, start=0):\n    counter = start\n\n    for value in items:\n        print(counter, value)\n        counter += 1\n    \nenum(items)\nitems = [8, 23, 45, 12, 78]\n\ndef enum(items, start=0):\n    counter = start\n\n    for value in items:\n        print(counter, value)\n        counter += 1\n    \nenum(items)\nResult:#    0 8\n#    1 23\n#    2 45\n#    3 12\n#    4 78\n#    0 8\n#    1 23\n#    2 45\n#    3 12\n#    4 78\n7. Of course, we can't forget about while loop.whileitems = [8, 23, 45, 12, 78]\ncounter = 0\n\nwhile counter < len(items):\n    print(counter, items[counter])\n    counter += 1\nitems = [8, 23, 45, 12, 78]\ncounter = 0\n\nwhile counter < len(items):\n    print(counter, items[counter])\n    counter += 1\nResult:#    0 8\n#    1 23\n#    2 45\n#    3 12\n#    4 78\n#    0 8\n#    1 23\n#    2 45\n#    3 12\n#    4 78\n8. yield statement returning a generator object.yielddef createGenerator():        \n    items = [8, 23, 45, 12, 78]\n\n    for (j, k) in enumerate(items):\n        yield (j, k)\n        \n\ngenerator = createGenerator()\n\nfor i in generator:\n    print(i)\ndef createGenerator():        \n    items = [8, 23, 45, 12, 78]\n\n    for (j, k) in enumerate(items):\n        yield (j, k)\n        \n\ngenerator = createGenerator()\n\nfor i in generator:\n    print(i)\nResult:#    (0, 8)\n#    (1, 23)\n#    (2, 45)\n#    (3, 12)\n#    (4, 78)\n#    (0, 8)\n#    (1, 23)\n#    (2, 45)\n#    (3, 12)\n#    (4, 78)\n9. Inline expression with for-in loop and lambda.lambdaitems = [8, 23, 45, 12, 78]\n\nxerox = lambda upperBound: [(i, items[i]) for i in range(0, upperBound)]\nprint(xerox(5))\nitems = [8, 23, 45, 12, 78]\n\nxerox = lambda upperBound: [(i, items[i]) for i in range(0, upperBound)]\nprint(xerox(5))\nResult:#    [(0, 8), (1, 23), (2, 45), (3, 12), (4, 78)]\n#    [(0, 8), (1, 23), (2, 45), (3, 12), (4, 78)]\n",
                "As is the norm in Python, there are several ways to do this. In all examples assume: lst = [1, 2, 3, 4, 5]lst = [1, 2, 3, 4, 5]\nUsing enumerate (considered most idiomatic)\nUsing enumerate (considered most idiomatic)considered most idiomaticfor index, element in enumerate(lst):\n    # Do the things that need doing here\nfor index, element in enumerate(lst):\n    # Do the things that need doing here\nThis is also the safest option in my opinion because the chance of going into infinite recursion has been eliminated. Both the item and its index are held in variables and there is no need to write any further code to access the item.\nCreating a variable to hold the index (using for)\nCreating a variable to hold the index (using for)using forforfor index in range(len(lst)):   # or xrange\n    # you will have to write extra code to get the element\nfor index in range(len(lst)):   # or xrange\n    # you will have to write extra code to get the element\n\nCreating a variable to hold the index (using while)\nCreating a variable to hold the index (using while)using whilewhileindex = 0\nwhile index < len(lst):\n    # You will have to write extra code to get the element\n    index += 1  # escape infinite recursion\nindex = 0\nwhile index < len(lst):\n    # You will have to write extra code to get the element\n    index += 1  # escape infinite recursion\n\nThere is always another way\nThere is always another wayAs explained before, there are other ways to do this that have not been explained here and they may even apply more in other situations. For example, using itertools.chain with for. It handles nested loops better than the other examples.For exampleitertools.chain",
                "Old fashioned way:for ix in range(len(ints)):\n    print(ints[ix])\nfor ix in range(len(ints)):\n    print(ints[ix])\nList comprehension:[ (ix, ints[ix]) for ix in range(len(ints))]\n\n>>> ints\n[1, 2, 3, 4, 5]\n>>> for ix in range(len(ints)): print ints[ix]\n... \n1\n2\n3\n4\n5\n>>> [ (ix, ints[ix]) for ix in range(len(ints))]\n[(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)]\n>>> lc = [ (ix, ints[ix]) for ix in range(len(ints))]\n>>> for tup in lc:\n...     print(tup)\n... \n(0, 1)\n(1, 2)\n(2, 3)\n(3, 4)\n(4, 5)\n>>> \n[ (ix, ints[ix]) for ix in range(len(ints))]\n\n>>> ints\n[1, 2, 3, 4, 5]\n>>> for ix in range(len(ints)): print ints[ix]\n... \n1\n2\n3\n4\n5\n>>> [ (ix, ints[ix]) for ix in range(len(ints))]\n[(0, 1), (1, 2), (2, 3), (3, 4), (4, 5)]\n>>> lc = [ (ix, ints[ix]) for ix in range(len(ints))]\n>>> for tup in lc:\n...     print(tup)\n... \n(0, 1)\n(1, 2)\n(2, 3)\n(3, 4)\n(4, 5)\n>>> \n",
                "Accessing indexes & Performance Benchmarking of approachesThe fastest way to access indexes of list within loop in Python 3.7 is to use the enumerate method for small, medium and huge lists.Python 3.7enumerate methodPlease see different approaches which can be used to iterate over list and access index value and their performance metrics (which I suppose would be useful for you) in code samples below:different approachestheir performance metrics# Using range\ndef range_loop(iterable):\n    for i in range(len(iterable)):\n        1 + iterable[i]\n\n# Using enumerate\ndef enumerate_loop(iterable):\n    for i, val in enumerate(iterable):\n        1 + val\n\n# Manual indexing\ndef manual_indexing_loop(iterable):\n    index = 0\n    for item in iterable:\n        1 + item\n        index += 1\n# Using range\ndef range_loop(iterable):\n    for i in range(len(iterable)):\n        1 + iterable[i]\n\n# Using enumerate\ndef enumerate_loop(iterable):\n    for i, val in enumerate(iterable):\n        1 + val\n\n# Manual indexing\ndef manual_indexing_loop(iterable):\n    index = 0\n    for item in iterable:\n        1 + item\n        index += 1\nSee performance metrics for each method below:from timeit import timeit\n\ndef measure(l, number=10000):\n    print(\"Measure speed for list with %d items\" % len(l))\n    print(\"range: \", timeit(lambda :range_loop(l), number=number))\n    print(\"enumerate: \", timeit(lambda :enumerate_loop(l), number=number))\n    print(\"manual_indexing: \", timeit(lambda :manual_indexing_loop(l), number=number))\n\n# Measure speed for list with 1000 items\nmeasure(range(1000))\n# range:  1.161622366\n# enumerate:  0.5661940879999996\n# manual_indexing:  0.610455682\n\n# Measure speed for list with 100000 items\nmeasure(range(10000))\n# range:  11.794482958\n# enumerate:  6.197628574000001\n# manual_indexing:  6.935181098000001\n\n# Measure speed for list with 10000000 items\nmeasure(range(10000000), number=100)\n# range:  121.416859069\n# enumerate:  62.718909123\n# manual_indexing:  69.59575057400002\nfrom timeit import timeit\n\ndef measure(l, number=10000):\n    print(\"Measure speed for list with %d items\" % len(l))\n    print(\"range: \", timeit(lambda :range_loop(l), number=number))\n    print(\"enumerate: \", timeit(lambda :enumerate_loop(l), number=number))\n    print(\"manual_indexing: \", timeit(lambda :manual_indexing_loop(l), number=number))\n\n# Measure speed for list with 1000 items\nmeasure(range(1000))\n# range:  1.161622366\n# enumerate:  0.5661940879999996\n# manual_indexing:  0.610455682\n\n# Measure speed for list with 100000 items\nmeasure(range(10000))\n# range:  11.794482958\n# enumerate:  6.197628574000001\n# manual_indexing:  6.935181098000001\n\n# Measure speed for list with 10000000 items\nmeasure(range(10000000), number=100)\n# range:  121.416859069\n# enumerate:  62.718909123\n# manual_indexing:  69.59575057400002\nAs the result, using enumerate method is the fastest method for iteration when the index needed.enumerateAdding some useful links below:\nWhat is the difference between range and xrange functions in Python 2.X?\n\nWhat is faster for loop using enumerate or for loop using xrange in Python?\n\nrange(len(list)) or enumerate(list)?\n\nWhat is the difference between range and xrange functions in Python 2.X?\nWhat is the difference between range and xrange functions in Python 2.X?What is the difference between range and xrange functions in Python 2.X?What is the difference between range and xrange functions in Python 2.X?What is faster for loop using enumerate or for loop using xrange in Python?\nWhat is faster for loop using enumerate or for loop using xrange in Python?What is faster for loop using enumerate or for loop using xrange in Python?What is faster for loop using enumerate or for loop using xrange in Python?range(len(list)) or enumerate(list)?\nrange(len(list)) or enumerate(list)?range(len(list)) or enumerate(list)?range(len(list)) or enumerate(list)?",
                "You can use enumerate and embed expressions inside string literals to obtain the solution.enumerateThis is a simple way:a=[4,5,6,8]\nfor b, val in enumerate(a):\n    print('item #{} = {}'.format(b+1, val))\na=[4,5,6,8]\nfor b, val in enumerate(a):\n    print('item #{} = {}'.format(b+1, val))\n",
                "First of all, the indexes will be from 0 to 4. Programming languages start counting from 0; don't forget that or you will come across an index-out-of-bounds exception. All you need in the for loop is a variable counting from 0 to 4 like so:forfor x in range(0, 5):\nfor x in range(0, 5):\nKeep in mind that I wrote 0 to 5 because the loop stops one number before the maximum. :)To get the value of an index, uselist[index]\nlist[index]\n",
                "You can do it with this code:ints = [8, 23, 45, 12, 78]\nindex = 0\n\nfor value in (ints):\n    index +=1\n    print index, value\nints = [8, 23, 45, 12, 78]\nindex = 0\n\nfor value in (ints):\n    index +=1\n    print index, value\nUse this code if you need to reset the index value at the end of the loop:ints = [8, 23, 45, 12, 78]\nindex = 0\n\nfor value in (ints):\n    index +=1\n    print index, value\n    if index >= len(ints)-1:\n        index = 0\nints = [8, 23, 45, 12, 78]\nindex = 0\n\nfor value in (ints):\n    index +=1\n    print index, value\n    if index >= len(ints)-1:\n        index = 0\n",
                "According to this discussion: object's list indexobject's list indexobject's list indexLoop counter iterationThe current idiom for looping over the indices makes use of the built-in range function:rangefor i in range(len(sequence)):\n    # Work with index i\nfor i in range(len(sequence)):\n    # Work with index i\nLooping over both elements and indices can be achieved either by the old idiom or by using the new zip built-in function:zipfor i in range(len(sequence)):\n    e = sequence[i]\n    # Work with index i and element e\nfor i in range(len(sequence)):\n    e = sequence[i]\n    # Work with index i and element e\norfor i, e in zip(range(len(sequence)), sequence):\n    # Work with index i and element e\nfor i, e in zip(range(len(sequence)), sequence):\n    # Work with index i and element e\nvia PEP 212 \u2013 Loop Counter Iteration.PEP 212 \u2013 Loop Counter IterationPEP 212 \u2013 Loop Counter Iteration",
                "In your question, you write \"how do I access the loop index, from 1 to 5 in this case?\"\"how do I access the loop index, from 1 to 5 in this case?\"However, the index for a list runs from zero.  So, then we need to know if what you actually want is the index and item for each item in a list, or whether you really want numbers starting from 1.  Fortunately, in Python, it is easy to do either or both.First, to clarify, the enumerate function iteratively returns the index and corresponding item for each item in a list.enumeratealist = [1, 2, 3, 4, 5]\n\nfor n, a in enumerate(alist):\n    print(\"%d %d\" % (n, a))\nalist = [1, 2, 3, 4, 5]\n\nfor n, a in enumerate(alist):\n    print(\"%d %d\" % (n, a))\nThe output for the above is then,0 1\n1 2\n2 3\n3 4\n4 5\n0 1\n1 2\n2 3\n3 4\n4 5\nNotice that the index runs from 0. This kind of indexing is common among modern programming languages including Python and C.If you want your loop to span a part of the list, you can use the standard Python syntax for a part of the list. For example, to loop from the second item in a list up to but not including the last item, you could usefor n, a in enumerate(alist[1:-1]):\n    print(\"%d %d\" % (n, a))\nfor n, a in enumerate(alist[1:-1]):\n    print(\"%d %d\" % (n, a))\nNote that once again, the output index runs from 0,0 2\n1 3\n2 4\n0 2\n1 3\n2 4\nThat brings us to the start=n switch for enumerate().  This simply offsets the index, you can equivalently simply add a number to the index inside the loop.start=nenumerate()for n, a in enumerate(alist, start=1):\n    print(\"%d %d\" % (n, a))\nfor n, a in enumerate(alist, start=1):\n    print(\"%d %d\" % (n, a))\nfor which the output is1 1\n2 2\n3 3\n4 4\n5 5\n1 1\n2 2\n3 3\n4 4\n5 5\n",
                "If I were to iterate nums = [1, 2, 3, 4, 5] I would donums = [1, 2, 3, 4, 5]for i, num in enumerate(nums, start=1):\n    print(i, num)\nfor i, num in enumerate(nums, start=1):\n    print(i, num)\nOr get the length as l = len(nums)l = len(nums)for i in range(l):\n    print(i+1, nums[i])\nfor i in range(l):\n    print(i+1, nums[i])\n",
                "If there is no duplicate value in the list:for i in ints:\n    indx = ints.index(i)\n    print(i, indx)\nfor i in ints:\n    indx = ints.index(i)\n    print(i, indx)\n",
                "You can also try this:data = ['itemA.ABC', 'itemB.defg', 'itemC.drug', 'itemD.ashok']\nx = []\nfor (i, item) in enumerate(data):\n      a = (i, str(item).split('.'))\n      x.append(a)\nfor index, value in x:\n     print(index, value)\ndata = ['itemA.ABC', 'itemB.defg', 'itemC.drug', 'itemD.ashok']\nx = []\nfor (i, item) in enumerate(data):\n      a = (i, str(item).split('.'))\n      x.append(a)\nfor index, value in x:\n     print(index, value)\nThe output is 0 ['itemA', 'ABC']\n1 ['itemB', 'defg']\n2 ['itemC', 'drug']\n3 ['itemD', 'ashok']\n0 ['itemA', 'ABC']\n1 ['itemB', 'defg']\n2 ['itemC', 'drug']\n3 ['itemD', 'ashok']\n",
                "You can use the index method:indexints = [8, 23, 45, 12, 78]\ninds = [ints.index(i) for i in ints]\nints = [8, 23, 45, 12, 78]\ninds = [ints.index(i) for i in ints]\nIt is highlighted in a comment that this method doesn\u2019t work if there are duplicates in ints. The method below should work for any values in ints:intsintsints = [8, 8, 8, 23, 45, 12, 78]\ninds = [tup[0] for tup in enumerate(ints)]\nints = [8, 8, 8, 23, 45, 12, 78]\ninds = [tup[0] for tup in enumerate(ints)]\nOr alternativelyints = [8, 8, 8, 23, 45, 12, 78]\ninds = [tup for tup in enumerate(ints)]\nints = [8, 8, 8, 23, 45, 12, 78]\ninds = [tup for tup in enumerate(ints)]\nif you want to get both the index and the value in ints as a list of tuples.intsIt uses the method of enumerate in the selected answer to this question, but with list comprehension, making it faster with less code.enumerate",
                "A simple answer using a while loop:whilearr = [8, 23, 45, 12, 78]\ni = 0\nwhile i < len(arr):\n    print(\"Item \", i + 1, \" = \", arr[i])\n    i += 1\narr = [8, 23, 45, 12, 78]\ni = 0\nwhile i < len(arr):\n    print(\"Item \", i + 1, \" = \", arr[i])\n    i += 1\nOutput:Item  1  =  8\nItem  2  =  23\nItem  3  =  45\nItem  4  =  12\nItem  5  =  78\nItem  1  =  8\nItem  2  =  23\nItem  3  =  45\nItem  4  =  12\nItem  5  =  78\n",
                "You can simply use a variable such as count to count the number of elements in the list:countints = [8, 23, 45, 12, 78]\ncount = 0\nfor i in ints:\n    count = count + 1\n    print('item #{} = {}'.format(count, i))\nints = [8, 23, 45, 12, 78]\ncount = 0\nfor i in ints:\n    count = count + 1\n    print('item #{} = {}'.format(count, i))\n",
                "To print a tuple of (index, value) in a list comprehension using a for loop:forints = [8, 23, 45, 12, 78]\nprint [(i,ints[i]) for i in range(len(ints))]\nints = [8, 23, 45, 12, 78]\nprint [(i,ints[i]) for i in range(len(ints))]\nOutput:[(0, 8), (1, 23), (2, 45), (3, 12), (4, 78)]\n[(0, 8), (1, 23), (2, 45), (3, 12), (4, 78)]\n",
                "In addition to all the excellent answers above, here is a solution to this problem when working with pandas Series objects. In many cases, pandas Series have custom/unique indices (for example, unique identifier strings) that can't be accessed with the enumerate() function.enumerate()\nxs = pd.Series([8, 23, 45])\n\nxs.index = ['G923002', 'G923004', 'G923005']\n\nprint(xs)\n\nOutput:\n#    G923002     8\n#    G923004    23\n#    G923005    45\n#    dtype: int64\n\nxs = pd.Series([8, 23, 45])\n\nxs.index = ['G923002', 'G923004', 'G923005']\n\nprint(xs)\nxs = pd.Series([8, 23, 45])\n\nxs.index = ['G923002', 'G923004', 'G923005']\n\nprint(xs)\nOutput:#    G923002     8\n#    G923004    23\n#    G923005    45\n#    dtype: int64\n#    G923002     8\n#    G923004    23\n#    G923005    45\n#    dtype: int64\nWe can see below that enumerate() doesn't give us the desired result:enumerate()\nfor id, x in enumerate(xs):\n    print(\"id #{} = {}\".format(id, x))\n\nOutput:\n#    id #0 = 8\n#    id #1 = 23\n#    id #2 = 45\n\nfor id, x in enumerate(xs):\n    print(\"id #{} = {}\".format(id, x))\nfor id, x in enumerate(xs):\n    print(\"id #{} = {}\".format(id, x))\nOutput:#    id #0 = 8\n#    id #1 = 23\n#    id #2 = 45\n#    id #0 = 8\n#    id #1 = 23\n#    id #2 = 45\nWe can access the indices of a pandas Series in a for loop using .items():.items()\nfor id, x in xs.items():\n    print(\"id #{} = {}\".format(id, x))\n\nOutput:\n#    id #G923002 = 8\n#    id #G923004 = 23\n#    id #G923005 = 45\n\nfor id, x in xs.items():\n    print(\"id #{} = {}\".format(id, x))\nfor id, x in xs.items():\n    print(\"id #{} = {}\".format(id, x))\nOutput:#    id #G923002 = 8\n#    id #G923004 = 23\n#    id #G923005 = 45\n#    id #G923002 = 8\n#    id #G923004 = 23\n#    id #G923005 = 45\n",
                "One-liner lovers:[index for index, datum in enumerate(data) if 'a' in datum]\n[index for index, datum in enumerate(data) if 'a' in datum]\nExplaination:>>> data = ['a','ab','bb','ba','alskdhkjl','hkjferht','lal']\n>>> data\n['a', 'ab', 'bb', 'ba', 'alskdhkjl', 'hkjferht', 'lal']\n>>> [index for index, datum in enumerate(data) if 'a' in datum]\n[0, 1, 3, 4, 6]\n>>> [index for index, datum in enumerate(data) if 'b' in datum]\n[1, 2, 3]\n>>>\n>>> data = ['a','ab','bb','ba','alskdhkjl','hkjferht','lal']\n>>> data\n['a', 'ab', 'bb', 'ba', 'alskdhkjl', 'hkjferht', 'lal']\n>>> [index for index, datum in enumerate(data) if 'a' in datum]\n[0, 1, 3, 4, 6]\n>>> [index for index, datum in enumerate(data) if 'b' in datum]\n[1, 2, 3]\n>>>\nPoints to take:\nPython list doesn't provide an index; if you are using for\nIf you enumerate a list it will return you ANOTHER list\n\nBUT that list will have a different type\nit will wrap each and every element with an index as tuple\nwe can access tuples as variables, separated with comma(,)\n\n\nPython list doesn't provide an index; if you are using forlistforIf you enumerate a list it will return you ANOTHER list\n\nBUT that list will have a different type\nit will wrap each and every element with an index as tuple\nwe can access tuples as variables, separated with comma(,)\n\nenumeratelistlist\nBUT that list will have a different type\nit will wrap each and every element with an index as tuple\nwe can access tuples as variables, separated with comma(,)\nBUT that list will have a different typeit will wrap each and every element with an index as tupletuplewe can access tuples as variables, separated with comma(,),Thanks. Keep me in your prayers.Thanks. Keep me in your prayers.",
                "You can use range(len(some_list)) and then lookup the index like thisrange(len(some_list))xs = [8, 23, 45]\nfor i in range(len(xs)):\n    print(\"item #{} = {}\".format(i + 1, xs[i]))\nxs = [8, 23, 45]\nfor i in range(len(xs)):\n    print(\"item #{} = {}\".format(i + 1, xs[i]))\nOr use the Python\u2019s built-in enumerate function which allows you to loop over a list and retrieve the index and the value of each item in the listenumeratexs = [8, 23, 45]\nfor idx, val in enumerate(xs, start=1):\n    print(\"item #{} = {}\".format(idx, val))\nxs = [8, 23, 45]\nfor idx, val in enumerate(xs, start=1):\n    print(\"item #{} = {}\".format(idx, val))\n",
                "It can be achieved with the following code:xs = [8, 23, 45]\nfor x, n in zip(xs, range(1, len(xs)+1)):\n    print(\"item #{} = {}\".format(n, x))\nxs = [8, 23, 45]\nfor x, n in zip(xs, range(1, len(xs)+1)):\n    print(\"item #{} = {}\".format(n, x))\nHere, range(1, len(xs)+1); If you expect the output to start from 1 instead of 0, you need to start the range from 1 and add 1 to the total length estimated since python starts indexing the number from 0 by default.Final Output:\nitem #1 = 8\nitem #2 = 23\nitem #3 = 45\nFinal Output:\nitem #1 = 8\nitem #2 = 23\nitem #3 = 45\n",
                "A loop with a \"counter\" variable set as an initialiser that will be a parameter, in formatting the string, as the item number.The for loop accesses the \"listos\" variable which is the list. As we access the list by \"i\", \"i\" is formatted as the item price (or whatever it is).forlistos = [8, 23, 45, 12, 78]\ncounter = 1\nfor i in listos:\n    print('Item #{} = {}'.format(counter, i))\n    counter += 1\nlistos = [8, 23, 45, 12, 78]\ncounter = 1\nfor i in listos:\n    print('Item #{} = {}'.format(counter, i))\n    counter += 1\nOutput:Item #1 = 8\nItem #2 = 23\nItem #3 = 45\nItem #4 = 12\nItem #5 = 78\nItem #1 = 8\nItem #2 = 23\nItem #3 = 45\nItem #4 = 12\nItem #5 = 78\n",
                "This serves the purpose well enough:list1 = [10, 'sumit', 43.21, 'kumar', '43', 'test', 3]\nfor x in list1:\n    print('index:', list1.index(x), 'value:', x)\nlist1 = [10, 'sumit', 43.21, 'kumar', '43', 'test', 3]\nfor x in list1:\n    print('index:', list1.index(x), 'value:', x)\n"
            ]
        },
        {
            "tag": "flat_list",
            "patterns": [
                "How do I make a flat list out of a list of lists?",
                "how do i make a flat list?",
                "how do i create a flat list of lists?",
                "tell me the best way to make a flat list out of multiple lists?",
                "what can i do to make a list out of a list?",
                "how do you get a flat list out of a list?"
            ],
            "responses": [
                "Given a list of lists l,lflat_list = [item for sublist in l for item in sublist]\nflat_list = [item for sublist in l for item in sublist]\nwhich means:flat_list = []\nfor sublist in l:\n    for item in sublist:\n        flat_list.append(item)\nflat_list = []\nfor sublist in l:\n    for item in sublist:\n        flat_list.append(item)\nis faster than the shortcuts posted so far. (l is the list to flatten.)lHere is the corresponding function:def flatten(l):\n    return [item for sublist in l for item in sublist]\ndef flatten(l):\n    return [item for sublist in l for item in sublist]\nAs evidence, you can use the timeit module in the standard library:timeit$ python -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' '[item for sublist in l for item in sublist]'\n10000 loops, best of 3: 143 usec per loop\n$ python -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' 'sum(l, [])'\n1000 loops, best of 3: 969 usec per loop\n$ python -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' 'reduce(lambda x,y: x+y,l)'\n1000 loops, best of 3: 1.1 msec per loop\n$ python -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' '[item for sublist in l for item in sublist]'\n10000 loops, best of 3: 143 usec per loop\n$ python -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' 'sum(l, [])'\n1000 loops, best of 3: 969 usec per loop\n$ python -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' 'reduce(lambda x,y: x+y,l)'\n1000 loops, best of 3: 1.1 msec per loop\nExplanation: the shortcuts based on + (including the implied use in sum) are, of necessity, O(L**2) when there are L sublists -- as the intermediate result list keeps getting longer, at each step a new intermediate result list object gets allocated, and all the items in the previous intermediate result must be copied over (as well as a few new ones added at the end). So, for simplicity and without actual loss of generality, say you have L sublists of I items each: the first I items are copied back and forth L-1 times, the second I items L-2 times, and so on; total number of copies is I times the sum of x for x from 1 to L excluded, i.e., I * (L**2)/2.+sumO(L**2)I * (L**2)/2The list comprehension just generates one list, once, and copies each item over (from its original place of residence to the result list) also exactly once.",
                "You can use itertools.chain():itertools.chain()itertools.chain()>>> import itertools\n>>> list2d = [[1,2,3], [4,5,6], [7], [8,9]]\n>>> merged = list(itertools.chain(*list2d))\n>>> import itertools\n>>> list2d = [[1,2,3], [4,5,6], [7], [8,9]]\n>>> merged = list(itertools.chain(*list2d))\nOr you can use itertools.chain.from_iterable() which doesn't require unpacking the list with the * operator:itertools.chain.from_iterable()itertools.chain.from_iterable()*>>> import itertools\n>>> list2d = [[1,2,3], [4,5,6], [7], [8,9]]\n>>> merged = list(itertools.chain.from_iterable(list2d))\n>>> import itertools\n>>> list2d = [[1,2,3], [4,5,6], [7], [8,9]]\n>>> merged = list(itertools.chain.from_iterable(list2d))\nThis approach is arguably more readable than [item for sublist in l for item in sublist] and appears to be faster too:[item for sublist in l for item in sublist]$ python3 -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99;import itertools' 'list(itertools.chain.from_iterable(l))'\n20000 loops, best of 5: 10.8 usec per loop\n$ python3 -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' '[item for sublist in l for item in sublist]'\n10000 loops, best of 5: 21.7 usec per loop\n$ python3 -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' 'sum(l, [])'\n1000 loops, best of 5: 258 usec per loop\n$ python3 -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99;from functools import reduce' 'reduce(lambda x,y: x+y,l)'\n1000 loops, best of 5: 292 usec per loop\n$ python3 --version\nPython 3.7.5rc1\n$ python3 -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99;import itertools' 'list(itertools.chain.from_iterable(l))'\n20000 loops, best of 5: 10.8 usec per loop\n$ python3 -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' '[item for sublist in l for item in sublist]'\n10000 loops, best of 5: 21.7 usec per loop\n$ python3 -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99' 'sum(l, [])'\n1000 loops, best of 5: 258 usec per loop\n$ python3 -mtimeit -s'l=[[1,2,3],[4,5,6], [7], [8,9]]*99;from functools import reduce' 'reduce(lambda x,y: x+y,l)'\n1000 loops, best of 5: 292 usec per loop\n$ python3 --version\nPython 3.7.5rc1\n",
                "Note from the author: This is very inefficient. But fun, because monoids are awesome.Note from the authormonoids>>> xss = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\n>>> sum(xss, [])\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n>>> xss = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\n>>> sum(xss, [])\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\nsum sums the elements of the iterable xss, and uses the second argument as the initial value [] for the sum. (The default initial value is 0, which is not a list.)sumxss[]0Because you are summing nested lists, you actually get [1,3]+[2,4] as a result of sum([[1,3],[2,4]],[]), which is equal to [1,3,2,4].[1,3]+[2,4]sum([[1,3],[2,4]],[])[1,3,2,4]Note that only works on lists of lists. For lists of lists of lists, you'll need another solution.",
                "I tested most suggested solutions with perfplot (a pet project of mine, essentially a wrapper around timeit), and foundperfplottimeitimport functools\nimport operator\nfunctools.reduce(operator.iconcat, a, [])\nimport functools\nimport operator\nfunctools.reduce(operator.iconcat, a, [])\nto be the fastest solution, both when many small lists and few long lists are concatenated. (operator.iadd is equally fast.)operator.iaddA simpler and also acceptable variant isout = []\nfor sublist in a:\n    out.extend(sublist)\nout = []\nfor sublist in a:\n    out.extend(sublist)\nIf the number of sublists is large, this performs a little worse than the above suggestion.Code to reproduce the plot:import functools\nimport itertools\nimport operator\n\nimport numpy as np\nimport perfplot\n\n\ndef forfor(a):\n    return [item for sublist in a for item in sublist]\n\n\ndef sum_brackets(a):\n    return sum(a, [])\n\n\ndef functools_reduce(a):\n    return functools.reduce(operator.concat, a)\n\n\ndef functools_reduce_iconcat(a):\n    return functools.reduce(operator.iconcat, a, [])\n\n\ndef itertools_chain(a):\n    return list(itertools.chain.from_iterable(a))\n\n\ndef numpy_flat(a):\n    return list(np.array(a).flat)\n\n\ndef numpy_concatenate(a):\n    return list(np.concatenate(a))\n\n\ndef extend(a):\n    out = []\n    for sublist in a:\n        out.extend(sublist)\n    return out\n\n\nb = perfplot.bench(\n    setup=lambda n: [list(range(10))] * n,\n    # setup=lambda n: [list(range(n))] * 10,\n    kernels=[\n        forfor,\n        sum_brackets,\n        functools_reduce,\n        functools_reduce_iconcat,\n        itertools_chain,\n        numpy_flat,\n        numpy_concatenate,\n        extend,\n    ],\n    n_range=[2 ** k for k in range(16)],\n    xlabel=\"num lists (of length 10)\",\n    # xlabel=\"len lists (10 lists total)\"\n)\nb.save(\"out.png\")\nb.show()\nimport functools\nimport itertools\nimport operator\n\nimport numpy as np\nimport perfplot\n\n\ndef forfor(a):\n    return [item for sublist in a for item in sublist]\n\n\ndef sum_brackets(a):\n    return sum(a, [])\n\n\ndef functools_reduce(a):\n    return functools.reduce(operator.concat, a)\n\n\ndef functools_reduce_iconcat(a):\n    return functools.reduce(operator.iconcat, a, [])\n\n\ndef itertools_chain(a):\n    return list(itertools.chain.from_iterable(a))\n\n\ndef numpy_flat(a):\n    return list(np.array(a).flat)\n\n\ndef numpy_concatenate(a):\n    return list(np.concatenate(a))\n\n\ndef extend(a):\n    out = []\n    for sublist in a:\n        out.extend(sublist)\n    return out\n\n\nb = perfplot.bench(\n    setup=lambda n: [list(range(10))] * n,\n    # setup=lambda n: [list(range(n))] * 10,\n    kernels=[\n        forfor,\n        sum_brackets,\n        functools_reduce,\n        functools_reduce_iconcat,\n        itertools_chain,\n        numpy_flat,\n        numpy_concatenate,\n        extend,\n    ],\n    n_range=[2 ** k for k in range(16)],\n    xlabel=\"num lists (of length 10)\",\n    # xlabel=\"len lists (10 lists total)\"\n)\nb.save(\"out.png\")\nb.show()\n",
                "Using functools.reduce, which adds an accumulated list xs to the next list ys:functools.reducefunctools.reducexsysfrom functools import reduce\nxss = [[1,2,3], [4,5,6], [7], [8,9]]\nout = reduce(lambda xs, ys: xs + ys, xss)\nfrom functools import reduce\nxss = [[1,2,3], [4,5,6], [7], [8,9]]\nout = reduce(lambda xs, ys: xs + ys, xss)\nOutput:[1, 2, 3, 4, 5, 6, 7, 8, 9]\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\nA faster way using operator.concat:operator.concatoperator.concatfrom functools import reduce\nimport operator\nxss = [[1,2,3], [4,5,6], [7], [8,9]]\nout = reduce(operator.concat, xss)\nfrom functools import reduce\nimport operator\nxss = [[1,2,3], [4,5,6], [7], [8,9]]\nout = reduce(operator.concat, xss)\nOutput:[1, 2, 3, 4, 5, 6, 7, 8, 9]\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
                "Here is a general approach that applies to numbers, strings, nested lists and mixed containers.  This can flatten both simple and complicated containers (see also Demo).numbersstringsnestedmixedDemoCodeCodefrom typing import Iterable \n#from collections import Iterable                            # < py38\n\n\ndef flatten(items):\n    \"\"\"Yield items from any nested iterable; see Reference.\"\"\"\n    for x in items:\n        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n            for sub_x in flatten(x):\n                yield sub_x\n        else:\n            yield x\nfrom typing import Iterable \n#from collections import Iterable                            # < py38\n\n\ndef flatten(items):\n    \"\"\"Yield items from any nested iterable; see Reference.\"\"\"\n    for x in items:\n        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n            for sub_x in flatten(x):\n                yield sub_x\n        else:\n            yield x\nNotes:Notes\nIn Python 3, yield from flatten(x) can replace for sub_x in flatten(x): yield sub_x\nIn Python 3.8, abstract base classes are moved from collection.abc to the typing module.\nIn Python 3, yield from flatten(x) can replace for sub_x in flatten(x): yield sub_xyield from flatten(x)for sub_x in flatten(x): yield sub_xIn Python 3.8, abstract base classes are moved from collection.abc to the typing module.abstract base classesmovedcollection.abctypingDemoDemosimple = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\nlist(flatten(simple))\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\ncomplicated = [[1, [2]], (3, 4, {5, 6}, 7), 8, \"9\"]              # numbers, strs, nested & mixed\nlist(flatten(complicated))\n# [1, 2, 3, 4, 5, 6, 7, 8, '9']\nsimple = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\nlist(flatten(simple))\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\ncomplicated = [[1, [2]], (3, 4, {5, 6}, 7), 8, \"9\"]              # numbers, strs, nested & mixed\nlist(flatten(complicated))\n# [1, 2, 3, 4, 5, 6, 7, 8, '9']\nReferenceReference\nThis solution is modified from a recipe in Beazley, D. and B. Jones.  Recipe 4.14, Python Cookbook 3rd Ed., O'Reilly Media Inc. Sebastopol, CA: 2013.\nFound an earlier SO post, possibly the original demonstration.\nThis solution is modified from a recipe in Beazley, D. and B. Jones.  Recipe 4.14, Python Cookbook 3rd Ed., O'Reilly Media Inc. Sebastopol, CA: 2013.Beazley, D. and B. Jones.  Recipe 4.14, Python Cookbook 3rd Ed., O'Reilly Media Inc. Sebastopol, CA: 2013.Found an earlier SO post, possibly the original demonstration.SO post",
                "To flatten a data-structure that is deeply nested, use iteration_utilities.deepflatten1:iteration_utilities.deepflatteniteration_utilities.deepflatten1>>> from iteration_utilities import deepflatten\n\n>>> l = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\n>>> list(deepflatten(l, depth=1))\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n>>> l = [[1, 2, 3], [4, [5, 6]], 7, [8, 9]]\n>>> list(deepflatten(l))\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n>>> from iteration_utilities import deepflatten\n\n>>> l = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\n>>> list(deepflatten(l, depth=1))\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n>>> l = [[1, 2, 3], [4, [5, 6]], 7, [8, 9]]\n>>> list(deepflatten(l))\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\nIt's a generator so you need to cast the result to a list or explicitly iterate over it.listTo flatten only one level and if each of the items is itself iterable you can also use iteration_utilities.flatten which itself is just a thin wrapper around itertools.chain.from_iterable:iteration_utilities.flatteniteration_utilities.flattenitertools.chain.from_iterableitertools.chain.from_iterable>>> from iteration_utilities import flatten\n>>> l = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\n>>> list(flatten(l))\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n>>> from iteration_utilities import flatten\n>>> l = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\n>>> list(flatten(l))\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\nJust to add some timings (based on Nico Schl\u00f6mer's answer that didn't include the function presented in this answer):Nico Schl\u00f6mer's answerIt's a log-log plot to accommodate for the huge range of values spanned. For qualitative reasoning: Lower is better.The results show that if the iterable contains only a few inner iterables then sum will be fastest, however for long iterables only the itertools.chain.from_iterable, iteration_utilities.deepflatten or the nested comprehension have reasonable performance with itertools.chain.from_iterable being the fastest (as already noticed by Nico Schl\u00f6mer).sumitertools.chain.from_iterableiteration_utilities.deepflattenitertools.chain.from_iterablefrom itertools import chain\nfrom functools import reduce\nfrom collections import Iterable  # or from collections.abc import Iterable\nimport operator\nfrom iteration_utilities import deepflatten\n\ndef nested_list_comprehension(lsts):\n    return [item for sublist in lsts for item in sublist]\n\ndef itertools_chain_from_iterable(lsts):\n    return list(chain.from_iterable(lsts))\n\ndef pythons_sum(lsts):\n    return sum(lsts, [])\n\ndef reduce_add(lsts):\n    return reduce(lambda x, y: x + y, lsts)\n\ndef pylangs_flatten(lsts):\n    return list(flatten(lsts))\n\ndef flatten(items):\n    \"\"\"Yield items from any nested iterable; see REF.\"\"\"\n    for x in items:\n        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n            yield from flatten(x)\n        else:\n            yield x\n\ndef reduce_concat(lsts):\n    return reduce(operator.concat, lsts)\n\ndef iteration_utilities_deepflatten(lsts):\n    return list(deepflatten(lsts, depth=1))\n\n\nfrom simple_benchmark import benchmark\n\nb = benchmark(\n    [nested_list_comprehension, itertools_chain_from_iterable, pythons_sum, reduce_add,\n     pylangs_flatten, reduce_concat, iteration_utilities_deepflatten],\n    arguments={2**i: [[0]*5]*(2**i) for i in range(1, 13)},\n    argument_name='number of inner lists'\n)\n\nb.plot()\nfrom itertools import chain\nfrom functools import reduce\nfrom collections import Iterable  # or from collections.abc import Iterable\nimport operator\nfrom iteration_utilities import deepflatten\n\ndef nested_list_comprehension(lsts):\n    return [item for sublist in lsts for item in sublist]\n\ndef itertools_chain_from_iterable(lsts):\n    return list(chain.from_iterable(lsts))\n\ndef pythons_sum(lsts):\n    return sum(lsts, [])\n\ndef reduce_add(lsts):\n    return reduce(lambda x, y: x + y, lsts)\n\ndef pylangs_flatten(lsts):\n    return list(flatten(lsts))\n\ndef flatten(items):\n    \"\"\"Yield items from any nested iterable; see REF.\"\"\"\n    for x in items:\n        if isinstance(x, Iterable) and not isinstance(x, (str, bytes)):\n            yield from flatten(x)\n        else:\n            yield x\n\ndef reduce_concat(lsts):\n    return reduce(operator.concat, lsts)\n\ndef iteration_utilities_deepflatten(lsts):\n    return list(deepflatten(lsts, depth=1))\n\n\nfrom simple_benchmark import benchmark\n\nb = benchmark(\n    [nested_list_comprehension, itertools_chain_from_iterable, pythons_sum, reduce_add,\n     pylangs_flatten, reduce_concat, iteration_utilities_deepflatten],\n    arguments={2**i: [[0]*5]*(2**i) for i in range(1, 13)},\n    argument_name='number of inner lists'\n)\n\nb.plot()\n1 Disclaimer: I'm the author of that library1 Disclaimer: I'm the author of that library",
                "The following seems simplest to me:>>> import numpy as np\n>>> l = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\n>>> print(np.concatenate(l))\n[1 2 3 4 5 6 7 8 9]\n>>> import numpy as np\n>>> l = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\n>>> print(np.concatenate(l))\n[1 2 3 4 5 6 7 8 9]\n",
                "Consider installing the more_itertools package.more_itertoolsmore_itertools> pip install more_itertools\n> pip install more_itertools\nIt ships with an implementation for flatten (source, from the itertools recipes):flattenflattensourceitertools recipesimport more_itertools\n\n\nlst = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\nlist(more_itertools.flatten(lst))\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\nimport more_itertools\n\n\nlst = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\nlist(more_itertools.flatten(lst))\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\nNote: as mentioned in the docs, flatten requires a list of lists.  See below on flattening more irregular inputs.Note: as mentioned in the docs, flatten requires a list of lists.  See below on flattening more irregular inputs.docsflattenAs of version 2.4, you can flatten more complicated, nested iterables with more_itertools.collapse (source, contributed by  abarnet).more_itertools.collapsemore_itertools.collapsesourcelst = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\nlist(more_itertools.collapse(lst)) \n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nlst = [[1, 2, 3], [[4, 5, 6]], [[[7]]], 8, 9]              # complex nesting\nlist(more_itertools.collapse(lst))\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\nlst = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\nlist(more_itertools.collapse(lst)) \n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nlst = [[1, 2, 3], [[4, 5, 6]], [[[7]]], 8, 9]              # complex nesting\nlist(more_itertools.collapse(lst))\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
                "The reason your function didn't work is because the extend extends an array in-place and doesn't return it. You can still return x from lambda, using something like this:extendreduce(lambda x,y: x.extend(y) or x, l)\nreduce(lambda x,y: x.extend(y) or x, l)\nNote: extend is more efficient than + on lists.",
                "matplotlib.cbook.flatten() will work for nested lists even if they nest more deeply than the example.matplotlib.cbook.flatten()import matplotlib\nl = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\nprint(list(matplotlib.cbook.flatten(l)))\nl2 = [[1, 2, 3], [4, 5, 6], [7], [8, [9, 10, [11, 12, [13]]]]]\nprint(list(matplotlib.cbook.flatten(l2)))\nimport matplotlib\nl = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\nprint(list(matplotlib.cbook.flatten(l)))\nl2 = [[1, 2, 3], [4, 5, 6], [7], [8, [9, 10, [11, 12, [13]]]]]\nprint(list(matplotlib.cbook.flatten(l2)))\nResult:[1, 2, 3, 4, 5, 6, 7, 8, 9]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\nThis is 18x faster than underscore._.flatten:Average time over 1000 trials of matplotlib.cbook.flatten: 2.55e-05 sec\nAverage time over 1000 trials of underscore._.flatten: 4.63e-04 sec\n(time for underscore._)/(time for matplotlib.cbook) = 18.1233394636\nAverage time over 1000 trials of matplotlib.cbook.flatten: 2.55e-05 sec\nAverage time over 1000 trials of underscore._.flatten: 4.63e-04 sec\n(time for underscore._)/(time for matplotlib.cbook) = 18.1233394636\n",
                "According your list [[1, 2, 3], [4, 5, 6], [7], [8, 9]] which is 1 list level, we can simply use sum(list,[]) without using any libraries[[1, 2, 3], [4, 5, 6], [7], [8, 9]]sum(list,[])sum([[1, 2, 3], [4, 5, 6], [7], [8, 9]],[])\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\nsum([[1, 2, 3], [4, 5, 6], [7], [8, 9]],[])\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\nTo extend the advantage of this method when there is a tuple or number existing inside. Simply adding a mapping function for each element by map to the listmap#For only tuple\nsum(list(map(list,[[1, 2, 3], (4, 5, 6), (7,), [8, 9]])),[])\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n#In general\n\ndef convert(x):\n    if type(x) is int or type(x) is float:\n           return [x]\n    else:\n           return list(x)\n\nsum(list(map(convert,[[1, 2, 3], (4, 5, 6), 7, [8, 9]])),[])\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\n#For only tuple\nsum(list(map(list,[[1, 2, 3], (4, 5, 6), (7,), [8, 9]])),[])\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n#In general\n\ndef convert(x):\n    if type(x) is int or type(x) is float:\n           return [x]\n    else:\n           return list(x)\n\nsum(list(map(convert,[[1, 2, 3], (4, 5, 6), 7, [8, 9]])),[])\n# [1, 2, 3, 4, 5, 6, 7, 8, 9]\nIn here, there is a clear explanation of the drawback in terms of memory for this approach. In short, it recursively creates list objects, which should be avoided :(here",
                "One can also use NumPy's flat:flatimport numpy as np\nlist(np.array(l).flat)\nimport numpy as np\nlist(np.array(l).flat)\nIt only works when sublists have identical dimensions.",
                "You can use the list extend method. It shows to be the fastest:listextendflat_list = []\nfor sublist in l:\n    flat_list.extend(sublist)\nflat_list = []\nfor sublist in l:\n    flat_list.extend(sublist)\nPerformance:import functools\nimport itertools\nimport numpy\nimport operator\nimport perfplot\n\n\ndef functools_reduce_iconcat(a):\n    return functools.reduce(operator.iconcat, a, [])\n\n\ndef itertools_chain(a):\n    return list(itertools.chain.from_iterable(a))\n\n\ndef numpy_flat(a):\n    return list(numpy.array(a).flat)\n\n\ndef extend(a):\n    n = []\n\n    list(map(n.extend, a))\n\n    return n\n\n\nperfplot.show(\n    setup = lambda n: [list(range(10))] * n,\n    kernels = [\n        functools_reduce_iconcat, extend, itertools_chain, numpy_flat\n        ],\n    n_range = [2**k for k in range(16)],\n    xlabel = 'num lists',\n    )\nimport functools\nimport itertools\nimport numpy\nimport operator\nimport perfplot\n\n\ndef functools_reduce_iconcat(a):\n    return functools.reduce(operator.iconcat, a, [])\n\n\ndef itertools_chain(a):\n    return list(itertools.chain.from_iterable(a))\n\n\ndef numpy_flat(a):\n    return list(numpy.array(a).flat)\n\n\ndef extend(a):\n    n = []\n\n    list(map(n.extend, a))\n\n    return n\n\n\nperfplot.show(\n    setup = lambda n: [list(range(10))] * n,\n    kernels = [\n        functools_reduce_iconcat, extend, itertools_chain, numpy_flat\n        ],\n    n_range = [2**k for k in range(16)],\n    xlabel = 'num lists',\n    )\nOutput:Output:",
                "There are several answers with the same recursive appending scheme as below, but none makes use of try, which makes the solution more robust and Pythonic.tryPythonicdef flatten(itr):\n    for x in itr:\n        try:\n            yield from flatten(x)\n        except TypeError:\n            yield x\ndef flatten(itr):\n    for x in itr:\n        try:\n            yield from flatten(x)\n        except TypeError:\n            yield x\nUsage: this is a generator, and you typically want to enclose it in an iterable builder like list() or tuple() or use it in a for loop.Usagelist()tuple()forAdvantages of this solution are:\nworks with any kind of iterable (even future ones!)\nworks with any combination and deepness of nesting\nworks also if top level contains bare items\nno dependencies\nfast and efficient (you can flatten the nested iterable partially, without wasting time on the remaining part you don't need)\nversatile (you can use it to build an iterable of your choice or in a loop)\nworks with any kind of iterable (even future ones!)works with any combination and deepness of nestingworks also if top level contains bare itemsno dependenciesfast and efficient (you can flatten the nested iterable partially, without wasting time on the remaining part you don't need)versatile (you can use it to build an iterable of your choice or in a loop)N.B.: Since all iterables are flattened, strings are decomposed into sequences of single characters. If you don't like/want such behavior, you can use the following version which filters out from flattening iterables like strings and bytes:alldef flatten(itr):\n    if type(itr) in (str,bytes):\n        yield itr\n    else:\n        for x in itr:\n            try:\n                yield from flatten(x)\n            except TypeError:\n                yield x\ndef flatten(itr):\n    if type(itr) in (str,bytes):\n        yield itr\n    else:\n        for x in itr:\n            try:\n                yield from flatten(x)\n            except TypeError:\n                yield x\n",
                "Note: Below applies to Python 3.3+ because it uses yield_from.  six is also a third-party package, though it is stable.  Alternately, you could use sys.version.Noteyield_fromyield_fromsixsys.versionIn the case of obj = [[1, 2,], [3, 4], [5, 6]], all of the solutions here are good, including list comprehension and itertools.chain.from_iterable.obj = [[1, 2,], [3, 4], [5, 6]]itertools.chain.from_iterableHowever, consider this slightly more complex case:>>> obj = [[1, 2, 3], [4, 5], 6, 'abc', [7], [8, [9, 10]]]\n>>> obj = [[1, 2, 3], [4, 5], 6, 'abc', [7], [8, [9, 10]]]\nThere are several problems here:\nOne element, 6, is just a scalar; it's not iterable, so the above routes will fail here.\nOne element, 'abc', is technically iterable (all strs are).  However, reading between the lines a bit, you don't want to treat it as such--you want to treat it as a single element.\nThe final element, [8, [9, 10]] is itself a nested iterable.  Basic list comprehension and chain.from_iterable only extract \"1 level down.\"\nOne element, 6, is just a scalar; it's not iterable, so the above routes will fail here.6One element, 'abc', is technically iterable (all strs are).  However, reading between the lines a bit, you don't want to treat it as such--you want to treat it as a single element.'abc'isstrThe final element, [8, [9, 10]] is itself a nested iterable.  Basic list comprehension and chain.from_iterable only extract \"1 level down.\"[8, [9, 10]]chain.from_iterableYou can remedy this as follows:>>> from collections import Iterable\n>>> from six import string_types\n\n>>> def flatten(obj):\n...     for i in obj:\n...         if isinstance(i, Iterable) and not isinstance(i, string_types):\n...             yield from flatten(i)\n...         else:\n...             yield i\n\n\n>>> list(flatten(obj))\n[1, 2, 3, 4, 5, 6, 'abc', 7, 8, 9, 10]\n>>> from collections import Iterable\n>>> from six import string_types\n\n>>> def flatten(obj):\n...     for i in obj:\n...         if isinstance(i, Iterable) and not isinstance(i, string_types):\n...             yield from flatten(i)\n...         else:\n...             yield i\n\n\n>>> list(flatten(obj))\n[1, 2, 3, 4, 5, 6, 'abc', 7, 8, 9, 10]\nHere, you check that the sub-element (1) is iterable with Iterable, an ABC from itertools, but also want to ensure that (2) the element is not \"string-like.\"IterableIterableitertoolsnot",
                "If you are willing to give up a tiny amount of speed for a cleaner look, then you could use numpy.concatenate().tolist() or numpy.concatenate().ravel().tolist():numpy.concatenate().tolist()numpy.concatenate().ravel().tolist()import numpy\n\nl = [[1, 2, 3], [4, 5, 6], [7], [8, 9]] * 99\n\n%timeit numpy.concatenate(l).ravel().tolist()\n1000 loops, best of 3: 313 \u00b5s per loop\n\n%timeit numpy.concatenate(l).tolist()\n1000 loops, best of 3: 312 \u00b5s per loop\n\n%timeit [item for sublist in l for item in sublist]\n1000 loops, best of 3: 31.5 \u00b5s per loop\nimport numpy\n\nl = [[1, 2, 3], [4, 5, 6], [7], [8, 9]] * 99\n\n%timeit numpy.concatenate(l).ravel().tolist()\n1000 loops, best of 3: 313 \u00b5s per loop\n\n%timeit numpy.concatenate(l).tolist()\n1000 loops, best of 3: 312 \u00b5s per loop\n\n%timeit [item for sublist in l for item in sublist]\n1000 loops, best of 3: 31.5 \u00b5s per loop\nYou can find out more here in the documentation, numpy.concatenate and numpy.ravel.numpy.concatenatenumpy.ravel",
                "def flatten(alist):\n    if alist == []:\n        return []\n    elif type(alist) is not list:\n        return [alist]\n    else:\n        return flatten(alist[0]) + flatten(alist[1:])\ndef flatten(alist):\n    if alist == []:\n        return []\n    elif type(alist) is not list:\n        return [alist]\n    else:\n        return flatten(alist[0]) + flatten(alist[1:])\n",
                "I wanted a solution which can deal with multiple nesting ([[1], [[[2]], [3]]], [1, 2, 3] for example), but would also not be recursive (I had a big level of recursion and I got a recursion error.[[1], [[[2]], [3]]], [1, 2, 3]This is what I came up with:def _flatten(l) -> Iterator[Any]:\n    stack = l.copy()\n    while stack:\n        item = stack.pop()\n        if isinstance(item, list):\n            stack.extend(item)\n        else:\n            yield item\n\n\ndef flatten(l) -> Iterator[Any]:\n    return reversed(list(_flatten(l)))\ndef _flatten(l) -> Iterator[Any]:\n    stack = l.copy()\n    while stack:\n        item = stack.pop()\n        if isinstance(item, list):\n            stack.extend(item)\n        else:\n            yield item\n\n\ndef flatten(l) -> Iterator[Any]:\n    return reversed(list(_flatten(l)))\nand tests:@pytest.mark.parametrize('input_list, expected_output', [\n    ([1, 2, 3], [1, 2, 3]),\n    ([[1], 2, 3], [1, 2, 3]),\n    ([[1], [2], 3], [1, 2, 3]),\n    ([[1], [2], [3]], [1, 2, 3]),\n    ([[1], [[2]], [3]], [1, 2, 3]),\n    ([[1], [[[2]], [3]]], [1, 2, 3]),\n])\ndef test_flatten(input_list, expected_output):\n    assert list(flatten(input_list)) == expected_output\n@pytest.mark.parametrize('input_list, expected_output', [\n    ([1, 2, 3], [1, 2, 3]),\n    ([[1], 2, 3], [1, 2, 3]),\n    ([[1], [2], 3], [1, 2, 3]),\n    ([[1], [2], [3]], [1, 2, 3]),\n    ([[1], [[2]], [3]], [1, 2, 3]),\n    ([[1], [[[2]], [3]]], [1, 2, 3]),\n])\ndef test_flatten(input_list, expected_output):\n    assert list(flatten(input_list)) == expected_output\n",
                "This may not be the most efficient way, but I thought to put a one-liner (actually a two-liner). Both versions will work on arbitrary hierarchy nested lists, and exploits language features (Python\u00a03.5) and recursion.def make_list_flat (l):\n    flist = []\n    flist.extend ([l]) if (type (l) is not list) else [flist.extend (make_list_flat (e)) for e in l]\n    return flist\n\na = [[1, 2], [[[[3, 4, 5], 6]]], 7, [8, [9, [10, 11], 12, [13, 14, [15, [[16, 17], 18]]]]]]\nflist = make_list_flat(a)\nprint (flist)\ndef make_list_flat (l):\n    flist = []\n    flist.extend ([l]) if (type (l) is not list) else [flist.extend (make_list_flat (e)) for e in l]\n    return flist\n\na = [[1, 2], [[[[3, 4, 5], 6]]], 7, [8, [9, [10, 11], 12, [13, 14, [15, [[16, 17], 18]]]]]]\nflist = make_list_flat(a)\nprint (flist)\nThe output is[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\nThis works in a depth first manner. The recursion goes down until it finds a non-list element, then extends the local variable flist and then rolls back it to the parent. Whenever flist is returned, it is extended to the parent's flist in the list comprehension. Therefore, at the root, a flat list is returned.flistflistflistThe above one creates several local lists and returns them which are used to extend the parent's list. I think the way around for this may be creating a gloabl flist, like below.flista = [[1, 2], [[[[3, 4, 5], 6]]], 7, [8, [9, [10, 11], 12, [13, 14, [15, [[16, 17], 18]]]]]]\nflist = []\ndef make_list_flat (l):\n    flist.extend ([l]) if (type (l) is not list) else [make_list_flat (e) for e in l]\n\nmake_list_flat(a)\nprint (flist)\na = [[1, 2], [[[[3, 4, 5], 6]]], 7, [8, [9, [10, 11], 12, [13, 14, [15, [[16, 17], 18]]]]]]\nflist = []\ndef make_list_flat (l):\n    flist.extend ([l]) if (type (l) is not list) else [make_list_flat (e) for e in l]\n\nmake_list_flat(a)\nprint (flist)\nThe output is again[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\nAlthough I am not sure at this time about the efficiency.",
                "Not a one-liner, but seeing all the answers here, I guess this long list missed some pattern matching, so here it is :)The two methods are probably not efficient, but anyway, it's easy to read (to me at least; perhaps I'm spoiled by functional programming):def flat(x):\n    match x:\n        case []:\n            return []\n        case [[*sublist], *r]:\n            return [*sublist, *flat(r)]\ndef flat(x):\n    match x:\n        case []:\n            return []\n        case [[*sublist], *r]:\n            return [*sublist, *flat(r)]\nThe second version considers lists of lists of lists... whatever the nesting:def flat(x):\n    match x:\n        case []:\n            return []\n        case [[*sublist], *r]:\n            return [*flat(sublist), *flat(r)]\n        case [h, *r]:\n            return [h, *flat(r)]\ndef flat(x):\n    match x:\n        case []:\n            return []\n        case [[*sublist], *r]:\n            return [*flat(sublist), *flat(r)]\n        case [h, *r]:\n            return [h, *flat(r)]\n",
                "Another unusual approach that works for hetero- and homogeneous lists of integers:from typing import List\n\n\ndef flatten(l: list) -> List[int]:\n    \"\"\"Flatten an arbitrary deep nested list of lists of integers.\n\n    Examples:\n        >>> flatten([1, 2, [1, [10]]])\n        [1, 2, 1, 10]\n\n    Args:\n        l: Union[l, Union[int, List[int]]\n\n    Returns:\n        Flatted list of integer\n    \"\"\"\n    return [int(i.strip('[ ]')) for i in str(l).split(',')]\nfrom typing import List\n\n\ndef flatten(l: list) -> List[int]:\n    \"\"\"Flatten an arbitrary deep nested list of lists of integers.\n\n    Examples:\n        >>> flatten([1, 2, [1, [10]]])\n        [1, 2, 1, 10]\n\n    Args:\n        l: Union[l, Union[int, List[int]]\n\n    Returns:\n        Flatted list of integer\n    \"\"\"\n    return [int(i.strip('[ ]')) for i in str(l).split(',')]\n",
                "A non-recursive function to flatten lists of lists of any depth:def flatten_list(list1):\n    out = []\n    inside = list1\n    while inside:\n        x = inside.pop(0)\n        if isinstance(x, list):\n            inside[0:0] = x\n        else:\n            out.append(x)\n    return out\n\nl = [[[1,2],3,[4,[[5,6],7],[8]]],[9,10,11]]\nflatten_list(l)\n# [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\ndef flatten_list(list1):\n    out = []\n    inside = list1\n    while inside:\n        x = inside.pop(0)\n        if isinstance(x, list):\n            inside[0:0] = x\n        else:\n            out.append(x)\n    return out\n\nl = [[[1,2],3,[4,[[5,6],7],[8]]],[9,10,11]]\nflatten_list(l)\n# [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
                "If you want to unnest everything and keep a distinct list of elements, you could use this as well.list_of_lists = [[1,2], [2,3], [3,4]]\nlist(set.union(*[set(s) for s in list_of_lists]))\nlist_of_lists = [[1,2], [2,3], [3,4]]\nlist(set.union(*[set(s) for s in list_of_lists]))\n",
                "If you have a numpy array a:aa = np.array([[1,2], [3,4]])\na.flatten('C')\na = np.array([[1,2], [3,4]])\na.flatten('C')\nproduces:[1, 2, 3, 4]\n[1, 2, 3, 4]\nnp.flatten also accepts other parameters:np.flatten\nC:\nF\nA\nK\nC:CFFAAKKMore details about parameters are available here.here",
                "For a list containing multiple list here a recursive solution that work for me and that i hope is correct:# Question 4\ndef flatten(input_ls=[]) -> []:\n    res_ls = []\n    res_ls = flatten_recursive(input_ls, res_ls)\n\n    print(\"Final flatten list solution is: \\n\", res_ls)\n\n    return res_ls\n\n\ndef flatten_recursive(input_ls=[], res_ls=[]) -> []:\n    tmp_ls = []\n\n    for i in input_ls:\n        if isinstance(i, int):\n            res_ls.append(i)\n        else:\n            tmp_ls = i\n            tmp_ls.append(flatten_recursive(i, res_ls))\n\n    print(res_ls)\n    return res_ls\n\n\nflatten([0, 1, [2, 3], 4, [5, 6]])  # test\nflatten([0, [[[1]]], [[2, 3], [4, [[5, 6]]]]])\n# Question 4\ndef flatten(input_ls=[]) -> []:\n    res_ls = []\n    res_ls = flatten_recursive(input_ls, res_ls)\n\n    print(\"Final flatten list solution is: \\n\", res_ls)\n\n    return res_ls\n\n\ndef flatten_recursive(input_ls=[], res_ls=[]) -> []:\n    tmp_ls = []\n\n    for i in input_ls:\n        if isinstance(i, int):\n            res_ls.append(i)\n        else:\n            tmp_ls = i\n            tmp_ls.append(flatten_recursive(i, res_ls))\n\n    print(res_ls)\n    return res_ls\n\n\nflatten([0, 1, [2, 3], 4, [5, 6]])  # test\nflatten([0, [[[1]]], [[2, 3], [4, [[5, 6]]]]])\nOutput:[0, 1, 2, 3]\n[0, 1, 2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6]\nFinal flatten list solution is: \n [0, 1, 2, 3, 4, 5, 6]\n[0, 1]\n[0, 1]\n[0, 1]\n[0, 1, 2, 3]\n[0, 1, 2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6]\nFinal flatten list solution is: \n [0, 1, 2, 3, 4, 5, 6]\n[0, 1, 2, 3]\n[0, 1, 2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6]\nFinal flatten list solution is: \n [0, 1, 2, 3, 4, 5, 6]\n[0, 1]\n[0, 1]\n[0, 1]\n[0, 1, 2, 3]\n[0, 1, 2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6]\n[0, 1, 2, 3, 4, 5, 6]\nFinal flatten list solution is: \n [0, 1, 2, 3, 4, 5, 6]\n",
                "I would suggest using generators with yield statement and yield from.\nHere's an example:yieldyield fromfrom collections.abc import Iterable\n\ndef flatten(items, ignore_types=(bytes, str)):\n    \"\"\"\n       Flatten all of the nested lists to the one. Ignoring flatting of iterable types str and bytes by default.\n    \"\"\"\n    for x in items:\n        if isinstance(x, Iterable) and not isinstance(x, ignore_types):\n            yield from flatten(x)\n        else:\n            yield x\n\nvalues = [7, [4, 3, 5, [7, 3], (3, 4), ('A', {'B', 'C'})]]\n\nfor v in flatten(values):\n    print(v)\nfrom collections.abc import Iterable\n\ndef flatten(items, ignore_types=(bytes, str)):\n    \"\"\"\n       Flatten all of the nested lists to the one. Ignoring flatting of iterable types str and bytes by default.\n    \"\"\"\n    for x in items:\n        if isinstance(x, Iterable) and not isinstance(x, ignore_types):\n            yield from flatten(x)\n        else:\n            yield x\n\nvalues = [7, [4, 3, 5, [7, 3], (3, 4), ('A', {'B', 'C'})]]\n\nfor v in flatten(values):\n    print(v)\n",
                "If I want to add something to the great previous answers, here is my recursive flatten function which can flatten not only nested lists, but also any given container or any generally any object which can throw out items. This does also work for any depth of nesting and it is a lazy iterator which yields the items as requested:flattendef flatten(iterable):\n    # These types won't considered a sequence or generally a container\n    exclude = str, bytes\n\n    for i in iterable:\n        try:\n            if isinstance(i, exclude):\n                raise TypeError\n            iter(i)\n        except TypeError:\n            yield i\n        else:\n            yield from flatten(i)\ndef flatten(iterable):\n    # These types won't considered a sequence or generally a container\n    exclude = str, bytes\n\n    for i in iterable:\n        try:\n            if isinstance(i, exclude):\n                raise TypeError\n            iter(i)\n        except TypeError:\n            yield i\n        else:\n            yield from flatten(i)\nThis way, you can exclude types you don't want to be flattened, like str or what else.strThe idea is if an object can pass the iter() it's ready to yield items. So the iterable can have even generator expressions as an item.iter()Someone could argue: Why did you write this that generic when the OP didn't ask for it? OK, you're right. I just felt like this might help someone (like it did for myself).Test cases:lst1 = [1, {3}, (1, 6), [[3, 8]], [[[5]]], 9, ((((2,),),),)]\nlst2 = ['3', B'A', [[[(i ** 2 for i in range(3))]]], range(3)]\n\nprint(list(flatten(lst1)))\nprint(list(flatten(lst2)))\nlst1 = [1, {3}, (1, 6), [[3, 8]], [[[5]]], 9, ((((2,),),),)]\nlst2 = ['3', B'A', [[[(i ** 2 for i in range(3))]]], range(3)]\n\nprint(list(flatten(lst1)))\nprint(list(flatten(lst2)))\nOutput:[1, 3, 1, 6, 3, 8, 5, 9, 2]\n['3', b'A', 0, 1, 4, 0, 1, 2]\n[1, 3, 1, 6, 3, 8, 5, 9, 2]\n['3', b'A', 0, 1, 4, 0, 1, 2]\n",
                "Simplest Way to do in python without any librarySimplest Way to do in python without any libraryThis function will work for even multidimensional list alsoThis function will work for even multidimensional list alsousing recursion we can achieve any combination of list inside list, we can flatten it without using any library.#Devil\nx = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\n\n\noutput = []\ndef flatten(v):\n    if isinstance(v, int):\n        output.append(v)\n    if isinstance(v, list):\n        for i in range(0, len(v)):\n            flatten(v[i])\n\nflatten(x)\nprint(\"Output:\", output)\n#Output: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n#Adding more dimensions \nx = [ [1, [2, 3, [4, 5], [6]], 7 ], [8, [9, [10]]] ]\nflatten(x)\nprint(\"Output:\", output)\n#Output: [1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n#Devil\nx = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\n\n\noutput = []\ndef flatten(v):\n    if isinstance(v, int):\n        output.append(v)\n    if isinstance(v, list):\n        for i in range(0, len(v)):\n            flatten(v[i])\n\nflatten(x)\nprint(\"Output:\", output)\n#Output: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n#Adding more dimensions \nx = [ [1, [2, 3, [4, 5], [6]], 7 ], [8, [9, [10]]] ]\nflatten(x)\nprint(\"Output:\", output)\n#Output: [1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
                "Considering the list has just integers:import re\nl = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\nlist(map(int,re.sub('(\\[|\\])','',str(l)).split(',')))\nimport re\nl = [[1, 2, 3], [4, 5, 6], [7], [8, 9]]\nlist(map(int,re.sub('(\\[|\\])','',str(l)).split(',')))\n"
            ]
        },
        {
            "tag": "methods",
            "patterns": [
                "Difference between @staticmethod and @classmethod",
                "the difference between static method and class method",
                "the difference between staticmethod and classmethod"
            ],
            "responses": [
                "Maybe a bit of example code will help: Notice the difference in the call signatures of foo, class_foo and static_foo:fooclass_foostatic_fooclass A(object):\n    def foo(self, x):\n        print(f\"executing foo({self}, {x})\")\n\n    @classmethod\n    def class_foo(cls, x):\n        print(f\"executing class_foo({cls}, {x})\")\n\n    @staticmethod\n    def static_foo(x):\n        print(f\"executing static_foo({x})\")\n\na = A()\nclass A(object):\n    def foo(self, x):\n        print(f\"executing foo({self}, {x})\")\n\n    @classmethod\n    def class_foo(cls, x):\n        print(f\"executing class_foo({cls}, {x})\")\n\n    @staticmethod\n    def static_foo(x):\n        print(f\"executing static_foo({x})\")\n\na = A()\nBelow is the usual way an object instance calls a method. The object instance, a, is implicitly passed as the first argument.aa.foo(1)\n# executing foo(<__main__.A object at 0xb7dbef0c>, 1)\na.foo(1)\n# executing foo(<__main__.A object at 0xb7dbef0c>, 1)\nWith classmethods, the class of the object instance is implicitly passed as the first argument instead of self.With classmethodsselfa.class_foo(1)\n# executing class_foo(<class '__main__.A'>, 1)\na.class_foo(1)\n# executing class_foo(<class '__main__.A'>, 1)\nYou can also call class_foo using the class. In fact, if you define something to be\na classmethod, it is probably because you intend to call it from the class rather than from a class instance. A.foo(1) would have raised a TypeError, but A.class_foo(1) works just fine:class_fooA.foo(1)A.class_foo(1)A.class_foo(1)\n# executing class_foo(<class '__main__.A'>, 1)\nA.class_foo(1)\n# executing class_foo(<class '__main__.A'>, 1)\nOne use people have found for class methods is to create inheritable alternative constructors.inheritable alternative constructorsWith staticmethods, neither self (the object instance) nor  cls (the class) is implicitly passed as the first argument. They behave like plain functions except that you can call them from an instance or the class:With staticmethodsselfclsa.static_foo(1)\n# executing static_foo(1)\n\nA.static_foo('hi')\n# executing static_foo(hi)\na.static_foo(1)\n# executing static_foo(1)\n\nA.static_foo('hi')\n# executing static_foo(hi)\nStaticmethods are used to group functions which have some logical connection with a class to the class.foo is just a function, but when you call a.foo you don't just get the function,\nyou get a \"partially applied\" version of the function with the object instance a bound as the first argument to the function. foo expects 2 arguments, while a.foo only expects 1 argument.fooa.fooafooa.fooa is bound to foo. That is what is meant by the term \"bound\" below:afooprint(a.foo)\n# <bound method A.foo of <__main__.A object at 0xb7d52f0c>>\nprint(a.foo)\n# <bound method A.foo of <__main__.A object at 0xb7d52f0c>>\nWith a.class_foo, a is not bound to class_foo, rather the class A is bound to class_foo.a.class_fooaclass_fooAclass_fooprint(a.class_foo)\n# <bound method type.class_foo of <class '__main__.A'>>\nprint(a.class_foo)\n# <bound method type.class_foo of <class '__main__.A'>>\nHere, with a staticmethod, even though it is a method, a.static_foo just returns\na good 'ole function with no arguments bound. static_foo expects 1 argument, and\na.static_foo expects 1 argument too.a.static_foostatic_fooa.static_fooprint(a.static_foo)\n# <function static_foo at 0xb7d479cc>\nprint(a.static_foo)\n# <function static_foo at 0xb7d479cc>\nAnd of course the same thing happens when you call static_foo with the class A instead.static_fooAprint(A.static_foo)\n# <function static_foo at 0xb7d479cc>\nprint(A.static_foo)\n# <function static_foo at 0xb7d479cc>\n",
                "A staticmethod is a method that knows nothing about the class or instance it was called on. It just gets the arguments that were passed, no implicit first argument. It is basically useless in Python -- you can just use a module function instead of a staticmethod.staticmethodA classmethod, on the other hand, is a method that gets passed the class it was called on, or the class of the instance it was called on, as first argument. This is useful when you want the method to be a factory for the class: since it gets the actual class it was called on as first argument, you can always instantiate the right class, even when subclasses are involved. Observe for instance how dict.fromkeys(), a classmethod, returns an instance of the subclass when called on a subclass:classmethoddict.fromkeys()>>> class DictSubclass(dict):\n...     def __repr__(self):\n...         return \"DictSubclass\"\n... \n>>> dict.fromkeys(\"abc\")\n{'a': None, 'c': None, 'b': None}\n>>> DictSubclass.fromkeys(\"abc\")\nDictSubclass\n>>> \n>>> class DictSubclass(dict):\n...     def __repr__(self):\n...         return \"DictSubclass\"\n... \n>>> dict.fromkeys(\"abc\")\n{'a': None, 'c': None, 'b': None}\n>>> DictSubclass.fromkeys(\"abc\")\nDictSubclass\n>>> \n",
                "Basically @classmethod makes a method whose first argument is the class it's called from (rather than the class instance), @staticmethod does not have any implicit arguments.@classmethod@staticmethod",
                "To decide whether to use @staticmethod or @classmethod you have to look inside your method. If your method accesses other variables/methods in your class then use @classmethod. On the other hand, if your method does not touches any other parts of the class then use @staticmethod.@staticmethod@classmethodIf your method accesses other variables/methods in your class then use @classmethodclass Apple:\n\n    _counter = 0\n\n    @staticmethod\n    def about_apple():\n        print('Apple is good for you.')\n\n        # note you can still access other member of the class\n        # but you have to use the class instance \n        # which is not very nice, because you have repeat yourself\n        # \n        # For example:\n        # @staticmethod\n        #    print('Number of apples have been juiced: %s' % Apple._counter)\n        #\n        # @classmethod\n        #    print('Number of apples have been juiced: %s' % cls._counter)\n        #\n        #    @classmethod is especially useful when you move your function to another class,\n        #       you don't have to rename the referenced class \n\n    @classmethod\n    def make_apple_juice(cls, number_of_apples):\n        print('Making juice:')\n        for i in range(number_of_apples):\n            cls._juice_this(i)\n\n    @classmethod\n    def _juice_this(cls, apple):\n        print('Juicing apple %d...' % apple)\n        cls._counter += 1\nclass Apple:\n\n    _counter = 0\n\n    @staticmethod\n    def about_apple():\n        print('Apple is good for you.')\n\n        # note you can still access other member of the class\n        # but you have to use the class instance \n        # which is not very nice, because you have repeat yourself\n        # \n        # For example:\n        # @staticmethod\n        #    print('Number of apples have been juiced: %s' % Apple._counter)\n        #\n        # @classmethod\n        #    print('Number of apples have been juiced: %s' % cls._counter)\n        #\n        #    @classmethod is especially useful when you move your function to another class,\n        #       you don't have to rename the referenced class \n\n    @classmethod\n    def make_apple_juice(cls, number_of_apples):\n        print('Making juice:')\n        for i in range(number_of_apples):\n            cls._juice_this(i)\n\n    @classmethod\n    def _juice_this(cls, apple):\n        print('Juicing apple %d...' % apple)\n        cls._counter += 1\n",
                "Official python docs:Official python docs:@classmethod@classmethod\nA class method receives the class as\n  implicit first argument, just like an\n  instance method receives the instance.\n  To declare a class method, use this\n  idiom:\nclass C:\n    @classmethod\n    def f(cls, arg1, arg2, ...): ... \n\nThe @classmethod form is a function\n  decorator \u2013 see the description of\n  function definitions in Function\n  definitions for details.\nIt can be called either on the class\n  (such as C.f()) or on an instance\n  (such as C().f()). The instance is\n  ignored except for its class. If a\n  class method is called for a derived\n  class, the derived class object is\n  passed as the implied first argument.\nClass methods are different than C++\n  or Java static methods. If you want\n  those, see staticmethod() in this\n  section.\nA class method receives the class as\n  implicit first argument, just like an\n  instance method receives the instance.\n  To declare a class method, use this\n  idiom:class C:\n    @classmethod\n    def f(cls, arg1, arg2, ...): ... \nclass C:\n    @classmethod\n    def f(cls, arg1, arg2, ...): ... \nThe @classmethod form is a function\n  decorator \u2013 see the description of\n  function definitions in Function\n  definitions for details.@classmethoddecoratordecoratorFunction\n  definitionsFunction\n  definitionsIt can be called either on the class\n  (such as C.f()) or on an instance\n  (such as C().f()). The instance is\n  ignored except for its class. If a\n  class method is called for a derived\n  class, the derived class object is\n  passed as the implied first argument.C.f()C().f()Class methods are different than C++\n  or Java static methods. If you want\n  those, see staticmethod() in this\n  section.staticmethod()staticmethod()@staticmethod@staticmethod\nA static method does not receive an\n  implicit first argument. To declare a\n  static method, use this idiom:\nclass C:\n    @staticmethod\n    def f(arg1, arg2, ...): ... \n\nThe @staticmethod form is a function\n  decorator \u2013 see the description of\n  function definitions in Function\n  definitions for details.\nIt can be called either on the class\n  (such as C.f()) or on an instance\n  (such as C().f()). The instance is\n  ignored except for its class.\nStatic methods in Python are similar\n  to those found in Java or C++. For a\n  more advanced concept, see\n  classmethod() in this section.\nA static method does not receive an\n  implicit first argument. To declare a\n  static method, use this idiom:class C:\n    @staticmethod\n    def f(arg1, arg2, ...): ... \nclass C:\n    @staticmethod\n    def f(arg1, arg2, ...): ... \nThe @staticmethod form is a function\n  decorator \u2013 see the description of\n  function definitions in Function\n  definitions for details.@staticmethoddecoratordecoratorFunction\n  definitionsFunction\n  definitionsIt can be called either on the class\n  (such as C.f()) or on an instance\n  (such as C().f()). The instance is\n  ignored except for its class.C.f()C().f()Static methods in Python are similar\n  to those found in Java or C++. For a\n  more advanced concept, see\n  classmethod() in this section.classmethod()classmethod()",
                "Here is a short article on this questionHere\n@staticmethod function is nothing more than a function defined inside a class. It is callable without instantiating the class first. It\u2019s definition is immutable via inheritance.\n@classmethod function also callable without instantiating the class, but its definition follows Sub class, not Parent class, via inheritance. That\u2019s because the first argument for @classmethod function must always be cls (class).\n@staticmethod function is nothing more than a function defined inside a class. It is callable without instantiating the class first. It\u2019s definition is immutable via inheritance.@classmethod function also callable without instantiating the class, but its definition follows Sub class, not Parent class, via inheritance. That\u2019s because the first argument for @classmethod function must always be cls (class).",
                "\nWhat is the difference between @staticmethod and @classmethod in Python?\nWhat is the difference between @staticmethod and @classmethod in Python?You may have seen Python code like this pseudocode, which demonstrates the signatures of the various method types and provides a docstring to explain each:class Foo(object):\n\n    def a_normal_instance_method(self, arg_1, kwarg_2=None):\n        '''\n        Return a value that is a function of the instance with its\n        attributes, and other arguments such as arg_1 and kwarg2\n        '''\n\n    @staticmethod\n    def a_static_method(arg_0):\n        '''\n        Return a value that is a function of arg_0. It does not know the \n        instance or class it is called from.\n        '''\n\n    @classmethod\n    def a_class_method(cls, arg1):\n        '''\n        Return a value that is a function of the class and other arguments.\n        respects subclassing, it is called with the class it is called from.\n        '''\nclass Foo(object):\n\n    def a_normal_instance_method(self, arg_1, kwarg_2=None):\n        '''\n        Return a value that is a function of the instance with its\n        attributes, and other arguments such as arg_1 and kwarg2\n        '''\n\n    @staticmethod\n    def a_static_method(arg_0):\n        '''\n        Return a value that is a function of arg_0. It does not know the \n        instance or class it is called from.\n        '''\n\n    @classmethod\n    def a_class_method(cls, arg1):\n        '''\n        Return a value that is a function of the class and other arguments.\n        respects subclassing, it is called with the class it is called from.\n        '''\nThe Normal Instance MethodFirst I'll explain a_normal_instance_method. This is precisely called an \"instance method\". When an instance method is used, it is used as a partial function (as opposed to a total function, defined for all values when viewed in source code) that is, when used, the first of the arguments is predefined as the instance of the object, with all of its given attributes. It has the instance of the object bound to it, and it must be called from an instance of the object. Typically, it will access various attributes of the instance.a_normal_instance_methodinstance methodFor example, this is an instance of a string:', '\n', '\nif we use the instance method, join on this string, to join another iterable,\nit quite obviously is a function of the instance, in addition to being a function of the iterable list, ['a', 'b', 'c']:join['a', 'b', 'c']>>> ', '.join(['a', 'b', 'c'])\n'a, b, c'\n>>> ', '.join(['a', 'b', 'c'])\n'a, b, c'\nBound methodsInstance methods can be bound via a dotted lookup for use later.For example, this binds the str.join method to the ':' instance:str.join':'>>> join_with_colons = ':'.join \n>>> join_with_colons = ':'.join \nAnd later we can use this as a function that already has the first argument bound to it. In this way, it works like a partial function on the instance:>>> join_with_colons('abcde')\n'a:b:c:d:e'\n>>> join_with_colons(['FF', 'FF', 'FF', 'FF', 'FF', 'FF'])\n'FF:FF:FF:FF:FF:FF'\n>>> join_with_colons('abcde')\n'a:b:c:d:e'\n>>> join_with_colons(['FF', 'FF', 'FF', 'FF', 'FF', 'FF'])\n'FF:FF:FF:FF:FF:FF'\nStatic MethodThe static method does not take the instance as an argument.notIt is very similar to a module level function.However, a module level function must live in the module and be specially imported to other places where it is used.If it is attached to the object, however, it will follow the object conveniently through importing and inheritance as well.An example of a static method is str.maketrans, moved from the string module in Python 3.  It makes a translation table suitable for consumption by str.translate. It does seem rather silly when used from an instance of a string, as demonstrated below, but importing the function from the string module is rather clumsy, and it's nice to be able to call it from the class, as in str.maketransstr.maketransstringstr.translatestringstr.maketrans# demonstrate same function whether called from instance or not:\n>>> ', '.maketrans('ABC', 'abc')\n{65: 97, 66: 98, 67: 99}\n>>> str.maketrans('ABC', 'abc')\n{65: 97, 66: 98, 67: 99}\n# demonstrate same function whether called from instance or not:\n>>> ', '.maketrans('ABC', 'abc')\n{65: 97, 66: 98, 67: 99}\n>>> str.maketrans('ABC', 'abc')\n{65: 97, 66: 98, 67: 99}\nIn python 2, you have to import this function from the increasingly less useful string module:>>> import string\n>>> 'ABCDEFG'.translate(string.maketrans('ABC', 'abc'))\n'abcDEFG'\n>>> import string\n>>> 'ABCDEFG'.translate(string.maketrans('ABC', 'abc'))\n'abcDEFG'\nClass MethodA class method is a similar to an instance method in that it takes an implicit first argument, but instead of taking the instance, it takes the class. Frequently these are used as alternative constructors for better semantic usage and it will support inheritance.The most canonical example of a builtin classmethod is dict.fromkeys. It is used as an alternative constructor of dict, (well suited for when you know what your keys are and want a default value for them.)dict.fromkeys>>> dict.fromkeys(['a', 'b', 'c'])\n{'c': None, 'b': None, 'a': None}\n>>> dict.fromkeys(['a', 'b', 'c'])\n{'c': None, 'b': None, 'a': None}\nWhen we subclass dict, we can use the same constructor, which creates an instance of the subclass.>>> class MyDict(dict): 'A dict subclass, use to demo classmethods'\n>>> md = MyDict.fromkeys(['a', 'b', 'c'])\n>>> md\n{'a': None, 'c': None, 'b': None}\n>>> type(md)\n<class '__main__.MyDict'>\n>>> class MyDict(dict): 'A dict subclass, use to demo classmethods'\n>>> md = MyDict.fromkeys(['a', 'b', 'c'])\n>>> md\n{'a': None, 'c': None, 'b': None}\n>>> type(md)\n<class '__main__.MyDict'>\nSee the pandas source code for other similar examples of alternative constructors, and see also the official Python documentation on classmethod and staticmethod.pandas source codeclassmethodclassmethodstaticmethodstaticmethod",
                "I started learning programming language with C++ and then Java and then Python and so this question bothered me a lot as well, until I understood the simple usage of each. Class Method: Python unlike Java and C++ doesn't have constructor overloading.  And so to achieve this you could use classmethod. Following example will explain this Class Method:classmethodLet's consider we have a Person class which takes two arguments first_name and last_name and creates the instance of Person. Personfirst_namelast_namePersonclass Person(object):\n\n    def __init__(self, first_name, last_name):\n        self.first_name = first_name\n        self.last_name = last_name\nclass Person(object):\n\n    def __init__(self, first_name, last_name):\n        self.first_name = first_name\n        self.last_name = last_name\nNow, if the requirement comes where you need to create a class using a single name only, just a first_name, you can't do something like this in Python. first_namecan'tThis will give you an error when you will try to create an object (instance).class Person(object):\n\n    def __init__(self, first_name, last_name):\n        self.first_name = first_name\n        self.last_name = last_name\n\n    def __init__(self, first_name):\n        self.first_name = first_name\nclass Person(object):\n\n    def __init__(self, first_name, last_name):\n        self.first_name = first_name\n        self.last_name = last_name\n\n    def __init__(self, first_name):\n        self.first_name = first_name\nHowever, you could achieve the same thing using @classmethod as mentioned below @classmethodclass Person(object):\n\n    def __init__(self, first_name, last_name):\n        self.first_name = first_name\n        self.last_name = last_name\n\n    @classmethod\n    def get_person(cls, first_name):\n        return cls(first_name, \"\")\nclass Person(object):\n\n    def __init__(self, first_name, last_name):\n        self.first_name = first_name\n        self.last_name = last_name\n\n    @classmethod\n    def get_person(cls, first_name):\n        return cls(first_name, \"\")\nStatic Method: This is rather simple, it's not bound to instance or class and you can simply call that using class name. Static Method:So let's say in above example you need a validation that first_name should not exceed 20 characters, you can simply do this. first_name@staticmethod  \ndef validate_name(name):\n    return len(name) <= 20\n@staticmethod  \ndef validate_name(name):\n    return len(name) <= 20\nand you could simply call using class nameclass namePerson.validate_name(\"Gaurang Shah\")\nPerson.validate_name(\"Gaurang Shah\")\n",
                "Only the first argument differs:Only the first argument differs\nnormal method: the current object is automatically passed as an (additional) first argument\nclassmethod: the class of the current object is automatically passed as an (additional) fist argument\nstaticmethod: no extra arguments are automatically passed. What you passed to the function is what you get.\nnormal method: the current object is automatically passed as an (additional) first argumentthe current objectclassmethod: the class of the current object is automatically passed as an (additional) fist argumentthe class of the current objectstaticmethod: no extra arguments are automatically passed. What you passed to the function is what you get.no extra argumentsIn more detail...normal methodThe \"standard\" method, as in every object oriented language. When an object's method is called, it is automatically given an extra argument self as its first argument. That is, methodselfdef f(self, x, y)\ndef f(self, x, y)\nmust be called with 2 arguments. self is automatically passed, and it is the object itself. Similar to the this that magically appears in eg. java/c++, only in python it is shown explicitly.selfthe object itselfthis\nactually, the first argument does not have to be called self, but it's the standard convention, so keep it\nactually, the first argument does not have to be called self, but it's the standard convention, so keep ithave toselfclass methodWhen the method is decorated@classmethod\ndef f(cls, x, y)\n@classmethod\ndef f(cls, x, y)\nthe automatically provided argument is not self, but the class of self.is notselfthe class ofselfstatic methodWhen the method is decorated@staticmethod\ndef f(x, y)\n@staticmethod\ndef f(x, y)\nthe method is not given any automatic argument at all. It is only given the parameters that it is called with.is not givenusages\nclassmethod is mostly used for alternative constructors.\nstaticmethod does not use the state of the object, or even the structure of the class itself. It could be a function external to a class. It only put inside the class for grouping functions with similar functionality (for example, like Java's Math class static methods)\nclassmethod is mostly used for alternative constructors.classmethodstaticmethod does not use the state of the object, or even the structure of the class itself. It could be a function external to a class. It only put inside the class for grouping functions with similar functionality (for example, like Java's Math class static methods)staticmethodMathclass Point\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @classmethod\n    def frompolar(cls, radius, angle):\n        \"\"\"The `cls` argument is the `Point` class itself\"\"\"\n        return cls(radius * cos(angle), radius * sin(angle))\n\n    @staticmethod\n    def angle(x, y):\n        \"\"\"this could be outside the class, but we put it here \njust because we think it is logically related to the class.\"\"\"\n        return atan(y, x)\n\n\np1 = Point(3, 2)\np2 = Point.frompolar(3, pi/4)\n\nangle = Point.angle(3, 2)\n\nclass Point\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n\n    @classmethod\n    def frompolar(cls, radius, angle):\n        \"\"\"The `cls` argument is the `Point` class itself\"\"\"\n        return cls(radius * cos(angle), radius * sin(angle))\n\n    @staticmethod\n    def angle(x, y):\n        \"\"\"this could be outside the class, but we put it here \njust because we think it is logically related to the class.\"\"\"\n        return atan(y, x)\n\n\np1 = Point(3, 2)\np2 = Point.frompolar(3, pi/4)\n\nangle = Point.angle(3, 2)\n\n",
                "I think a better question is \"When would you use @classmethod vs @staticmethod?\"@classmethod@staticmethod@classmethod allows you easy access to private members that are associated to the class definition. this is a great way to do singletons, or factory classes that control the number of instances of the created objects exist.@classmethod@staticmethod provides marginal performance gains, but I have yet to see a productive use of a static method within a class that couldn't be achieved as a standalone function outside the class.@staticmethod",
                "Static Methods:Static Methods:\nSimple functions with no self argument. \nWork on class attributes; not on instance attributes.\nCan be called through both class and instance.\nThe built-in function staticmethod()is used to create them.\nSimple functions with no self argument. Work on class attributes; not on instance attributes.Can be called through both class and instance.The built-in function staticmethod()is used to create them.Benefits of Static Methods:Benefits of Static Methods:\nIt localizes the function name in the classscope\nIt moves the function code closer to where it is used\nMore convenient to import versus module-level functions since each method does not have to be specially imported\n@staticmethod\ndef some_static_method(*args, **kwds):\n    pass\n\nIt localizes the function name in the classscopeIt moves the function code closer to where it is usedMore convenient to import versus module-level functions since each method does not have to be specially imported\n@staticmethod\ndef some_static_method(*args, **kwds):\n    pass\nMore convenient to import versus module-level functions since each method does not have to be specially imported@staticmethod\ndef some_static_method(*args, **kwds):\n    pass\n@staticmethod\ndef some_static_method(*args, **kwds):\n    pass\nClass Methods:Class Methods:\nFunctions that have first argument as classname.\nCan be called through both class and instance.\nThese are created with classmethod in-built function.\n @classmethod\n def some_class_method(cls, *args, **kwds):\n     pass\n\nFunctions that have first argument as classname.Can be called through both class and instance.These are created with classmethod in-built function.\n @classmethod\n def some_class_method(cls, *args, **kwds):\n     pass\nThese are created with classmethod in-built function. @classmethod\n def some_class_method(cls, *args, **kwds):\n     pass\n @classmethod\n def some_class_method(cls, *args, **kwds):\n     pass\n",
                "@decorators were added in python 2.4 If you're using python < 2.4 you can use the classmethod() and staticmethod() function.For example, if you want to create a factory method (A function returning an instance of a different implementation of a class depending on what argument it gets) you can do something like:class Cluster(object):\n\n    def _is_cluster_for(cls, name):\n        \"\"\"\n        see if this class is the cluster with this name\n        this is a classmethod\n        \"\"\" \n        return cls.__name__ == name\n    _is_cluster_for = classmethod(_is_cluster_for)\n\n    #static method\n    def getCluster(name):\n        \"\"\"\n        static factory method, should be in Cluster class\n        returns a cluster object for the given name\n        \"\"\"\n        for cls in Cluster.__subclasses__():\n            if cls._is_cluster_for(name):\n                return cls()\n    getCluster = staticmethod(getCluster)\nclass Cluster(object):\n\n    def _is_cluster_for(cls, name):\n        \"\"\"\n        see if this class is the cluster with this name\n        this is a classmethod\n        \"\"\" \n        return cls.__name__ == name\n    _is_cluster_for = classmethod(_is_cluster_for)\n\n    #static method\n    def getCluster(name):\n        \"\"\"\n        static factory method, should be in Cluster class\n        returns a cluster object for the given name\n        \"\"\"\n        for cls in Cluster.__subclasses__():\n            if cls._is_cluster_for(name):\n                return cls()\n    getCluster = staticmethod(getCluster)\nAlso observe that this is a good example for using a classmethod and a static method,\nThe static method clearly belongs to the class, since it uses the class Cluster internally.\nThe classmethod only needs information about the class, and no instance of the object.Another benefit of making the _is_cluster_for method a classmethod is so a subclass can decide to change it's implementation, maybe because it is pretty generic and can handle more than one type of cluster, so just checking the name of the class would not be enough._is_cluster_for",
                "Let me tell the similarity between a method decorated with @classmethod vs @staticmethod first.Similarity: Both of them can be called on the Class itself, rather than just the instance of the class. So, both of them in a sense are Class's methods. Similarity:ClassinstanceClass's methodsDifference: A classmethod will receive the class itself as the first argument, while a staticmethod does not.Difference:So a static method is, in a sense, not bound to the Class itself and is just hanging in there just because it may have a related functionality. >>> class Klaus:\n        @classmethod\n        def classmthd(*args):\n            return args\n\n        @staticmethod\n        def staticmthd(*args):\n            return args\n\n# 1. Call classmethod without any arg\n>>> Klaus.classmthd()  \n(__main__.Klaus,)  # the class gets passed as the first argument\n\n# 2. Call classmethod with 1 arg\n>>> Klaus.classmthd('chumma')\n(__main__.Klaus, 'chumma')\n\n# 3. Call staticmethod without any arg\n>>> Klaus.staticmthd()  \n()\n\n# 4. Call staticmethod with 1 arg\n>>> Klaus.staticmthd('chumma')\n('chumma',)\n>>> class Klaus:\n        @classmethod\n        def classmthd(*args):\n            return args\n\n        @staticmethod\n        def staticmthd(*args):\n            return args\n\n# 1. Call classmethod without any arg\n>>> Klaus.classmthd()  \n(__main__.Klaus,)  # the class gets passed as the first argument\n\n# 2. Call classmethod with 1 arg\n>>> Klaus.classmthd('chumma')\n(__main__.Klaus, 'chumma')\n\n# 3. Call staticmethod without any arg\n>>> Klaus.staticmthd()  \n()\n\n# 4. Call staticmethod with 1 arg\n>>> Klaus.staticmthd('chumma')\n('chumma',)\n",
                "@staticmethod just disables the default function as method descriptor.  classmethod wraps your function in a container callable that passes a reference to the owning class as first argument:@staticmethod>>> class C(object):\n...  pass\n... \n>>> def f():\n...  pass\n... \n>>> staticmethod(f).__get__(None, C)\n<function f at 0x5c1cf0>\n>>> classmethod(f).__get__(None, C)\n<bound method type.f of <class '__main__.C'>>\n>>> class C(object):\n...  pass\n... \n>>> def f():\n...  pass\n... \n>>> staticmethod(f).__get__(None, C)\n<function f at 0x5c1cf0>\n>>> classmethod(f).__get__(None, C)\n<bound method type.f of <class '__main__.C'>>\nAs a matter of fact, classmethod has a runtime overhead but makes it possible to access the owning class.  Alternatively I recommend using a metaclass and putting the class methods on that metaclass:classmethod>>> class CMeta(type):\n...  def foo(cls):\n...   print cls\n... \n>>> class C(object):\n...  __metaclass__ = CMeta\n... \n>>> C.foo()\n<class '__main__.C'>\n>>> class CMeta(type):\n...  def foo(cls):\n...   print cls\n... \n>>> class C(object):\n...  __metaclass__ = CMeta\n... \n>>> C.foo()\n<class '__main__.C'>\n",
                "The definitive guide on how to use static, class or abstract methods in Python is one good link for this topic, and summary it as following.The definitive guide on how to use static, class or abstract methods in Python@staticmethod function is nothing more than a function defined inside a class. It is callable without instantiating the class first. It\u2019s definition is immutable via inheritance.@staticmethod@staticmethod\nPython does not have to instantiate a bound-method for object.\nIt eases the readability of the code, and it does not depend on the state of object itself;\nPython does not have to instantiate a bound-method for object.It eases the readability of the code, and it does not depend on the state of object itself;@classmethod function also callable without instantiating the class, but its definition follows Sub class, not Parent class, via inheritance, can be overridden by subclass. That\u2019s because the first argument for @classmethod function must always be cls (class).@classmethod@classmethod@classmethodcls\nFactory methods, that are used to create an instance for a class using for example some sort of pre-processing.\nStatic methods calling static methods: if you split a static methods in several static methods, you shouldn't hard-code the class name but use class methods\nFactory methods, that are used to create an instance for a class using for example some sort of pre-processing.Factory methodsStatic methods calling static methods: if you split a static methods in several static methods, you shouldn't hard-code the class name but use class methodsStatic methods calling static methods",
                "Another consideration with respect to staticmethod vs classmethod comes up with inheritance.  Say you have the following class:class Foo(object):\n    @staticmethod\n    def bar():\n        return \"In Foo\"\nclass Foo(object):\n    @staticmethod\n    def bar():\n        return \"In Foo\"\nAnd you then want to override bar() in a child class:bar()class Foo2(Foo):\n    @staticmethod\n    def bar():\n        return \"In Foo2\"\nclass Foo2(Foo):\n    @staticmethod\n    def bar():\n        return \"In Foo2\"\nThis works, but note that now the bar() implementation in the child class (Foo2) can no longer take advantage of anything specific to that class.  For example, say Foo2 had a method called magic() that you want to use in the Foo2 implementation of bar():bar()Foo2Foo2magic()Foo2bar()class Foo2(Foo):\n    @staticmethod\n    def bar():\n        return \"In Foo2\"\n    @staticmethod\n    def magic():\n        return \"Something useful you'd like to use in bar, but now can't\" \nclass Foo2(Foo):\n    @staticmethod\n    def bar():\n        return \"In Foo2\"\n    @staticmethod\n    def magic():\n        return \"Something useful you'd like to use in bar, but now can't\" \nThe workaround here would be to call Foo2.magic() in bar(), but then you're repeating yourself (if the name of Foo2 changes, you'll have to remember to update that bar() method).Foo2.magic()bar()Foo2bar()To me, this is a slight violation of the open/closed principle, since a decision made in Foo is impacting your ability to refactor common code in a derived class (ie it's less open to extension).  If bar() were a classmethod we'd be fine:open/closed principleFoobar()classmethodclass Foo(object):\n    @classmethod\n    def bar(cls):\n        return \"In Foo\"\n\nclass Foo2(Foo):\n    @classmethod\n    def bar(cls):\n        return \"In Foo2 \" + cls.magic()\n    @classmethod\n    def magic(cls):\n        return \"MAGIC\"\n\nprint Foo2().bar()\nclass Foo(object):\n    @classmethod\n    def bar(cls):\n        return \"In Foo\"\n\nclass Foo2(Foo):\n    @classmethod\n    def bar(cls):\n        return \"In Foo2 \" + cls.magic()\n    @classmethod\n    def magic(cls):\n        return \"MAGIC\"\n\nprint Foo2().bar()\nGives: In Foo2 MAGICIn Foo2 MAGICAlso: historical note: Guido Van Rossum (Python's creator) once referred to staticmethod's as \"an accident\": https://mail.python.org/pipermail/python-ideas/2012-May/014969.htmlstaticmethodhttps://mail.python.org/pipermail/python-ideas/2012-May/014969.html\nwe all know how limited static methods are. (They're basically an accident -- back in the Python 2.2 days when I was inventing new-style classes and descriptors, I meant to implement class methods but at first I didn't understand them and accidentally implemented static methods first. Then it was too late to remove them and only provide class methods.\nwe all know how limited static methods are. (They're basically an accident -- back in the Python 2.2 days when I was inventing new-style classes and descriptors, I meant to implement class methods but at first I didn't understand them and accidentally implemented static methods first. Then it was too late to remove them and only provide class methods.Also: https://mail.python.org/pipermail/python-ideas/2016-July/041189.htmlhttps://mail.python.org/pipermail/python-ideas/2016-July/041189.html\nHonestly, staticmethod was something of a mistake -- I was trying to do something like Java class methods but once it was released I found what was really needed was classmethod. But it was too late to get rid of staticmethod.\nHonestly, staticmethod was something of a mistake -- I was trying to do something like Java class methods but once it was released I found what was really needed was classmethod. But it was too late to get rid of staticmethod.",
                "I will try to explain the basic difference using an example.class A(object):\n    x = 0\n\n    def say_hi(self):\n        pass\n\n    @staticmethod\n    def say_hi_static():\n        pass\n\n    @classmethod\n    def say_hi_class(cls):\n        pass\n\n    def run_self(self):\n        self.x += 1\n        print self.x # outputs 1\n        self.say_hi()\n        self.say_hi_static()\n        self.say_hi_class()\n\n    @staticmethod\n    def run_static():\n        print A.x  # outputs 0\n        # A.say_hi() #  wrong\n        A.say_hi_static()\n        A.say_hi_class()\n\n    @classmethod\n    def run_class(cls):\n        print cls.x # outputs 0\n        # cls.say_hi() #  wrong\n        cls.say_hi_static()\n        cls.say_hi_class()\nclass A(object):\n    x = 0\n\n    def say_hi(self):\n        pass\n\n    @staticmethod\n    def say_hi_static():\n        pass\n\n    @classmethod\n    def say_hi_class(cls):\n        pass\n\n    def run_self(self):\n        self.x += 1\n        print self.x # outputs 1\n        self.say_hi()\n        self.say_hi_static()\n        self.say_hi_class()\n\n    @staticmethod\n    def run_static():\n        print A.x  # outputs 0\n        # A.say_hi() #  wrong\n        A.say_hi_static()\n        A.say_hi_class()\n\n    @classmethod\n    def run_class(cls):\n        print cls.x # outputs 0\n        # cls.say_hi() #  wrong\n        cls.say_hi_static()\n        cls.say_hi_class()\n1 - we can directly call static and classmethods without initializing# A.run_self() #  wrong\nA.run_static()\nA.run_class()\n# A.run_self() #  wrong\nA.run_static()\nA.run_class()\n2- Static method cannot call self method but can call other static and classmethod3- Static method belong to class and will not use object at all.4- Class method are not bound to an object but to a class.",
                "Python comes with several built-in decorators. The big three are:@classmethod\n@staticmethod\n@property\n@classmethod\n@staticmethod\n@property\nFirst let's note that any function of a class can be called with instance of this class (after we initialized this class).@classmethod is the way to call function not only as an instance of a class but also directly by the class itself as its first argument.@classmethodcall functiondirectly by the class itself@staticmethod is a way of putting a function into a class (because it logically belongs there), while indicating that it does not require access to the class (so we don't need to use self in function definition).@staticmethoddon't need to use selfselfLet's consider the following class:class DecoratorTest(object):\n\n    def __init__(self):\n        pass\n\n    def doubler(self, x):\n        return x*2\n\n    @classmethod\n    def class_doubler(cls, x): # we need to use 'cls' instead of 'self'; 'cls' reference to the class instead of an instance of the class\n        return x*2\n\n    @staticmethod\n    def static_doubler(x): # no need adding 'self' here; static_doubler() could be just a function not inside the class\n        return x*2\nclass DecoratorTest(object):\n\n    def __init__(self):\n        pass\n\n    def doubler(self, x):\n        return x*2\n\n    @classmethod\n    def class_doubler(cls, x): # we need to use 'cls' instead of 'self'; 'cls' reference to the class instead of an instance of the class\n        return x*2\n\n    @staticmethod\n    def static_doubler(x): # no need adding 'self' here; static_doubler() could be just a function not inside the class\n        return x*2\nLet's see how it works:decor = DecoratorTest()\n\nprint(decor.doubler(5))\n# 10\n\nprint(decor.class_doubler(5)) # a call with an instance of a class\n# 10\nprint(DecoratorTest.class_doubler(5)) # a direct call by the class itself\n# 10\n\n# staticmethod could be called in the same way as classmethod.\nprint(decor.static_doubler(5)) # as an instance of the class\n# 10\nprint(DecoratorTest.static_doubler(5)) # or as a direct call \n# 10\ndecor = DecoratorTest()\n\nprint(decor.doubler(5))\n# 10\n\nprint(decor.class_doubler(5)) # a call with an instance of a class\n# 10\nprint(DecoratorTest.class_doubler(5)) # a direct call by the class itself\n# 10\n\n# staticmethod could be called in the same way as classmethod.\nprint(decor.static_doubler(5)) # as an instance of the class\n# 10\nprint(DecoratorTest.static_doubler(5)) # or as a direct call \n# 10\nHere you can see some use cases for those methods.HereBonus: you can read about @property decorator here@propertyhere",
                "The difference occurs when there is inheritance.The difference occurs when there is inheritance.Suppose that there are two classes-- Parent and Child. If one wants to use @staticmethod, print_name method should be written twice because the name of the class should be written in the print line.class Parent:\n   _class_name = \"Parent\"\n\n   @staticmethod\n   def print_name():\n       print(Parent._class_name)\n\n\nclass Child(Parent):\n   _class_name = \"Child\"\n\n   @staticmethod\n   def print_name():\n       print(Child._class_name)\n\n\nParent.print_name()\nChild.print_name()\nclass Parent:\n   _class_name = \"Parent\"\n\n   @staticmethod\n   def print_name():\n       print(Parent._class_name)\n\n\nclass Child(Parent):\n   _class_name = \"Child\"\n\n   @staticmethod\n   def print_name():\n       print(Child._class_name)\n\n\nParent.print_name()\nChild.print_name()\nHowever, for @classmethod, it is not required to write print_name method twice.class Parent:\n    _class_name = \"Parent\"\n\n    @classmethod\n    def print_name(cls):\n        print(cls._class_name)\n\n\nclass Child(Parent):\n    _class_name = \"Child\"\n\n\nParent.print_name()\nChild.print_name()\nclass Parent:\n    _class_name = \"Parent\"\n\n    @classmethod\n    def print_name(cls):\n        print(cls._class_name)\n\n\nclass Child(Parent):\n    _class_name = \"Child\"\n\n\nParent.print_name()\nChild.print_name()\n",
                "Instance Method:Instance MethodInstance Method+ Can modify object instance state+Can+ Can modify class state+CanClass Method:Class MethodClass Method- Can't modify object instance state-Can't+ Can modify class state+CanStatic Method:Static MethodStatic Method- Can't modify object instance state-Can't- Can't modify class state-Can'tclass MyClass:\n    ''' \n    Instance method has a mandatory first attribute self which represent the instance itself. \n    Instance method must be called by a instantiated instance.\n    '''\n    def method(self):\n        return 'instance method called', self\n    \n    '''\n    Class method has a mandatory first attribute cls which represent the class itself. \n    Class method can be called by an instance or by the class directly. \n    Its most common using scenario is to define a factory method.\n    '''\n    @classmethod\n    def class_method(cls):\n        return 'class method called', cls\n    \n    '''\n    Static method doesn\u2019t have any attributes of instances or the class. \n    It also can be called by an instance or by the class directly. \n    Its most common using scenario is to define some helper or utility functions which are closely relative to the class.\n    '''\n    @staticmethod\n    def static_method():\n        return 'static method called'\n\n\nobj = MyClass()\nprint(obj.method())\nprint(obj.class_method()) # MyClass.class_method()\nprint(obj.static_method()) # MyClass.static_method()\nclass MyClass:\n    ''' \n    Instance method has a mandatory first attribute self which represent the instance itself. \n    Instance method must be called by a instantiated instance.\n    '''\n    def method(self):\n        return 'instance method called', self\n    \n    '''\n    Class method has a mandatory first attribute cls which represent the class itself. \n    Class method can be called by an instance or by the class directly. \n    Its most common using scenario is to define a factory method.\n    '''\n    @classmethod\n    def class_method(cls):\n        return 'class method called', cls\n    \n    '''\n    Static method doesn\u2019t have any attributes of instances or the class. \n    It also can be called by an instance or by the class directly. \n    Its most common using scenario is to define some helper or utility functions which are closely relative to the class.\n    '''\n    @staticmethod\n    def static_method():\n        return 'static method called'\n\n\nobj = MyClass()\nprint(obj.method())\nprint(obj.class_method()) # MyClass.class_method()\nprint(obj.static_method()) # MyClass.static_method()\noutput:('instance method called', <__main__.MyClass object at 0x100fb3940>)\n('class method called', <class '__main__.MyClass'>)\nstatic method called\n('instance method called', <__main__.MyClass object at 0x100fb3940>)\n('class method called', <class '__main__.MyClass'>)\nstatic method called\nThe instance method we actually had access to the object instance , right so this was an instance off a my class object whereas with the class method we have access to the class itself. But not to any of the objects,  because the class method doesn't really care about an object existing. However you can both call a class method and static method on an object instance. This is going to work it doesn't really make a difference, so again when you call static method here it's going to work and it's going to know which method you want to call.The Static methods are used to do some utility tasks, and class methods are used for factory methods. The factory methods can return class objects for different use cases.And finally, a short example for better understanding:class Student:\n    def __init__(self, first_name, last_name):\n        self.first_name = first_name\n        self.last_name = last_name\n\n    @classmethod\n    def get_from_string(cls, name_string: str):\n        first_name, last_name = name_string.split()\n        if Student.validate_name(first_name) and Student.validate_name(last_name):\n            return cls(first_name, last_name)\n        else:\n            print('Invalid Names')\n\n    @staticmethod\n    def validate_name(name):\n        return len(name) <= 10\n\n\nstackoverflow_student = Student.get_from_string('Name Surname')\nprint(stackoverflow_student.first_name) # Name\nprint(stackoverflow_student.last_name) # Surname\nclass Student:\n    def __init__(self, first_name, last_name):\n        self.first_name = first_name\n        self.last_name = last_name\n\n    @classmethod\n    def get_from_string(cls, name_string: str):\n        first_name, last_name = name_string.split()\n        if Student.validate_name(first_name) and Student.validate_name(last_name):\n            return cls(first_name, last_name)\n        else:\n            print('Invalid Names')\n\n    @staticmethod\n    def validate_name(name):\n        return len(name) <= 10\n\n\nstackoverflow_student = Student.get_from_string('Name Surname')\nprint(stackoverflow_student.first_name) # Name\nprint(stackoverflow_student.last_name) # Surname\n",
                "@classmethod : can be used to create a shared global access to all the instances created of that class..... like updating a record by multiple users....\nI particulary found it use ful when creating singletons as well..:)@static method:  has nothing to do with the class or instance being associated with ...but for readability can use static method",
                "My contribution demonstrates the difference amongst @classmethod, @staticmethod, and instance methods, including how an instance can indirectly call a @staticmethod. But instead of indirectly calling a @staticmethod from an instance, making it private may be more \"pythonic.\" Getting something from a private method isn't demonstrated here but it's basically the same concept.@classmethod@staticmethod@staticmethod@staticmethod#!python3\n\nfrom os import system\nsystem('cls')\n# %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %\n\nclass DemoClass(object):\n    # instance methods need a class instance and\n    # can access the instance through 'self'\n    def instance_method_1(self):\n        return 'called from inside the instance_method_1()'\n\n    def instance_method_2(self):\n        # an instance outside the class indirectly calls the static_method\n        return self.static_method() + ' via instance_method_2()'\n\n    # class methods don't need a class instance, they can't access the\n    # instance (self) but they have access to the class itself via 'cls'\n    @classmethod\n    def class_method(cls):\n        return 'called from inside the class_method()'\n\n    # static methods don't have access to 'cls' or 'self', they work like\n    # regular functions but belong to the class' namespace\n    @staticmethod\n    def static_method():\n        return 'called from inside the static_method()'\n# %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %\n\n# works even if the class hasn't been instantiated\nprint(DemoClass.class_method() + '\\n')\n''' called from inside the class_method() '''\n\n# works even if the class hasn't been instantiated\nprint(DemoClass.static_method() + '\\n')\n''' called from inside the static_method() '''\n# %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %\n\n# >>>>> all methods types can be called on a class instance <<<<<\n# instantiate the class\ndemoclassObj = DemoClass()\n\n# call instance_method_1()\nprint(democlassObj.instance_method_1() + '\\n')\n''' called from inside the instance_method_1() '''\n\n# # indirectly call static_method through instance_method_2(), there's really no use\n# for this since a @staticmethod can be called whether the class has been\n# instantiated or not\nprint(democlassObj.instance_method_2() + '\\n')\n''' called from inside the static_method() via instance_method_2() '''\n\n# call class_method()\nprint(democlassObj.class_method() + '\\n')\n'''  called from inside the class_method() '''\n\n# call static_method()\nprint(democlassObj.static_method())\n''' called from inside the static_method() '''\n\n\"\"\"\n# whether the class is instantiated or not, this doesn't work\nprint(DemoClass.instance_method_1() + '\\n')\n'''\nTypeError: TypeError: unbound method instancemethod() must be called with\nDemoClass instance as first argument (got nothing instead)\n'''\n\"\"\"\n#!python3\n\nfrom os import system\nsystem('cls')\n# %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %\n\nclass DemoClass(object):\n    # instance methods need a class instance and\n    # can access the instance through 'self'\n    def instance_method_1(self):\n        return 'called from inside the instance_method_1()'\n\n    def instance_method_2(self):\n        # an instance outside the class indirectly calls the static_method\n        return self.static_method() + ' via instance_method_2()'\n\n    # class methods don't need a class instance, they can't access the\n    # instance (self) but they have access to the class itself via 'cls'\n    @classmethod\n    def class_method(cls):\n        return 'called from inside the class_method()'\n\n    # static methods don't have access to 'cls' or 'self', they work like\n    # regular functions but belong to the class' namespace\n    @staticmethod\n    def static_method():\n        return 'called from inside the static_method()'\n# %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %\n\n# works even if the class hasn't been instantiated\nprint(DemoClass.class_method() + '\\n')\n''' called from inside the class_method() '''\n\n# works even if the class hasn't been instantiated\nprint(DemoClass.static_method() + '\\n')\n''' called from inside the static_method() '''\n# %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %   %\n\n# >>>>> all methods types can be called on a class instance <<<<<\n# instantiate the class\ndemoclassObj = DemoClass()\n\n# call instance_method_1()\nprint(democlassObj.instance_method_1() + '\\n')\n''' called from inside the instance_method_1() '''\n\n# # indirectly call static_method through instance_method_2(), there's really no use\n# for this since a @staticmethod can be called whether the class has been\n# instantiated or not\nprint(democlassObj.instance_method_2() + '\\n')\n''' called from inside the static_method() via instance_method_2() '''\n\n# call class_method()\nprint(democlassObj.class_method() + '\\n')\n'''  called from inside the class_method() '''\n\n# call static_method()\nprint(democlassObj.static_method())\n''' called from inside the static_method() '''\n\n\"\"\"\n# whether the class is instantiated or not, this doesn't work\nprint(DemoClass.instance_method_1() + '\\n')\n'''\nTypeError: TypeError: unbound method instancemethod() must be called with\nDemoClass instance as first argument (got nothing instead)\n'''\n\"\"\"\n",
                "A class method receives the class as implicit first argument, just like an instance method receives the instance. It is a method which is bound to the class and not the object of the class.It has access to the state of the class as it takes a class parameter that points to the class and not the object instance. It can modify a class state that would apply across all the instances of the class. For example it can modify a class variable that will be applicable to all the instances. On the other hand, a static method does not receive an implicit first argument, compared to class methods or instance methods. And can\u2019t access or modify class state. It only belongs to the class because from design point of view that is the correct way. But in terms of functionality is not bound, at runtime, to the class.as a guideline, use static methods as utilities, use class methods for example as factory . Or maybe to define a singleton. And use instance methods to model the state and behavior of instances.Hope I was clear ! ",
                "You might want to consider the difference between:class A:\n    def foo():  # no self parameter, no decorator\n        pass\nclass A:\n    def foo():  # no self parameter, no decorator\n        pass\nandclass B:\n    @staticmethod\n    def foo():  # no self parameter\n        pass\nclass B:\n    @staticmethod\n    def foo():  # no self parameter\n        pass\nThis has changed between python2 and python3:python2:>>> A.foo()\nTypeError\n>>> A().foo()\nTypeError\n>>> B.foo()\n>>> B().foo()\n>>> A.foo()\nTypeError\n>>> A().foo()\nTypeError\n>>> B.foo()\n>>> B().foo()\npython3:>>> A.foo()\n>>> A().foo()\nTypeError\n>>> B.foo()\n>>> B().foo()\n>>> A.foo()\n>>> A().foo()\nTypeError\n>>> B.foo()\n>>> B().foo()\nSo using  @staticmethod for methods only called directly from the class has become optional in python3. If you want to call them from both class and instance, you still need to use the @staticmethod decorator.@staticmethod@staticmethodThe other cases have been well covered by unutbus answer.",
                "Class methods, as the name suggests, are used to make changes to classes and not the objects. To make changes to classes, they will modify the class attributes(not object attributes), since that is how you update classes.\nThis is the reason that class methods take the class(conventionally denoted by 'cls') as the first argument.class A(object):\n    m=54\n\n    @classmethod\n    def class_method(cls):\n        print \"m is %d\" % cls.m\nclass A(object):\n    m=54\n\n    @classmethod\n    def class_method(cls):\n        print \"m is %d\" % cls.m\nStatic methods on the other hand, are used to perform functionalities that are not bound to the class i.e. they will not read or write class variables. Hence, static methods do not take classes as arguments. They are used so that classes can perform functionalities that are not directly related to the purpose of the class.class X(object):\n    m=54 #will not be referenced\n\n    @staticmethod\n    def static_method():\n        print \"Referencing/calling a variable or function outside this class. E.g. Some global variable/function.\"\nclass X(object):\n    m=54 #will not be referenced\n\n    @staticmethod\n    def static_method():\n        print \"Referencing/calling a variable or function outside this class. E.g. Some global variable/function.\"\n",
                "I think giving a purely Python version of staticmethod and classmethod would help to understand the difference between them at language level (Refers to Descriptor Howto Guide).staticmethodclassmethodDescriptor Howto GuideBoth of them are non-data descriptors (It would be easier to understand them if you are familiar with descriptors first).descriptorsclass StaticMethod(object):\n    \"Emulate PyStaticMethod_Type() in Objects/funcobject.c\"\n\n    def __init__(self, f):\n        self.f = f\n\n    def __get__(self, obj, objtype=None):\n        return self.f\n\n\nclass ClassMethod(object):\n    \"Emulate PyClassMethod_Type() in Objects/funcobject.c\"\n    def __init__(self, f):\n        self.f = f\n\n    def __get__(self, obj, cls=None):\n        def inner(*args, **kwargs):\n            if cls is None:\n                cls = type(obj)\n            return self.f(cls, *args, **kwargs)\n        return inner\nclass StaticMethod(object):\n    \"Emulate PyStaticMethod_Type() in Objects/funcobject.c\"\n\n    def __init__(self, f):\n        self.f = f\n\n    def __get__(self, obj, objtype=None):\n        return self.f\n\n\nclass ClassMethod(object):\n    \"Emulate PyClassMethod_Type() in Objects/funcobject.c\"\n    def __init__(self, f):\n        self.f = f\n\n    def __get__(self, obj, cls=None):\n        def inner(*args, **kwargs):\n            if cls is None:\n                cls = type(obj)\n            return self.f(cls, *args, **kwargs)\n        return inner\n",
                "Analyze @staticmethod literally providing different insights.literallyA normal method of a class is an implicit dynamic method which takes the instance as first argument.\nIn contrast, a staticmethod does not take the instance as first argument, so is called 'static'.dynamic'static'A staticmethod is indeed such a normal function the same as those outside a class definition.\nIt is luckily grouped into the class just in order to stand closer where it is applied, or you might scroll around to find it.",
                "One pretty important practical difference occurs when subclassing. If you don't mind, I'll hijack @unutbu's example:class A: \n    def foo(self, x): \n        print(\"executing foo(%s, %s)\" % (self, x)) \n \n    @classmethod\n    def class_foo(cls, x): \n        print(\"executing class_foo(%s, %s)\" % (cls, x))\n \n    @staticmethod \n    def static_foo(x): \n        print(\"executing static_foo(%s)\" % x)\n\nclass B(A):\n    pass\nclass A: \n    def foo(self, x): \n        print(\"executing foo(%s, %s)\" % (self, x)) \n \n    @classmethod\n    def class_foo(cls, x): \n        print(\"executing class_foo(%s, %s)\" % (cls, x))\n \n    @staticmethod \n    def static_foo(x): \n        print(\"executing static_foo(%s)\" % x)\n\nclass B(A):\n    pass\nIn class_foo, the method knows which class it is called on:class_fooA.class_foo(1)\n# => executing class_foo(<class '__main__.A'>, 1)\nB.class_foo(1)\n# => executing class_foo(<class '__main__.B'>, 1)\nA.class_foo(1)\n# => executing class_foo(<class '__main__.A'>, 1)\nB.class_foo(1)\n# => executing class_foo(<class '__main__.B'>, 1)\nIn static_foo, there is no way to determine whether it is called on A or B:static_fooABA.static_foo(1)\n# => executing static_foo(1)\nB.static_foo(1)\n# => executing static_foo(1)\nA.static_foo(1)\n# => executing static_foo(1)\nB.static_foo(1)\n# => executing static_foo(1)\nNote that this doesn't mean you can't use other methods in a staticmethod, you just have to reference the class directly, which means subclasses' staticmethods will still reference the parent class:staticmethodclass A:\n    @classmethod\n    def class_qux(cls, x):\n        print(f\"executing class_qux({cls}, {x})\")\n    \n    @classmethod\n    def class_bar(cls, x):\n        cls.class_qux(x)\n\n    @staticmethod\n    def static_bar(x):\n        A.class_qux(x)\n\nclass B(A):\n    pass\n\nA.class_bar(1)\n# => executing class_qux(<class '__main__.A'>, 1)\nB.class_bar(1)\n# => executing class_qux(<class '__main__.B'>, 1)\nA.static_bar(1)\n# => executing class_qux(<class '__main__.A'>, 1)\nB.static_bar(1)\n# => executing class_qux(<class '__main__.A'>, 1)\nclass A:\n    @classmethod\n    def class_qux(cls, x):\n        print(f\"executing class_qux({cls}, {x})\")\n    \n    @classmethod\n    def class_bar(cls, x):\n        cls.class_qux(x)\n\n    @staticmethod\n    def static_bar(x):\n        A.class_qux(x)\n\nclass B(A):\n    pass\n\nA.class_bar(1)\n# => executing class_qux(<class '__main__.A'>, 1)\nB.class_bar(1)\n# => executing class_qux(<class '__main__.B'>, 1)\nA.static_bar(1)\n# => executing class_qux(<class '__main__.A'>, 1)\nB.static_bar(1)\n# => executing class_qux(<class '__main__.A'>, 1)\n",
                "tldr;A staticmethod is essentially a function bound to a class (and consequently its instances)staticmethodA classmethod is essentially an inheritable staticmethod.classmethodstaticmethodFor details, see the excellent answers by others.",
                "First let's start with an example code that we'll use to understand both concepts:class Employee:\n\n    NO_OF_EMPLOYEES = 0\n  \n    def __init__(self, first_name, last_name, salary):\n        self.first_name = first_name\n        self.last_name = last_name\n        self.salary = salary\n        self.increment_employees()\n\n    def give_raise(self, amount):\n        self.salary += amount\n\n    @classmethod\n    def employee_from_full_name(cls, full_name, salary):\n        split_name = full_name.split(' ')\n        first_name = split_name[0]\n        last_name = split_name[1]\n        return cls(first_name, last_name, salary)\n\n    @classmethod\n    def increment_employees(cls):\n        cls.NO_OF_EMPLOYEES += 1\n\n    @staticmethod\n    def get_employee_legal_obligations_txt():\n        legal_obligations = \"\"\"\n        1. An employee must complete 8 hours per working day\n        2. ...\n        \"\"\"\n        return legal_obligations\nclass Employee:\n\n    NO_OF_EMPLOYEES = 0\n  \n    def __init__(self, first_name, last_name, salary):\n        self.first_name = first_name\n        self.last_name = last_name\n        self.salary = salary\n        self.increment_employees()\n\n    def give_raise(self, amount):\n        self.salary += amount\n\n    @classmethod\n    def employee_from_full_name(cls, full_name, salary):\n        split_name = full_name.split(' ')\n        first_name = split_name[0]\n        last_name = split_name[1]\n        return cls(first_name, last_name, salary)\n\n    @classmethod\n    def increment_employees(cls):\n        cls.NO_OF_EMPLOYEES += 1\n\n    @staticmethod\n    def get_employee_legal_obligations_txt():\n        legal_obligations = \"\"\"\n        1. An employee must complete 8 hours per working day\n        2. ...\n        \"\"\"\n        return legal_obligations\nClass methodClass methodA class method accepts the class itself as an implicit argument and -optionally- any other arguments specified in the definition. It\u2019s important to understand that a class method, does not have access to object instances (like instance methods do). Therefore, class methods cannot be used to alter the state of an instantiated object but instead, they are capable of changing the class state which is shared amongst all the instances of that class.\nClass methods are typically useful when we need to access the class itself \u2014 for example, when we want to create a factory method, that is a method that creates instances of the class. In other words, class methods can serve as alternative constructors.In our example code, an instance of Employee can be constructed by providing three arguments; first_name , last_name and salary.Employeefirst_namelast_namesalaryemployee_1 = Employee('Andrew', 'Brown', 85000)\nprint(employee_1.first_name)\nprint(employee_1.salary)\n\n'Andrew'\n85000\nemployee_1 = Employee('Andrew', 'Brown', 85000)\nprint(employee_1.first_name)\nprint(employee_1.salary)\n\n'Andrew'\n85000\nNow let\u2019s assume that there\u2019s a chance that the name of an Employee can be provided in a single field in which the first and last names are separated by a whitespace. In this case, we could possibly use our class method called employee_from_full_name that accepts three arguments in total. The first one, is the class itself, which is an implicit argument which means that it won\u2019t be provided when calling the method \u2014 Python will automatically do this for us:employee_from_full_nameemployee_2 = Employee.employee_from_full_name('John Black', 95000)\nprint(employee_2.first_name)\nprint(employee_2.salary)\n\n'John'\n95000\nemployee_2 = Employee.employee_from_full_name('John Black', 95000)\nprint(employee_2.first_name)\nprint(employee_2.salary)\n\n'John'\n95000\nNote that it is also possible to call employee_from_full_name from object instances although in this context it doesn\u2019t make a lot of sense:employee_from_full_nameemployee_1 = Employee('Andrew', 'Brown', 85000)\nemployee_2 = employee_1.employee_from_full_name('John Black', 95000)\nemployee_1 = Employee('Andrew', 'Brown', 85000)\nemployee_2 = employee_1.employee_from_full_name('John Black', 95000)\nAnother reason why we might want to create a class method, is when we need to change the state of the class. In our example, the class variable NO_OF_EMPLOYEES keeps track of the number of employees currently working for the company. This method is called every time a new instance of Employee is created and it updates the count accordingly:NO_OF_EMPLOYEESemployee_1 = Employee('Andrew', 'Brown', 85000)\nprint(f'Number of employees: {Employee.NO_OF_EMPLOYEES}')\nemployee_2 = Employee.employee_from_full_name('John Black', 95000)\nprint(f'Number of employees: {Employee.NO_OF_EMPLOYEES}')\n\nNumber of employees: 1\nNumber of employees: 2\nemployee_1 = Employee('Andrew', 'Brown', 85000)\nprint(f'Number of employees: {Employee.NO_OF_EMPLOYEES}')\nemployee_2 = Employee.employee_from_full_name('John Black', 95000)\nprint(f'Number of employees: {Employee.NO_OF_EMPLOYEES}')\n\nNumber of employees: 1\nNumber of employees: 2\nStatic methodsStatic methodsOn the other hand, in static methods neither the instance (i.e. self) nor the class itself (i.e. cls) is passed as an implicit argument. This means that such methods, are not capable of accessing the class itself or its instances.\nNow one could argue that static methods are not useful in the context of classes as they can also be placed in helper modules instead of adding them as members of the class. In object oriented programming, it is important to structure your classes into logical chunks and thus, static methods are quite useful when we need to add a method under a class simply because it logically belongs to the class.\nIn our example, the static method named get_employee_legal_obligations_txt simply returns a string that contains the legal obligations of every single employee of a company. This function, does not interact with the class itself nor with any instance. It could have been placed into a different helper module however, it is only relevant to this class and therefore we have to place it under the Employee class.selfclsget_employee_legal_obligations_txtA static method can be access directly from the class itselfprint(Employee.get_employee_legal_obligations_txt())\n\n\n    1. An employee must complete 8 hours per working day\n    2. ...\nprint(Employee.get_employee_legal_obligations_txt())\n\n\n    1. An employee must complete 8 hours per working day\n    2. ...\nor from an instance of the class:employee_1 = Employee('Andrew', 'Brown', 85000)\nprint(employee_1.get_employee_legal_obligations_txt())\n\n\n    1. An employee must complete 8 hours per working day\n    2. ...\nemployee_1 = Employee('Andrew', 'Brown', 85000)\nprint(employee_1.get_employee_legal_obligations_txt())\n\n\n    1. An employee must complete 8 hours per working day\n    2. ...\nReferencesReferences\nWhat's the difference between static and class methods in Python?\nWhat's the difference between static and class methods in Python?What's the difference between static and class methods in Python?"
            ]
        },
        {
            "tag": "slicing",
            "patterns": [
                "Understanding slicing",
                "the understanding of slicing",
                "How to slice a list", 
                "List slicing",
                "How does list slicing work?"
            ],
            "responses": [
                "The syntax is:a[start:stop]  # items start through stop-1\na[start:]      # items start through the rest of the array\na[:stop]       # items from the beginning through stop-1\na[:]           # a copy of the whole array\na[start:stop]  # items start through stop-1\na[start:]      # items start through the rest of the array\na[:stop]       # items from the beginning through stop-1\na[:]           # a copy of the whole array\nThere is also the step value, which can be used with any of the above:stepa[start:stop:step] # start through not past stop, by step\na[start:stop:step] # start through not past stop, by step\nThe key point to remember is that the :stop value represents the first value that is not in the selected slice. So, the difference between stop and start is the number of elements selected (if step is 1, the default).:stopnotstopstartstepThe other feature is that start or stop may be a negative number, which means it counts from the end of the array instead of the beginning. So:startstopnegativea[-1]    # last item in the array\na[-2:]   # last two items in the array\na[:-2]   # everything except the last two items\na[-1]    # last item in the array\na[-2:]   # last two items in the array\na[:-2]   # everything except the last two items\nSimilarly, step may be a negative number:stepa[::-1]    # all items in the array, reversed\na[1::-1]   # the first two items, reversed\na[:-3:-1]  # the last two items, reversed\na[-3::-1]  # everything except the last two items, reversed\na[::-1]    # all items in the array, reversed\na[1::-1]   # the first two items, reversed\na[:-3:-1]  # the last two items, reversed\na[-3::-1]  # everything except the last two items, reversed\nPython is kind to the programmer if there are fewer items than you ask for. For example, if you ask for a[:-2] and a only contains one element, you get an empty list instead of an error. Sometimes you would prefer the error, so you have to be aware that this may happen.a[:-2]aRelationship with the slice objectsliceA slice object can represent a slicing operation, i.e.:slice objectslicea[start:stop:step]\na[start:stop:step]\nis equivalent to:a[slice(start, stop, step)]\na[slice(start, stop, step)]\nSlice objects also behave slightly differently depending on the number of arguments, similarly to range(), i.e. both slice(stop) and slice(start, stop[, step]) are supported.\nTo skip specifying a given argument, one might use None, so that e.g. a[start:] is equivalent to a[slice(start, None)] or a[::-1] is equivalent to a[slice(None, None, -1)].range()slice(stop)slice(start, stop[, step])Nonea[start:]a[slice(start, None)]a[::-1]a[slice(None, None, -1)]While the :-based notation is very helpful for simple slicing, the explicit use of slice() objects simplifies the programmatic generation of slicing.:slice()",
                "The Python tutorial talks about it (scroll down a bit until you get to the part about slicing).Python tutorialThe ASCII art diagram is helpful too for remembering how slices work: +---+---+---+---+---+---+\n | P | y | t | h | o | n |\n +---+---+---+---+---+---+\n 0   1   2   3   4   5   6\n-6  -5  -4  -3  -2  -1\n +---+---+---+---+---+---+\n | P | y | t | h | o | n |\n +---+---+---+---+---+---+\n 0   1   2   3   4   5   6\n-6  -5  -4  -3  -2  -1\n\nOne way to remember how slices work is to think of the indices as pointing between characters, with the left edge of the first character numbered 0. Then the right edge of the last character of a string of n characters has index n.\nOne way to remember how slices work is to think of the indices as pointing between characters, with the left edge of the first character numbered 0. Then the right edge of the last character of a string of n characters has index n.betweennn",
                "Enumerating the possibilities allowed by the grammar for the sequence x:x>>> x[:]                # [x[0],   x[1],          ..., x[-1]    ]\n>>> x[low:]             # [x[low], x[low+1],      ..., x[-1]    ]\n>>> x[:high]            # [x[0],   x[1],          ..., x[high-1]]\n>>> x[low:high]         # [x[low], x[low+1],      ..., x[high-1]]\n>>> x[::stride]         # [x[0],   x[stride],     ..., x[-1]    ]\n>>> x[low::stride]      # [x[low], x[low+stride], ..., x[-1]    ]\n>>> x[:high:stride]     # [x[0],   x[stride],     ..., x[high-1]]\n>>> x[low:high:stride]  # [x[low], x[low+stride], ..., x[high-1]]\n>>> x[:]                # [x[0],   x[1],          ..., x[-1]    ]\n>>> x[low:]             # [x[low], x[low+1],      ..., x[-1]    ]\n>>> x[:high]            # [x[0],   x[1],          ..., x[high-1]]\n>>> x[low:high]         # [x[low], x[low+1],      ..., x[high-1]]\n>>> x[::stride]         # [x[0],   x[stride],     ..., x[-1]    ]\n>>> x[low::stride]      # [x[low], x[low+stride], ..., x[-1]    ]\n>>> x[:high:stride]     # [x[0],   x[stride],     ..., x[high-1]]\n>>> x[low:high:stride]  # [x[low], x[low+stride], ..., x[high-1]]\nOf course, if (high-low)%stride != 0, then the end point will be a little lower than high-1.(high-low)%stride != 0high-1If stride is negative, the ordering is changed a bit since we're counting down:stride>>> x[::-stride]        # [x[-1],   x[-1-stride],   ..., x[0]    ]\n>>> x[high::-stride]    # [x[high], x[high-stride], ..., x[0]    ]\n>>> x[:low:-stride]     # [x[-1],   x[-1-stride],   ..., x[low+1]]\n>>> x[high:low:-stride] # [x[high], x[high-stride], ..., x[low+1]]\n>>> x[::-stride]        # [x[-1],   x[-1-stride],   ..., x[0]    ]\n>>> x[high::-stride]    # [x[high], x[high-stride], ..., x[0]    ]\n>>> x[:low:-stride]     # [x[-1],   x[-1-stride],   ..., x[low+1]]\n>>> x[high:low:-stride] # [x[high], x[high-stride], ..., x[low+1]]\nExtended slicing (with commas and ellipses) are mostly used only by special data structures (like NumPy); the basic sequences don't support them.>>> class slicee:\n...     def __getitem__(self, item):\n...         return repr(item)\n...\n>>> slicee()[0, 1:2, ::5, ...]\n'(0, slice(1, 2, None), slice(None, None, 5), Ellipsis)'\n>>> class slicee:\n...     def __getitem__(self, item):\n...         return repr(item)\n...\n>>> slicee()[0, 1:2, ::5, ...]\n'(0, slice(1, 2, None), slice(None, None, 5), Ellipsis)'\n",
                "The answers above don't discuss slice assignment. To understand slice assignment, it's helpful to add another concept to the ASCII art:                +---+---+---+---+---+---+\n                | P | y | t | h | o | n |\n                +---+---+---+---+---+---+\nSlice position: 0   1   2   3   4   5   6\nIndex position:   0   1   2   3   4   5\n\n>>> p = ['P','y','t','h','o','n']\n# Why the two sets of numbers:\n# indexing gives items, not lists\n>>> p[0]\n 'P'\n>>> p[5]\n 'n'\n\n# Slicing gives lists\n>>> p[0:1]\n ['P']\n>>> p[0:2]\n ['P','y']\n                +---+---+---+---+---+---+\n                | P | y | t | h | o | n |\n                +---+---+---+---+---+---+\nSlice position: 0   1   2   3   4   5   6\nIndex position:   0   1   2   3   4   5\n\n>>> p = ['P','y','t','h','o','n']\n# Why the two sets of numbers:\n# indexing gives items, not lists\n>>> p[0]\n 'P'\n>>> p[5]\n 'n'\n\n# Slicing gives lists\n>>> p[0:1]\n ['P']\n>>> p[0:2]\n ['P','y']\nOne heuristic is, for a slice from zero to n, think: \"zero is the beginning, start at the beginning and take n items in a list\".>>> p[5] # the last of six items, indexed from zero\n 'n'\n>>> p[0:5] # does NOT include the last item!\n ['P','y','t','h','o']\n>>> p[0:6] # not p[0:5]!!!\n ['P','y','t','h','o','n']\n>>> p[5] # the last of six items, indexed from zero\n 'n'\n>>> p[0:5] # does NOT include the last item!\n ['P','y','t','h','o']\n>>> p[0:6] # not p[0:5]!!!\n ['P','y','t','h','o','n']\nAnother heuristic is, \"for any slice, replace the start by zero, apply the previous heuristic to get the end of the list, then count the first number back up to chop items off the beginning\">>> p[0:4] # Start at the beginning and count out 4 items\n ['P','y','t','h']\n>>> p[1:4] # Take one item off the front\n ['y','t','h']\n>>> p[2:4] # Take two items off the front\n ['t','h']\n# etc.\n>>> p[0:4] # Start at the beginning and count out 4 items\n ['P','y','t','h']\n>>> p[1:4] # Take one item off the front\n ['y','t','h']\n>>> p[2:4] # Take two items off the front\n ['t','h']\n# etc.\nThe first rule of slice assignment is that since slicing returns a list, slice assignment requires a list (or other iterable):returnsrequires>>> p[2:3]\n ['t']\n>>> p[2:3] = ['T']\n>>> p\n ['P','y','T','h','o','n']\n>>> p[2:3] = 't'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: can only assign an iterable\n>>> p[2:3]\n ['t']\n>>> p[2:3] = ['T']\n>>> p\n ['P','y','T','h','o','n']\n>>> p[2:3] = 't'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: can only assign an iterable\nThe second rule of slice assignment, which you can also see above, is that whatever portion of the list is returned by slice indexing, that's the same portion that is changed by slice assignment:>>> p[2:4]\n ['T','h']\n>>> p[2:4] = ['t','r']\n>>> p\n ['P','y','t','r','o','n']\n>>> p[2:4]\n ['T','h']\n>>> p[2:4] = ['t','r']\n>>> p\n ['P','y','t','r','o','n']\nThe third rule of slice assignment is, the assigned list (iterable) doesn't have to have the same length; the indexed slice is simply sliced out and replaced en masse by whatever is being assigned:>>> p = ['P','y','t','h','o','n'] # Start over\n>>> p[2:4] = ['s','p','a','m']\n>>> p\n ['P','y','s','p','a','m','o','n']\n>>> p = ['P','y','t','h','o','n'] # Start over\n>>> p[2:4] = ['s','p','a','m']\n>>> p\n ['P','y','s','p','a','m','o','n']\nThe trickiest part to get used to is assignment to empty slices. Using heuristic 1 and 2 it's easy to get your head around indexing an empty slice:indexing>>> p = ['P','y','t','h','o','n']\n>>> p[0:4]\n ['P','y','t','h']\n>>> p[1:4]\n ['y','t','h']\n>>> p[2:4]\n ['t','h']\n>>> p[3:4]\n ['h']\n>>> p[4:4]\n []\n>>> p = ['P','y','t','h','o','n']\n>>> p[0:4]\n ['P','y','t','h']\n>>> p[1:4]\n ['y','t','h']\n>>> p[2:4]\n ['t','h']\n>>> p[3:4]\n ['h']\n>>> p[4:4]\n []\nAnd then once you've seen that, slice assignment to the empty slice makes sense too:>>> p = ['P','y','t','h','o','n']\n>>> p[2:4] = ['x','y'] # Assigned list is same length as slice\n>>> p\n ['P','y','x','y','o','n'] # Result is same length\n>>> p = ['P','y','t','h','o','n']\n>>> p[3:4] = ['x','y'] # Assigned list is longer than slice\n>>> p\n ['P','y','t','x','y','o','n'] # The result is longer\n>>> p = ['P','y','t','h','o','n']\n>>> p[4:4] = ['x','y']\n>>> p\n ['P','y','t','h','x','y','o','n'] # The result is longer still\n>>> p = ['P','y','t','h','o','n']\n>>> p[2:4] = ['x','y'] # Assigned list is same length as slice\n>>> p\n ['P','y','x','y','o','n'] # Result is same length\n>>> p = ['P','y','t','h','o','n']\n>>> p[3:4] = ['x','y'] # Assigned list is longer than slice\n>>> p\n ['P','y','t','x','y','o','n'] # The result is longer\n>>> p = ['P','y','t','h','o','n']\n>>> p[4:4] = ['x','y']\n>>> p\n ['P','y','t','h','x','y','o','n'] # The result is longer still\nNote that, since we are not changing the second number of the slice (4), the inserted items always stack right up against the 'o', even when we're assigning to the empty slice. So the position for the empty slice assignment is the logical extension of the positions for the non-empty slice assignments.Backing up a little bit, what happens when you keep going with our procession of counting up the slice beginning?>>> p = ['P','y','t','h','o','n']\n>>> p[0:4]\n ['P','y','t','h']\n>>> p[1:4]\n ['y','t','h']\n>>> p[2:4]\n ['t','h']\n>>> p[3:4]\n ['h']\n>>> p[4:4]\n []\n>>> p[5:4]\n []\n>>> p[6:4]\n []\n>>> p = ['P','y','t','h','o','n']\n>>> p[0:4]\n ['P','y','t','h']\n>>> p[1:4]\n ['y','t','h']\n>>> p[2:4]\n ['t','h']\n>>> p[3:4]\n ['h']\n>>> p[4:4]\n []\n>>> p[5:4]\n []\n>>> p[6:4]\n []\nWith slicing, once you're done, you're done; it doesn't start slicing backwards. In Python you don't get negative strides unless you explicitly ask for them by using a negative number.>>> p[5:3:-1]\n ['n','o']\n>>> p[5:3:-1]\n ['n','o']\nThere are some weird consequences to the \"once you're done, you're done\" rule:>>> p[4:4]\n []\n>>> p[5:4]\n []\n>>> p[6:4]\n []\n>>> p[6]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: list index out of range\n>>> p[4:4]\n []\n>>> p[5:4]\n []\n>>> p[6:4]\n []\n>>> p[6]\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nIndexError: list index out of range\nIn fact, compared to indexing, Python slicing is bizarrely error-proof:>>> p[100:200]\n []\n>>> p[int(2e99):int(1e99)]\n []\n>>> p[100:200]\n []\n>>> p[int(2e99):int(1e99)]\n []\nThis can come in handy sometimes, but it can also lead to somewhat strange behavior:>>> p\n ['P', 'y', 't', 'h', 'o', 'n']\n>>> p[int(2e99):int(1e99)] = ['p','o','w','e','r']\n>>> p\n ['P', 'y', 't', 'h', 'o', 'n', 'p', 'o', 'w', 'e', 'r']\n>>> p\n ['P', 'y', 't', 'h', 'o', 'n']\n>>> p[int(2e99):int(1e99)] = ['p','o','w','e','r']\n>>> p\n ['P', 'y', 't', 'h', 'o', 'n', 'p', 'o', 'w', 'e', 'r']\nDepending on your application, that might... or might not... be what you were hoping for there!Below is the text of my original answer. It has been useful to many people, so I didn't want to delete it.>>> r=[1,2,3,4]\n>>> r[1:1]\n[]\n>>> r[1:1]=[9,8]\n>>> r\n[1, 9, 8, 2, 3, 4]\n>>> r[1:1]=['blah']\n>>> r\n[1, 'blah', 9, 8, 2, 3, 4]\n>>> r=[1,2,3,4]\n>>> r[1:1]\n[]\n>>> r[1:1]=[9,8]\n>>> r\n[1, 9, 8, 2, 3, 4]\n>>> r[1:1]=['blah']\n>>> r\n[1, 'blah', 9, 8, 2, 3, 4]\nThis may also clarify the difference between slicing and indexing.",
                "\nExplain Python's slice notation\nExplain Python's slice notationIn short, the colons (:) in subscript notation (subscriptable[subscriptarg]) make slice notation, which has the optional arguments start, stop, and step::subscriptable[subscriptarg]startstopstepsliceable[start:stop:step]\nsliceable[start:stop:step]\nPython slicing is a computationally fast way to methodically access parts of your data. In my opinion, to be even an intermediate Python programmer, it's one aspect of the language that it is necessary to be familiar with.Important DefinitionsTo begin with, let's define a few terms:\nstart: the beginning index of the slice, it will include the element at this index unless it is the same as stop, defaults to 0, i.e. the first index. If it's negative, it means to start n items from the end.\nstop: the ending index of the slice, it does not include the element at this index, defaults to length of the sequence being sliced, that is, up to and including the end.\nstep: the amount by which the index increases, defaults to 1. If it's negative, you're slicing over the iterable in reverse.\nstart: the beginning index of the slice, it will include the element at this index unless it is the same as stop, defaults to 0, i.e. the first index. If it's negative, it means to start n items from the end.start:startstopnstop: the ending index of the slice, it does not include the element at this index, defaults to length of the sequence being sliced, that is, up to and including the end.stop:stopnotstep: the amount by which the index increases, defaults to 1. If it's negative, you're slicing over the iterable in reverse.step:stepHow Indexing WorksYou can make any of these positive or negative numbers. The meaning of the positive numbers is straightforward, but for negative numbers, just like indexes in Python, you count backwards from the end for the start and stop, and for the step, you simply decrement your index. This example is from the documentation's tutorial, but I've modified it slightly to indicate which item in a sequence each index references:startstopstepfrom the documentation's tutorial +---+---+---+---+---+---+\n | P | y | t | h | o | n |\n +---+---+---+---+---+---+\n   0   1   2   3   4   5 \n  -6  -5  -4  -3  -2  -1\n +---+---+---+---+---+---+\n | P | y | t | h | o | n |\n +---+---+---+---+---+---+\n   0   1   2   3   4   5 \n  -6  -5  -4  -3  -2  -1\nHow Slicing WorksTo use slice notation with a sequence that supports it, you must include at least one colon in the square brackets that follow the sequence (which actually implement the __getitem__ method of the sequence, according to the Python data model.)implement the __getitem__ method of the sequence, according to the Python data model__getitem__Slice notation works like this:sequence[start:stop:step]\nsequence[start:stop:step]\nAnd recall that there are defaults for start, stop, and step, so to access the defaults, simply leave out the argument.startstopstepSlice notation to get the last nine elements from a list (or any other sequence that supports it, like a string) would look like this:my_list[-9:]\nmy_list[-9:]\nWhen I see this, I read the part in the brackets as \"9th from the end, to the end.\" (Actually, I abbreviate it mentally as \"-9, on\")Explanation:The full notation ismy_list[-9:None:None]\nmy_list[-9:None:None]\nand to substitute the defaults (actually when step is negative, stop's default is -len(my_list) - 1, so None for stop really just means it goes to whichever end step takes it to):stepstop-len(my_list) - 1Nonemy_list[-9:len(my_list):1]\nmy_list[-9:len(my_list):1]\nThe colon, :,  is what tells Python you're giving it a slice and not a regular index. That's why the idiomatic way of making a shallow copy of lists in Python 2 iscolon:list_copy = sequence[:]\nlist_copy = sequence[:]\nAnd clearing them is with:del my_list[:]\ndel my_list[:]\n(Python 3 gets a list.copy and list.clear method.)list.copylist.clearWhen step is negative, the defaults for start and stop changestepstartstopBy default, when the step argument is empty (or None), it is assigned to +1.stepNone+1But you can pass in a negative integer, and the list (or most other standard sliceables) will be sliced from the end to the beginning.Thus a negative slice will change the defaults for start and stop!startstopConfirming this in the sourceI like to encourage users to read the source as well as the documentation. The source code for slice objects and this logic is found here. First we determine if step is negative:source code for slice objects and this logic is found herestep\nstep_is_negative = step_sign < 0;\n\nstep_is_negative = step_sign < 0;\nstep_is_negative = step_sign < 0;\nIf so, the lower bound is -1  meaning we slice all the way up to and including the beginning, and the upper bound is the length minus 1, meaning we start at the end. (Note that the semantics of this -1 is different from a -1 that users may pass indexes in Python indicating the last item.)-1-1different-1\nif (step_is_negative) {\n    lower = PyLong_FromLong(-1L);\n    if (lower == NULL)\n        goto error;\n\n    upper = PyNumber_Add(length, lower);\n    if (upper == NULL)\n        goto error;\n}\n\nif (step_is_negative) {\n    lower = PyLong_FromLong(-1L);\n    if (lower == NULL)\n        goto error;\n\n    upper = PyNumber_Add(length, lower);\n    if (upper == NULL)\n        goto error;\n}\nif (step_is_negative) {\n    lower = PyLong_FromLong(-1L);\n    if (lower == NULL)\n        goto error;\n\n    upper = PyNumber_Add(length, lower);\n    if (upper == NULL)\n        goto error;\n}\nOtherwise step is positive, and the lower bound will be zero and the upper bound (which we go up to but not including) the length of the sliced list.step\nelse {\n    lower = _PyLong_Zero;\n    Py_INCREF(lower);\n    upper = length;\n    Py_INCREF(upper);\n}\n\nelse {\n    lower = _PyLong_Zero;\n    Py_INCREF(lower);\n    upper = length;\n    Py_INCREF(upper);\n}\nelse {\n    lower = _PyLong_Zero;\n    Py_INCREF(lower);\n    upper = length;\n    Py_INCREF(upper);\n}\nThen, we may need to apply the defaults for start and stop\u2014the default then for start is calculated as the upper bound when step is negative:startstopstartstep\nif (self->start == Py_None) {\n    start = step_is_negative ? upper : lower;\n    Py_INCREF(start);\n}\n\nif (self->start == Py_None) {\n    start = step_is_negative ? upper : lower;\n    Py_INCREF(start);\n}\nif (self->start == Py_None) {\n    start = step_is_negative ? upper : lower;\n    Py_INCREF(start);\n}\nand stop, the lower bound:stop\nif (self->stop == Py_None) {\n    stop = step_is_negative ? lower : upper;\n    Py_INCREF(stop);\n}\n\nif (self->stop == Py_None) {\n    stop = step_is_negative ? lower : upper;\n    Py_INCREF(stop);\n}\nif (self->stop == Py_None) {\n    stop = step_is_negative ? lower : upper;\n    Py_INCREF(stop);\n}\nGive your slices a descriptive name!You may find it useful to separate forming the slice from passing it to the list.__getitem__ method (that's what the square brackets do). Even if you're not new to it, it keeps your code more readable so that others that may have to read your code can more readily understand what you're doing.list.__getitem__that's what the square brackets doHowever, you can't just assign some integers separated by colons to a variable. You need to use the slice object:last_nine_slice = slice(-9, None)\nlast_nine_slice = slice(-9, None)\nThe second argument, None, is required, so that the first argument is interpreted as the start argument otherwise it would be the stop argument.Nonestartotherwise it would be the stop argumentstopYou can then pass the slice object to your sequence:>>> list(range(100))[last_nine_slice]\n[91, 92, 93, 94, 95, 96, 97, 98, 99]\n>>> list(range(100))[last_nine_slice]\n[91, 92, 93, 94, 95, 96, 97, 98, 99]\nIt's interesting that ranges also take slices:>>> range(100)[last_nine_slice]\nrange(91, 100)\n>>> range(100)[last_nine_slice]\nrange(91, 100)\nMemory Considerations:Since slices of Python lists create new objects in memory, another important function to be aware of is itertools.islice. Typically you'll want to iterate over a slice, not just have it created statically in memory. islice is perfect for this. A caveat, it doesn't support negative arguments to start, stop, or step, so if that's an issue you may need to calculate indices or reverse the iterable in advance.itertools.isliceislicestartstopsteplength = 100\nlast_nine_iter = itertools.islice(list(range(length)), length-9, None, 1)\nlist_last_nine = list(last_nine_iter)\nlength = 100\nlast_nine_iter = itertools.islice(list(range(length)), length-9, None, 1)\nlist_last_nine = list(last_nine_iter)\nand now:>>> list_last_nine\n[91, 92, 93, 94, 95, 96, 97, 98, 99]\n>>> list_last_nine\n[91, 92, 93, 94, 95, 96, 97, 98, 99]\nThe fact that list slices make a copy is a feature of lists themselves. If you're slicing advanced objects like a Pandas DataFrame, it may return a view on the original, and not a copy.",
                "And a couple of things that weren't immediately obvious to me when I first saw the slicing syntax:>>> x = [1,2,3,4,5,6]\n>>> x[::-1]\n[6,5,4,3,2,1]\n>>> x = [1,2,3,4,5,6]\n>>> x[::-1]\n[6,5,4,3,2,1]\nEasy way to reverse sequences!And if you wanted, for some reason, every second item in the reversed sequence:>>> x = [1,2,3,4,5,6]\n>>> x[::-2]\n[6,4,2]\n>>> x = [1,2,3,4,5,6]\n>>> x[::-2]\n[6,4,2]\n",
                "In Python 2.7Slicing in Python[a:b:c]\n\nlen = length of string, tuple or list\n\nc -- default is +1. The sign of c indicates forward or backward, absolute value of c indicates steps. Default is forward with step size 1. Positive means forward, negative means backward.\n\na --  When c is positive or blank, default is 0. When c is negative, default is -1.\n\nb --  When c is positive or blank, default is len. When c is negative, default is -(len+1).\n[a:b:c]\n\nlen = length of string, tuple or list\n\nc -- default is +1. The sign of c indicates forward or backward, absolute value of c indicates steps. Default is forward with step size 1. Positive means forward, negative means backward.\n\na --  When c is positive or blank, default is 0. When c is negative, default is -1.\n\nb --  When c is positive or blank, default is len. When c is negative, default is -(len+1).\nUnderstanding index assignment is very important.In forward direction, starts at 0 and ends at len-1\n\nIn backward direction, starts at -1 and ends at -len\nIn forward direction, starts at 0 and ends at len-1\n\nIn backward direction, starts at -1 and ends at -len\nWhen you say [a:b:c], you are saying depending on the sign of c (forward or backward), start at a and end at b (excluding element at bth index). Use the indexing rule above and remember you will only find elements in this range:-len, -len+1, -len+2, ..., 0, 1, 2,3,4 , len -1\n-len, -len+1, -len+2, ..., 0, 1, 2,3,4 , len -1\nBut this range continues in both directions infinitely:...,-len -2 ,-len-1,-len, -len+1, -len+2, ..., 0, 1, 2,3,4 , len -1, len, len +1, len+2 , ....\n...,-len -2 ,-len-1,-len, -len+1, -len+2, ..., 0, 1, 2,3,4 , len -1, len, len +1, len+2 , ....\nFor example:             0    1    2   3    4   5   6   7   8   9   10   11\n             a    s    t   r    i   n   g\n    -9  -8  -7   -6   -5  -4   -3  -2  -1\n             0    1    2   3    4   5   6   7   8   9   10   11\n             a    s    t   r    i   n   g\n    -9  -8  -7   -6   -5  -4   -3  -2  -1\nIf your choice of a, b, and c allows overlap with the range above as you traverse using rules for a,b,c above you will either get a list with elements (touched during traversal) or you will get an empty list.One last thing: if a and b are equal, then also you get an empty list:>>> l1\n[2, 3, 4]\n\n>>> l1[:]\n[2, 3, 4]\n\n>>> l1[::-1] # a default is -1 , b default is -(len+1)\n[4, 3, 2]\n\n>>> l1[:-4:-1] # a default is -1\n[4, 3, 2]\n\n>>> l1[:-3:-1] # a default is -1\n[4, 3]\n\n>>> l1[::] # c default is +1, so a default is 0, b default is len\n[2, 3, 4]\n\n>>> l1[::-1] # c is -1 , so a default is -1 and b default is -(len+1)\n[4, 3, 2]\n\n\n>>> l1[-100:-200:-1] # Interesting\n[]\n\n>>> l1[-1:-200:-1] # Interesting\n[4, 3, 2]\n\n\n>>> l1[-1:-1:1]\n[]\n\n\n>>> l1[-1:5:1] # Interesting\n[4]\n\n\n>>> l1[1:-7:1]\n[]\n\n>>> l1[1:-7:-1] # Interesting\n[3, 2]\n\n>>> l1[:-2:-2] # a default is -1, stop(b) at -2 , step(c) by 2 in reverse direction\n[4]\n>>> l1\n[2, 3, 4]\n\n>>> l1[:]\n[2, 3, 4]\n\n>>> l1[::-1] # a default is -1 , b default is -(len+1)\n[4, 3, 2]\n\n>>> l1[:-4:-1] # a default is -1\n[4, 3, 2]\n\n>>> l1[:-3:-1] # a default is -1\n[4, 3]\n\n>>> l1[::] # c default is +1, so a default is 0, b default is len\n[2, 3, 4]\n\n>>> l1[::-1] # c is -1 , so a default is -1 and b default is -(len+1)\n[4, 3, 2]\n\n\n>>> l1[-100:-200:-1] # Interesting\n[]\n\n>>> l1[-1:-200:-1] # Interesting\n[4, 3, 2]\n\n\n>>> l1[-1:-1:1]\n[]\n\n\n>>> l1[-1:5:1] # Interesting\n[4]\n\n\n>>> l1[1:-7:1]\n[]\n\n>>> l1[1:-7:-1] # Interesting\n[3, 2]\n\n>>> l1[:-2:-2] # a default is -1, stop(b) at -2 , step(c) by 2 in reverse direction\n[4]\n",
                "Found this great table at http://wiki.python.org/moin/MovingToPythonFromOtherLanguageshttp://wiki.python.org/moin/MovingToPythonFromOtherLanguagesPython indexes and slices for a six-element list.\nIndexes enumerate the elements, slices enumerate the spaces between the elements.\n\nIndex from rear:    -6  -5  -4  -3  -2  -1      a=[0,1,2,3,4,5]    a[1:]==[1,2,3,4,5]\nIndex from front:    0   1   2   3   4   5      len(a)==6          a[:5]==[0,1,2,3,4]\n                   +---+---+---+---+---+---+    a[0]==0            a[:-2]==[0,1,2,3]\n                   | a | b | c | d | e | f |    a[5]==5            a[1:2]==[1]\n                   +---+---+---+---+---+---+    a[-1]==5           a[1:-1]==[1,2,3,4]\nSlice from front:  :   1   2   3   4   5   :    a[-2]==4\nSlice from rear:   :  -5  -4  -3  -2  -1   :\n                                                b=a[:]\n                                                b==[0,1,2,3,4,5] (shallow copy of a)Python indexes and slices for a six-element list.\nIndexes enumerate the elements, slices enumerate the spaces between the elements.\n\nIndex from rear:    -6  -5  -4  -3  -2  -1      a=[0,1,2,3,4,5]    a[1:]==[1,2,3,4,5]\nIndex from front:    0   1   2   3   4   5      len(a)==6          a[:5]==[0,1,2,3,4]\n                   +---+---+---+---+---+---+    a[0]==0            a[:-2]==[0,1,2,3]\n                   | a | b | c | d | e | f |    a[5]==5            a[1:2]==[1]\n                   +---+---+---+---+---+---+    a[-1]==5           a[1:-1]==[1,2,3,4]\nSlice from front:  :   1   2   3   4   5   :    a[-2]==4\nSlice from rear:   :  -5  -4  -3  -2  -1   :\n                                                b=a[:]\n                                                b==[0,1,2,3,4,5] (shallow copy of a)",
                "After using it a bit I realise that the simplest description is that it is exactly the same as the arguments in a for loop...for(from:to:step)\n(from:to:step)\nAny of them are optional:(:to:step)\n(from::step)\n(from:to)\n(:to:step)\n(from::step)\n(from:to)\nThen the negative indexing just needs you to add the length of the string to the negative indices to understand it.This works for me anyway...",
                "I find it easier to remember how it works, and then I can figure out any specific start/stop/step combination.It's instructive to understand range() first:range()def range(start=0, stop, step=1):  # Illegal syntax, but that's the effect\n    i = start\n    while (i < stop if step > 0 else i > stop):\n        yield i\n        i += step\ndef range(start=0, stop, step=1):  # Illegal syntax, but that's the effect\n    i = start\n    while (i < stop if step > 0 else i > stop):\n        yield i\n        i += step\nBegin from start, increment by step, do not reach stop.  Very simple.startstepstopThe thing to remember about negative step is that stop is always the excluded end, whether it's higher or lower. If you want same slice in opposite order, it's much cleaner to do the reversal separately: e.g. 'abcde'[1:-2][::-1] slices off one char from left, two from right, then reverses. (See also reversed().)stop'abcde'[1:-2][::-1]reversed()reversed()Sequence slicing is same, except it first normalizes negative indexes, and it can never go outside the sequence:TODO: The code below had a bug with \"never go outside the sequence\" when abs(step)>1; I think I patched it to be correct, but it's hard to understand.TODOthinkdef this_is_how_slicing_works(seq, start=None, stop=None, step=1):\n    if start is None:\n        start = (0 if step > 0 else len(seq)-1)\n    elif start < 0:\n        start += len(seq)\n    if not 0 <= start < len(seq):  # clip if still outside bounds\n        start = (0 if step > 0 else len(seq)-1)\n    if stop is None:\n        stop = (len(seq) if step > 0 else -1)  # really -1, not last element\n    elif stop < 0:\n        stop += len(seq)\n    for i in range(start, stop, step):\n        if 0 <= i < len(seq):\n            yield seq[i]\ndef this_is_how_slicing_works(seq, start=None, stop=None, step=1):\n    if start is None:\n        start = (0 if step > 0 else len(seq)-1)\n    elif start < 0:\n        start += len(seq)\n    if not 0 <= start < len(seq):  # clip if still outside bounds\n        start = (0 if step > 0 else len(seq)-1)\n    if stop is None:\n        stop = (len(seq) if step > 0 else -1)  # really -1, not last element\n    elif stop < 0:\n        stop += len(seq)\n    for i in range(start, stop, step):\n        if 0 <= i < len(seq):\n            yield seq[i]\nDon't worry about the is None details - just remember that omitting start and/or stop always does the right thing to give you the whole sequence.is NonestartstopNormalizing negative indexes first allows start and/or stop to be counted from the end independently: 'abcde'[1:-2] == 'abcde'[1:3] == 'bc' despite range(1,-2) == [].\nThe normalization is sometimes thought of as \"modulo the length\", but note it adds the length just once: e.g. 'abcde'[-53:42] is just the whole string.'abcde'[1:-2] == 'abcde'[1:3] == 'bc'range(1,-2) == []'abcde'[-53:42]",
                "I use the \"an index points between elements\" method of thinking about it myself, but one way of describing it which sometimes helps others get it is this:mylist[X:Y]\nmylist[X:Y]\nX is the index of the first element you want.\nY is the index of the first element you don't want.don't",
                "Index:\n      ------------>\n  0   1   2   3   4\n+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n  0  -4  -3  -2  -1\n      <------------\n\nSlice:\n    <---------------|\n|--------------->\n:   1   2   3   4   :\n+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n:  -4  -3  -2  -1   :\n|--------------->\n    <---------------|\nIndex:\n      ------------>\n  0   1   2   3   4\n+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n  0  -4  -3  -2  -1\n      <------------\n\nSlice:\n    <---------------|\n|--------------->\n:   1   2   3   4   :\n+---+---+---+---+---+\n| a | b | c | d | e |\n+---+---+---+---+---+\n:  -4  -3  -2  -1   :\n|--------------->\n    <---------------|\nI hope this will help you to model the list in Python.Reference: http://wiki.python.org/moin/MovingToPythonFromOtherLanguageshttp://wiki.python.org/moin/MovingToPythonFromOtherLanguages",
                "This is how I teach slices to newbies:Understanding the difference between indexing and slicing:Understanding the difference between indexing and slicing:Wiki Python has this amazing picture which clearly distinguishes indexing and slicing.It is a list with six elements in it. To understand slicing better, consider that list as a set of six boxes placed together. Each box has an alphabet in it.Indexing is like dealing with the contents of box. You can check contents of any box. But you can't check the contents of multiple boxes at once. You can even replace the contents of the box. But you can't place two balls in one box or replace two balls at a time.In [122]: alpha = ['a', 'b', 'c', 'd', 'e', 'f']\n\nIn [123]: alpha\nOut[123]: ['a', 'b', 'c', 'd', 'e', 'f']\n\nIn [124]: alpha[0]\nOut[124]: 'a'\n\nIn [127]: alpha[0] = 'A'\n\nIn [128]: alpha\nOut[128]: ['A', 'b', 'c', 'd', 'e', 'f']\n\nIn [129]: alpha[0,1]\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-129-c7eb16585371> in <module>()\n----> 1 alpha[0,1]\n\nTypeError: list indices must be integers, not tuple\nIn [122]: alpha = ['a', 'b', 'c', 'd', 'e', 'f']\n\nIn [123]: alpha\nOut[123]: ['a', 'b', 'c', 'd', 'e', 'f']\n\nIn [124]: alpha[0]\nOut[124]: 'a'\n\nIn [127]: alpha[0] = 'A'\n\nIn [128]: alpha\nOut[128]: ['A', 'b', 'c', 'd', 'e', 'f']\n\nIn [129]: alpha[0,1]\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-129-c7eb16585371> in <module>()\n----> 1 alpha[0,1]\n\nTypeError: list indices must be integers, not tuple\nSlicing is like dealing with boxes themselves. You can pick up the first box and place it on another table. To pick up the box, all you need to know is the position of beginning and ending of the box.You can even pick up the first three boxes or the last two boxes or all boxes between 1 and 4. So, you can pick any set of boxes if you know the beginning and ending. These positions are called start and stop positions.The interesting thing is that you can replace multiple boxes at once. Also you can place multiple boxes wherever you like.In [130]: alpha[0:1]\nOut[130]: ['A']\n\nIn [131]: alpha[0:1] = 'a'\n\nIn [132]: alpha\nOut[132]: ['a', 'b', 'c', 'd', 'e', 'f']\n\nIn [133]: alpha[0:2] = ['A', 'B']\n\nIn [134]: alpha\nOut[134]: ['A', 'B', 'c', 'd', 'e', 'f']\n\nIn [135]: alpha[2:2] = ['x', 'xx']\n\nIn [136]: alpha\nOut[136]: ['A', 'B', 'x', 'xx', 'c', 'd', 'e', 'f']\nIn [130]: alpha[0:1]\nOut[130]: ['A']\n\nIn [131]: alpha[0:1] = 'a'\n\nIn [132]: alpha\nOut[132]: ['a', 'b', 'c', 'd', 'e', 'f']\n\nIn [133]: alpha[0:2] = ['A', 'B']\n\nIn [134]: alpha\nOut[134]: ['A', 'B', 'c', 'd', 'e', 'f']\n\nIn [135]: alpha[2:2] = ['x', 'xx']\n\nIn [136]: alpha\nOut[136]: ['A', 'B', 'x', 'xx', 'c', 'd', 'e', 'f']\nSlicing With Step:Slicing With Step:Till now you have picked boxes continuously. But sometimes you need to pick up discretely. For example, you can pick up every second box. You can even pick up every third box from the end. This value is called step size. This represents the gap between your successive pickups. The step size should be positive if You are picking boxes from the beginning to end and vice versa.In [137]: alpha = ['a', 'b', 'c', 'd', 'e', 'f']\n\nIn [142]: alpha[1:5:2]\nOut[142]: ['b', 'd']\n\nIn [143]: alpha[-1:-5:-2]\nOut[143]: ['f', 'd']\n\nIn [144]: alpha[1:5:-2]\nOut[144]: []\n\nIn [145]: alpha[-1:-5:2]\nOut[145]: []\nIn [137]: alpha = ['a', 'b', 'c', 'd', 'e', 'f']\n\nIn [142]: alpha[1:5:2]\nOut[142]: ['b', 'd']\n\nIn [143]: alpha[-1:-5:-2]\nOut[143]: ['f', 'd']\n\nIn [144]: alpha[1:5:-2]\nOut[144]: []\n\nIn [145]: alpha[-1:-5:2]\nOut[145]: []\nHow Python Figures Out Missing Parameters:How Python Figures Out Missing Parameters:When slicing, if you leave out any parameter, Python tries to figure it out automatically.If you check the source code of CPython, you will find a function called PySlice_GetIndicesEx() which figures out indices to a slice for any given parameters. Here is the logical equivalent code in Python.CPythonThis function takes a Python object and optional parameters for slicing and returns the start, stop, step, and slice length for the requested slice.def py_slice_get_indices_ex(obj, start=None, stop=None, step=None):\n\n    length = len(obj)\n\n    if step is None:\n        step = 1\n    if step == 0:\n        raise Exception(\"Step cannot be zero.\")\n\n    if start is None:\n        start = 0 if step > 0 else length - 1\n    else:\n        if start < 0:\n            start += length\n        if start < 0:\n            start = 0 if step > 0 else -1\n        if start >= length:\n            start = length if step > 0 else length - 1\n\n    if stop is None:\n        stop = length if step > 0 else -1\n    else:\n        if stop < 0:\n            stop += length\n        if stop < 0:\n            stop = 0 if step > 0 else -1\n        if stop >= length:\n            stop = length if step > 0 else length - 1\n\n    if (step < 0 and stop >= start) or (step > 0 and start >= stop):\n        slice_length = 0\n    elif step < 0:\n        slice_length = (stop - start + 1)/(step) + 1\n    else:\n        slice_length = (stop - start - 1)/(step) + 1\n\n    return (start, stop, step, slice_length)\ndef py_slice_get_indices_ex(obj, start=None, stop=None, step=None):\n\n    length = len(obj)\n\n    if step is None:\n        step = 1\n    if step == 0:\n        raise Exception(\"Step cannot be zero.\")\n\n    if start is None:\n        start = 0 if step > 0 else length - 1\n    else:\n        if start < 0:\n            start += length\n        if start < 0:\n            start = 0 if step > 0 else -1\n        if start >= length:\n            start = length if step > 0 else length - 1\n\n    if stop is None:\n        stop = length if step > 0 else -1\n    else:\n        if stop < 0:\n            stop += length\n        if stop < 0:\n            stop = 0 if step > 0 else -1\n        if stop >= length:\n            stop = length if step > 0 else length - 1\n\n    if (step < 0 and stop >= start) or (step > 0 and start >= stop):\n        slice_length = 0\n    elif step < 0:\n        slice_length = (stop - start + 1)/(step) + 1\n    else:\n        slice_length = (stop - start - 1)/(step) + 1\n\n    return (start, stop, step, slice_length)\nThis is the intelligence that is present behind slices. Since Python has an built-in function called slice, you can pass some parameters and check how smartly it calculates missing parameters.In [21]: alpha = ['a', 'b', 'c', 'd', 'e', 'f']\n\nIn [22]: s = slice(None, None, None)\n\nIn [23]: s\nOut[23]: slice(None, None, None)\n\nIn [24]: s.indices(len(alpha))\nOut[24]: (0, 6, 1)\n\nIn [25]: range(*s.indices(len(alpha)))\nOut[25]: [0, 1, 2, 3, 4, 5]\n\nIn [26]: s = slice(None, None, -1)\n\nIn [27]: range(*s.indices(len(alpha)))\nOut[27]: [5, 4, 3, 2, 1, 0]\n\nIn [28]: s = slice(None, 3, -1)\n\nIn [29]: range(*s.indices(len(alpha)))\nOut[29]: [5, 4]\nIn [21]: alpha = ['a', 'b', 'c', 'd', 'e', 'f']\n\nIn [22]: s = slice(None, None, None)\n\nIn [23]: s\nOut[23]: slice(None, None, None)\n\nIn [24]: s.indices(len(alpha))\nOut[24]: (0, 6, 1)\n\nIn [25]: range(*s.indices(len(alpha)))\nOut[25]: [0, 1, 2, 3, 4, 5]\n\nIn [26]: s = slice(None, None, -1)\n\nIn [27]: range(*s.indices(len(alpha)))\nOut[27]: [5, 4, 3, 2, 1, 0]\n\nIn [28]: s = slice(None, 3, -1)\n\nIn [29]: range(*s.indices(len(alpha)))\nOut[29]: [5, 4]\nNote: This post was originally written in my blog, The Intelligence Behind Python Slices.Note:The Intelligence Behind Python SlicesThe Intelligence Behind Python Slices",
                "Python slicing notation:a[start:end:step]\na[start:end:step]\n\nFor start and end, negative values are interpreted as being relative to the end of the sequence.\nPositive indices for end indicate the position after the last element to be included.\nBlank values are defaulted as follows: [+0:-0:1].\nUsing a negative step reverses the interpretation of start and end\nFor start and end, negative values are interpreted as being relative to the end of the sequence.startendPositive indices for end indicate the position after the last element to be included.endafterBlank values are defaulted as follows: [+0:-0:1].[+0:-0:1]Using a negative step reverses the interpretation of start and endstartendThe notation extends to (numpy) matrices and multidimensional arrays.  For example, to slice entire columns you can use:m[::,0:2:] ## slice the first two columns\nm[::,0:2:] ## slice the first two columns\nSlices hold references, not copies, of the array elements.  If you want to make a separate copy an array, you can use deepcopy().deepcopy()deepcopy()",
                "You can also use slice assignment to remove one or more elements from a list:r = [1, 'blah', 9, 8, 2, 3, 4]\n>>> r[1:4] = []\n>>> r\n[1, 2, 3, 4]\nr = [1, 'blah', 9, 8, 2, 3, 4]\n>>> r[1:4] = []\n>>> r\n[1, 2, 3, 4]\n",
                "This is just for some extra info...\nConsider the list below >>> l=[12,23,345,456,67,7,945,467]\n>>> l=[12,23,345,456,67,7,945,467]\nFew other tricks for reversing the list:>>> l[len(l):-len(l)-1:-1]\n[467, 945, 7, 67, 456, 345, 23, 12]\n\n>>> l[:-len(l)-1:-1]\n[467, 945, 7, 67, 456, 345, 23, 12]\n\n>>> l[len(l)::-1]\n[467, 945, 7, 67, 456, 345, 23, 12]\n\n>>> l[::-1]\n[467, 945, 7, 67, 456, 345, 23, 12]\n\n>>> l[-1:-len(l)-1:-1]\n[467, 945, 7, 67, 456, 345, 23, 12]\n>>> l[len(l):-len(l)-1:-1]\n[467, 945, 7, 67, 456, 345, 23, 12]\n\n>>> l[:-len(l)-1:-1]\n[467, 945, 7, 67, 456, 345, 23, 12]\n\n>>> l[len(l)::-1]\n[467, 945, 7, 67, 456, 345, 23, 12]\n\n>>> l[::-1]\n[467, 945, 7, 67, 456, 345, 23, 12]\n\n>>> l[-1:-len(l)-1:-1]\n[467, 945, 7, 67, 456, 345, 23, 12]\n",
                "1. Slice NotationTo make it simple, remember slice has only one form\uff1aslice has only one form\uff1as[start:end:step]\ns[start:end:step]\nand here is how it works:\ns: an object that can be sliced\nstart: first index to start iteration\nend: last index, NOTE that end index will not be included in the resulted slice\nstep: pick element every step index\ns: an object that can be slicedsstart: first index to start iterationstartend: last index, NOTE that end index will not be included in the resulted sliceendNOTE that end index will not be included in the resulted sliceendstep: pick element every step indexstepstepAnother import thing: all start,end, step can be omitted! And if they are omitted, their default value will be used: 0,len(s),1 accordingly.all start,end, step can be omitted!startendstep0len(s)1So possible variations are:# Mostly used variations\ns[start:end]\ns[start:]\ns[:end]\n\n# Step-related variations\ns[:end:step]\ns[start::step]\ns[::step]\n\n# Make a copy\ns[:]\n# Mostly used variations\ns[start:end]\ns[start:]\ns[:end]\n\n# Step-related variations\ns[:end:step]\ns[start::step]\ns[::step]\n\n# Make a copy\ns[:]\nNOTE: If start >= end (considering only when step>0), Python will return a empty slice [].start >= endstep>0[]2. PitfallsThe above part explains the core features on how slice works, and it will work on most occasions. However, there can be pitfalls you should watch out, and this part explains them.Negative indexesThe very first thing that confuses Python learners is that an index can be negative!\nDon't panic: a negative index means count backwards.an index can be negative!a negative index means count backwards.For example:s[-5:]    # Start at the 5th index from the end of array,\n          # thus returning the last 5 elements.\ns[:-5]    # Start at index 0, and end until the 5th index from end of array,\n          # thus returning s[0:len(s)-5].\ns[-5:]    # Start at the 5th index from the end of array,\n          # thus returning the last 5 elements.\ns[:-5]    # Start at index 0, and end until the 5th index from end of array,\n          # thus returning s[0:len(s)-5].\nNegative stepMaking things more confusing is that step can be negative too!step can be negative too!stepA negative step means iterate the array backwards: from the end to start, with the end index included, and the start index excluded from the result.A negative step means iterate the array backwards: from the end to start, with the end index included, and the start index excluded from the result.NOTE: when step is negative, the default value for start is len(s) (while end does not equal to 0, because s[::-1] contains s[0]). For example:NOTEstartlen(s)end0s[::-1]s[0]s[::-1]            # Reversed slice\ns[len(s)::-1]      # The same as above, reversed slice\ns[0:len(s):-1]     # Empty list\ns[::-1]            # Reversed slice\ns[len(s)::-1]      # The same as above, reversed slice\ns[0:len(s):-1]     # Empty list\nOut of range error?Be surprised: slice does not raise an IndexError when the index is out of range!slice does not raise an IndexError when the index is out of range!If the index is out of range, Python will try its best to set the index to 0 or len(s) according to the situation. For example:0len(s)s[:len(s)+5]      # The same as s[:len(s)]\ns[-len(s)-5::]    # The same as s[0:]\ns[len(s)+5::-1]   # The same as s[len(s)::-1], and the same as s[::-1]\ns[:len(s)+5]      # The same as s[:len(s)]\ns[-len(s)-5::]    # The same as s[0:]\ns[len(s)+5::-1]   # The same as s[len(s)::-1], and the same as s[::-1]\n3. ExamplesLet's finish this answer with examples, explaining everything we have discussed:# Create our array for demonstration\nIn [1]: s = [i for i in range(10)]\n\nIn [2]: s\nOut[2]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nIn [3]: s[2:]   # From index 2 to last index\nOut[3]: [2, 3, 4, 5, 6, 7, 8, 9]\n\nIn [4]: s[:8]   # From index 0 up to index 8\nOut[4]: [0, 1, 2, 3, 4, 5, 6, 7]\n\nIn [5]: s[4:7]  # From index 4 (included) up to index 7(excluded)\nOut[5]: [4, 5, 6]\n\nIn [6]: s[:-2]  # Up to second last index (negative index)\nOut[6]: [0, 1, 2, 3, 4, 5, 6, 7]\n\nIn [7]: s[-2:]  # From second last index (negative index)\nOut[7]: [8, 9]\n\nIn [8]: s[::-1] # From last to first in reverse order (negative step)\nOut[8]: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\nIn [9]: s[::-2] # All odd numbers in reversed order\nOut[9]: [9, 7, 5, 3, 1]\n\nIn [11]: s[-2::-2] # All even numbers in reversed order\nOut[11]: [8, 6, 4, 2, 0]\n\nIn [12]: s[3:15]   # End is out of range, and Python will set it to len(s).\nOut[12]: [3, 4, 5, 6, 7, 8, 9]\n\nIn [14]: s[5:1]    # Start > end; return empty list\nOut[14]: []\n\nIn [15]: s[11]     # Access index 11 (greater than len(s)) will raise an IndexError\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-15-79ffc22473a3> in <module>()\n----> 1 s[11]\n\nIndexError: list index out of range\n# Create our array for demonstration\nIn [1]: s = [i for i in range(10)]\n\nIn [2]: s\nOut[2]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nIn [3]: s[2:]   # From index 2 to last index\nOut[3]: [2, 3, 4, 5, 6, 7, 8, 9]\n\nIn [4]: s[:8]   # From index 0 up to index 8\nOut[4]: [0, 1, 2, 3, 4, 5, 6, 7]\n\nIn [5]: s[4:7]  # From index 4 (included) up to index 7(excluded)\nOut[5]: [4, 5, 6]\n\nIn [6]: s[:-2]  # Up to second last index (negative index)\nOut[6]: [0, 1, 2, 3, 4, 5, 6, 7]\n\nIn [7]: s[-2:]  # From second last index (negative index)\nOut[7]: [8, 9]\n\nIn [8]: s[::-1] # From last to first in reverse order (negative step)\nOut[8]: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n\nIn [9]: s[::-2] # All odd numbers in reversed order\nOut[9]: [9, 7, 5, 3, 1]\n\nIn [11]: s[-2::-2] # All even numbers in reversed order\nOut[11]: [8, 6, 4, 2, 0]\n\nIn [12]: s[3:15]   # End is out of range, and Python will set it to len(s).\nOut[12]: [3, 4, 5, 6, 7, 8, 9]\n\nIn [14]: s[5:1]    # Start > end; return empty list\nOut[14]: []\n\nIn [15]: s[11]     # Access index 11 (greater than len(s)) will raise an IndexError\n---------------------------------------------------------------------------\nIndexError                                Traceback (most recent call last)\n<ipython-input-15-79ffc22473a3> in <module>()\n----> 1 s[11]\n\nIndexError: list index out of range\n",
                "As a general rule, writing code with a lot of hardcoded index values leads to a readability\nand maintenance mess. For example, if you come back to the code a year later, you\u2019ll\nlook at it and wonder what you were thinking when you wrote it. The solution shown\nis simply a way of more clearly stating what your code is actually doing.\nIn general, the built-in slice() creates a slice object that can be used anywhere a slice\nis allowed. For example:>>> items = [0, 1, 2, 3, 4, 5, 6]\n>>> a = slice(2, 4)\n>>> items[2:4]\n[2, 3]\n>>> items[a]\n[2, 3]\n>>> items[a] = [10,11]\n>>> items\n[0, 1, 10, 11, 4, 5, 6]\n>>> del items[a]\n>>> items\n[0, 1, 4, 5, 6]\n>>> items = [0, 1, 2, 3, 4, 5, 6]\n>>> a = slice(2, 4)\n>>> items[2:4]\n[2, 3]\n>>> items[a]\n[2, 3]\n>>> items[a] = [10,11]\n>>> items\n[0, 1, 10, 11, 4, 5, 6]\n>>> del items[a]\n>>> items\n[0, 1, 4, 5, 6]\nIf you have a slice instance s, you can get more information about it by looking at its\ns.start, s.stop, and s.step attributes, respectively. For example:\n>>> a = slice(10, 50, 2)\n>>> a.start\n10\n>>> a.stop\n50\n>>> a.step\n2\n>>>\n\n>>> a = slice(10, 50, 2)\n>>> a.start\n10\n>>> a.stop\n50\n>>> a.step\n2\n>>>\n>>> a = slice(10, 50, 2)\n>>> a.start\n10\n>>> a.stop\n50\n>>> a.step\n2\n>>>\n",
                "The previous answers don't discuss multi-dimensional array slicing which is possible using the famous NumPy package:NumPySlicing can also be applied to multi-dimensional arrays.Slicing can also be applied to multi-dimensional arrays.# Here, a is a NumPy array\n\n>>> a\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n>>> a[:2, 0:3:2]\narray([[1, 3],\n       [5, 7]])\n# Here, a is a NumPy array\n\n>>> a\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n>>> a[:2, 0:3:2]\narray([[1, 3],\n       [5, 7]])\nThe \":2\" before the comma operates on the first dimension and the \"0:3:2\" after the comma operates on the second dimension.:20:3:2",
                "The rules of slicing are as follows:[lower bound : upper bound : step size]\n[lower bound : upper bound : step size]\nI- Convert upper bound  and lower bound into common signs.I-upper boundlower boundII- Then check if the step size is a positive or a negative value.II-step sizepositivenegative(i) If the step size is a positive value, upper bound should be greater than lower bound, otherwise empty string is printed. For example:(i)step sizepositive valueupper boundgreater thanlower boundempty stringFor examples=\"Welcome\"\ns1=s[0:3:1]\nprint(s1)\ns=\"Welcome\"\ns1=s[0:3:1]\nprint(s1)\nThe output:Wel\nWel\nHowever if we run the following code:s=\"Welcome\"\ns1=s[3:0:1]\nprint(s1)\ns=\"Welcome\"\ns1=s[3:0:1]\nprint(s1)\nIt will return an empty string.empty string(ii) If the step size if a negative value, upper bound should be lesser than lower bound, otherwise empty string will be printed. For example:(ii)step sizenegative valueupper boundlesser thanlower boundempty strings=\"Welcome\"\ns1=s[3:0:-1]\nprint(s1)\ns=\"Welcome\"\ns1=s[3:0:-1]\nprint(s1)\nThe output:cle\ncle\nBut if we run the following code:s=\"Welcome\"\ns1=s[0:5:-1]\nprint(s1)\ns=\"Welcome\"\ns1=s[0:5:-1]\nprint(s1)\nThe output will be an empty string.empty stringThus in the code:str = 'abcd'\nl = len(str)\nstr2 = str[l-1:0:-1]    #str[3:0:-1] \nprint(str2)\nstr2 = str[l-1:-1:-1]    #str[3:-1:-1]\nprint(str2)\nstr = 'abcd'\nl = len(str)\nstr2 = str[l-1:0:-1]    #str[3:0:-1] \nprint(str2)\nstr2 = str[l-1:-1:-1]    #str[3:-1:-1]\nprint(str2)\nIn the first str2=str[l-1:0:-1], the upper bound is lesser than the lower bound, thus dcb is printed.str2=str[l-1:0:-1]upper boundlesser thanlower bounddcbHowever in str2=str[l-1:-1:-1], the upper bound is not less than the lower bound (upon converting lower bound into negative value which is -1: since index of last element is -1 as well as 3).str2=str[l-1:-1:-1]upper boundnot less thanlower boundlower boundnegative value-1index",
                "In my opinion, you will understand and memorize better the Python string slicing notation if you look at it the following way (read on).Let's work with the following string ...azString = \"abcdefghijklmnopqrstuvwxyz\"\nazString = \"abcdefghijklmnopqrstuvwxyz\"\nFor those who don't know, you can create any substring from azString using the notation azString[x:y]azStringazString[x:y]Coming from other programming languages, that's when the common sense gets compromised. What are x and y?I had to sit down and run several scenarios in my quest for a memorization technique that will help me remember what x and y are and help me slice strings properly at the first attempt.My conclusion is that x and y should be seen as the boundary indexes that are surrounding the strings that we want to extra. So we should see the expression as azString[index1, index2] or even more clearer as azString[index_of_first_character, index_after_the_last_character].azString[index1, index2]azString[index_of_first_character, index_after_the_last_character]Here is an example visualization of that ...Letters   a b c d e f g h i j ...\n         \u2191 \u2191 \u2191 \u2191 \u2191 \u2191 \u2191 \u2191 \u2191 \u2191\n             \u250a           \u250a\nIndexes  0 1 2 3 4 5 6 7 8 9 ...\n             \u250a           \u250a\ncdefgh    index1       index2\nLetters   a b c d e f g h i j ...\n         \u2191 \u2191 \u2191 \u2191 \u2191 \u2191 \u2191 \u2191 \u2191 \u2191\n             \u250a           \u250a\nIndexes  0 1 2 3 4 5 6 7 8 9 ...\n             \u250a           \u250a\ncdefgh    index1       index2\nSo all you have to do is setting index1 and index2 to the values that will surround the desired substring. For instance, to get the substring \"cdefgh\", you can use azString[2:8], because the index on the left side of \"c\" is 2 and the one on the right size of \"h\" is 8.azString[2:8]Remember that we are setting the boundaries. And those boundaries are the positions where you could place some brackets that will be wrapped around the substring like this ...a b [ c d e f g h ] i j[]That trick works all the time and is easy to memorize.",
                "I personally think about it like a for loop:fora[start:end:step]\n# for(i = start; i < end; i += step)\na[start:end:step]\n# for(i = start; i < end; i += step)\nAlso, note that negative values for start and end are relative to the end of the list and computed in the example above by given_index + a.shape[0].startendgiven_index + a.shape[0]",
                "#!/usr/bin/env python\n\ndef slicegraphical(s, lista):\n\n    if len(s) > 9:\n        print \"\"\"Enter a string of maximum 9 characters,\n    so the printig would looki nice\"\"\"\n        return 0;\n    # print \" \",\n    print '  '+'+---' * len(s) +'+'\n    print ' ',\n    for letter in s:\n        print '| {}'.format(letter),\n    print '|'\n    print \" \",; print '+---' * len(s) +'+'\n\n    print \" \",\n    for letter in range(len(s) +1):\n        print '{}  '.format(letter),\n    print \"\"\n    for letter in range(-1*(len(s)), 0):\n        print ' {}'.format(letter),\n    print ''\n    print ''\n\n\n    for triada in lista:\n        if len(triada) == 3:\n            if triada[0]==None and triada[1] == None and triada[2] == None:\n                # 000\n                print s+'[   :   :   ]' +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] == None and triada[1] == None and triada[2] != None:\n                # 001\n                print s+'[   :   :{0:2d} ]'.format(triada[2], '','') +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] == None and triada[1] != None and triada[2] == None:\n                # 010\n                print s+'[   :{0:2d} :   ]'.format(triada[1]) +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] == None and triada[1] != None and triada[2] != None:\n                # 011\n                print s+'[   :{0:2d} :{1:2d} ]'.format(triada[1], triada[2]) +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] != None and triada[1] == None and triada[2] == None:\n                # 100\n                print s+'[{0:2d} :   :   ]'.format(triada[0]) +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] != None and triada[1] == None and triada[2] != None:\n                # 101\n                print s+'[{0:2d} :   :{1:2d} ]'.format(triada[0], triada[2]) +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] != None and triada[1] != None and triada[2] == None:\n                # 110\n                print s+'[{0:2d} :{1:2d} :   ]'.format(triada[0], triada[1]) +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] != None and triada[1] != None and triada[2] != None:\n                # 111\n                print s+'[{0:2d} :{1:2d} :{2:2d} ]'.format(triada[0], triada[1], triada[2]) +' = ', s[triada[0]:triada[1]:triada[2]]\n\n        elif len(triada) == 2:\n            if triada[0] == None and triada[1] == None:\n                # 00\n                print s+'[   :   ]    ' + ' = ', s[triada[0]:triada[1]]\n            elif triada[0] == None and triada[1] != None:\n                # 01\n                print s+'[   :{0:2d} ]    '.format(triada[1]) + ' = ', s[triada[0]:triada[1]]\n            elif triada[0] != None and triada[1] == None:\n                # 10\n                print s+'[{0:2d} :   ]    '.format(triada[0]) + ' = ', s[triada[0]:triada[1]]\n            elif triada[0] != None and triada[1] != None:\n                # 11\n                print s+'[{0:2d} :{1:2d} ]    '.format(triada[0],triada[1]) + ' = ', s[triada[0]:triada[1]]\n\n        elif len(triada) == 1:\n            print s+'[{0:2d} ]        '.format(triada[0]) + ' = ', s[triada[0]]\n\n\nif __name__ == '__main__':\n    # Change \"s\" to what ever string you like, make it 9 characters for\n    # better representation.\n    s = 'COMPUTERS'\n\n    # add to this list different lists to experement with indexes\n    # to represent ex. s[::], use s[None, None,None], otherwise you get an error\n    # for s[2:] use s[2:None]\n\n    lista = [[4,7],[2,5,2],[-5,1,-1],[4],[-4,-6,-1], [2,-3,1],[2,-3,-1], [None,None,-1],[-5,None],[-5,0,-1],[-5,None,-1],[-1,1,-2]]\n\n    slicegraphical(s, lista)\n#!/usr/bin/env python\n\ndef slicegraphical(s, lista):\n\n    if len(s) > 9:\n        print \"\"\"Enter a string of maximum 9 characters,\n    so the printig would looki nice\"\"\"\n        return 0;\n    # print \" \",\n    print '  '+'+---' * len(s) +'+'\n    print ' ',\n    for letter in s:\n        print '| {}'.format(letter),\n    print '|'\n    print \" \",; print '+---' * len(s) +'+'\n\n    print \" \",\n    for letter in range(len(s) +1):\n        print '{}  '.format(letter),\n    print \"\"\n    for letter in range(-1*(len(s)), 0):\n        print ' {}'.format(letter),\n    print ''\n    print ''\n\n\n    for triada in lista:\n        if len(triada) == 3:\n            if triada[0]==None and triada[1] == None and triada[2] == None:\n                # 000\n                print s+'[   :   :   ]' +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] == None and triada[1] == None and triada[2] != None:\n                # 001\n                print s+'[   :   :{0:2d} ]'.format(triada[2], '','') +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] == None and triada[1] != None and triada[2] == None:\n                # 010\n                print s+'[   :{0:2d} :   ]'.format(triada[1]) +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] == None and triada[1] != None and triada[2] != None:\n                # 011\n                print s+'[   :{0:2d} :{1:2d} ]'.format(triada[1], triada[2]) +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] != None and triada[1] == None and triada[2] == None:\n                # 100\n                print s+'[{0:2d} :   :   ]'.format(triada[0]) +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] != None and triada[1] == None and triada[2] != None:\n                # 101\n                print s+'[{0:2d} :   :{1:2d} ]'.format(triada[0], triada[2]) +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] != None and triada[1] != None and triada[2] == None:\n                # 110\n                print s+'[{0:2d} :{1:2d} :   ]'.format(triada[0], triada[1]) +' = ', s[triada[0]:triada[1]:triada[2]]\n            elif triada[0] != None and triada[1] != None and triada[2] != None:\n                # 111\n                print s+'[{0:2d} :{1:2d} :{2:2d} ]'.format(triada[0], triada[1], triada[2]) +' = ', s[triada[0]:triada[1]:triada[2]]\n\n        elif len(triada) == 2:\n            if triada[0] == None and triada[1] == None:\n                # 00\n                print s+'[   :   ]    ' + ' = ', s[triada[0]:triada[1]]\n            elif triada[0] == None and triada[1] != None:\n                # 01\n                print s+'[   :{0:2d} ]    '.format(triada[1]) + ' = ', s[triada[0]:triada[1]]\n            elif triada[0] != None and triada[1] == None:\n                # 10\n                print s+'[{0:2d} :   ]    '.format(triada[0]) + ' = ', s[triada[0]:triada[1]]\n            elif triada[0] != None and triada[1] != None:\n                # 11\n                print s+'[{0:2d} :{1:2d} ]    '.format(triada[0],triada[1]) + ' = ', s[triada[0]:triada[1]]\n\n        elif len(triada) == 1:\n            print s+'[{0:2d} ]        '.format(triada[0]) + ' = ', s[triada[0]]\n\n\nif __name__ == '__main__':\n    # Change \"s\" to what ever string you like, make it 9 characters for\n    # better representation.\n    s = 'COMPUTERS'\n\n    # add to this list different lists to experement with indexes\n    # to represent ex. s[::], use s[None, None,None], otherwise you get an error\n    # for s[2:] use s[2:None]\n\n    lista = [[4,7],[2,5,2],[-5,1,-1],[4],[-4,-6,-1], [2,-3,1],[2,-3,-1], [None,None,-1],[-5,None],[-5,0,-1],[-5,None,-1],[-1,1,-2]]\n\n    slicegraphical(s, lista)\nYou can run this script and experiment with it, below is some samples that I got from the script.  +---+---+---+---+---+---+---+---+---+\n  | C | O | M | P | U | T | E | R | S |\n  +---+---+---+---+---+---+---+---+---+\n  0   1   2   3   4   5   6   7   8   9   \n -9  -8  -7  -6  -5  -4  -3  -2  -1 \n\nCOMPUTERS[ 4 : 7 ]     =  UTE\nCOMPUTERS[ 2 : 5 : 2 ] =  MU\nCOMPUTERS[-5 : 1 :-1 ] =  UPM\nCOMPUTERS[ 4 ]         =  U\nCOMPUTERS[-4 :-6 :-1 ] =  TU\nCOMPUTERS[ 2 :-3 : 1 ] =  MPUT\nCOMPUTERS[ 2 :-3 :-1 ] =  \nCOMPUTERS[   :   :-1 ] =  SRETUPMOC\nCOMPUTERS[-5 :   ]     =  UTERS\nCOMPUTERS[-5 : 0 :-1 ] =  UPMO\nCOMPUTERS[-5 :   :-1 ] =  UPMOC\nCOMPUTERS[-1 : 1 :-2 ] =  SEUM\n[Finished in 0.9s]\n  +---+---+---+---+---+---+---+---+---+\n  | C | O | M | P | U | T | E | R | S |\n  +---+---+---+---+---+---+---+---+---+\n  0   1   2   3   4   5   6   7   8   9   \n -9  -8  -7  -6  -5  -4  -3  -2  -1 \n\nCOMPUTERS[ 4 : 7 ]     =  UTE\nCOMPUTERS[ 2 : 5 : 2 ] =  MU\nCOMPUTERS[-5 : 1 :-1 ] =  UPM\nCOMPUTERS[ 4 ]         =  U\nCOMPUTERS[-4 :-6 :-1 ] =  TU\nCOMPUTERS[ 2 :-3 : 1 ] =  MPUT\nCOMPUTERS[ 2 :-3 :-1 ] =  \nCOMPUTERS[   :   :-1 ] =  SRETUPMOC\nCOMPUTERS[-5 :   ]     =  UTERS\nCOMPUTERS[-5 : 0 :-1 ] =  UPMO\nCOMPUTERS[-5 :   :-1 ] =  UPMOC\nCOMPUTERS[-1 : 1 :-2 ] =  SEUM\n[Finished in 0.9s]\nWhen using a negative step, notice that the answer is shifted to the right by 1.",
                "My brain seems happy to accept that lst[start:end] contains the start-th item. I might even say that it is a 'natural assumption'.lst[start:end]startBut occasionally a doubt creeps in and my brain asks for reassurance that it does not contain the end-th element.endIn these moments I rely on this simple theorem:for any n,    lst = lst[:n] + lst[n:]\nfor any n,    lst = lst[:n] + lst[n:]\nThis pretty property tells me that lst[start:end] does not contain the end-th item because it is in lst[end:].lst[start:end]endlst[end:]Note that this theorem is true for any n at all. For example, you can check thatnlst = range(10)\nlst[:-42] + lst[-42:] == lst\nlst = range(10)\nlst[:-42] + lst[-42:] == lst\nreturns True.True",
                "In Python, the most basic form for slicing is the following:l[start:end]\nl[start:end]\nwhere l is some collection, start is an inclusive index, and end is an exclusive index.lstartendIn [1]: l = list(range(10))\n\nIn [2]: l[:5] # First five elements\nOut[2]: [0, 1, 2, 3, 4]\n\nIn [3]: l[-5:] # Last five elements\nOut[3]: [5, 6, 7, 8, 9]\nIn [1]: l = list(range(10))\n\nIn [2]: l[:5] # First five elements\nOut[2]: [0, 1, 2, 3, 4]\n\nIn [3]: l[-5:] # Last five elements\nOut[3]: [5, 6, 7, 8, 9]\nWhen slicing from the start, you can omit the zero index, and when slicing to the end, you can omit the final index since it is redundant, so do not be verbose:In [5]: l[:3] == l[0:3]\nOut[5]: True\n\nIn [6]: l[7:] == l[7:len(l)]\nOut[6]: True\nIn [5]: l[:3] == l[0:3]\nOut[5]: True\n\nIn [6]: l[7:] == l[7:len(l)]\nOut[6]: True\nNegative integers are useful when doing offsets relative to the end of a collection:In [7]: l[:-1] # Include all elements but the last one\nOut[7]: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n\nIn [8]: l[-3:] # Take the last three elements\nOut[8]: [7, 8, 9]\nIn [7]: l[:-1] # Include all elements but the last one\nOut[7]: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n\nIn [8]: l[-3:] # Take the last three elements\nOut[8]: [7, 8, 9]\nIt is possible to provide indices that are out of bounds when slicing such as:In [9]: l[:20] # 20 is out of index bounds, and l[20] will raise an IndexError exception\nOut[9]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nIn [11]: l[-20:] # -20 is out of index bounds, and l[-20] will raise an IndexError exception\nOut[11]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nIn [9]: l[:20] # 20 is out of index bounds, and l[20] will raise an IndexError exception\nOut[9]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n\nIn [11]: l[-20:] # -20 is out of index bounds, and l[-20] will raise an IndexError exception\nOut[11]: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nKeep in mind that the result of slicing a collection is a whole new collection. In addition, when using slice notation in assignments, the length of the slice assignments do not need to be the same. The values before and after the assigned slice will be kept, and the collection will shrink or grow to contain the new values:In [16]: l[2:6] = list('abc') # Assigning fewer elements than the ones contained in the sliced collection l[2:6]\n\nIn [17]: l\nOut[17]: [0, 1, 'a', 'b', 'c', 6, 7, 8, 9]\n\nIn [18]: l[2:5] = list('hello') # Assigning more elements than the ones contained in the sliced collection l [2:5]\n\nIn [19]: l\nOut[19]: [0, 1, 'h', 'e', 'l', 'l', 'o', 6, 7, 8, 9]\nIn [16]: l[2:6] = list('abc') # Assigning fewer elements than the ones contained in the sliced collection l[2:6]\n\nIn [17]: l\nOut[17]: [0, 1, 'a', 'b', 'c', 6, 7, 8, 9]\n\nIn [18]: l[2:5] = list('hello') # Assigning more elements than the ones contained in the sliced collection l [2:5]\n\nIn [19]: l\nOut[19]: [0, 1, 'h', 'e', 'l', 'l', 'o', 6, 7, 8, 9]\nIf you omit the start and end index, you will make a copy of the collection:In [14]: l_copy = l[:]\n\nIn [15]: l == l_copy and l is not l_copy\nOut[15]: True\nIn [14]: l_copy = l[:]\n\nIn [15]: l == l_copy and l is not l_copy\nOut[15]: True\nIf the start and end indexes are omitted when performing an assignment operation, the entire content of the collection will be replaced with a copy of what is referenced:In [20]: l[:] = list('hello...')\n\nIn [21]: l\nOut[21]: ['h', 'e', 'l', 'l', 'o', '.', '.', '.']\nIn [20]: l[:] = list('hello...')\n\nIn [21]: l\nOut[21]: ['h', 'e', 'l', 'l', 'o', '.', '.', '.']\nBesides basic slicing, it is also possible to apply the following notation:l[start:end:step]\nl[start:end:step]\nwhere l is a collection, start is an inclusive index, end is an exclusive index, and step is a stride that can be used to take every nth item in l.lstartendstepnthlIn [22]: l = list(range(10))\n\nIn [23]: l[::2] # Take the elements which indexes are even\nOut[23]: [0, 2, 4, 6, 8]\n\nIn [24]: l[1::2] # Take the elements which indexes are odd\nOut[24]: [1, 3, 5, 7, 9]\nIn [22]: l = list(range(10))\n\nIn [23]: l[::2] # Take the elements which indexes are even\nOut[23]: [0, 2, 4, 6, 8]\n\nIn [24]: l[1::2] # Take the elements which indexes are odd\nOut[24]: [1, 3, 5, 7, 9]\nUsing step provides a useful trick to reverse a collection in Python:stepIn [25]: l[::-1]\nOut[25]: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\nIn [25]: l[::-1]\nOut[25]: [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\nIt is also possible to use negative integers for step as the following example:stepIn[28]:  l[::-2]\nOut[28]: [9, 7, 5, 3, 1]\nIn[28]:  l[::-2]\nOut[28]: [9, 7, 5, 3, 1]\nHowever, using a negative value for step could become very confusing. Moreover, in order to be Pythonic, you should avoid using start, end, and step in a single slice. In case this is required, consider doing this in two assignments (one to slice, and the other to stride).stepPythonicstartendstepIn [29]: l = l[::2] # This step is for striding\n\nIn [30]: l\nOut[30]: [0, 2, 4, 6, 8]\n\nIn [31]: l = l[1:-1] # This step is for slicing\n\nIn [32]: l\nOut[32]: [2, 4, 6]\nIn [29]: l = l[::2] # This step is for striding\n\nIn [30]: l\nOut[30]: [0, 2, 4, 6, 8]\n\nIn [31]: l = l[1:-1] # This step is for slicing\n\nIn [32]: l\nOut[32]: [2, 4, 6]\n",
                "I want to add one Hello, World! example that explains the basics of slices for the very beginners. It helped me a lot.Hello, World!Let's have a list with six values ['P', 'Y', 'T', 'H', 'O', 'N']:['P', 'Y', 'T', 'H', 'O', 'N']+---+---+---+---+---+---+\n| P | Y | T | H | O | N |\n+---+---+---+---+---+---+\n  0   1   2   3   4   5\n+---+---+---+---+---+---+\n| P | Y | T | H | O | N |\n+---+---+---+---+---+---+\n  0   1   2   3   4   5\nNow the simplest slices of that list are its sublists. The notation is [<index>:<index>] and the key is to read it like this:[<index>:<index>][ start cutting before this index : end cutting before this index ]\n[ start cutting before this index : end cutting before this index ]\nNow if you make a slice [2:5] of the list above, this will happen:[2:5]        |           |\n+---+---|---+---+---|---+\n| P | Y | T | H | O | N |\n+---+---|---+---+---|---+\n  0   1 | 2   3   4 | 5\n        |           |\n+---+---|---+---+---|---+\n| P | Y | T | H | O | N |\n+---+---|---+---+---|---+\n  0   1 | 2   3   4 | 5\nYou made a cut before the element with index 2 and another cut before the element with index 5. So the result will be a slice between those two cuts, a list ['T', 'H', 'O'].before2before5['T', 'H', 'O']",
                "Most of the previous answers clears up questions about slice notation.The extended indexing syntax used for slicing is aList[start:stop:step], and basic examples are:aList[start:stop:step]:More slicing examples: 15 Extended Slices15 Extended Slices",
                "The below is the example of an index of a string: +---+---+---+---+---+\n | H | e | l | p | A |\n +---+---+---+---+---+\n 0   1   2   3   4   5\n-5  -4  -3  -2  -1\n\nstr=\"Name string\"\n +---+---+---+---+---+\n | H | e | l | p | A |\n +---+---+---+---+---+\n 0   1   2   3   4   5\n-5  -4  -3  -2  -1\n\nstr=\"Name string\"\nSlicing example: [start:end:step]str[start:end] # Items start through end-1\nstr[start:]    # Items start through the rest of the array\nstr[:end]      # Items from the beginning through end-1\nstr[:]         # A copy of the whole array\nstr[start:end] # Items start through end-1\nstr[start:]    # Items start through the rest of the array\nstr[:end]      # Items from the beginning through end-1\nstr[:]         # A copy of the whole array\nBelow is the example usage:print str[0] = N\nprint str[0:2] = Na\nprint str[0:7] = Name st\nprint str[0:7:2] = Nm t\nprint str[0:-1:2] = Nm ti\nprint str[0] = N\nprint str[0:2] = Na\nprint str[0:7] = Name st\nprint str[0:7:2] = Nm t\nprint str[0:-1:2] = Nm ti\n",
                "If you feel negative indices in slicing is confusing, here's a very easy way to think about it: just replace the negative index with len - index. So for example, replace -3 with len(list) - 3.len - indexlen(list) - 3The best way to illustrate what slicing does internally is just show it in code that implements this operation:def slice(list, start = None, end = None, step = 1):\n  # Take care of missing start/end parameters\n  start = 0 if start is None else start\n  end = len(list) if end is None else end\n\n  # Take care of negative start/end parameters\n  start = len(list) + start if start < 0 else start\n  end = len(list) + end if end < 0 else end\n\n  # Now just execute a for-loop with start, end and step\n  return [list[i] for i in range(start, end, step)]\ndef slice(list, start = None, end = None, step = 1):\n  # Take care of missing start/end parameters\n  start = 0 if start is None else start\n  end = len(list) if end is None else end\n\n  # Take care of negative start/end parameters\n  start = len(list) + start if start < 0 else start\n  end = len(list) + end if end < 0 else end\n\n  # Now just execute a for-loop with start, end and step\n  return [list[i] for i in range(start, end, step)]\n",
                "I don't think that the Python tutorial diagram (cited in various other answers) is good as this suggestion works for positive stride, but does not for a negative stride.Python tutorialThis is the diagram: +---+---+---+---+---+---+\n | P | y | t | h | o | n |\n +---+---+---+---+---+---+\n 0   1   2   3   4   5   6\n-6  -5  -4  -3  -2  -1\n\n +---+---+---+---+---+---+\n | P | y | t | h | o | n |\n +---+---+---+---+---+---+\n 0   1   2   3   4   5   6\n-6  -5  -4  -3  -2  -1\n\nFrom the diagram, I expect a[-4,-6,-1] to be yP but it is ty.a[-4,-6,-1]yPty>>> a = \"Python\"\n>>> a[2:4:1] # as expected\n'th'\n>>> a[-4:-6:-1] # off by 1\n'ty'\n>>> a = \"Python\"\n>>> a[2:4:1] # as expected\n'th'\n>>> a[-4:-6:-1] # off by 1\n'ty'\nWhat always work is to think in characters or slots and use indexing as a half-open interval -- right-open if positive stride, left-open if negative stride.This way, I can think of a[-4:-6:-1] as a(-6,-4] in interval terminology.a[-4:-6:-1]a(-6,-4] +---+---+---+---+---+---+\n | P | y | t | h | o | n |\n +---+---+---+---+---+---+\n   0   1   2   3   4   5  \n  -6  -5  -4  -3  -2  -1\n\n +---+---+---+---+---+---+---+---+---+---+---+---+\n | P | y | t | h | o | n | P | y | t | h | o | n |\n +---+---+---+---+---+---+---+---+---+---+---+---+\n  -6  -5  -4  -3  -2  -1   0   1   2   3   4   5  \n +---+---+---+---+---+---+\n | P | y | t | h | o | n |\n +---+---+---+---+---+---+\n   0   1   2   3   4   5  \n  -6  -5  -4  -3  -2  -1\n\n +---+---+---+---+---+---+---+---+---+---+---+---+\n | P | y | t | h | o | n | P | y | t | h | o | n |\n +---+---+---+---+---+---+---+---+---+---+---+---+\n  -6  -5  -4  -3  -2  -1   0   1   2   3   4   5  \n"
            ]
        },
        {
            "tag": "list_index",
            "patterns": [
                "Finding the index of an item in a list",
                "finding the index of an item in a list", 
                "How to find the index of element"
            ],
            "responses": [
                "The simplest case is handled by the built-in .index method of the list:built-in .index method of the list.index\nlist.index(x[, start[, end]])\n\nReturn zero-based index in the list of the first item whose value is equal to x. Raises a ValueError if there is no such item.\nThe optional arguments start and end are interpreted as in the slice notation and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the start argument.\nlist.index(x[, start[, end]])\nlist.index(x[, start[, end]])\nReturn zero-based index in the list of the first item whose value is equal to x. Raises a ValueError if there is no such item.xValueErrorValueErrorThe optional arguments start and end are interpreted as in the slice notation and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the start argument.startendslice notationThus, we can do:>>> [\"foo\", \"bar\", \"baz\"].index(\"bar\")\n1\n>>> [\"foo\", \"bar\", \"baz\"].index(\"bar\")\n1\nCaveatsLinear time-complexity in list lengthAn index call checks every element of the list in order, until it finds a match. If the list is long, and if there is no guarantee that the value will be near the beginning, this can slow down the code.indexThis problem can only be completely avoided by using a different data structure. However, if the element is known to be within a certain part of the list, the start and end parameters can be used to narrow the search.startendFor example:>>> import timeit\n>>> timeit.timeit('l.index(999_999)', setup='l = list(range(0, 1_000_000))', number=1000)\n9.356267921015387\n>>> timeit.timeit('l.index(999_999, 999_990, 1_000_000)', setup='l = list(range(0, 1_000_000))', number=1000)\n0.0004404920036904514\n>>> import timeit\n>>> timeit.timeit('l.index(999_999)', setup='l = list(range(0, 1_000_000))', number=1000)\n9.356267921015387\n>>> timeit.timeit('l.index(999_999, 999_990, 1_000_000)', setup='l = list(range(0, 1_000_000))', number=1000)\n0.0004404920036904514\nThe second call is orders of magnitude faster, because it only has to search through 10 elements, rather than all 1 million.Only the index of the first match is returnedfirst matchA call to index searches through the list in order until it finds a match, and stops there. If there could be more than one occurrence of the value, and all indices are needed, index cannot solve the problem:indexstops there.index>>> [1, 1].index(1) # the `1` index is not found.\n0\n>>> [1, 1].index(1) # the `1` index is not found.\n0\nInstead, use a list comprehension or generator expression to do the search, with enumerate to get indices:list comprehension or generator expression to do the searchenumerate to get indicesenumerate>>> # A list comprehension gives a list of indices directly:\n>>> [i for i, e in enumerate([1, 2, 1]) if e == 1]\n[0, 2]\n>>> # A generator comprehension gives us an iterable object...\n>>> g = (i for i, e in enumerate([1, 2, 1]) if e == 1)\n>>> # which can be used in a `for` loop, or manually iterated with `next`:\n>>> next(g)\n0\n>>> next(g)\n2\n>>> # A list comprehension gives a list of indices directly:\n>>> [i for i, e in enumerate([1, 2, 1]) if e == 1]\n[0, 2]\n>>> # A generator comprehension gives us an iterable object...\n>>> g = (i for i, e in enumerate([1, 2, 1]) if e == 1)\n>>> # which can be used in a `for` loop, or manually iterated with `next`:\n>>> next(g)\n0\n>>> next(g)\n2\nThe list comprehension and generator expression techniques still work if there is only one match, and are more generalizable.Raises an exception if there is no matchAs noted in the documentation above, using .index will raise an exception if the searched-for value is not in the list:.index>>> [1, 1].index(2)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: 2 is not in list\n>>> [1, 1].index(2)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nValueError: 2 is not in list\nIf this is a concern, either explicitly check first using item in my_list, or handle the exception with try/except as appropriate.explicitly check firstitem in my_listtryexceptThe explicit check is simple and readable, but it must iterate the list a second time. See What is the EAFP principle in Python? for more guidance on this choice.What is the EAFP principle in Python?",
                "The majority of answers explain how to find a single index, but their methods do not return multiple indexes if the item is in the list multiple times. Use enumerate():a single indexenumerate()enumerate()for i, j in enumerate(['foo', 'bar', 'baz']):\n    if j == 'bar':\n        print(i)\nfor i, j in enumerate(['foo', 'bar', 'baz']):\n    if j == 'bar':\n        print(i)\nThe index() function only returns the first occurrence, while enumerate() returns all occurrences.index()enumerate()As a list comprehension:[i for i, j in enumerate(['foo', 'bar', 'baz']) if j == 'bar']\n[i for i, j in enumerate(['foo', 'bar', 'baz']) if j == 'bar']\nHere's also another small solution with itertools.count() (which is pretty much the same approach as enumerate):itertools.count()itertools.count()from itertools import izip as zip, count # izip for maximum efficiency\n[i for i, j in zip(count(), ['foo', 'bar', 'baz']) if j == 'bar']\nfrom itertools import izip as zip, count # izip for maximum efficiency\n[i for i, j in zip(count(), ['foo', 'bar', 'baz']) if j == 'bar']\nThis is more efficient for larger lists than using enumerate():enumerate()$ python -m timeit -s \"from itertools import izip as zip, count\" \"[i for i, j in zip(count(), ['foo', 'bar', 'baz']*500) if j == 'bar']\"\n10000 loops, best of 3: 174 usec per loop\n$ python -m timeit \"[i for i, j in enumerate(['foo', 'bar', 'baz']*500) if j == 'bar']\"\n10000 loops, best of 3: 196 usec per loop\n$ python -m timeit -s \"from itertools import izip as zip, count\" \"[i for i, j in zip(count(), ['foo', 'bar', 'baz']*500) if j == 'bar']\"\n10000 loops, best of 3: 174 usec per loop\n$ python -m timeit \"[i for i, j in enumerate(['foo', 'bar', 'baz']*500) if j == 'bar']\"\n10000 loops, best of 3: 196 usec per loop\n",
                "To get all indexes:indexes = [i for i, x in enumerate(xs) if x == 'foo']\nindexes = [i for i, x in enumerate(xs) if x == 'foo']\n",
                "index() returns the first index of value!index()first\n|  index(...)\n   |      L.index(value, [start, [stop]]) -> integer -- return first index of value\n|  index(...)\n   |      L.index(value, [start, [stop]]) -> integer -- return first index of valuedef all_indices(value, qlist):\n    indices = []\n    idx = -1\n    while True:\n        try:\n            idx = qlist.index(value, idx+1)\n            indices.append(idx)\n        except ValueError:\n            break\n    return indices\n\nall_indices(\"foo\", [\"foo\",\"bar\",\"baz\",\"foo\"])\ndef all_indices(value, qlist):\n    indices = []\n    idx = -1\n    while True:\n        try:\n            idx = qlist.index(value, idx+1)\n            indices.append(idx)\n        except ValueError:\n            break\n    return indices\n\nall_indices(\"foo\", [\"foo\",\"bar\",\"baz\",\"foo\"])\n",
                "A problem will arise if the element is not in the list. This function handles the issue:# if element is found it returns index of element else returns None\n\ndef find_element_in_list(element, list_element):\n    try:\n        index_element = list_element.index(element)\n        return index_element\n    except ValueError:\n        return None\n# if element is found it returns index of element else returns None\n\ndef find_element_in_list(element, list_element):\n    try:\n        index_element = list_element.index(element)\n        return index_element\n    except ValueError:\n        return None\n",
                "a = [\"foo\",\"bar\",\"baz\",'bar','any','much']\n\nindexes = [index for index in range(len(a)) if a[index] == 'bar']\na = [\"foo\",\"bar\",\"baz\",'bar','any','much']\n\nindexes = [index for index in range(len(a)) if a[index] == 'bar']\n",
                "You have to set a condition to check if the element you're searching is in the listif 'your_element' in mylist:\n    print mylist.index('your_element')\nelse:\n    print None\nif 'your_element' in mylist:\n    print mylist.index('your_element')\nelse:\n    print None\n",
                "If you want all indexes, then you can use NumPy:NumPyimport numpy as np\n\narray = [1, 2, 1, 3, 4, 5, 1]\nitem = 1\nnp_array = np.array(array)\nitem_index = np.where(np_array==item)\nprint item_index\n# Out: (array([0, 2, 6], dtype=int64),)\nimport numpy as np\n\narray = [1, 2, 1, 3, 4, 5, 1]\nitem = 1\nnp_array = np.array(array)\nitem_index = np.where(np_array==item)\nprint item_index\n# Out: (array([0, 2, 6], dtype=int64),)\nIt is clear, readable solution.",
                "All of the proposed functions here reproduce inherent language behavior but obscure what's going on.[i for i in range(len(mylist)) if mylist[i]==myterm]  # get the indices\n\n[each for each in mylist if each==myterm]             # get the items\n\nmylist.index(myterm) if myterm in mylist else None    # get the first index and fail quietly\n[i for i in range(len(mylist)) if mylist[i]==myterm]  # get the indices\n\n[each for each in mylist if each==myterm]             # get the items\n\nmylist.index(myterm) if myterm in mylist else None    # get the first index and fail quietly\nWhy write a function with exception handling if the language provides the methods to do what you want itself?",
                "\nFinding the index of an item given a list containing it in Python\nFor a list [\"foo\", \"bar\", \"baz\"] and an item in the list \"bar\", what's the cleanest way to get its index (1) in Python?\nFinding the index of an item given a list containing it in PythonFor a list [\"foo\", \"bar\", \"baz\"] and an item in the list \"bar\", what's the cleanest way to get its index (1) in Python?[\"foo\", \"bar\", \"baz\"]\"bar\"Well, sure, there's the index method, which returns the index of the first occurrence:>>> l = [\"foo\", \"bar\", \"baz\"]\n>>> l.index('bar')\n1\n>>> l = [\"foo\", \"bar\", \"baz\"]\n>>> l.index('bar')\n1\nThere are a couple of issues with this method:\nif the value isn't in the list, you'll get a ValueError\nif more than one of the value is in the list, you only get the index for the first one\nif the value isn't in the list, you'll get a ValueErrorValueErrorif more than one of the value is in the list, you only get the index for the first oneNo valuesIf the value could be missing, you need to catch the ValueError.ValueErrorYou can do so with a reusable definition like this:def index(a_list, value):\n    try:\n        return a_list.index(value)\n    except ValueError:\n        return None\ndef index(a_list, value):\n    try:\n        return a_list.index(value)\n    except ValueError:\n        return None\nAnd use it like this:>>> print(index(l, 'quux'))\nNone\n>>> print(index(l, 'bar'))\n1\n>>> print(index(l, 'quux'))\nNone\n>>> print(index(l, 'bar'))\n1\nAnd the downside of this is that you will probably have a check for if the returned value is or is not None:isis notresult = index(a_list, value)\nif result is not None:\n    do_something(result)\nresult = index(a_list, value)\nif result is not None:\n    do_something(result)\nMore than one value in the listIf you could have more occurrences, you'll not get complete information with list.index:notlist.index>>> l.append('bar')\n>>> l\n['foo', 'bar', 'baz', 'bar']\n>>> l.index('bar')              # nothing at index 3?\n1\n>>> l.append('bar')\n>>> l\n['foo', 'bar', 'baz', 'bar']\n>>> l.index('bar')              # nothing at index 3?\n1\nYou might enumerate into a list comprehension the indexes:>>> [index for index, v in enumerate(l) if v == 'bar']\n[1, 3]\n>>> [index for index, v in enumerate(l) if v == 'boink']\n[]\n>>> [index for index, v in enumerate(l) if v == 'bar']\n[1, 3]\n>>> [index for index, v in enumerate(l) if v == 'boink']\n[]\nIf you have no occurrences, you can check for that with boolean check of the result, or just do nothing if you loop over the results:indexes = [index for index, v in enumerate(l) if v == 'boink']\nfor index in indexes:\n    do_something(index)\nindexes = [index for index, v in enumerate(l) if v == 'boink']\nfor index in indexes:\n    do_something(index)\nBetter data munging with pandasIf you have pandas, you can easily get this information with a Series object:>>> import pandas as pd\n>>> series = pd.Series(l)\n>>> series\n0    foo\n1    bar\n2    baz\n3    bar\ndtype: object\n>>> import pandas as pd\n>>> series = pd.Series(l)\n>>> series\n0    foo\n1    bar\n2    baz\n3    bar\ndtype: object\nA comparison check will return a series of booleans:>>> series == 'bar'\n0    False\n1     True\n2    False\n3     True\ndtype: bool\n>>> series == 'bar'\n0    False\n1     True\n2    False\n3     True\ndtype: bool\nPass that series of booleans to the series via subscript notation, and you get just the matching members:>>> series[series == 'bar']\n1    bar\n3    bar\ndtype: object\n>>> series[series == 'bar']\n1    bar\n3    bar\ndtype: object\nIf you want just the indexes, the index attribute returns a series of integers:>>> series[series == 'bar'].index\nInt64Index([1, 3], dtype='int64')\n>>> series[series == 'bar'].index\nInt64Index([1, 3], dtype='int64')\nAnd if you want them in a list or tuple, just pass them to the constructor:>>> list(series[series == 'bar'].index)\n[1, 3]\n>>> list(series[series == 'bar'].index)\n[1, 3]\nYes, you could use a list comprehension with enumerate too, but that's just not as elegant, in my opinion - you're doing tests for equality in Python, instead of letting builtin code written in C handle it:>>> [i for i, value in enumerate(l) if value == 'bar']\n[1, 3]\n>>> [i for i, value in enumerate(l) if value == 'bar']\n[1, 3]\nIs this an XY problem?XY problem\nThe XY problem is asking about your attempted solution rather than your actual problem.\nThe XY problem is asking about your attempted solution rather than your actual problem.Why do you think you need the index given an element in a list?If you already know the value, why do you care where it is in a list?If the value isn't there, catching the ValueError is rather verbose - and I prefer to avoid that.ValueErrorI'm usually iterating over the list anyways, so I'll usually keep a pointer to any interesting information, getting the index with enumerate.index with enumerate.If you're munging data, you should probably be using pandas - which has far more elegant tools than the pure Python workarounds I've shown.I do not recall needing list.index, myself. However, I have looked through the Python standard library, and I see some excellent uses for it.list.indexThere are many, many uses for it in idlelib, for GUI and text parsing.idlelibThe keyword module uses it to find comment markers in the module to automatically regenerate the list of keywords in it via metaprogramming.keywordIn Lib/mailbox.py it seems to be using it like an ordered mapping:key_list[key_list.index(old)] = new\nkey_list[key_list.index(old)] = new\nanddel key_list[key_list.index(key)]\ndel key_list[key_list.index(key)]\nIn Lib/http/cookiejar.py, seems to be used to get the next month:mon = MONTHS_LOWER.index(mon.lower())+1\nmon = MONTHS_LOWER.index(mon.lower())+1\nIn Lib/tarfile.py similar to distutils to get a slice up to an item:members = members[:members.index(tarinfo)]\nmembers = members[:members.index(tarinfo)]\nIn Lib/pickletools.py:numtopop = before.index(markobject)\nnumtopop = before.index(markobject)\nWhat these usages seem to have in common is that they seem to operate on lists of constrained sizes (important because of O(n) lookup time for list.index), and they're mostly used in parsing (and UI in the case of Idle).list.indexWhile there are use-cases for it, they are fairly uncommon. If you find yourself looking for this answer, ask yourself if what you're doing is the most direct usage of the tools provided by the language for your use-case.",
                "Getting all the occurrences and the position of one or more (identical) items in a listWith enumerate(alist) you can store the first element (n) that is the index of the list when the element x is equal to what you look for.>>> alist = ['foo', 'spam', 'egg', 'foo']\n>>> foo_indexes = [n for n,x in enumerate(alist) if x=='foo']\n>>> foo_indexes\n[0, 3]\n>>>\n>>> alist = ['foo', 'spam', 'egg', 'foo']\n>>> foo_indexes = [n for n,x in enumerate(alist) if x=='foo']\n>>> foo_indexes\n[0, 3]\n>>>\nLet's make our function findindexThis function takes the item and the list as arguments and return the position of the item in the list, like we saw before.def indexlist(item2find, list_or_string):\n  \"Returns all indexes of an item in a list or a string\"\n  return [n for n,item in enumerate(list_or_string) if item==item2find]\n\nprint(indexlist(\"1\", \"010101010\"))\ndef indexlist(item2find, list_or_string):\n  \"Returns all indexes of an item in a list or a string\"\n  return [n for n,item in enumerate(list_or_string) if item==item2find]\n\nprint(indexlist(\"1\", \"010101010\"))\nOutputOutput[1, 3, 5, 7]\n[1, 3, 5, 7]\nSimplefor n, i in enumerate([1, 2, 3, 4, 1]):\n    if i == 1:\n        print(n)\nfor n, i in enumerate([1, 2, 3, 4, 1]):\n    if i == 1:\n        print(n)\nOutput:0\n4\n0\n4\n",
                "me = [\"foo\", \"bar\", \"baz\"]\nme.index(\"bar\") \nme = [\"foo\", \"bar\", \"baz\"]\nme.index(\"bar\") \nYou can apply this for any member of the list to get their index",
                "All indexes with the zip function:zipzipget_indexes = lambda x, xs: [i for (y, i) in zip(xs, range(len(xs))) if x == y]\n\nprint get_indexes(2, [1, 2, 3, 4, 5, 6, 3, 2, 3, 2])\nprint get_indexes('f', 'xsfhhttytffsafweef')\nget_indexes = lambda x, xs: [i for (y, i) in zip(xs, range(len(xs))) if x == y]\n\nprint get_indexes(2, [1, 2, 3, 4, 5, 6, 3, 2, 3, 2])\nprint get_indexes('f', 'xsfhhttytffsafweef')\n",
                "Simply you can go witha = [['hand', 'head'], ['phone', 'wallet'], ['lost', 'stock']]\nb = ['phone', 'lost']\n\nres = [[x[0] for x in a].index(y) for y in b]\na = [['hand', 'head'], ['phone', 'wallet'], ['lost', 'stock']]\nb = ['phone', 'lost']\n\nres = [[x[0] for x in a].index(y) for y in b]\n",
                "Another option>>> a = ['red', 'blue', 'green', 'red']\n>>> b = 'red'\n>>> offset = 0;\n>>> indices = list()\n>>> for i in range(a.count(b)):\n...     indices.append(a.index(b,offset))\n...     offset = indices[-1]+1\n... \n>>> indices\n[0, 3]\n>>> \n>>> a = ['red', 'blue', 'green', 'red']\n>>> b = 'red'\n>>> offset = 0;\n>>> indices = list()\n>>> for i in range(a.count(b)):\n...     indices.append(a.index(b,offset))\n...     offset = indices[-1]+1\n... \n>>> indices\n[0, 3]\n>>> \n",
                "And now, for something completely different...  ... like confirming the existence of the item before getting the index.  The nice thing about this approach is the function always returns a list of indices -- even if it is an empty list.  It works with strings as well.def indices(l, val):\n    \"\"\"Always returns a list containing the indices of val in the_list\"\"\"\n    retval = []\n    last = 0\n    while val in l[last:]:\n            i = l[last:].index(val)\n            retval.append(last + i)\n            last += i + 1   \n    return retval\n\nl = ['bar','foo','bar','baz','bar','bar']\nq = 'bar'\nprint indices(l,q)\nprint indices(l,'bat')\nprint indices('abcdaababb','a')\ndef indices(l, val):\n    \"\"\"Always returns a list containing the indices of val in the_list\"\"\"\n    retval = []\n    last = 0\n    while val in l[last:]:\n            i = l[last:].index(val)\n            retval.append(last + i)\n            last += i + 1   \n    return retval\n\nl = ['bar','foo','bar','baz','bar','bar']\nq = 'bar'\nprint indices(l,q)\nprint indices(l,'bat')\nprint indices('abcdaababb','a')\nWhen pasted into an interactive python window:Python 2.7.6 (v2.7.6:3a1db0d2747e, Nov 10 2013, 00:42:54) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> def indices(the_list, val):\n...     \"\"\"Always returns a list containing the indices of val in the_list\"\"\"\n...     retval = []\n...     last = 0\n...     while val in the_list[last:]:\n...             i = the_list[last:].index(val)\n...             retval.append(last + i)\n...             last += i + 1   \n...     return retval\n... \n>>> l = ['bar','foo','bar','baz','bar','bar']\n>>> q = 'bar'\n>>> print indices(l,q)\n[0, 2, 4, 5]\n>>> print indices(l,'bat')\n[]\n>>> print indices('abcdaababb','a')\n[0, 4, 5, 7]\n>>> \nPython 2.7.6 (v2.7.6:3a1db0d2747e, Nov 10 2013, 00:42:54) \n[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> def indices(the_list, val):\n...     \"\"\"Always returns a list containing the indices of val in the_list\"\"\"\n...     retval = []\n...     last = 0\n...     while val in the_list[last:]:\n...             i = the_list[last:].index(val)\n...             retval.append(last + i)\n...             last += i + 1   \n...     return retval\n... \n>>> l = ['bar','foo','bar','baz','bar','bar']\n>>> q = 'bar'\n>>> print indices(l,q)\n[0, 2, 4, 5]\n>>> print indices(l,'bat')\n[]\n>>> print indices('abcdaababb','a')\n[0, 4, 5, 7]\n>>> \nUpdateAfter another year of heads-down python development, I'm a bit embarrassed by my original answer, so to set the record straight, one can certainly use the above code; however, the much more idiomatic way to get the same behavior would be to use list comprehension, along with the enumerate() function.  muchSomething like this:  def indices(l, val):\n    \"\"\"Always returns a list containing the indices of val in the_list\"\"\"\n    return [index for index, value in enumerate(l) if value == val]\n\nl = ['bar','foo','bar','baz','bar','bar']\nq = 'bar'\nprint indices(l,q)\nprint indices(l,'bat')\nprint indices('abcdaababb','a')\ndef indices(l, val):\n    \"\"\"Always returns a list containing the indices of val in the_list\"\"\"\n    return [index for index, value in enumerate(l) if value == val]\n\nl = ['bar','foo','bar','baz','bar','bar']\nq = 'bar'\nprint indices(l,q)\nprint indices(l,'bat')\nprint indices('abcdaababb','a')\nWhich, when pasted into an interactive python window yields:Python 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) \n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> def indices(l, val):\n...     \"\"\"Always returns a list containing the indices of val in the_list\"\"\"\n...     return [index for index, value in enumerate(l) if value == val]\n... \n>>> l = ['bar','foo','bar','baz','bar','bar']\n>>> q = 'bar'\n>>> print indices(l,q)\n[0, 2, 4, 5]\n>>> print indices(l,'bat')\n[]\n>>> print indices('abcdaababb','a')\n[0, 4, 5, 7]\n>>> \nPython 2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 11:07:58) \n[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> def indices(l, val):\n...     \"\"\"Always returns a list containing the indices of val in the_list\"\"\"\n...     return [index for index, value in enumerate(l) if value == val]\n... \n>>> l = ['bar','foo','bar','baz','bar','bar']\n>>> q = 'bar'\n>>> print indices(l,q)\n[0, 2, 4, 5]\n>>> print indices(l,'bat')\n[]\n>>> print indices('abcdaababb','a')\n[0, 4, 5, 7]\n>>> \nAnd now, after reviewing this question and all the answers, I realize that this is exactly what FMc suggested in his earlier answer.  At the time I originally answered this question, I didn't even see that answer, because I didn't understand it.  I hope that my somewhat more verbose example will aid understanding.  FMcearlier answerseeIf the single line of code above still doesn't make sense to you, I highly recommend you Google 'python list comprehension' and take a few minutes to familiarize yourself.  It's just one of the many powerful features that make it a joy to use Python to develop code.doesn't",
                "Here's a two-liner using Python's index() function:index()LIST = ['foo' ,'boo', 'shoo']\nprint(LIST.index('boo'))\nLIST = ['foo' ,'boo', 'shoo']\nprint(LIST.index('boo'))\nOutput: 11",
                "A variant on the answer from FMc and user7177 will give a dict that can return all indices for any entry:>>> a = ['foo','bar','baz','bar','any', 'foo', 'much']\n>>> l = dict(zip(set(a), map(lambda y: [i for i,z in enumerate(a) if z is y ], set(a))))\n>>> l['foo']\n[0, 5]\n>>> l ['much']\n[6]\n>>> l\n{'baz': [2], 'foo': [0, 5], 'bar': [1, 3], 'any': [4], 'much': [6]}\n>>> \n>>> a = ['foo','bar','baz','bar','any', 'foo', 'much']\n>>> l = dict(zip(set(a), map(lambda y: [i for i,z in enumerate(a) if z is y ], set(a))))\n>>> l['foo']\n[0, 5]\n>>> l ['much']\n[6]\n>>> l\n{'baz': [2], 'foo': [0, 5], 'bar': [1, 3], 'any': [4], 'much': [6]}\n>>> \nYou could also use this as a one liner to get all indices for a single entry. There are no guarantees for efficiency, though I did use set(a) to reduce the number of times the lambda is called.",
                "Finding index of item x in list L:idx = L.index(x) if (x in L) else -1\nidx = L.index(x) if (x in L) else -1\n",
                "This solution is not as powerful as others, but if you're a beginner and only know about forloops it's still possible to find the first index of an item while avoiding the ValueError:fordef find_element(p,t):\n    i = 0\n    for e in p:\n        if e == t:\n            return i\n        else:\n            i +=1\n    return -1\ndef find_element(p,t):\n    i = 0\n    for e in p:\n        if e == t:\n            return i\n        else:\n            i +=1\n    return -1\n",
                "There is a chance that that value may not be present so to avoid this ValueError, we can check if that actually exists in the list .list =  [\"foo\", \"bar\", \"baz\"]\n\nitem_to_find = \"foo\"\n\nif item_to_find in list:\n      index = list.index(item_to_find)\n      print(\"Index of the item is \" + str(index))\nelse:\n    print(\"That word does not exist\") \nlist =  [\"foo\", \"bar\", \"baz\"]\n\nitem_to_find = \"foo\"\n\nif item_to_find in list:\n      index = list.index(item_to_find)\n      print(\"Index of the item is \" + str(index))\nelse:\n    print(\"That word does not exist\") \n",
                "List comprehension would be the best option to acquire a compact implementation in finding the index of an item in a list.a_list = [\"a\", \"b\", \"a\"]\nprint([index for (index , item) in enumerate(a_list) if item == \"a\"])\na_list = [\"a\", \"b\", \"a\"]\nprint([index for (index , item) in enumerate(a_list) if item == \"a\"])\n",
                "It just uses the python function array.index() and with a simple Try / Except it returns the position of the record if it is found in the list and return -1 if it is not found in the list (like on JavaScript with the function indexOf()).array.index()indexOf()fruits = ['apple', 'banana', 'cherry']\n\ntry:\n  pos = fruits.index(\"mango\")\nexcept:\n  pos = -1\nfruits = ['apple', 'banana', 'cherry']\n\ntry:\n  pos = fruits.index(\"mango\")\nexcept:\n  pos = -1\nIn this case \"mango\" is not present in the list fruits so the pos variable is -1, if I had searched for \"cherry\" the pos variable would be 2.fruitspospos",
                "There is a more functional answer to this.list(filter(lambda x: x[1]==\"bar\",enumerate([\"foo\", \"bar\", \"baz\", \"bar\", \"baz\", \"bar\", \"a\", \"b\", \"c\"])))\nlist(filter(lambda x: x[1]==\"bar\",enumerate([\"foo\", \"bar\", \"baz\", \"bar\", \"baz\", \"bar\", \"a\", \"b\", \"c\"])))\nMore generic form:def get_index_of(lst, element):\n    return list(map(lambda x: x[0],\\\n       (list(filter(lambda x: x[1]==element, enumerate(lst))))))\ndef get_index_of(lst, element):\n    return list(map(lambda x: x[0],\\\n       (list(filter(lambda x: x[1]==element, enumerate(lst))))))\n",
                "For one comparable# Throws ValueError if nothing is found\nsome_list = ['foo', 'bar', 'baz'].index('baz')\n# some_list == 2\n# Throws ValueError if nothing is found\nsome_list = ['foo', 'bar', 'baz'].index('baz')\n# some_list == 2\nCustom predicatesome_list = [item1, item2, item3]\n\n# Throws StopIteration if nothing is found\n# *unless* you provide a second parameter to `next`\nindex_of_value_you_like = next(\n    i for i, item in enumerate(some_list)\n    if item.matches_your_criteria())\nsome_list = [item1, item2, item3]\n\n# Throws StopIteration if nothing is found\n# *unless* you provide a second parameter to `next`\nindex_of_value_you_like = next(\n    i for i, item in enumerate(some_list)\n    if item.matches_your_criteria())\nFinding index of all items by predicateindex_of_staff_members = [\n    i for i, user in enumerate(users)\n    if user.is_staff()]\nindex_of_staff_members = [\n    i for i, user in enumerate(users)\n    if user.is_staff()]\n",
                "Python index() method throws an error if the item was not found. So instead you can make it similar to the indexOf() function of JavaScript which returns -1 if the item was not found:index()indexOf()-1try:\n    index = array.index('search_keyword')\nexcept ValueError:\n    index = -1\ntry:\n    index = array.index('search_keyword')\nexcept ValueError:\n    index = -1\n",
                "name =\"bar\"\nlist = [[\"foo\", 1], [\"bar\", 2], [\"baz\", 3]]\nnew_list=[]\nfor item in list:\n    new_list.append(item[0])\nprint(new_list)\ntry:\n    location= new_list.index(name)\nexcept:\n    location=-1\nprint (location)\nname =\"bar\"\nlist = [[\"foo\", 1], [\"bar\", 2], [\"baz\", 3]]\nnew_list=[]\nfor item in list:\n    new_list.append(item[0])\nprint(new_list)\ntry:\n    location= new_list.index(name)\nexcept:\n    location=-1\nprint (location)\nThis accounts for if the string is not in the list too, if it isn't in the list then location = -1location = -1",
                "If you are going to find an index once then using \"index\" method is fine. However, if you are going to search your data more than once then I recommend using bisect module. Keep in mind that using bisect module data must be sorted. So you sort data once and then you can use bisect.\nUsing bisect module on my machine is about 20 times faster than using index method.bisectbisectHere is an example of code using Python 3.8 and above syntax:import bisect\nfrom timeit import timeit\n\ndef bisect_search(container, value):\n    return (\n      index \n      if (index := bisect.bisect_left(container, value)) < len(container) \n      and container[index] == value else -1\n    )\n\ndata = list(range(1000))\n# value to search\nvalue = 666\n\n# times to test\nttt = 1000\n\nt1 = timeit(lambda: data.index(value), number=ttt)\nt2 = timeit(lambda: bisect_search(data, value), number=ttt)\n\nprint(f\"{t1=:.4f}, {t2=:.4f}, diffs {t1/t2=:.2f}\")\nimport bisect\nfrom timeit import timeit\n\ndef bisect_search(container, value):\n    return (\n      index \n      if (index := bisect.bisect_left(container, value)) < len(container) \n      and container[index] == value else -1\n    )\n\ndata = list(range(1000))\n# value to search\nvalue = 666\n\n# times to test\nttt = 1000\n\nt1 = timeit(lambda: data.index(value), number=ttt)\nt2 = timeit(lambda: bisect_search(data, value), number=ttt)\n\nprint(f\"{t1=:.4f}, {t2=:.4f}, diffs {t1/t2=:.2f}\")\nOutput:t1=0.0400, t2=0.0020, diffs t1/t2=19.60\nt1=0.0400, t2=0.0020, diffs t1/t2=19.60\n",
                "Since Python lists are zero-based, we can use the zip built-in function as follows:>>> [i for i,j in zip(range(len(haystack)), haystack) if j == 'needle' ]\n>>> [i for i,j in zip(range(len(haystack)), haystack) if j == 'needle' ]\nwhere \"haystack\" is the list in question and \"needle\" is the item to look for.(Note: Here we are iterating using i to get the indexes, but if we need rather to focus on the items we can switch to j.)",
                "If performance is of concern:It is mentioned in numerous answers that the built-in method of list.index(item) method is an O(n) algorithm. It is fine if you need to perform this once. But if you need to access the indices of elements a number of times, it makes more sense to first create a dictionary (O(n)) of item-index pairs, and then access the index at O(1) every time you need it.list.index(item)If you are sure that the items in your list are never repeated, you can easily:myList = [\"foo\", \"bar\", \"baz\"]\n\n# Create the dictionary\nmyDict = dict((e,i) for i,e in enumerate(myList))\n\n# Lookup\nmyDict[\"bar\"] # Returns 1\n# myDict.get(\"blah\") if you don't want an error to be raised if element not found.\nmyList = [\"foo\", \"bar\", \"baz\"]\n\n# Create the dictionary\nmyDict = dict((e,i) for i,e in enumerate(myList))\n\n# Lookup\nmyDict[\"bar\"] # Returns 1\n# myDict.get(\"blah\") if you don't want an error to be raised if element not found.\nIf you may have duplicate elements, and need to return all of their indices:from collections import defaultdict as dd\nmyList = [\"foo\", \"bar\", \"bar\", \"baz\", \"foo\"]\n\n# Create the dictionary\nmyDict = dd(list)\nfor i,e in enumerate(myList):\n    myDict[e].append(i)\n\n# Lookup\nmyDict[\"foo\"] # Returns [0, 4]\nfrom collections import defaultdict as dd\nmyList = [\"foo\", \"bar\", \"bar\", \"baz\", \"foo\"]\n\n# Create the dictionary\nmyDict = dd(list)\nfor i,e in enumerate(myList):\n    myDict[e].append(i)\n\n# Lookup\nmyDict[\"foo\"] # Returns [0, 4]\n"
            ]
        },
        {
            "tag": "dictionary_iteration",
            "patterns": [
                "Iterating over dictionaries using 'for' loops",
                "the iteration of dictionaries using loops for"
            ],
            "responses": [
                "key is just a variable name.  keyfor key in d:\nfor key in d:\nwill simply loop over the keys in the dictionary, rather than the keys and values.  To loop over both key and value you can use the following:For Python 3.x:for key, value in d.items():\nfor key, value in d.items():\nFor Python 2.x:for key, value in d.iteritems():\nfor key, value in d.iteritems():\nTo test for yourself, change the word key to poop.keypoopIn Python 3.x, iteritems() was replaced with simply items(), which returns a set-like view backed by the dict, like iteritems() but even better. \nThis is also available in 2.7 as viewitems(). iteritems()items()iteritems()viewitems()The operation items() will work for both 2 and 3, but in 2 it will return a list of the dictionary's (key, value) pairs, which will not reflect changes to the dict that happen after the items() call. If you want the 2.x behavior in 3.x, you can call list(d.items()).items()(key, value)items()list(d.items())",
                "It's not that key is a special word, but that dictionaries implement the iterator protocol.  You could do this in your class, e.g. see this question for how to build class iterators.this questionIn the case of dictionaries, it's implemented at the C level.  The details are available in PEP 234.  In particular, the section titled \"Dictionary Iterators\":PEP 234\n\nDictionaries implement a tp_iter slot that returns an efficient\n  iterator that iterates over the keys of the dictionary. [...] This \n  means that we can write\nfor k in dict: ...\n\nwhich is equivalent to, but much faster than\nfor k in dict.keys(): ...\n\nas long as the restriction on modifications to the dictionary\n  (either by the loop or by another thread) are not violated.\nAdd methods to dictionaries that return different kinds of\n  iterators explicitly:\nfor key in dict.iterkeys(): ...\n\nfor value in dict.itervalues(): ...\n\nfor key, value in dict.iteritems(): ...\n\nThis means that for x in dict is shorthand for for x in\n   dict.iterkeys().\n\n\nDictionaries implement a tp_iter slot that returns an efficient\n  iterator that iterates over the keys of the dictionary. [...] This \n  means that we can write\nfor k in dict: ...\n\nwhich is equivalent to, but much faster than\nfor k in dict.keys(): ...\n\nas long as the restriction on modifications to the dictionary\n  (either by the loop or by another thread) are not violated.\nAdd methods to dictionaries that return different kinds of\n  iterators explicitly:\nfor key in dict.iterkeys(): ...\n\nfor value in dict.itervalues(): ...\n\nfor key, value in dict.iteritems(): ...\n\nThis means that for x in dict is shorthand for for x in\n   dict.iterkeys().\nDictionaries implement a tp_iter slot that returns an efficient\n  iterator that iterates over the keys of the dictionary. [...] This \n  means that we can write\nfor k in dict: ...\n\nwhich is equivalent to, but much faster than\nfor k in dict.keys(): ...\n\nas long as the restriction on modifications to the dictionary\n  (either by the loop or by another thread) are not violated.Dictionaries implement a tp_iter slot that returns an efficient\n  iterator that iterates over the keys of the dictionary. [...] This \n  means that we can writefor k in dict: ...\nfor k in dict: ...\nwhich is equivalent to, but much faster thanfor k in dict.keys(): ...\nfor k in dict.keys(): ...\nas long as the restriction on modifications to the dictionary\n  (either by the loop or by another thread) are not violated.Add methods to dictionaries that return different kinds of\n  iterators explicitly:\nfor key in dict.iterkeys(): ...\n\nfor value in dict.itervalues(): ...\n\nfor key, value in dict.iteritems(): ...\n\nThis means that for x in dict is shorthand for for x in\n   dict.iterkeys().Add methods to dictionaries that return different kinds of\n  iterators explicitly:for key in dict.iterkeys(): ...\n\nfor value in dict.itervalues(): ...\n\nfor key, value in dict.iteritems(): ...\nfor key in dict.iterkeys(): ...\n\nfor value in dict.itervalues(): ...\n\nfor key, value in dict.iteritems(): ...\nThis means that for x in dict is shorthand for for x in\n   dict.iterkeys().for x in dictfor x in\n   dict.iterkeys()In Python 3, dict.iterkeys(), dict.itervalues() and dict.iteritems() are no longer supported. Use dict.keys(), dict.values() and dict.items() instead.dict.iterkeys()dict.itervalues()dict.iteritems()dict.keys()dict.values()dict.items()",
                "Iterating over a dict iterates through its keys in no particular order, as you can see here:dict(This is no longer the case in Python 3.6, but note that it's not guaranteed behaviour yet.)no longer the case in Python 3.6no longer the case in Python 3.6not guaranteed>>> d = {'x': 1, 'y': 2, 'z': 3}\n>>> list(d)\n['y', 'x', 'z']\n>>> d.keys()\n['y', 'x', 'z']\n>>> d = {'x': 1, 'y': 2, 'z': 3}\n>>> list(d)\n['y', 'x', 'z']\n>>> d.keys()\n['y', 'x', 'z']\nFor your example, it is a better idea to use dict.items():dict.items()>>> d.items()\n[('y', 2), ('x', 1), ('z', 3)]\n>>> d.items()\n[('y', 2), ('x', 1), ('z', 3)]\nThis gives you a list of tuples. When you loop over them like this, each tuple is unpacked into k and v automatically:kvfor k,v in d.items():\n    print(k, 'corresponds to', v)\nfor k,v in d.items():\n    print(k, 'corresponds to', v)\nUsing k and v as variable names when looping over a dict is quite common if the body of the loop is only a few lines. For more complicated loops it may be a good idea to use more descriptive names:kvdictfor letter, number in d.items():\n    print(letter, 'corresponds to', number)\nfor letter, number in d.items():\n    print(letter, 'corresponds to', number)\nIt's a good idea to get into the habit of using format strings:for letter, number in d.items():\n    print('{0} corresponds to {1}'.format(letter, number))\nfor letter, number in d.items():\n    print('{0} corresponds to {1}'.format(letter, number))\n",
                "key is simply a variable.keyFor Python2.X:Python2.X>>> d = {'x': 1, 'y': 2, 'z': 3} \n>>> for my_var in d:\n>>>     print my_var, 'corresponds to', d[my_var]\n\nx corresponds to 1\ny corresponds to 2\nz corresponds to 3\n>>> d = {'x': 1, 'y': 2, 'z': 3} \n>>> for my_var in d:\n>>>     print my_var, 'corresponds to', d[my_var]\n\nx corresponds to 1\ny corresponds to 2\nz corresponds to 3\n... or better,d = {'x': 1, 'y': 2, 'z': 3} \n\nfor the_key, the_value in d.iteritems():\n    print the_key, 'corresponds to', the_value\nd = {'x': 1, 'y': 2, 'z': 3} \n\nfor the_key, the_value in d.iteritems():\n    print the_key, 'corresponds to', the_value\nFor Python3.X:Python3.Xd = {'x': 1, 'y': 2, 'z': 3} \n\nfor the_key, the_value in d.items():\n    print(the_key, 'corresponds to', the_value)\nd = {'x': 1, 'y': 2, 'z': 3} \n\nfor the_key, the_value in d.items():\n    print(the_key, 'corresponds to', the_value)\n",
                "When you iterate through dictionaries using the for .. in ..-syntax, it always iterates over the keys (the values are accessible using dictionary[key]).for .. in ..dictionary[key]To iterate over key-value pairs, use the following:\nfor k,v in dict.iteritems() in Python 2\nfor k,v in dict.items() in Python 3\nfor k,v in dict.iteritems() in Python 2for k,v in dict.iteritems()for k,v in dict.items() in Python 3for k,v in dict.items()",
                "This is a very common looping idiom. in is an operator. For when to use for key in dict and when it must be for key in dict.keys() see David Goodger's Idiomatic Python article (archived copy).infor key in dictfor key in dict.keys()David Goodger's Idiomatic Python article (archived copy)",
                "I have a use case where I have to iterate through the dict to get the key, value pair, also the index indicating where I am. This is how I do it:d = {'x': 1, 'y': 2, 'z': 3} \nfor i, (key, value) in enumerate(d.items()):\n   print(i, key, value)\nd = {'x': 1, 'y': 2, 'z': 3} \nfor i, (key, value) in enumerate(d.items()):\n   print(i, key, value)\nNote that the parentheses around the key, value are important, without them, you'd get an ValueError \"not enough values to unpack\".ValueError",
                "\nIterating over dictionaries using 'for' loops\nd = {'x': 1, 'y': 2, 'z': 3} \nfor key in d:\n    ...\n\nHow does Python recognize that it needs only to read the key from the\n  dictionary? Is key a special word in Python? Or is it simply a\n  variable?\nIterating over dictionaries using 'for' loopsd = {'x': 1, 'y': 2, 'z': 3} \nfor key in d:\n    ...\nd = {'x': 1, 'y': 2, 'z': 3} \nfor key in d:\n    ...\nHow does Python recognize that it needs only to read the key from the\n  dictionary? Is key a special word in Python? Or is it simply a\n  variable?It's not just for loops. The important word here is \"iterating\".forA dictionary is a mapping of keys to values:d = {'x': 1, 'y': 2, 'z': 3} \nd = {'x': 1, 'y': 2, 'z': 3} \nAny time we iterate over it, we iterate over the keys. The variable name key is only intended to be descriptive - and it is quite apt for the purpose.keyThis happens in a list comprehension:>>> [k for k in d]\n['x', 'y', 'z']\n>>> [k for k in d]\n['x', 'y', 'z']\nIt happens when we pass the dictionary to list (or any other collection type object):>>> list(d)\n['x', 'y', 'z']\n>>> list(d)\n['x', 'y', 'z']\nThe way Python iterates is, in a context where it needs to, it calls the __iter__ method of the object (in this case the dictionary) which returns an iterator (in this case, a keyiterator object):__iter__>>> d.__iter__()\n<dict_keyiterator object at 0x7fb1747bee08>\n>>> d.__iter__()\n<dict_keyiterator object at 0x7fb1747bee08>\nWe shouldn't use these special methods ourselves, instead, use the respective builtin function to call it, iter:iter>>> key_iterator = iter(d)\n>>> key_iterator\n<dict_keyiterator object at 0x7fb172fa9188>\n>>> key_iterator = iter(d)\n>>> key_iterator\n<dict_keyiterator object at 0x7fb172fa9188>\nIterators have a __next__ method - but we call it with the builtin function, next:__next__next>>> next(key_iterator)\n'x'\n>>> next(key_iterator)\n'y'\n>>> next(key_iterator)\n'z'\n>>> next(key_iterator)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration\n>>> next(key_iterator)\n'x'\n>>> next(key_iterator)\n'y'\n>>> next(key_iterator)\n'z'\n>>> next(key_iterator)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nStopIteration\nWhen an iterator is exhausted, it raises StopIteration. This is how Python knows to exit a for loop, or a list comprehension, or a generator expression, or any other iterative context. Once an iterator raises StopIteration it will always raise it - if you want to iterate again, you need a new one.StopIterationforStopIteration>>> list(key_iterator)\n[]\n>>> new_key_iterator = iter(d)\n>>> list(new_key_iterator)\n['x', 'y', 'z']\n>>> list(key_iterator)\n[]\n>>> new_key_iterator = iter(d)\n>>> list(new_key_iterator)\n['x', 'y', 'z']\nReturning to dictsWe've seen dicts iterating in many contexts. What we've seen is that any time we iterate over a dict, we get the keys. Back to the original example:\nd = {'x': 1, 'y': 2, 'z': 3} \nfor key in d:\n\nd = {'x': 1, 'y': 2, 'z': 3} \nfor key in d:\nd = {'x': 1, 'y': 2, 'z': 3} \nfor key in d:\nIf we change the variable name, we still get the keys. Let's try it:>>> for each_key in d:\n...     print(each_key, '=>', d[each_key])\n... \nx => 1\ny => 2\nz => 3\n>>> for each_key in d:\n...     print(each_key, '=>', d[each_key])\n... \nx => 1\ny => 2\nz => 3\nIf we want to iterate over the values, we need to use the .values method of dicts, or for both together, .items:.values.items>>> list(d.values())\n[1, 2, 3]\n>>> list(d.items())\n[('x', 1), ('y', 2), ('z', 3)]\n>>> list(d.values())\n[1, 2, 3]\n>>> list(d.items())\n[('x', 1), ('y', 2), ('z', 3)]\nIn the example given, it would be more efficient to iterate over the items like this:for a_key, corresponding_value in d.items():\n    print(a_key, corresponding_value)\nfor a_key, corresponding_value in d.items():\n    print(a_key, corresponding_value)\nBut for academic purposes, the question's example is just fine.",
                "For Iterating through dictionaries, The below code can be used.dictionary= {1:\"a\", 2:\"b\", 3:\"c\"}\n\n#To iterate over the keys\nfor key in dictionary.keys():\n    print(key)\n\n#To Iterate over the values\nfor value in dictionary.values():\n    print(value)\n\n#To Iterate both the keys and values\nfor key, value in dictionary.items():\n    print(key,'\\t', value)\ndictionary= {1:\"a\", 2:\"b\", 3:\"c\"}\n\n#To iterate over the keys\nfor key in dictionary.keys():\n    print(key)\n\n#To Iterate over the values\nfor value in dictionary.values():\n    print(value)\n\n#To Iterate both the keys and values\nfor key, value in dictionary.items():\n    print(key,'\\t', value)\n",
                "You can check the implementation of CPython's dicttype on GitHub. This is the signature of method that implements the dict iterator:dicttype_PyDict_Next(PyObject *op, Py_ssize_t *ppos, PyObject **pkey,\n             PyObject **pvalue, Py_hash_t *phash)\n_PyDict_Next(PyObject *op, Py_ssize_t *ppos, PyObject **pkey,\n             PyObject **pvalue, Py_hash_t *phash)\nCPython dictobject.cCPython dictobject.c",
                "To iterate over keys, it is slower but better to use my_dict.keys(). If you tried to do something like this:my_dict.keys()for key in my_dict:\n    my_dict[key+\"-1\"] = my_dict[key]-1\nfor key in my_dict:\n    my_dict[key+\"-1\"] = my_dict[key]-1\nit would create a runtime error because you are changing the keys while the program is running. If you are absolutely set on reducing time, use the for key in my_dict way, but you have been warned.for key in my_dict",
                "If you are looking for a clear and visual example:cat  = {'name': 'Snowy', 'color': 'White' ,'age': 14}\nfor key , value in cat.items():\n   print(key, ': ', value)\ncat  = {'name': 'Snowy', 'color': 'White' ,'age': 14}\nfor key , value in cat.items():\n   print(key, ': ', value)\nResult:name:  Snowy\ncolor:  White\nage:  14\nname:  Snowy\ncolor:  White\nage:  14\n",
                "This will print the output in sorted order by values in ascending order.sortedvaluesd = {'x': 3, 'y': 1, 'z': 2}\n\ndef by_value(item):\n    return item[1]\n\nfor key, value in sorted(d.items(), key=by_value):\n    print(key, '->', value)\nd = {'x': 3, 'y': 1, 'z': 2}\n\ndef by_value(item):\n    return item[1]\n\nfor key, value in sorted(d.items(), key=by_value):\n    print(key, '->', value)\nOutput:\ny -> 1\nz -> 2\nx -> 3\n\ny -> 1\nz -> 2\nx -> 3\ny -> 1\nz -> 2\nx -> 3\n",
                "Let's get straight to the point. If the word key is just a variable, as you have mentioned then the main thing to note is that when you run a 'FOR LOOP' over a dictionary it runs through only the 'keys' and ignores the 'values'.'FOR LOOP''keys''values'd = {'x': 1, 'y': 2, 'z': 3} \nfor key in d:\n    print (key, 'corresponds to', d[key])\nd = {'x': 1, 'y': 2, 'z': 3} \nfor key in d:\n    print (key, 'corresponds to', d[key])\nrather try this:d = {'x': 1, 'y': 2, 'z': 3} \nfor i in d:\n    print (i, 'corresponds to', d[i])\nd = {'x': 1, 'y': 2, 'z': 3} \nfor i in d:\n    print (i, 'corresponds to', d[i])\nbut if you use a function like:d = {'x': 1, 'y': 2, 'z': 3}\nprint(d.keys())\nd = {'x': 1, 'y': 2, 'z': 3}\nprint(d.keys())\nin the above case 'keys' is just not a variable, its a function.'keys'",
                "A dictionary in Python is a collection of key-value pairs. Each key is connected to a value, and you can use a key to access the value associated with that key. A key's value can be a number, a string, a list, or even another dictionary. In this case, threat each \"key-value pair\" as a separate row in the table: d is your table with two columns. the key is the first column, key[value] is your second column. Your for loop is a standard way to iterate over a table."
            ]
        },
        {
            "tag": "global_var",
            "patterns": [
                "Using global variables in a function",
                "use of global variables in a function",
                "the use of global variables in a function",
                "using global variables in a function"
            ],
            "responses": [
                "You can use a global variable within other functions by declaring it as global within each function that assigns a value to it:globalglobvar = 0\n\ndef set_globvar_to_one():\n    global globvar    # Needed to modify global copy of globvar\n    globvar = 1\n\ndef print_globvar():\n    print(globvar)     # No need for global declaration to read value of globvar\n\nset_globvar_to_one()\nprint_globvar()       # Prints 1\nglobvar = 0\n\ndef set_globvar_to_one():\n    global globvar    # Needed to modify global copy of globvar\n    globvar = 1\n\ndef print_globvar():\n    print(globvar)     # No need for global declaration to read value of globvar\n\nset_globvar_to_one()\nprint_globvar()       # Prints 1\nSince it's unclear whether globvar = 1 is creating a local variable or changing a global variable, Python defaults to creating a local variable, and makes you explicitly choose the other behavior with the global keyword.globvar = 1globalSee other answers if you want to share a global variable across modules.",
                "If I'm understanding your situation correctly, what you're seeing is the result of how Python handles local (function) and global (module) namespaces.Say you've got a module like this:# sample.py\n_my_global = 5\n\ndef func1():\n    _my_global = 42\n\ndef func2():\n    print _my_global\n\nfunc1()\nfunc2()\n# sample.py\n_my_global = 5\n\ndef func1():\n    _my_global = 42\n\ndef func2():\n    print _my_global\n\nfunc1()\nfunc2()\nYou might expecting this to print 42, but instead it prints 5.  As has already been mentioned, if you add a 'global' declaration to func1(), then func2() will print 42.globalfunc1()func2()def func1():\n    global _my_global \n    _my_global = 42\ndef func1():\n    global _my_global \n    _my_global = 42\nWhat's going on here is that Python assumes that any name that is assigned to, anywhere within a function, is local to that function unless explicitly told otherwise.  If it is only reading from a name, and the name doesn't exist locally, it will try to look up the name in any containing scopes (e.g. the module's global scope).assigned toreadingWhen you assign 42 to the name _my_global, therefore, Python creates a local variable that shadows the global variable of the same name.  That local goes out of scope and is garbage-collected when func1() returns; meanwhile, func2() can never see anything other than the (unmodified) global name.  Note that this namespace decision happens at compile time, not at runtime -- if you were to read the value of _my_global inside func1() before you assign to it, you'd get an UnboundLocalError, because Python has already decided that it must be a local variable but it has not had any value associated with it yet.  But by using the 'global' statement, you tell Python that it should look elsewhere for the name instead of assigning to it locally._my_globalgarbage-collectedfunc1()func2()_my_globalfunc1()UnboundLocalErrorglobal(I believe that this behavior originated largely through an optimization of local namespaces -- without this behavior,  Python's VM would need to perform at least three name lookups each time a new name is assigned to inside a function (to ensure that the name didn't already exist at module/builtin level), which would significantly slow down a very common operation.)",
                "You may want to explore the notion of namespaces. In Python, the module is the natural place for global data:namespacesmoduleglobal\nEach module has its own private symbol table, which is used as the global symbol table by all functions defined in the module. Thus, the author of a module can use global variables in the module without worrying about accidental clashes with a user\u2019s global variables. On the other hand, if you know what you are doing you can touch a module\u2019s global variables with the same notation used to refer to its functions, modname.itemname.\nEach module has its own private symbol table, which is used as the global symbol table by all functions defined in the module. Thus, the author of a module can use global variables in the module without worrying about accidental clashes with a user\u2019s global variables. On the other hand, if you know what you are doing you can touch a module\u2019s global variables with the same notation used to refer to its functions, modname.itemname.modname.itemnameA specific use of global-in-a-module is described here - How do I share global variables across modules?, and for completeness the contents are shared here:How do I share global variables across modules?\nThe canonical way to share information across modules within a single program is to create a special configuration module (often called config or cfg). Just import the configuration module in all modules of your application; the module then becomes available as a global name. Because there is only one instance of each module, any changes made to the module object get reflected everywhere. For example:\nThe canonical way to share information across modules within a single program is to create a special configuration module (often called config or cfg). Just import the configuration module in all modules of your application; the module then becomes available as a global name. Because there is only one instance of each module, any changes made to the module object get reflected everywhere. For example:configcfg\nFile: config.py\nFile: config.py\nx = 0   # Default value of the 'x' configuration setting\n\nx = 0   # Default value of the 'x' configuration setting\nx = 0   # Default value of the 'x' configuration setting\n\nFile: mod.py\nFile: mod.pyimport config\nconfig.x = 1\nimport config\nconfig.x = 1\n\nFile: main.py\nFile: main.pyimport config\nimport mod\nprint config.x\nimport config\nimport mod\nprint config.x\n",
                "Python uses a simple heuristic to decide which scope it should load a variable from, between local and global.  If a variable name appears on the left hand side of an assignment, but is not declared global, it is assumed to be local.  If it does not appear on the left hand side of an assignment, it is assumed to be global.  >>> import dis\n>>> def foo():\n...     global bar\n...     baz = 5\n...     print bar\n...     print baz\n...     print quux\n... \n>>> dis.disassemble(foo.func_code)\n  3           0 LOAD_CONST               1 (5)\n              3 STORE_FAST               0 (baz)\n\n  4           6 LOAD_GLOBAL              0 (bar)\n              9 PRINT_ITEM          \n             10 PRINT_NEWLINE       \n\n  5          11 LOAD_FAST                0 (baz)\n             14 PRINT_ITEM          \n             15 PRINT_NEWLINE       \n\n  6          16 LOAD_GLOBAL              1 (quux)\n             19 PRINT_ITEM          \n             20 PRINT_NEWLINE       \n             21 LOAD_CONST               0 (None)\n             24 RETURN_VALUE        \n>>> \n>>> import dis\n>>> def foo():\n...     global bar\n...     baz = 5\n...     print bar\n...     print baz\n...     print quux\n... \n>>> dis.disassemble(foo.func_code)\n  3           0 LOAD_CONST               1 (5)\n              3 STORE_FAST               0 (baz)\n\n  4           6 LOAD_GLOBAL              0 (bar)\n              9 PRINT_ITEM          \n             10 PRINT_NEWLINE       \n\n  5          11 LOAD_FAST                0 (baz)\n             14 PRINT_ITEM          \n             15 PRINT_NEWLINE       \n\n  6          16 LOAD_GLOBAL              1 (quux)\n             19 PRINT_ITEM          \n             20 PRINT_NEWLINE       \n             21 LOAD_CONST               0 (None)\n             24 RETURN_VALUE        \n>>> \nSee how baz, which appears on the left side of an assignment in foo(), is the only LOAD_FAST variable.foo()LOAD_FAST",
                "If you want to refer to a global variable in a function, you can use the global keyword to declare which variables are global. You don't have to use it in all cases (as someone here incorrectly claims) - if the name referenced in an expression cannot be found in local scope or scopes in the functions in which this function is defined, it is looked up among global variables.globalHowever, if you assign to a new variable not declared as global in the function, it is implicitly declared as local, and it can overshadow any existing global variable with the same name.Also, global variables are useful, contrary to some OOP zealots who claim otherwise - especially for smaller scripts, where OOP is overkill.",
                "\nIf I create a global variable in one function, how can I use that variable in another function?\nIf I create a global variable in one function, how can I use that variable in another function?We can create a global with the following function:def create_global_variable():\n    global global_variable # must declare it to be a global first\n    # modifications are thus reflected on the module's global scope\n    global_variable = 'Foo' \ndef create_global_variable():\n    global global_variable # must declare it to be a global first\n    # modifications are thus reflected on the module's global scope\n    global_variable = 'Foo' \nWriting a function does not actually run its code. So we call the create_global_variable function:create_global_variable>>> create_global_variable()\n>>> create_global_variable()\nUsing globals without modificationYou can just use it, so long as you don't expect to change which object it points to: For example, def use_global_variable():\n    return global_variable + '!!!'\ndef use_global_variable():\n    return global_variable + '!!!'\nand now we can use the global variable:>>> use_global_variable()\n'Foo!!!'\n>>> use_global_variable()\n'Foo!!!'\nModification of the global variable from inside a functionTo point the global variable at a different object, you are required to use the global keyword again:def change_global_variable():\n    global global_variable\n    global_variable = 'Bar'\ndef change_global_variable():\n    global global_variable\n    global_variable = 'Bar'\nNote that after writing this function, the code actually changing it has still not run:>>> use_global_variable()\n'Foo!!!'\n>>> use_global_variable()\n'Foo!!!'\nSo after calling the function:>>> change_global_variable()\n>>> change_global_variable()\nwe can see that the global variable has been changed. The global_variable name now points to 'Bar':global_variable'Bar'>>> use_global_variable()\n'Bar!!!'\n>>> use_global_variable()\n'Bar!!!'\nNote that \"global\" in Python is not truly global - it's only global to the module level. So it is only available to functions written in the modules in which it is global. Functions remember the module in which they are written, so when they are exported into other modules, they still look in the module in which they were created to find global variables.Local variables with the same nameIf you create a local variable with the same name, it will overshadow a global variable:def use_local_with_same_name_as_global():\n    # bad name for a local variable, though.\n    global_variable = 'Baz' \n    return global_variable + '!!!'\n\n>>> use_local_with_same_name_as_global()\n'Baz!!!'\ndef use_local_with_same_name_as_global():\n    # bad name for a local variable, though.\n    global_variable = 'Baz' \n    return global_variable + '!!!'\n\n>>> use_local_with_same_name_as_global()\n'Baz!!!'\nBut using that misnamed local variable does not change the global variable:>>> use_global_variable()\n'Bar!!!'\n>>> use_global_variable()\n'Bar!!!'\nNote that you should avoid using the local variables with the same names as globals unless you know precisely what you are doing and have a very good reason to do so. I have not yet encountered such a reason.We get the same behavior in classesA follow on comment asks:\nwhat to do if I want to create a global variable inside a function inside a class and want to use that variable inside another function inside another class?\nwhat to do if I want to create a global variable inside a function inside a class and want to use that variable inside another function inside another class?Here I demonstrate we get the same behavior in methods as we do in regular functions:class Foo:\n    def foo(self):\n        global global_variable\n        global_variable = 'Foo'\n\nclass Bar:\n    def bar(self):\n        return global_variable + '!!!'\n\nFoo().foo()\nclass Foo:\n    def foo(self):\n        global global_variable\n        global_variable = 'Foo'\n\nclass Bar:\n    def bar(self):\n        return global_variable + '!!!'\n\nFoo().foo()\nAnd now:>>> Bar().bar()\n'Foo!!!'\n>>> Bar().bar()\n'Foo!!!'\nBut I would suggest instead of using global variables you use class attributes, to avoid cluttering the module namespace. Also note we don't use self arguments here - these could be class methods (handy if mutating the class attribute from the usual cls argument) or static methods (no self or cls).selfclsselfcls",
                "In addition to already existing answers and to make this more confusing:\nIn Python, variables that are only referenced inside a function are\n  implicitly global. If a variable is assigned a new value anywhere\n  within the function\u2019s body, it\u2019s assumed to be a local. If a variable\n  is ever assigned a new value inside the function, the variable is\n  implicitly local, and you need to explicitly declare it as \u2018global\u2019.\nThough a bit surprising at first, a moment\u2019s consideration explains\n  this. On one hand, requiring global for assigned variables provides a\n  bar against unintended side-effects. On the other hand, if global was\n  required for all global references, you\u2019d be using global all the\n  time. You\u2019d have to declare as global every reference to a built-in\n  function or to a component of an imported module. This clutter would\n  defeat the usefulness of the global declaration for identifying\n  side-effects.\nIn Python, variables that are only referenced inside a function are\n  implicitly global. If a variable is assigned a new value anywhere\n  within the function\u2019s body, it\u2019s assumed to be a local. If a variable\n  is ever assigned a new value inside the function, the variable is\n  implicitly local, and you need to explicitly declare it as \u2018global\u2019.implicitly globallocalThough a bit surprising at first, a moment\u2019s consideration explains\n  this. On one hand, requiring global for assigned variables provides a\n  bar against unintended side-effects. On the other hand, if global was\n  required for all global references, you\u2019d be using global all the\n  time. You\u2019d have to declare as global every reference to a built-in\n  function or to a component of an imported module. This clutter would\n  defeat the usefulness of the global declaration for identifying\n  side-effects.Source: What are the rules for local and global variables in Python?.What are the rules for local and global variables in Python?What are the rules for local and global variables in Python?",
                "With parallel execution, global variables can cause unexpected results if you don't understand what is happening. Here is an example of using a global variable within multiprocessing. We can clearly see that each process works with its own copy of the variable:import multiprocessing\nimport os\nimport random\nimport sys\nimport time\n\ndef worker(new_value):\n    old_value = get_value()\n    set_value(random.randint(1, 99))\n    print('pid=[{pid}] '\n          'old_value=[{old_value:2}] '\n          'new_value=[{new_value:2}] '\n          'get_value=[{get_value:2}]'.format(\n          pid=str(os.getpid()),\n          old_value=old_value,\n          new_value=new_value,\n          get_value=get_value()))\n\ndef get_value():\n    global global_variable\n    return global_variable\n\ndef set_value(new_value):\n    global global_variable\n    global_variable = new_value\n\nglobal_variable = -1\n\nprint('before set_value(), get_value() = [%s]' % get_value())\nset_value(new_value=-2)\nprint('after  set_value(), get_value() = [%s]' % get_value())\n\nprocessPool = multiprocessing.Pool(processes=5)\nprocessPool.map(func=worker, iterable=range(15))\nimport multiprocessing\nimport os\nimport random\nimport sys\nimport time\n\ndef worker(new_value):\n    old_value = get_value()\n    set_value(random.randint(1, 99))\n    print('pid=[{pid}] '\n          'old_value=[{old_value:2}] '\n          'new_value=[{new_value:2}] '\n          'get_value=[{get_value:2}]'.format(\n          pid=str(os.getpid()),\n          old_value=old_value,\n          new_value=new_value,\n          get_value=get_value()))\n\ndef get_value():\n    global global_variable\n    return global_variable\n\ndef set_value(new_value):\n    global global_variable\n    global_variable = new_value\n\nglobal_variable = -1\n\nprint('before set_value(), get_value() = [%s]' % get_value())\nset_value(new_value=-2)\nprint('after  set_value(), get_value() = [%s]' % get_value())\n\nprocessPool = multiprocessing.Pool(processes=5)\nprocessPool.map(func=worker, iterable=range(15))\nOutput:Output:before set_value(), get_value() = [-1]\nafter  set_value(), get_value() = [-2]\npid=[53970] old_value=[-2] new_value=[ 0] get_value=[23]\npid=[53971] old_value=[-2] new_value=[ 1] get_value=[42]\npid=[53970] old_value=[23] new_value=[ 4] get_value=[50]\npid=[53970] old_value=[50] new_value=[ 6] get_value=[14]\npid=[53971] old_value=[42] new_value=[ 5] get_value=[31]\npid=[53972] old_value=[-2] new_value=[ 2] get_value=[44]\npid=[53973] old_value=[-2] new_value=[ 3] get_value=[94]\npid=[53970] old_value=[14] new_value=[ 7] get_value=[21]\npid=[53971] old_value=[31] new_value=[ 8] get_value=[34]\npid=[53972] old_value=[44] new_value=[ 9] get_value=[59]\npid=[53973] old_value=[94] new_value=[10] get_value=[87]\npid=[53970] old_value=[21] new_value=[11] get_value=[21]\npid=[53971] old_value=[34] new_value=[12] get_value=[82]\npid=[53972] old_value=[59] new_value=[13] get_value=[ 4]\npid=[53973] old_value=[87] new_value=[14] get_value=[70]\nbefore set_value(), get_value() = [-1]\nafter  set_value(), get_value() = [-2]\npid=[53970] old_value=[-2] new_value=[ 0] get_value=[23]\npid=[53971] old_value=[-2] new_value=[ 1] get_value=[42]\npid=[53970] old_value=[23] new_value=[ 4] get_value=[50]\npid=[53970] old_value=[50] new_value=[ 6] get_value=[14]\npid=[53971] old_value=[42] new_value=[ 5] get_value=[31]\npid=[53972] old_value=[-2] new_value=[ 2] get_value=[44]\npid=[53973] old_value=[-2] new_value=[ 3] get_value=[94]\npid=[53970] old_value=[14] new_value=[ 7] get_value=[21]\npid=[53971] old_value=[31] new_value=[ 8] get_value=[34]\npid=[53972] old_value=[44] new_value=[ 9] get_value=[59]\npid=[53973] old_value=[94] new_value=[10] get_value=[87]\npid=[53970] old_value=[21] new_value=[11] get_value=[21]\npid=[53971] old_value=[34] new_value=[12] get_value=[82]\npid=[53972] old_value=[59] new_value=[13] get_value=[ 4]\npid=[53973] old_value=[87] new_value=[14] get_value=[70]\n",
                "As it turns out the answer is always simple.Here is a small sample module with a simple way to show it in a main definition:maindef five(enterAnumber,sumation):\n    global helper\n    helper  = enterAnumber + sumation\n\ndef isTheNumber():\n    return helper\ndef five(enterAnumber,sumation):\n    global helper\n    helper  = enterAnumber + sumation\n\ndef isTheNumber():\n    return helper\nHere is how to show it in a main definition:mainimport TestPy\n\ndef main():\n    atest  = TestPy\n    atest.five(5,8)\n    print(atest.isTheNumber())\n\nif __name__ == '__main__':\n    main()\nimport TestPy\n\ndef main():\n    atest  = TestPy\n    atest.five(5,8)\n    print(atest.isTheNumber())\n\nif __name__ == '__main__':\n    main()\nThis simple code works just like that, and it will execute. I hope it helps.",
                "What you are saying is to use the method like this:globvar = 5\n\ndef f():\n    var = globvar\n    print(var)\n\nf()  # Prints 5\nglobvar = 5\n\ndef f():\n    var = globvar\n    print(var)\n\nf()  # Prints 5\nBut the better way is to use the global variable like this:globvar = 5\ndef f():\n    global globvar\n    print(globvar)\nf()   #prints 5\nglobvar = 5\ndef f():\n    global globvar\n    print(globvar)\nf()   #prints 5\nBoth give the same output.",
                "You need to reference the global variable in every function you want to use.As follows:var = \"test\"\n\ndef printGlobalText():\n    global var #wWe are telling to explicitly use the global version\n    var = \"global from printGlobalText fun.\"\n    print \"var from printGlobalText: \" + var\n\ndef printLocalText():\n    #We are NOT telling to explicitly use the global version, so we are creating a local variable\n    var = \"local version from printLocalText fun\"\n    print \"var from printLocalText: \" + var\n\nprintGlobalText()\nprintLocalText()\n\"\"\"\nOutput Result:\nvar from printGlobalText: global from printGlobalText fun.\nvar from printLocalText: local version from printLocalText\n[Finished in 0.1s]\n\"\"\"\nvar = \"test\"\n\ndef printGlobalText():\n    global var #wWe are telling to explicitly use the global version\n    var = \"global from printGlobalText fun.\"\n    print \"var from printGlobalText: \" + var\n\ndef printLocalText():\n    #We are NOT telling to explicitly use the global version, so we are creating a local variable\n    var = \"local version from printLocalText fun\"\n    print \"var from printLocalText: \" + var\n\nprintGlobalText()\nprintLocalText()\n\"\"\"\nOutput Result:\nvar from printGlobalText: global from printGlobalText fun.\nvar from printLocalText: local version from printLocalText\n[Finished in 0.1s]\n\"\"\"\n",
                "Try this:def x1():\n    global x\n    x += 1\n    print('x1: ', x)\n\ndef x2():\n    global x\n    x = x+1\n    print('x2: ', x)\n\nx = 5\nprint('x:  ', x)\nx1()\nx2()\n\n# Output:\n# x:   5\n# x1:  6\n# x2:  7\ndef x1():\n    global x\n    x += 1\n    print('x1: ', x)\n\ndef x2():\n    global x\n    x = x+1\n    print('x2: ', x)\n\nx = 5\nprint('x:  ', x)\nx1()\nx2()\n\n# Output:\n# x:   5\n# x1:  6\n# x2:  7\n",
                "You're not actually storing the global in a local variable, just creating a local reference to the same object that your original global reference refers to. Remember that pretty much everything in Python is a name referring to an object, and nothing gets copied in usual operation.If you didn't have to explicitly specify when an identifier was to refer to a predefined global, then you'd presumably have to explicitly specify when an identifier is a new local variable instead (for example, with something like the 'var' command seen in JavaScript). Since local variables are more common than global variables in any serious and non-trivial system, Python's system makes more sense in most cases.You could have a language which attempted to guess, using a global variable if it existed or creating a local variable if it didn't. However, that would be very error-prone. For example, importing another module could inadvertently introduce a global variable by that name, changing the behaviour of your program.could",
                "In case you have a local variable with the same name, you might want to use the globals() function.globals() functionglobals()globals()['your_global_var'] = 42\nglobals()['your_global_var'] = 42\n",
                "Following on and as an add on, use a file to contain all global variables all declared locally and then import as:import asFile initval.py:initval.pyStocksin = 300\nPrices = []\nStocksin = 300\nPrices = []\nFile getstocks.py:getstocks.pyimport initval as iv\n\ndef getmystocks(): \n    iv.Stocksin = getstockcount()\n\n\ndef getmycharts():\n    for ic in range(iv.Stocksin):\nimport initval as iv\n\ndef getmystocks(): \n    iv.Stocksin = getstockcount()\n\n\ndef getmycharts():\n    for ic in range(iv.Stocksin):\n",
                "Writing to explicit elements of a global array does not apparently need the global declaration, though writing to it \"wholesale\" does have that requirement:import numpy as np\n\nhostValue = 3.14159\nhostArray = np.array([2., 3.])\nhostMatrix = np.array([[1.0, 0.0],[ 0.0, 1.0]])\n\ndef func1():\n    global hostValue    # mandatory, else local.\n    hostValue = 2.0\n\ndef func2():\n    global hostValue    # mandatory, else UnboundLocalError.\n    hostValue += 1.0\n\ndef func3():\n    global hostArray    # mandatory, else local.\n    hostArray = np.array([14., 15.])\n\ndef func4():            # no need for globals\n    hostArray[0] = 123.4\n\ndef func5():            # no need for globals\n    hostArray[1] += 1.0\n\ndef func6():            # no need for globals\n    hostMatrix[1][1] = 12.\n\ndef func7():            # no need for globals\n    hostMatrix[0][0] += 0.33\n\nfunc1()\nprint \"After func1(), hostValue = \", hostValue\nfunc2()\nprint \"After func2(), hostValue = \", hostValue\nfunc3()\nprint \"After func3(), hostArray = \", hostArray\nfunc4()\nprint \"After func4(), hostArray = \", hostArray\nfunc5()\nprint \"After func5(), hostArray = \", hostArray\nfunc6()\nprint \"After func6(), hostMatrix = \\n\", hostMatrix\nfunc7()\nprint \"After func7(), hostMatrix = \\n\", hostMatrix\nimport numpy as np\n\nhostValue = 3.14159\nhostArray = np.array([2., 3.])\nhostMatrix = np.array([[1.0, 0.0],[ 0.0, 1.0]])\n\ndef func1():\n    global hostValue    # mandatory, else local.\n    hostValue = 2.0\n\ndef func2():\n    global hostValue    # mandatory, else UnboundLocalError.\n    hostValue += 1.0\n\ndef func3():\n    global hostArray    # mandatory, else local.\n    hostArray = np.array([14., 15.])\n\ndef func4():            # no need for globals\n    hostArray[0] = 123.4\n\ndef func5():            # no need for globals\n    hostArray[1] += 1.0\n\ndef func6():            # no need for globals\n    hostMatrix[1][1] = 12.\n\ndef func7():            # no need for globals\n    hostMatrix[0][0] += 0.33\n\nfunc1()\nprint \"After func1(), hostValue = \", hostValue\nfunc2()\nprint \"After func2(), hostValue = \", hostValue\nfunc3()\nprint \"After func3(), hostArray = \", hostArray\nfunc4()\nprint \"After func4(), hostArray = \", hostArray\nfunc5()\nprint \"After func5(), hostArray = \", hostArray\nfunc6()\nprint \"After func6(), hostMatrix = \\n\", hostMatrix\nfunc7()\nprint \"After func7(), hostMatrix = \\n\", hostMatrix\n",
                "I'm adding this as I haven't seen it in any of the other answers and it might be useful for someone struggling with something similar. The globals() function returns a mutable global symbol dictionary where you can \"magically\" make data available for the rest of your code. \nFor example:globals()globals()from pickle import load\ndef loaditem(name):\n    with open(r\"C:\\pickle\\file\\location\"+\"\\{}.dat\".format(name), \"rb\") as openfile:\n        globals()[name] = load(openfile)\n    return True\nfrom pickle import load\ndef loaditem(name):\n    with open(r\"C:\\pickle\\file\\location\"+\"\\{}.dat\".format(name), \"rb\") as openfile:\n        globals()[name] = load(openfile)\n    return True\nand from pickle import dump\ndef dumpfile(name):\n    with open(name+\".dat\", \"wb\") as outfile:\n        dump(globals()[name], outfile)\n    return True\nfrom pickle import dump\ndef dumpfile(name):\n    with open(name+\".dat\", \"wb\") as outfile:\n        dump(globals()[name], outfile)\n    return True\nWill just let you dump/load variables out of and into the global namespace. Super convenient, no muss, no fuss. Pretty sure it's Python 3 only.",
                "Reference the class namespace where you want the change to show up.  In this example, runner is using max from the file config. I want my test to change the value of max when runner is using it.maxmaxmain/config.pymain/config.pymax = 15000\nmax = 15000\nmain/runner.pymain/runner.pyfrom main import config\ndef check_threads():\n    return max < thread_count \nfrom main import config\ndef check_threads():\n    return max < thread_count \ntests/runner_test.pytests/runner_test.pyfrom main import runner                # <----- 1. add file\nfrom main.runner import check_threads\nclass RunnerTest(unittest):\n   def test_threads(self):\n       runner.max = 0                  # <----- 2. set global \n       check_threads()\nfrom main import runner                # <----- 1. add file\nfrom main.runner import check_threads\nclass RunnerTest(unittest):\n   def test_threads(self):\n       runner.max = 0                  # <----- 2. set global \n       check_threads()\n",
                "global_var = 10  # will be considered as a global variable\n\n\ndef func_1():\n    global global_var  # access variable using variable keyword\n    global_var += 1\n\n\ndef func_2():\n    global global_var\n    global_var *= 2\n    print(f\"func_2: {global_var}\")\n\n\nfunc_1()\nfunc_2()\nprint(\"Global scope:\", global_var) # will print 22\nglobal_var = 10  # will be considered as a global variable\n\n\ndef func_1():\n    global global_var  # access variable using variable keyword\n    global_var += 1\n\n\ndef func_2():\n    global global_var\n    global_var *= 2\n    print(f\"func_2: {global_var}\")\n\n\nfunc_1()\nfunc_2()\nprint(\"Global scope:\", global_var) # will print 22\nExplanation:global_var is a global variable and all functions and classes can access that variable.global_varThe func_1() accessed that global variable using the keyword global which points to the variable which is written in the global scope. If I didn't write the global keyword the variable global_var inside func_1 is considered a local variable that is only usable inside the function. Then inside func_1, I have incremented that global variable by 1.func_1()globalglobal_varfunc_1func_1The same happened in func_2().func_2()After calling func_1 and func_2, you'll see the global_var is changedfunc_1func_2global_var",
                "Globals are fine - Except with MultiprocessingGlobals in connection with multiprocessing on different platforms/envrionments \nas Windows/Mac OS on the one side and Linux on the other are troublesome.I will show you this with a simple example pointing out a problem which I run into some time ago. If you want to understand, why things are different on Windows/MacOs and Linux you \nneed to know that, the default mechanism to start a new process on ...\nWindows/MacOs is 'spawn'\nLinux is 'fork'\nWindows/MacOs is 'spawn'Linux is 'fork'They are different in Memory allocation an initialisation ... (but I don't go into this\nhere). Let's have a look at the problem/example ...import multiprocessing\n\ncounter = 0\n\ndef do(task_id):\n    global counter\n    counter +=1\n    print(f'task {task_id}: counter = {counter}')\n\nif __name__ == '__main__':\n\n    pool = multiprocessing.Pool(processes=4)\n    task_ids = list(range(4))\n    pool.map(do, task_ids)\nimport multiprocessing\n\ncounter = 0\n\ndef do(task_id):\n    global counter\n    counter +=1\n    print(f'task {task_id}: counter = {counter}')\n\nif __name__ == '__main__':\n\n    pool = multiprocessing.Pool(processes=4)\n    task_ids = list(range(4))\n    pool.map(do, task_ids)\nWindowsIf you run this on Windows (And I suppose on MacOS too), you get the following output ...task 0: counter = 1\ntask 1: counter = 2\ntask 2: counter = 3\ntask 3: counter = 4\ntask 0: counter = 1\ntask 1: counter = 2\ntask 2: counter = 3\ntask 3: counter = 4\nLinuxIf you run this on Linux, you get the following instead. task 0: counter = 1\ntask 1: counter = 1\ntask 2: counter = 1\ntask 3: counter = 1\ntask 0: counter = 1\ntask 1: counter = 1\ntask 2: counter = 1\ntask 3: counter = 1\n",
                "There are 2 ways to declare a variable as global:1. assign variable inside functions and use global line1. assign variable inside functions and use global linedef declare_a_global_variable():\n    global global_variable_1\n    global_variable_1 = 1\n\n# Note to use the function to global variables\ndeclare_a_global_variable() \ndef declare_a_global_variable():\n    global global_variable_1\n    global_variable_1 = 1\n\n# Note to use the function to global variables\ndeclare_a_global_variable() \n2. assign variable outside functions:2. assign variable outside functions:global_variable_2 = 2\nglobal_variable_2 = 2\nNow we can use these declared global variables in the other functions:def declare_a_global_variable():\n    global global_variable_1\n    global_variable_1 = 1\n\n# Note to use the function to global variables\ndeclare_a_global_variable() \nglobal_variable_2 = 2\n\ndef print_variables():\n    print(global_variable_1)\n    print(global_variable_2)\nprint_variables() # prints 1 & 2\ndef declare_a_global_variable():\n    global global_variable_1\n    global_variable_1 = 1\n\n# Note to use the function to global variables\ndeclare_a_global_variable() \nglobal_variable_2 = 2\n\ndef print_variables():\n    print(global_variable_1)\n    print(global_variable_2)\nprint_variables() # prints 1 & 2\nNote 1:Note 1:If you want to change a global variable inside another function like update_variables() you should use global line in that function before assigning the variable:update_variables()global_variable_1 = 1\nglobal_variable_2 = 2\n\ndef update_variables():\n    global global_variable_1\n    global_variable_1 = 11\n    global_variable_2 = 12 # will update just locally for this function\n\nupdate_variables()\nprint(global_variable_1) # prints 11\nprint(global_variable_2) # prints 2\nglobal_variable_1 = 1\nglobal_variable_2 = 2\n\ndef update_variables():\n    global global_variable_1\n    global_variable_1 = 11\n    global_variable_2 = 12 # will update just locally for this function\n\nupdate_variables()\nprint(global_variable_1) # prints 11\nprint(global_variable_2) # prints 2\nNote 2:Note 2:There is a exception for note 1 for list and dictionary variables while not using global line inside a function:# declaring some global variables\nvariable = 'peter'\nlist_variable_1 = ['a','b']\nlist_variable_2 = ['c','d']\n\ndef update_global_variables():\n    \"\"\"without using global line\"\"\"\n    variable = 'PETER' # won't update in global scope\n    list_variable_1 = ['A','B'] # won't update in global scope\n    list_variable_2[0] = 'C' # updated in global scope surprisingly this way\n    list_variable_2[1] = 'D' # updated in global scope surprisingly this way\n\nupdate_global_variables()\n\nprint('variable is: %s'%variable) # prints peter\nprint('list_variable_1 is: %s'%list_variable_1) # prints ['a', 'b']\nprint('list_variable_2 is: %s'%list_variable_2) # prints ['C', 'D']\n# declaring some global variables\nvariable = 'peter'\nlist_variable_1 = ['a','b']\nlist_variable_2 = ['c','d']\n\ndef update_global_variables():\n    \"\"\"without using global line\"\"\"\n    variable = 'PETER' # won't update in global scope\n    list_variable_1 = ['A','B'] # won't update in global scope\n    list_variable_2[0] = 'C' # updated in global scope surprisingly this way\n    list_variable_2[1] = 'D' # updated in global scope surprisingly this way\n\nupdate_global_variables()\n\nprint('variable is: %s'%variable) # prints peter\nprint('list_variable_1 is: %s'%list_variable_1) # prints ['a', 'b']\nprint('list_variable_2 is: %s'%list_variable_2) # prints ['C', 'D']\n",
                "Though this has been answered, I am giving solution again as I prefer single line\nThis is if you wish to create global variable within functiondef someFunc():\n    x=20\n    globals()['y']=50\nsomeFunc() # invoking function so that variable Y is created globally \nprint(y) # output 50\nprint(x) #NameError: name 'x' is not defined as x was defined locally within function\ndef someFunc():\n    x=20\n    globals()['y']=50\nsomeFunc() # invoking function so that variable Y is created globally \nprint(y) # output 50\nprint(x) #NameError: name 'x' is not defined as x was defined locally within function\n",
                "Like this code:myVar = 12\n\ndef myFunc():\n  myVar += 12\nmyVar = 12\n\ndef myFunc():\n  myVar += 12\nKey:Key:If you declare a variable outside the strings, it become global.If you declare a variable inside the strings, it become local.If you want to declare a global variable inside the strings, use the keyword global before the variable you want to declare:globalmyVar = 124\ndef myFunc():\n  global myVar2\n  myVar2 = 100\nmyFunc()\nprint(myVar2)\nmyVar = 124\ndef myFunc():\n  global myVar2\n  myVar2 = 100\nmyFunc()\nprint(myVar2)\nand then you have 100 in the document.",
                "Initialized = 0  #Here This Initialized is global variable  \n\ndef Initialize():\n     print(\"Initialized!\")\n     Initialized = 1  #This is local variable and assigning 1 to local variable\nwhile Initialized == 0:  \nInitialized = 0  #Here This Initialized is global variable  \n\ndef Initialize():\n     print(\"Initialized!\")\n     Initialized = 1  #This is local variable and assigning 1 to local variable\nwhile Initialized == 0:  \nHere we are comparing global variable Initialized that 0, so while loop condition got true     Initialize()\n     Initialize()\nFunction will get called.Loop will be infinite#if we do Initialized=1 then loop will terminate  \n\nelse:\n    print(\"Lets do something else now!\")\n#if we do Initialized=1 then loop will terminate  \n\nelse:\n    print(\"Lets do something else now!\")\n",
                "if you want to access global var you just add global keyword inside your function\nex:\nglobal_var = 'yeah'def someFunc():\n   global global_var;\n   print(nam_of_var)\ndef someFunc():\n   global global_var;\n   print(nam_of_var)\n"
            ]
        },
        {
            "tag": "dataframe",
            "patterns": [
                "How to iterate over rows in a DataFrame in Pandas",
                "how can you iterate over the rows in a dataframe in pandas?",
                "how do i iterate over rows in a dataframe with pandas?"
            ],
            "responses": [
                "DataFrame.iterrows is a generator which yields both the index and row (as a Series):DataFrame.iterrowsDataFrame.iterrowsimport pandas as pd\n\ndf = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\ndf = df.reset_index()  # make sure indexes pair with number of rows\n\nfor index, row in df.iterrows():\n    print(row['c1'], row['c2'])\nimport pandas as pd\n\ndf = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\ndf = df.reset_index()  # make sure indexes pair with number of rows\n\nfor index, row in df.iterrows():\n    print(row['c1'], row['c2'])\n10 100\n11 110\n12 120\n10 100\n11 110\n12 120\n",
                "\nHow to iterate over rows in a DataFrame in Pandas\nHow to iterate over rows in a DataFrame in PandasAnswer: DON'T*!*Iteration in Pandas is an anti-pattern and is something you should only do when you have exhausted every other option. You should not use any function with \"iter\" in its name for more than a few thousand rows or you will have to get used to a lot of waiting.iterlotDo you want to print a DataFrame? Use DataFrame.to_string().DataFrame.to_string()DataFrame.to_string()DataFrame.to_string()Do you want to compute something? In that case, search for methods in this order (list modified from here):here\nVectorization\nCython routines\nList Comprehensions (vanilla for loop)\nDataFrame.apply(): i) \u00a0Reductions that can be performed in Cython, ii) Iteration in Python space\nDataFrame.itertuples() and iteritems()\nDataFrame.iterrows()\nVectorizationCython routinesCythonList Comprehensions (vanilla for loop)forDataFrame.apply(): i) \u00a0Reductions that can be performed in Cython, ii) Iteration in Python spaceDataFrame.apply()DataFrame.apply()DataFrame.apply()DataFrame.itertuples() and iteritems()DataFrame.itertuples()DataFrame.itertuples()DataFrame.itertuples()iteritems()iteritems()iteritems()DataFrame.iterrows()DataFrame.iterrows()DataFrame.iterrows()DataFrame.iterrows()iterrows and itertuples (both receiving many votes in answers to this question) should be used in very rare circumstances, such as generating row objects/nametuples for sequential processing, which is really the only thing these functions are useful for.iterrowsitertuplesAppeal to AuthorityAppeal to AuthorityThe documentation page on iteration has a huge red warning box that says:The documentation page\nIterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed [...].\nIterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed [...].* It's actually a little more complicated than \"don't\". df.iterrows() is the correct answer to this question, but \"vectorize your ops\" is the better one. I will concede that there are circumstances where iteration cannot be avoided (for example, some operations where the result depends on the value computed for the previous row). However, it takes some familiarity with the library to know when. If you're not sure whether you need an iterative solution, you probably don't. PS: To know more about my rationale for writing this answer, skip to the very bottom.* It's actually a little more complicated than \"don't\". df.iterrows() is the correct answer to this question, but \"vectorize your ops\" is the better one. I will concede that there are circumstances where iteration cannot be avoided (for example, some operations where the result depends on the value computed for the previous row). However, it takes some familiarity with the library to know when. If you're not sure whether you need an iterative solution, you probably don't. PS: To know more about my rationale for writing this answer, skip to the very bottom.df.iterrows()Faster than Looping: Vectorization, CythonVectorizationCythonA good number of basic operations and computations are \"vectorised\" by pandas (either through NumPy, or through Cythonized functions). This includes arithmetic, comparisons, (most) reductions, reshaping (such as pivoting), joins, and groupby operations. Look through the documentation on Essential Basic Functionality to find a suitable vectorised method for your problem.Essential Basic FunctionalityIf none exists, feel free to write your own using custom Cython extensions.Cython extensionsNext Best Thing: List Comprehensions*List Comprehensions*List comprehensions should be your next port of call if 1) there is no vectorized solution available, 2) performance is important, but not important enough to go through the hassle of cythonizing your code, and 3) you're trying to perform elementwise transformation on your code. There is a good amount of evidence to suggest that list comprehensions are sufficiently fast (and even sometimes faster) for many common Pandas tasks.good amount of evidenceThe formula is simple,# Iterating over one column - `f` is some function that processes your data\nresult = [f(x) for x in df['col']]\n# Iterating over two columns, use `zip`\nresult = [f(x, y) for x, y in zip(df['col1'], df['col2'])]\n# Iterating over multiple columns - same data type\nresult = [f(row[0], ..., row[n]) for row in df[['col1', ...,'coln']].to_numpy()]\n# Iterating over multiple columns - differing data type\nresult = [f(row[0], ..., row[n]) for row in zip(df['col1'], ..., df['coln'])]\n# Iterating over one column - `f` is some function that processes your data\nresult = [f(x) for x in df['col']]\n# Iterating over two columns, use `zip`\nresult = [f(x, y) for x, y in zip(df['col1'], df['col2'])]\n# Iterating over multiple columns - same data type\nresult = [f(row[0], ..., row[n]) for row in df[['col1', ...,'coln']].to_numpy()]\n# Iterating over multiple columns - differing data type\nresult = [f(row[0], ..., row[n]) for row in zip(df['col1'], ..., df['coln'])]\nIf you can encapsulate your business logic into a function, you can use a list comprehension that calls it. You can make arbitrarily complex things work through the simplicity and speed of raw Python code.CaveatsCaveatsList comprehensions assume that your data is easy to work with - what that means is your data types are consistent and you don't have NaNs, but this cannot always be guaranteed.\nThe first one is more obvious, but when dealing with NaNs, prefer in-built pandas methods if they exist (because they have much better corner-case handling logic), or ensure your business logic includes appropriate NaN handling logic.\nWhen dealing with mixed data types you should iterate over zip(df['A'], df['B'], ...) instead of df[['A', 'B']].to_numpy() as the latter implicitly upcasts data to the most common type. As an example if A is numeric and B is string, to_numpy() will cast the entire array to string, which may not be what you want. Fortunately zipping your columns together is the most straightforward workaround to this.\nThe first one is more obvious, but when dealing with NaNs, prefer in-built pandas methods if they exist (because they have much better corner-case handling logic), or ensure your business logic includes appropriate NaN handling logic.When dealing with mixed data types you should iterate over zip(df['A'], df['B'], ...) instead of df[['A', 'B']].to_numpy() as the latter implicitly upcasts data to the most common type. As an example if A is numeric and B is string, to_numpy() will cast the entire array to string, which may not be what you want. Fortunately zipping your columns together is the most straightforward workaround to this.zip(df['A'], df['B'], ...)df[['A', 'B']].to_numpy()to_numpy()zip*Your mileage may vary for the reasons outlined in the Caveats section above.*Your mileage may vary for the reasons outlined in the Caveats section above.CaveatsAn Obvious ExampleLet's demonstrate the difference with a simple example of adding two pandas columns A + B. This is a vectorizable operation, so it will be easy to contrast the performance of the methods discussed above.A + B\nBenchmarking code, for your reference. The line at the bottom measures a function written in numpandas, a style of Pandas that mixes heavily with NumPy to squeeze out maximum performance. Writing numpandas code should be avoided unless you know what you're doing. Stick to the API where you can (i.e., prefer vec over vec_numpy).\nI should mention, however, that it isn't always this cut and dry. Sometimes the answer to \"what is the best method for an operation\" is \"it depends on your data\". My advice is to test out different approaches on your data before settling on one.\n\nMy Personal Opinion *\nMost of the analyses performed on the various alternatives to the iter family has been through the lens of performance. However, in most situations you will typically be working on a reasonably sized dataset (nothing beyond a few thousand or 100K rows) and performance will come second to simplicity/readability of the solution.\nHere is my personal preference when selecting a method to use for a problem.\nFor the novice:\n\nVectorization (when possible); apply(); List Comprehensions; itertuples()/iteritems(); iterrows(); Cython\n\nFor the more experienced:\n\nVectorization (when possible); apply(); List Comprehensions; Cython; itertuples()/iteritems(); iterrows()\n\nVectorization prevails as the most idiomatic method for any problem that can be vectorized. Always seek to vectorize! When in doubt, consult the docs, or look on Stack Overflow for an existing question on your particular task.\nI do tend to go on about how bad apply is in a lot of my posts, but I do concede it is easier for a beginner to wrap their head around what it's doing. Additionally, there are quite a few use cases for apply has explained in this post of mine.\nCython ranks lower down on the list because it takes more time and effort to pull off correctly. You will usually never need to write code with pandas that demands this level of performance that even a list comprehension cannot satisfy.\n* As with any personal opinion, please take with heaps of salt!\n\nFurther Reading\n\n10 Minutes to pandas, and Essential Basic Functionality - Useful links that introduce you to Pandas and its library of vectorized*/cythonized functions.\n\nEnhancing Performance - A primer from the documentation on enhancing standard Pandas operations\n\nAre for-loops in pandas really bad? When should I care? - a detailed write-up by me on list comprehensions and their suitability for various operations (mainly ones involving non-numeric data)\n\nWhen should I (not) want to use pandas apply() in my code? - apply is slow (but not as slow as the iter* family. There are, however, situations where one can (or should) consider apply as a serious alternative, especially in some GroupBy operations).\n\n\n* Pandas string methods are \"vectorized\" in the sense that they are specified on the series but operate on each element. The underlying mechanisms are still iterative, because string operations are inherently hard to vectorize.\n\nWhy I Wrote this Answer\nA common trend I notice from new users is to ask questions of the form \"How can I iterate over my df to do X?\". Showing code that calls iterrows() while doing something inside a for loop. Here is why. A new user to the library who has not been introduced to the concept of vectorization will likely envision the code that solves their problem as iterating over their data to do something. Not knowing how to iterate over a DataFrame, the first thing they do is Google it and end up here, at this question. They then see the accepted answer telling them how to, and they close their eyes and run this code without ever first questioning if iteration is the right thing to do.\nThe aim of this answer is to help new users understand that iteration is not necessarily the solution to every problem, and that better, faster and more idiomatic solutions could exist, and that it is worth investing time in exploring them. I'm not trying to start a war of iteration vs. vectorization, but I want new users to be informed when developing solutions to their problems with this library.\nBenchmarking code, for your reference. The line at the bottom measures a function written in numpandas, a style of Pandas that mixes heavily with NumPy to squeeze out maximum performance. Writing numpandas code should be avoided unless you know what you're doing. Stick to the API where you can (i.e., prefer vec over vec_numpy).Benchmarking code, for your referencevecvec_numpyI should mention, however, that it isn't always this cut and dry. Sometimes the answer to \"what is the best method for an operation\" is \"it depends on your data\". My advice is to test out different approaches on your data before settling on one.My Personal Opinion **Most of the analyses performed on the various alternatives to the iter family has been through the lens of performance. However, in most situations you will typically be working on a reasonably sized dataset (nothing beyond a few thousand or 100K rows) and performance will come second to simplicity/readability of the solution.Here is my personal preference when selecting a method to use for a problem.For the novice:\nVectorization (when possible); apply(); List Comprehensions; itertuples()/iteritems(); iterrows(); Cython\nVectorization (when possible); apply(); List Comprehensions; itertuples()/iteritems(); iterrows(); CythonVectorization; apply(); List Comprehensions; itertuples()/iteritems(); iterrows(); Cythonapply()itertuples()iteritems()iterrows()For the more experienced:\nVectorization (when possible); apply(); List Comprehensions; Cython; itertuples()/iteritems(); iterrows()\nVectorization (when possible); apply(); List Comprehensions; Cython; itertuples()/iteritems(); iterrows()Vectorization; apply(); List Comprehensions; Cython; itertuples()/iteritems(); iterrows()apply()itertuples()iteritems()iterrows()Vectorization prevails as the most idiomatic method for any problem that can be vectorized. Always seek to vectorize! When in doubt, consult the docs, or look on Stack Overflow for an existing question on your particular task.I do tend to go on about how bad apply is in a lot of my posts, but I do concede it is easier for a beginner to wrap their head around what it's doing. Additionally, there are quite a few use cases for apply has explained in this post of mine.applyapplythis post of mineCython ranks lower down on the list because it takes more time and effort to pull off correctly. You will usually never need to write code with pandas that demands this level of performance that even a list comprehension cannot satisfy.* As with any personal opinion, please take with heaps of salt!* As with any personal opinion, please take with heaps of salt!Further Reading\n10 Minutes to pandas, and Essential Basic Functionality - Useful links that introduce you to Pandas and its library of vectorized*/cythonized functions.\n\nEnhancing Performance - A primer from the documentation on enhancing standard Pandas operations\n\nAre for-loops in pandas really bad? When should I care? - a detailed write-up by me on list comprehensions and their suitability for various operations (mainly ones involving non-numeric data)\n\nWhen should I (not) want to use pandas apply() in my code? - apply is slow (but not as slow as the iter* family. There are, however, situations where one can (or should) consider apply as a serious alternative, especially in some GroupBy operations).\n\n10 Minutes to pandas, and Essential Basic Functionality - Useful links that introduce you to Pandas and its library of vectorized*/cythonized functions.\n10 Minutes to pandas, and Essential Basic Functionality - Useful links that introduce you to Pandas and its library of vectorized*/cythonized functions.10 Minutes to pandasEssential Basic FunctionalityEnhancing Performance - A primer from the documentation on enhancing standard Pandas operations\nEnhancing Performance - A primer from the documentation on enhancing standard Pandas operationsEnhancing PerformanceAre for-loops in pandas really bad? When should I care? - a detailed write-up by me on list comprehensions and their suitability for various operations (mainly ones involving non-numeric data)\nAre for-loops in pandas really bad? When should I care? - a detailed write-up by me on list comprehensions and their suitability for various operations (mainly ones involving non-numeric data)Are for-loops in pandas really bad? When should I care?Are for-loops in pandas really bad? When should I care?When should I (not) want to use pandas apply() in my code? - apply is slow (but not as slow as the iter* family. There are, however, situations where one can (or should) consider apply as a serious alternative, especially in some GroupBy operations).\nWhen should I (not) want to use pandas apply() in my code? - apply is slow (but not as slow as the iter* family. There are, however, situations where one can (or should) consider apply as a serious alternative, especially in some GroupBy operations).When should I (not) want to use pandas apply() in my code?When should I (not) want to use pandas apply() in my code?applyiter*applyGroupBy* Pandas string methods are \"vectorized\" in the sense that they are specified on the series but operate on each element. The underlying mechanisms are still iterative, because string operations are inherently hard to vectorize.* Pandas string methods are \"vectorized\" in the sense that they are specified on the series but operate on each element. The underlying mechanisms are still iterative, because string operations are inherently hard to vectorize.Why I Wrote this AnswerA common trend I notice from new users is to ask questions of the form \"How can I iterate over my df to do X?\". Showing code that calls iterrows() while doing something inside a for loop. Here is why. A new user to the library who has not been introduced to the concept of vectorization will likely envision the code that solves their problem as iterating over their data to do something. Not knowing how to iterate over a DataFrame, the first thing they do is Google it and end up here, at this question. They then see the accepted answer telling them how to, and they close their eyes and run this code without ever first questioning if iteration is the right thing to do.iterrows()forThe aim of this answer is to help new users understand that iteration is not necessarily the solution to every problem, and that better, faster and more idiomatic solutions could exist, and that it is worth investing time in exploring them. I'm not trying to start a war of iteration vs. vectorization, but I want new users to be informed when developing solutions to their problems with this library.",
                "First consider if you really need to iterate over rows in a DataFrame. See this answer for alternatives.iteratethis answerIf you still need to iterate over rows, you can use methods below. Note some  important caveats which are not mentioned in any of the other answers.important caveats\nDataFrame.iterrows()\n  for index, row in df.iterrows():\n      print(row[\"c1\"], row[\"c2\"])\n\n\nDataFrame.itertuples()\n  for row in df.itertuples(index=True, name='Pandas'):\n      print(row.c1, row.c2)\n\n\nDataFrame.iterrows()\n  for index, row in df.iterrows():\n      print(row[\"c1\"], row[\"c2\"])\n\nDataFrame.iterrows()DataFrame.iterrows()  for index, row in df.iterrows():\n      print(row[\"c1\"], row[\"c2\"])\n  for index, row in df.iterrows():\n      print(row[\"c1\"], row[\"c2\"])\nDataFrame.itertuples()\n  for row in df.itertuples(index=True, name='Pandas'):\n      print(row.c1, row.c2)\n\nDataFrame.itertuples()DataFrame.itertuples()  for row in df.itertuples(index=True, name='Pandas'):\n      print(row.c1, row.c2)\n  for row in df.itertuples(index=True, name='Pandas'):\n      print(row.c1, row.c2)\nitertuples() is supposed to be faster than iterrows()itertuples()iterrows()But be aware, according to the docs (pandas 0.24.2 at the moment):\niterrows: dtype might not match from row to row\niterrows: dtype might not match from row to rowdtype\nBecause iterrows returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally much faster than iterrows()\nBecause iterrows returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally much faster than iterrows()does not preserve\niterrows: Do not modify rows\niterrows: Do not modify rows\nYou should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect.\nYou should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect.never modifyUse DataFrame.apply() instead:DataFrame.apply()    new_df = df.apply(lambda x: x * 2, axis = 1)\n    new_df = df.apply(lambda x: x * 2, axis = 1)\n\nitertuples:\nitertuples:\nThe column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (>255), regular tuples are returned.\nThe column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (>255), regular tuples are returned.See pandas docs on iteration for more details.pandas docs on iteration",
                "You should use df.iterrows(). Though iterating row-by-row is not especially efficient since Series objects have to be created.df.iterrows()df.iterrows()Series",
                "While iterrows() is a good option, sometimes itertuples() can be much faster:iterrows()itertuples()df = pd.DataFrame({'a': randn(1000), 'b': randn(1000),'N': randint(100, 1000, (1000)), 'x': 'x'})\n\n%timeit [row.a * 2 for idx, row in df.iterrows()]\n# => 10 loops, best of 3: 50.3 ms per loop\n\n%timeit [row[1] * 2 for row in df.itertuples()]\n# => 1000 loops, best of 3: 541 \u00b5s per loop\ndf = pd.DataFrame({'a': randn(1000), 'b': randn(1000),'N': randint(100, 1000, (1000)), 'x': 'x'})\n\n%timeit [row.a * 2 for idx, row in df.iterrows()]\n# => 10 loops, best of 3: 50.3 ms per loop\n\n%timeit [row[1] * 2 for row in df.itertuples()]\n# => 1000 loops, best of 3: 541 \u00b5s per loop\n",
                "You can use the df.iloc function as follows:df.ilocdf.ilocfor i in range(0, len(df)):\n    print(df.iloc[i]['c1'], df.iloc[i]['c2'])\nfor i in range(0, len(df)):\n    print(df.iloc[i]['c1'], df.iloc[i]['c2'])\n",
                "You can also use df.apply() to iterate over rows and access multiple columns for a function.df.apply()docs: DataFrame.apply()docs: DataFrame.apply()def valuation_formula(x, y):\n    return x * y * 0.5\n\ndf['price'] = df.apply(lambda row: valuation_formula(row['x'], row['y']), axis=1)\ndef valuation_formula(x, y):\n    return x * y * 0.5\n\ndf['price'] = df.apply(lambda row: valuation_formula(row['x'], row['y']), axis=1)\n",
                "How to iterate efficientlyIf you really have to iterate a Pandas dataframe, you will probably want to avoid using iterrows(). There are different methods and the usual iterrows() is far from being the best. itertuples() can be 100 times faster.avoid using iterrows()iterrows()itertuples() can be 100 times faster.In short:In short:\nAs a general rule, use df.itertuples(name=None). In particular, when you have a fixed number columns and less than 255 columns. See point (3)\nOtherwise, use df.itertuples() except if your columns have special characters such as spaces or '-'. See point (2)\nIt is possible to use itertuples() even if your dataframe has strange columns by using the last example. See point (4)\nOnly use iterrows() if you cannot the previous solutions. See point (1)\nAs a general rule, use df.itertuples(name=None). In particular, when you have a fixed number columns and less than 255 columns. See point (3)df.itertuples(name=None)See point (3)Otherwise, use df.itertuples() except if your columns have special characters such as spaces or '-'. See point (2)df.itertuples()See point (2)It is possible to use itertuples() even if your dataframe has strange columns by using the last example. See point (4)itertuples()See point (4)Only use iterrows() if you cannot the previous solutions. See point (1)iterrows()See point (1)Different methods to iterate over rows in a Pandas dataframe:Generate a random dataframe with a million rows and 4 columns:    df = pd.DataFrame(np.random.randint(0, 100, size=(1000000, 4)), columns=list('ABCD'))\n    print(df)\n    df = pd.DataFrame(np.random.randint(0, 100, size=(1000000, 4)), columns=list('ABCD'))\n    print(df)\n1) The usual iterrows() is convenient, but damn slow:iterrows()start_time = time.clock()\nresult = 0\nfor _, row in df.iterrows():\n    result += max(row['B'], row['C'])\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"1. Iterrows done in {} seconds, result = {}\".format(total_elapsed_time, result))\nstart_time = time.clock()\nresult = 0\nfor _, row in df.iterrows():\n    result += max(row['B'], row['C'])\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"1. Iterrows done in {} seconds, result = {}\".format(total_elapsed_time, result))\n2) The default itertuples() is already much faster, but it doesn't work with column names such as My Col-Name is very Strange (you should avoid this method if your columns are repeated or if a column name cannot be simply converted to a Python variable name).:itertuples()My Col-Name is very Strangestart_time = time.clock()\nresult = 0\nfor row in df.itertuples(index=False):\n    result += max(row.B, row.C)\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"2. Named Itertuples done in {} seconds, result = {}\".format(total_elapsed_time, result))\nstart_time = time.clock()\nresult = 0\nfor row in df.itertuples(index=False):\n    result += max(row.B, row.C)\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"2. Named Itertuples done in {} seconds, result = {}\".format(total_elapsed_time, result))\n3) The default itertuples() using name=None is even faster but not really convenient as you have to define a variable per column.itertuples()start_time = time.clock()\nresult = 0\nfor(_, col1, col2, col3, col4) in df.itertuples(name=None):\n    result += max(col2, col3)\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"3. Itertuples done in {} seconds, result = {}\".format(total_elapsed_time, result))\nstart_time = time.clock()\nresult = 0\nfor(_, col1, col2, col3, col4) in df.itertuples(name=None):\n    result += max(col2, col3)\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"3. Itertuples done in {} seconds, result = {}\".format(total_elapsed_time, result))\n4) Finally, the named itertuples() is slower than the previous point, but you do not have to define a variable per column and it works with column names such as My Col-Name is very Strange.itertuples()My Col-Name is very Strangestart_time = time.clock()\nresult = 0\nfor row in df.itertuples(index=False):\n    result += max(row[df.columns.get_loc('B')], row[df.columns.get_loc('C')])\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"4. Polyvalent Itertuples working even with special characters in the column name done in {} seconds, result = {}\".format(total_elapsed_time, result))\nstart_time = time.clock()\nresult = 0\nfor row in df.itertuples(index=False):\n    result += max(row[df.columns.get_loc('B')], row[df.columns.get_loc('C')])\n\ntotal_elapsed_time = round(time.clock() - start_time, 2)\nprint(\"4. Polyvalent Itertuples working even with special characters in the column name done in {} seconds, result = {}\".format(total_elapsed_time, result))\nOutput:         A   B   C   D\n0       41  63  42  23\n1       54   9  24  65\n2       15  34  10   9\n3       39  94  82  97\n4        4  88  79  54\n...     ..  ..  ..  ..\n999995  48  27   4  25\n999996  16  51  34  28\n999997   1  39  61  14\n999998  66  51  27  70\n999999  51  53  47  99\n\n[1000000 rows x 4 columns]\n\n1. Iterrows done in 104.96 seconds, result = 66151519\n2. Named Itertuples done in 1.26 seconds, result = 66151519\n3. Itertuples done in 0.94 seconds, result = 66151519\n4. Polyvalent Itertuples working even with special characters in the column name done in 2.94 seconds, result = 66151519\n         A   B   C   D\n0       41  63  42  23\n1       54   9  24  65\n2       15  34  10   9\n3       39  94  82  97\n4        4  88  79  54\n...     ..  ..  ..  ..\n999995  48  27   4  25\n999996  16  51  34  28\n999997   1  39  61  14\n999998  66  51  27  70\n999999  51  53  47  99\n\n[1000000 rows x 4 columns]\n\n1. Iterrows done in 104.96 seconds, result = 66151519\n2. Named Itertuples done in 1.26 seconds, result = 66151519\n3. Itertuples done in 0.94 seconds, result = 66151519\n4. Polyvalent Itertuples working even with special characters in the column name done in 2.94 seconds, result = 66151519\nThis article is a very interesting comparison between iterrows and itertuplesThis article is a very interesting comparison between iterrows and itertuples",
                "I was looking for How to iterate on rows and columns and ended here so:How to iterate on rowsandandcolumnsfor i, row in df.iterrows():\n    for j, column in row.iteritems():\n        print(column)\nfor i, row in df.iterrows():\n    for j, column in row.iteritems():\n        print(column)\n",
                "We have multiple options to do the same, and lots of folks have shared their answers.I found the below two methods easy and efficient to do:\nDataFrame.iterrows()\nDataFrame.itertuples()\nDataFrame.iterrows()DataFrame.iterrows()DataFrame.itertuples()DataFrame.itertuples()Example: import pandas as pd\n inp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\n df = pd.DataFrame(inp)\n print (df)\n\n # With the iterrows method\n\n for index, row in df.iterrows():\n     print(row[\"c1\"], row[\"c2\"])\n\n # With the itertuples method\n\n for row in df.itertuples(index=True, name='Pandas'):\n     print(row.c1, row.c2)\n import pandas as pd\n inp = [{'c1':10, 'c2':100}, {'c1':11,'c2':110}, {'c1':12,'c2':120}]\n df = pd.DataFrame(inp)\n print (df)\n\n # With the iterrows method\n\n for index, row in df.iterrows():\n     print(row[\"c1\"], row[\"c2\"])\n\n # With the itertuples method\n\n for row in df.itertuples(index=True, name='Pandas'):\n     print(row.c1, row.c2)\nNote: itertuples() is supposed to be faster than iterrows()",
                "You can write your own iterator that implements namedtuplenamedtuplefrom collections import namedtuple\n\ndef myiter(d, cols=None):\n    if cols is None:\n        v = d.values.tolist()\n        cols = d.columns.values.tolist()\n    else:\n        j = [d.columns.get_loc(c) for c in cols]\n        v = d.values[:, j].tolist()\n\n    n = namedtuple('MyTuple', cols)\n\n    for line in iter(v):\n        yield n(*line)\nfrom collections import namedtuple\n\ndef myiter(d, cols=None):\n    if cols is None:\n        v = d.values.tolist()\n        cols = d.columns.values.tolist()\n    else:\n        j = [d.columns.get_loc(c) for c in cols]\n        v = d.values[:, j].tolist()\n\n    n = namedtuple('MyTuple', cols)\n\n    for line in iter(v):\n        yield n(*line)\nThis is directly comparable to pd.DataFrame.itertuples.  I'm aiming at performing the same task with more efficiency.pd.DataFrame.itertuplesFor the given dataframe with my function:list(myiter(df))\n\n[MyTuple(c1=10, c2=100), MyTuple(c1=11, c2=110), MyTuple(c1=12, c2=120)]\nlist(myiter(df))\n\n[MyTuple(c1=10, c2=100), MyTuple(c1=11, c2=110), MyTuple(c1=12, c2=120)]\nOr with pd.DataFrame.itertuples:pd.DataFrame.itertupleslist(df.itertuples(index=False))\n\n[Pandas(c1=10, c2=100), Pandas(c1=11, c2=110), Pandas(c1=12, c2=120)]\nlist(df.itertuples(index=False))\n\n[Pandas(c1=10, c2=100), Pandas(c1=11, c2=110), Pandas(c1=12, c2=120)]\nA comprehensive test\nWe test making all columns available and subsetting the columns.  A comprehensive testdef iterfullA(d):\n    return list(myiter(d))\n\ndef iterfullB(d):\n    return list(d.itertuples(index=False))\n\ndef itersubA(d):\n    return list(myiter(d, ['col3', 'col4', 'col5', 'col6', 'col7']))\n\ndef itersubB(d):\n    return list(d[['col3', 'col4', 'col5', 'col6', 'col7']].itertuples(index=False))\n\nres = pd.DataFrame(\n    index=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    columns='iterfullA iterfullB itersubA itersubB'.split(),\n    dtype=float\n)\n\nfor i in res.index:\n    d = pd.DataFrame(np.random.randint(10, size=(i, 10))).add_prefix('col')\n    for j in res.columns:\n        stmt = '{}(d)'.format(j)\n        setp = 'from __main__ import d, {}'.format(j)\n        res.at[i, j] = timeit(stmt, setp, number=100)\n\nres.groupby(res.columns.str[4:-1], axis=1).plot(loglog=True);\ndef iterfullA(d):\n    return list(myiter(d))\n\ndef iterfullB(d):\n    return list(d.itertuples(index=False))\n\ndef itersubA(d):\n    return list(myiter(d, ['col3', 'col4', 'col5', 'col6', 'col7']))\n\ndef itersubB(d):\n    return list(d[['col3', 'col4', 'col5', 'col6', 'col7']].itertuples(index=False))\n\nres = pd.DataFrame(\n    index=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    columns='iterfullA iterfullB itersubA itersubB'.split(),\n    dtype=float\n)\n\nfor i in res.index:\n    d = pd.DataFrame(np.random.randint(10, size=(i, 10))).add_prefix('col')\n    for j in res.columns:\n        stmt = '{}(d)'.format(j)\n        setp = 'from __main__ import d, {}'.format(j)\n        res.at[i, j] = timeit(stmt, setp, number=100)\n\nres.groupby(res.columns.str[4:-1], axis=1).plot(loglog=True);\n",
                "To loop all rows in a dataframe you can use:dataframefor x in range(len(date_example.index)):\n    print date_example['Date'].iloc[x]\nfor x in range(len(date_example.index)):\n    print date_example['Date'].iloc[x]\n",
                " for ind in df.index:\n     print df['c1'][ind], df['c2'][ind]\n for ind in df.index:\n     print df['c1'][ind], df['c2'][ind]\n",
                "Update: cs95 has updated his answer to include plain numpy vectorization. You can simply refer to his answer.Updatehis answer\ncs95 shows that Pandas vectorization far outperforms other Pandas methods for computing stuff with dataframes.\nI wanted to add that if you first convert the dataframe to a NumPy array and then use vectorization, it's even faster than Pandas dataframe vectorization, (and that includes the time to turn it back into a dataframe series).\nIf you add the following functions to cs95's benchmark code, this becomes pretty evident:\ndef np_vectorization(df):\n    np_arr = df.to_numpy()\n    return pd.Series(np_arr[:,0] + np_arr[:,1], index=df.index)\n\ndef just_np_vectorization(df):\n    np_arr = df.to_numpy()\n    return np_arr[:,0] + np_arr[:,1]\n\n\ncs95 shows that Pandas vectorization far outperforms other Pandas methods for computing stuff with dataframes.cs95 showsI wanted to add that if you first convert the dataframe to a NumPy array and then use vectorization, it's even faster than Pandas dataframe vectorization, (and that includes the time to turn it back into a dataframe series).If you add the following functions to cs95's benchmark code, this becomes pretty evident:def np_vectorization(df):\n    np_arr = df.to_numpy()\n    return pd.Series(np_arr[:,0] + np_arr[:,1], index=df.index)\n\ndef just_np_vectorization(df):\n    np_arr = df.to_numpy()\n    return np_arr[:,0] + np_arr[:,1]\ndef np_vectorization(df):\n    np_arr = df.to_numpy()\n    return pd.Series(np_arr[:,0] + np_arr[:,1], index=df.index)\n\ndef just_np_vectorization(df):\n    np_arr = df.to_numpy()\n    return np_arr[:,0] + np_arr[:,1]\n",
                "Sometimes a useful pattern is:# Borrowing @KutalmisB df example\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]}, index=['a', 'b'])\n# The to_dict call results in a list of dicts\n# where each row_dict is a dictionary with k:v pairs of columns:value for that row\nfor row_dict in df.to_dict(orient='records'):\n    print(row_dict)\n# Borrowing @KutalmisB df example\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]}, index=['a', 'b'])\n# The to_dict call results in a list of dicts\n# where each row_dict is a dictionary with k:v pairs of columns:value for that row\nfor row_dict in df.to_dict(orient='records'):\n    print(row_dict)\nWhich results in:{'col1':1.0, 'col2':0.1}\n{'col1':2.0, 'col2':0.2}\n{'col1':1.0, 'col2':0.1}\n{'col1':2.0, 'col2':0.2}\n",
                "To loop all rows in a dataframe and use values of each row conveniently, namedtuples can be converted to ndarrays. For example:dataframeuseconvenientlynamedtuplesndarraydf = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]}, index=['a', 'b'])\ndf = pd.DataFrame({'col1': [1, 2], 'col2': [0.1, 0.2]}, index=['a', 'b'])\nIterating over the rows:for row in df.itertuples(index=False, name='Pandas'):\n    print np.asarray(row)\nfor row in df.itertuples(index=False, name='Pandas'):\n    print np.asarray(row)\nresults in:[ 1.   0.1]\n[ 2.   0.2]\n[ 1.   0.1]\n[ 2.   0.2]\nPlease note that if index=True, the index is added as the first element of the tuple, which may be undesirable for some applications.index=Truethe index is added as the first element of the tuple",
                "In short\nUse vectorization if possible\nIf an operation can't be vectorized - use list comprehensions\nIf you need a single object representing the entire row - use itertuples\nIf the above is too slow - try swifter.apply\nIf it's still too slow - try a Cython routine\nUse vectorization if possibleIf an operation can't be vectorized - use list comprehensionsIf you need a single object representing the entire row - use itertuplesIf the above is too slow - try swifter.applyswifter.applyIf it's still too slow - try a Cython routineCythonBenchmark",
                "There is a way to iterate throw rows while getting a DataFrame in return, and not a Series. I don't see anyone mentioning that you can pass index as a list for the row to be returned as a DataFrame:for i in range(len(df)):\n    row = df.iloc[[i]]\nfor i in range(len(df)):\n    row = df.iloc[[i]]\nNote the usage of double brackets. This returns a DataFrame with a single row.",
                "For both viewing and modifying values, I would use iterrows(). In a for loop and by using tuple unpacking (see the example: i, row), I use the row for only viewing the value and use i with the loc method when I want to modify values. As stated in previous answers, here you should not modify something you are iterating over.iterrows()i, rowrowilocfor i, row in df.iterrows():\n    df_column_A = df.loc[i, 'A']\n    if df_column_A == 'Old_Value':\n        df_column_A = 'New_value'  \nfor i, row in df.iterrows():\n    df_column_A = df.loc[i, 'A']\n    if df_column_A == 'Old_Value':\n        df_column_A = 'New_value'  \nHere the row in the loop is a copy of that row, and not a view of it. Therefore, you should NOT write something like row['A'] = 'New_Value', it will not modify the DataFrame. However, you can use i and loc and specify the DataFrame to do the work.rowrow['A'] = 'New_Value'iloc",
                "There are so many ways to iterate over the rows in Pandas dataframe. One very simple and intuitive way is:df = pd.DataFrame({'A':[1, 2, 3], 'B':[4, 5, 6], 'C':[7, 8, 9]})\nprint(df)\nfor i in range(df.shape[0]):\n    # For printing the second column\n    print(df.iloc[i, 1])\n\n    # For printing more than one columns\n    print(df.iloc[i, [0, 2]])\ndf = pd.DataFrame({'A':[1, 2, 3], 'B':[4, 5, 6], 'C':[7, 8, 9]})\nprint(df)\nfor i in range(df.shape[0]):\n    # For printing the second column\n    print(df.iloc[i, 1])\n\n    # For printing more than one columns\n    print(df.iloc[i, [0, 2]])\n",
                "The easiest way, use the apply functionapplydef print_row(row):\n   print row['c1'], row['c2']\n\ndf.apply(lambda row: print_row(row), axis=1)\ndef print_row(row):\n   print row['c1'], row['c2']\n\ndf.apply(lambda row: print_row(row), axis=1)\n",
                "Sometimes loops really are better than vectorized codeAs many answers here correctly point out, your default plan in Pandas should be to write vectorized code (with its implicit loops) rather than attempting an explicit loop yourself.  But the question remains whether you should ever write loops in Pandas, and if so what's the best way to loop in those situations.everI believe there is at least one general situation where loops are appropriate: when you need to calculate some function that depends on values in other rows in a somewhat complex manner.  In this case, the looping code is often simpler, more readable, and less error prone than vectorized code.I believe there is at least one general situation where loops are appropriate: when you need to calculate some function that depends on values in other rows in a somewhat complex manner.  In this case, the looping code is often simpler, more readable, and less error prone than vectorized code.otherThe looping code might even be faster too, as you'll see below, so loops might make sense in cases where speed is of utmost importance. But really, those are just going to be subsets of cases where you probably should have been working in numpy/numba (rather than Pandas) to begin with, because optimized numpy/numba will almost always be faster than Pandas.The looping code might even be faster tooLet's show this with an example.  Suppose you want to take a cumulative sum of a column, but reset it whenever some other column equals zero:import pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame( { 'x':[1,2,3,4,5,6], 'y':[1,1,1,0,1,1]  } )\n\n#   x  y  desired_result\n#0  1  1               1\n#1  2  1               3\n#2  3  1               6\n#3  4  0               4\n#4  5  1               9\n#5  6  1              15\nimport pandas as pd\nimport numpy as np\n\ndf = pd.DataFrame( { 'x':[1,2,3,4,5,6], 'y':[1,1,1,0,1,1]  } )\n\n#   x  y  desired_result\n#0  1  1               1\n#1  2  1               3\n#2  3  1               6\n#3  4  0               4\n#4  5  1               9\n#5  6  1              15\nThis is a good example where you could certainly write one line of Pandas to achieve this, although it's not especially readable, especially if you aren't fairly experienced with Pandas already:df.groupby( (df.y==0).cumsum() )['x'].cumsum()\ndf.groupby( (df.y==0).cumsum() )['x'].cumsum()\nThat's going to be fast enough for most situations, although you could also write faster code by avoiding the groupby, but it will likely be even less readable.groupbyAlternatively, what if we write this as a loop?  You could do something like the following with NumPy:import numba as nb\n\n@nb.jit(nopython=True)  # Optional\ndef custom_sum(x,y):\n    x_sum = x.copy()\n    for i in range(1,len(df)):\n        if y[i] > 0: x_sum[i] = x_sum[i-1] + x[i]\n    return x_sum\n\ndf['desired_result'] = custom_sum( df.x.to_numpy(), df.y.to_numpy() )\nimport numba as nb\n\n@nb.jit(nopython=True)  # Optional\ndef custom_sum(x,y):\n    x_sum = x.copy()\n    for i in range(1,len(df)):\n        if y[i] > 0: x_sum[i] = x_sum[i-1] + x[i]\n    return x_sum\n\ndf['desired_result'] = custom_sum( df.x.to_numpy(), df.y.to_numpy() )\nAdmittedly, there's a bit of overhead there required to convert DataFrame columns to NumPy arrays, but the core piece of code is just one line of code that you could read even if you didn't know anything about Pandas or NumPy:if y[i] > 0: x_sum[i] = x_sum[i-1] + x[i]\nif y[i] > 0: x_sum[i] = x_sum[i-1] + x[i]\nAnd this code is actually faster than the vectorized code.  In some quick tests with 100,000 rows, the above is about 10x faster than the groupby approach.  Note that one key to the speed there is numba, which is optional.  Without the \"@nb.jit\" line, the looping code is actually about 10x slower than the groupby approach.fastergroupbygroupbyClearly this example is simple enough that you would likely prefer the one line of pandas to writing a loop with its associated overhead.  However, there are more complex versions of this problem for which the readability or speed of the NumPy/numba loop approach likely makes sense.",
                "You can also do NumPy indexing for even greater speed ups. It's not really iterating but works much better than iteration for certain applications.subset = row['c1'][0:5]\nall = row['c1'][:]\nsubset = row['c1'][0:5]\nall = row['c1'][:]\nYou may also want to cast it to an array. These indexes/selections are supposed to act like NumPy arrays already, but I ran into issues and needed to castnp.asarray(all)\nimgs[:] = cv2.resize(imgs[:], (224,224) ) # Resize every image in an hdf5 file\nnp.asarray(all)\nimgs[:] = cv2.resize(imgs[:], (224,224) ) # Resize every image in an hdf5 file\n",
                "df.iterrows() returns tuple(a, b) where a is the index and b is the row.df.iterrows()tuple(a, b)aindexbrow",
                "Probably the most elegant solution (but certainly not the most efficient):for row in df.values:\n    c2 = row[1]\n    print(row)\n    # ...\n\nfor c1, c2 in df.values:\n    # ...\nfor row in df.values:\n    c2 = row[1]\n    print(row)\n    # ...\n\nfor c1, c2 in df.values:\n    # ...\nNote that:\nthe documentation explicitly recommends to use .to_numpy() instead\nthe produced NumPy array will have a dtype that fits all columns, in the worst case object\nthere are good reasons not to use a loop in the first place\nthe documentation explicitly recommends to use .to_numpy() insteadthe documentation.to_numpy()the produced NumPy array will have a dtype that fits all columns, in the worst case objectobjectthere are good reasons not to use a loop in the first placegood reasonsStill, I think this option should be included here, as a straightforward solution to a (one should think) trivial problem.",
                "This example uses iloc to isolate each digit in the data frame. import pandas as pd\n\n a = [1, 2, 3, 4]\n b = [5, 6, 7, 8]\n\n mjr = pd.DataFrame({'a':a, 'b':b})\n\n size = mjr.shape\n\n for i in range(size[0]):\n     for j in range(size[1]):\n         print(mjr.iloc[i, j])\nimport pandas as pd\n\n a = [1, 2, 3, 4]\n b = [5, 6, 7, 8]\n\n mjr = pd.DataFrame({'a':a, 'b':b})\n\n size = mjr.shape\n\n for i in range(size[0]):\n     for j in range(size[1]):\n         print(mjr.iloc[i, j])\n",
                "Disclaimer: Although here are so many answers which recommend not using an iterative (loop) approach (and I mostly agree), I would still see it as a reasonable approach for the following situation:Disclaimer:notExtend a dataframe with data from an APILet's say you have a large dataframe which contains incomplete user data. Now you have to extend this data with additional columns, for example, the user's age and gender.agegenderBoth values have to be fetched from a backend API. I'm assuming the API doesn't provide a \"batch\" endpoint (which would accept multiple user IDs at once). Otherwise, you should rather call the API only once.The costs (waiting time) for the network request surpass the iteration of the dataframe by far. We're talking about network round trip times of hundreds of milliseconds compared to the negligibly small gains in using alternative approaches to iterations.One expensive network request for each rowSo in this case, I would absolutely prefer using an iterative approach. Although the network request is expensive, it is guaranteed being triggered only once for each row in the dataframe. Here is an example using DataFrame.iterrows:DataFrame.iterrowsExamplefor index, row in users_df.iterrows():\n  user_id = row['user_id']\n\n  # Trigger expensive network request once for each row\n  response_dict = backend_api.get(f'/api/user-data/{user_id}')\n\n  # Extend dataframe with multiple data from response\n  users_df.at[index, 'age'] = response_dict.get('age')\n  users_df.at[index, 'gender'] = response_dict.get('gender')\nfor index, row in users_df.iterrows():\n  user_id = row['user_id']\n\n  # Trigger expensive network request once for each row\n  response_dict = backend_api.get(f'/api/user-data/{user_id}')\n\n  # Extend dataframe with multiple data from response\n  users_df.at[index, 'age'] = response_dict.get('age')\n  users_df.at[index, 'gender'] = response_dict.get('gender')\n",
                "Some libraries (e.g. a Java interop library that I use) require values to be passed in a row at a time, for example, if streaming data. To replicate the streaming nature, I 'stream' my dataframe values one by one, I wrote the below, which comes in handy from time to time.class DataFrameReader:\n  def __init__(self, df):\n    self._df = df\n    self._row = None\n    self._columns = df.columns.tolist()\n    self.reset()\n    self.row_index = 0\n\n  def __getattr__(self, key):\n    return self.__getitem__(key)\n\n  def read(self) -> bool:\n    self._row = next(self._iterator, None)\n    self.row_index += 1\n    return self._row is not None\n\n  def columns(self):\n    return self._columns\n\n  def reset(self) -> None:\n    self._iterator = self._df.itertuples()\n\n  def get_index(self):\n    return self._row[0]\n\n  def index(self):\n    return self._row[0]\n\n  def to_dict(self, columns: List[str] = None):\n    return self.row(columns=columns)\n\n  def tolist(self, cols) -> List[object]:\n    return [self.__getitem__(c) for c in cols]\n\n  def row(self, columns: List[str] = None) -> Dict[str, object]:\n    cols = set(self._columns if columns is None else columns)\n    return {c : self.__getitem__(c) for c in self._columns if c in cols}\n\n  def __getitem__(self, key) -> object:\n    # the df index of the row is at index 0\n    try:\n        if type(key) is list:\n            ix = [self._columns.index(key) + 1 for k in key]\n        else:\n            ix = self._columns.index(key) + 1\n        return self._row[ix]\n    except BaseException as e:\n        return None\n\n  def __next__(self) -> 'DataFrameReader':\n    if self.read():\n        return self\n    else:\n        raise StopIteration\n\n  def __iter__(self) -> 'DataFrameReader':\n    return self\nclass DataFrameReader:\n  def __init__(self, df):\n    self._df = df\n    self._row = None\n    self._columns = df.columns.tolist()\n    self.reset()\n    self.row_index = 0\n\n  def __getattr__(self, key):\n    return self.__getitem__(key)\n\n  def read(self) -> bool:\n    self._row = next(self._iterator, None)\n    self.row_index += 1\n    return self._row is not None\n\n  def columns(self):\n    return self._columns\n\n  def reset(self) -> None:\n    self._iterator = self._df.itertuples()\n\n  def get_index(self):\n    return self._row[0]\n\n  def index(self):\n    return self._row[0]\n\n  def to_dict(self, columns: List[str] = None):\n    return self.row(columns=columns)\n\n  def tolist(self, cols) -> List[object]:\n    return [self.__getitem__(c) for c in cols]\n\n  def row(self, columns: List[str] = None) -> Dict[str, object]:\n    cols = set(self._columns if columns is None else columns)\n    return {c : self.__getitem__(c) for c in self._columns if c in cols}\n\n  def __getitem__(self, key) -> object:\n    # the df index of the row is at index 0\n    try:\n        if type(key) is list:\n            ix = [self._columns.index(key) + 1 for k in key]\n        else:\n            ix = self._columns.index(key) + 1\n        return self._row[ix]\n    except BaseException as e:\n        return None\n\n  def __next__(self) -> 'DataFrameReader':\n    if self.read():\n        return self\n    else:\n        raise StopIteration\n\n  def __iter__(self) -> 'DataFrameReader':\n    return self\nWhich can be used:for row in DataFrameReader(df):\n  print(row.my_column_name)\n  print(row.to_dict())\n  print(row['my_column_name'])\n  print(row.tolist())\nfor row in DataFrameReader(df):\n  print(row.my_column_name)\n  print(row.to_dict())\n  print(row['my_column_name'])\n  print(row.tolist())\nAnd preserves the values/ name mapping for the rows being iterated. Obviously, is a lot slower than using apply and Cython as indicated above, but is necessary in some circumstances.",
                "As the accepted answer states, the fastest way to apply a function over rows is to use a vectorized function, the so-called NumPy ufuncs (universal functions).the accepted answervectorized functionufuncsBut what should you do when the function you want to apply isn't already implemented in NumPy?Well, using the vectorize decorator from numba, you can easily create ufuncs directly in Python like this:vectorizenumbafrom numba import vectorize, float64\n\n@vectorize([float64(float64)])\ndef f(x):\n    #x is your line, do something with it, and return a float\nfrom numba import vectorize, float64\n\n@vectorize([float64(float64)])\ndef f(x):\n    #x is your line, do something with it, and return a float\nThe documentation for this function is here: Creating NumPy universal functionsCreating NumPy universal functionsCreating NumPy universal functions",
                "Along with the great answers in this post I am going to propose Divide and Conquer approach, I am not writing this answer to abolish the other great answers but to fulfill them with another approach which was working efficiently for me. It has two steps of splitting and merging the pandas dataframe:Divide and ConquersplittingmergingPROS of Divide and Conquer:PROS of Divide and Conquer:\nYou don't need to use vectorization or any other methods to cast the type of your dataframe into another type\nYou don't need to Cythonize your code which normally takes extra time from you\nBoth iterrows() and itertuples() in my case were having the same performance over entire dataframe\nDepends on your choice of slicing index, you will be able to exponentially quicken the iteration. The higher index, the quicker your iteration process.\nYou don't need to use vectorization or any other methods to cast the type of your dataframe into another typeYou don't need to Cythonize your code which normally takes extra time from youBoth iterrows() and itertuples() in my case were having the same performance over entire dataframeiterrows()itertuples()Depends on your choice of slicing index, you will be able to exponentially quicken the iteration. The higher index, the quicker your iteration process.indexindexCONS of Divide and Conquer:CONS of Divide and Conquer:\nYou shouldn't have dependency over the iteration process to the same dataframe and different slice. Meaning if you want to read or write from other slice, it maybe difficult to do that.\nYou shouldn't have dependency over the iteration process to the same dataframe and different slice. Meaning if you want to read or write from other slice, it maybe difficult to do that.sliceslice===================    Divide and Conquer Approach    ====================================    Divide and Conquer Approach    =================Step 1: Splitting/SlicingStep 1: Splitting/SlicingIn this step, we are going to divide the iteration over the entire dataframe. Think that you are going to read a CSV file into pandas df then iterate over it. In may case I have 5,000,000 records and I am going to split it into 100,000 records.CSVNOTE: I need to reiterate as other runtime analysis explained in the other solutions in this page, \"number of records\" has exponential proportion of \"runtime\" on search on the df. Based on the benchmark on my data here are the results:NOTE:Number of records | Iteration rate [per second]\n========================================\n100,000           | 500\n500,000           | 200\n1,000,000         | 50\n5,000,000         | 20\nNumber of records | Iteration rate [per second]\n========================================\n100,000           | 500\n500,000           | 200\n1,000,000         | 50\n5,000,000         | 20\nStep 2: MergingStep 2: MergingThis is going to be an easy step, just merge all the written CSV files into one dataframe and write it into a bigger CSV file.Here is the sample code:# Step 1 (Splitting/Slicing)\nimport pandas as pd\ndf_all = pd.read_csv('C:/KtV.csv')\ndf_index = 100000\ndf_len = len(df)\nfor i in range(df_len // df_index + 1):\n    lower_bound = i * df_index\n    higher_bound = min(lower_bound + df_index, df_len)\n    # Splitting/slicing df (make sure to copy() otherwise it will be a view\n    df = df_all[lower_bound:higher_bound].copy()\n    '''\n    Write your iteration over the sliced df here\n    using iterrows() or intertuples() or ...\n    '''\n    # Writing into CSV files\n    df.to_csv('C:/KtV_prep_' + str(i) + '.csv')\n\n\n\n# Step 2 (Merging)\nfilename = 'C:/KtV_prep_'\ndf = (pd.read_csv(f) for f in [filename + str(i) + '.csv' for i in range(ktv_len // ktv_index + 1)])\ndf_prep_all = pd.concat(df)\ndf_prep_all.to_csv('C:/KtV_prep_all.csv')\n# Step 1 (Splitting/Slicing)\nimport pandas as pd\ndf_all = pd.read_csv('C:/KtV.csv')\ndf_index = 100000\ndf_len = len(df)\nfor i in range(df_len // df_index + 1):\n    lower_bound = i * df_index\n    higher_bound = min(lower_bound + df_index, df_len)\n    # Splitting/slicing df (make sure to copy() otherwise it will be a view\n    df = df_all[lower_bound:higher_bound].copy()\n    '''\n    Write your iteration over the sliced df here\n    using iterrows() or intertuples() or ...\n    '''\n    # Writing into CSV files\n    df.to_csv('C:/KtV_prep_' + str(i) + '.csv')\n\n\n\n# Step 2 (Merging)\nfilename = 'C:/KtV_prep_'\ndf = (pd.read_csv(f) for f in [filename + str(i) + '.csv' for i in range(ktv_len // ktv_index + 1)])\ndf_prep_all = pd.concat(df)\ndf_prep_all.to_csv('C:/KtV_prep_all.csv')\nReference:Reference:Efficient way of iteration over datafreameEfficient way of iteration over datafreameConcatenate CSV files into one Pandas DataframeConcatenate CSV files into one Pandas Dataframe"
            ]
        },
        {
            "tag": "time",
            "patterns": [
                "How do I get the current time?",
                "how do i know the current time?",
                "what can i do to get the current time?"
            ],
            "responses": [
                "Use datetime:datetimedatetime>>> import datetime\n>>> now = datetime.datetime.now()\n>>> now\ndatetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n>>> print(now)\n2009-01-06 15:08:24.789150\n>>> import datetime\n>>> now = datetime.datetime.now()\n>>> now\ndatetime.datetime(2009, 1, 6, 15, 8, 24, 78915)\n>>> print(now)\n2009-01-06 15:08:24.789150\nFor just the clock time without the date:>>> now.time()\ndatetime.time(15, 8, 24, 78915)\n>>> print(now.time())\n15:08:24.789150\n>>> now.time()\ndatetime.time(15, 8, 24, 78915)\n>>> print(now.time())\n15:08:24.789150\nTo save typing, you can import the datetime object from the datetime module:datetimedatetimedatetime>>> from datetime import datetime\n>>> from datetime import datetime\nThen remove the prefix datetime. from all of the above.datetime.",
                "Use time.strftime():time.strftime()time.strftime()>>> from time import gmtime, strftime\n>>> strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n'2009-01-05 22:14:39'\n>>> from time import gmtime, strftime\n>>> strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())\n'2009-01-05 22:14:39'\n",
                "from datetime import datetime\ndatetime.now().strftime('%Y-%m-%d %H:%M:%S')\nfrom datetime import datetime\ndatetime.now().strftime('%Y-%m-%d %H:%M:%S')\nExample output: '2013-09-18 11:16:32''2013-09-18 11:16:32'See list of strftime directives.strftime directivesstrftime",
                "Similar to Harley's answer, but use the str() function for a quick-n-dirty, slightly more human readable format:Harley's answerstr()>>> from datetime import datetime\n>>> str(datetime.now())\n'2011-05-03 17:45:35.177000'\n>>> from datetime import datetime\n>>> str(datetime.now())\n'2011-05-03 17:45:35.177000'\n",
                "\nHow do I get the current time in Python?\nHow do I get the current time in Python?The time moduletimeThe time module provides functions that tell us the time in \"seconds since the epoch\" as well as other utilities.timeimport time\nimport time\nUnix Epoch TimeThis is the format you should get timestamps in for saving in databases. It is a simple floating-point number that can be converted to an integer. It is also good for arithmetic in seconds, as it represents the number of seconds since Jan 1, 1970, 00:00:00, and it is memory light relative to the other representations of time we'll be looking at next:>>> time.time()\n1424233311.771502\n>>> time.time()\n1424233311.771502\nThis timestamp does not account for leap-seconds, so it's not linear - leap seconds are ignored. So while it is not equivalent to the international UTC standard, it is close, and therefore quite good for most cases of record-keeping.This is not ideal for human scheduling, however. If you have a future event you wish to take place at a certain point in time, you'll want to store that time with a string that can be parsed into a datetime object or a serialized datetime object (these will be described later).datetimedatetimetime.ctimetime.ctimeYou can also represent the current time in the way preferred by your operating system (which means it can change when you change your system preferences, so don't rely on this to be standard across all systems, as I've seen others expect). This is typically user friendly, but doesn't typically result in strings one can sort chronologically:>>> time.ctime()\n'Tue Feb 17 23:21:56 2015'\n>>> time.ctime()\n'Tue Feb 17 23:21:56 2015'\nYou can hydrate timestamps into human readable form with ctime as well:ctime>>> time.ctime(1424233311.771502)\n'Tue Feb 17 23:21:51 2015'\n>>> time.ctime(1424233311.771502)\n'Tue Feb 17 23:21:51 2015'\nThis conversion is also not good for record-keeping (except in text that will only be parsed by humans - and with improved Optical Character Recognition and Artificial Intelligence, I think the number of these cases will diminish).datetime moduledatetimeThe datetime module is also quite useful here:datetime>>> import datetime\n>>> import datetime\ndatetime.datetime.nowdatetime.datetime.nowThe datetime.now is a class method that returns the current time. It uses the time.localtime without the timezone info (if not given, otherwise see timezone aware below). It has a representation (which would allow you to recreate an equivalent object) echoed on the shell, but when printed (or coerced to a str), it is in human readable (and nearly ISO) format, and the lexicographic sort is equivalent to the chronological sort:datetime.nowtime.localtimestr>>> datetime.datetime.now()\ndatetime.datetime(2015, 2, 17, 23, 43, 49, 94252)\n>>> print(datetime.datetime.now())\n2015-02-17 23:43:51.782461\n>>> datetime.datetime.now()\ndatetime.datetime(2015, 2, 17, 23, 43, 49, 94252)\n>>> print(datetime.datetime.now())\n2015-02-17 23:43:51.782461\ndatetime's utcnowutcnowYou can get a datetime object in UTC time, a global standard, by doing this:>>> datetime.datetime.utcnow()\ndatetime.datetime(2015, 2, 18, 4, 53, 28, 394163)\n>>> print(datetime.datetime.utcnow())\n2015-02-18 04:53:31.783988\n>>> datetime.datetime.utcnow()\ndatetime.datetime(2015, 2, 18, 4, 53, 28, 394163)\n>>> print(datetime.datetime.utcnow())\n2015-02-18 04:53:31.783988\nUTC is a time standard that is nearly equivalent to the GMT timezone. (While GMT and UTC do not change for Daylight Savings Time, their users may switch to other timezones, like British Summer Time, during the Summer.)datetime timezone awareHowever, none of the datetime objects we've created so far can be easily converted to various timezones. We can solve that problem with the pytz module:pytz>>> import pytz\n>>> then = datetime.datetime.now(pytz.utc)\n>>> then\ndatetime.datetime(2015, 2, 18, 4, 55, 58, 753949, tzinfo=<UTC>)\n>>> import pytz\n>>> then = datetime.datetime.now(pytz.utc)\n>>> then\ndatetime.datetime(2015, 2, 18, 4, 55, 58, 753949, tzinfo=<UTC>)\nEquivalently, in Python 3 we have the timezone class with a utc timezone instance attached, which also makes the object timezone aware (but to convert to another timezone without the handy pytz module is left as an exercise to the reader):timezonetimezonepytz>>> datetime.datetime.now(datetime.timezone.utc)\ndatetime.datetime(2015, 2, 18, 22, 31, 56, 564191, tzinfo=datetime.timezone.utc)\n>>> datetime.datetime.now(datetime.timezone.utc)\ndatetime.datetime(2015, 2, 18, 22, 31, 56, 564191, tzinfo=datetime.timezone.utc)\nAnd we see we can easily convert to timezones from the original UTC object.>>> print(then)\n2015-02-18 04:55:58.753949+00:00\n>>> print(then.astimezone(pytz.timezone('US/Eastern')))\n2015-02-17 23:55:58.753949-05:00\n>>> print(then)\n2015-02-18 04:55:58.753949+00:00\n>>> print(then.astimezone(pytz.timezone('US/Eastern')))\n2015-02-17 23:55:58.753949-05:00\nYou can also make a naive datetime object aware with the pytz timezone localize method, or by replacing the tzinfo attribute (with replace, this is done blindly), but these are more last resorts than best practices:pytzlocalizereplace>>> pytz.utc.localize(datetime.datetime.utcnow())\ndatetime.datetime(2015, 2, 18, 6, 6, 29, 32285, tzinfo=<UTC>)\n>>> datetime.datetime.utcnow().replace(tzinfo=pytz.utc)\ndatetime.datetime(2015, 2, 18, 6, 9, 30, 728550, tzinfo=<UTC>)\n>>> pytz.utc.localize(datetime.datetime.utcnow())\ndatetime.datetime(2015, 2, 18, 6, 6, 29, 32285, tzinfo=<UTC>)\n>>> datetime.datetime.utcnow().replace(tzinfo=pytz.utc)\ndatetime.datetime(2015, 2, 18, 6, 9, 30, 728550, tzinfo=<UTC>)\nThe pytz module allows us to make our datetime objects timezone aware and convert the times to the hundreds of timezones available in the pytz module.pytzdatetimepytzOne could ostensibly serialize this object for UTC time and store that in a database, but it would require far more memory and be more prone to error than simply storing the Unix Epoch time, which I demonstrated first.thatThe other ways of viewing times are much more error-prone, especially when dealing with data that may come from different time zones. You want there to be no confusion as to which timezone a string or serialized datetime object was intended for.If you're displaying the time with Python for the user, ctime works nicely, not in a table (it doesn't typically sort well), but perhaps in a clock. However, I personally recommend, when dealing with time in Python, either using Unix time, or a timezone aware UTC datetime object.ctimedatetime",
                "Dofrom time import time\n\nt = time()\nfrom time import time\n\nt = time()\n\nt - float number, good for time interval measurement.\nt - float number, good for time interval measurement.tThere is some difference for Unix and Windows platforms.",
                ">>> from time import gmtime, strftime\n>>> strftime(\"%a, %d %b %Y %X +0000\", gmtime())\n'Tue, 06 Jan 2009 04:54:56 +0000'\n>>> from time import gmtime, strftime\n>>> strftime(\"%a, %d %b %Y %X +0000\", gmtime())\n'Tue, 06 Jan 2009 04:54:56 +0000'\nThat outputs the current GMT in the specified format. There is also a localtime() method. localtime()localtime()This page has more details.page",
                "The previous answers are all good suggestions, but I find it easiest to use ctime():ctime()In [2]: from time import ctime\nIn [3]: ctime()\nOut[3]: 'Thu Oct 31 11:40:53 2013'\nIn [2]: from time import ctime\nIn [3]: ctime()\nOut[3]: 'Thu Oct 31 11:40:53 2013'\nThis gives a nicely formatted string representation of the current local time. ",
                "The quickest way is:>>> import time\n>>> time.strftime(\"%Y%m%d\")\n'20130924'\n>>> import time\n>>> time.strftime(\"%Y%m%d\")\n'20130924'\n",
                "If you need current time as a time object:time>>> import datetime\n>>> now = datetime.datetime.now()\n>>> datetime.time(now.hour, now.minute, now.second)\ndatetime.time(11, 23, 44)\n>>> import datetime\n>>> now = datetime.datetime.now()\n>>> datetime.time(now.hour, now.minute, now.second)\ndatetime.time(11, 23, 44)\n",
                "You can use the time module:time>>> import time\n>>> print(time.strftime(\"%d/%m/%Y\"))\n06/02/2015\n>>> import time\n>>> print(time.strftime(\"%d/%m/%Y\"))\n06/02/2015\nThe use of the capital Y gives the full year, and using y would give 06/02/15.Yy06/02/15You could also use the following code to give a more lengthy time:>>> time.strftime(\"%a, %d %b %Y %H:%M:%S\")\n'Fri, 06 Feb 2015 17:45:09'\n>>> time.strftime(\"%a, %d %b %Y %H:%M:%S\")\n'Fri, 06 Feb 2015 17:45:09'\n",
                ".isoformat() is in the documentation, but not yet here\n(this is mighty similar to @Ray Vega's answer):.isoformat()>>> import datetime\n>>> datetime.datetime.now().isoformat()\n'2013-06-24T20:35:55.982000'\n>>> import datetime\n>>> datetime.datetime.now().isoformat()\n'2013-06-24T20:35:55.982000'\n",
                "Why not ask the U.S. Naval Observatory, the official timekeeper of the United States Navy?U.S. Naval Observatoryimport requests\nfrom lxml import html\n\npage = requests.get('http://tycho.usno.navy.mil/cgi-bin/timer.pl')\ntree = html.fromstring(page.content)\nprint(tree.xpath('//html//body//h3//pre/text()')[1])\nimport requests\nfrom lxml import html\n\npage = requests.get('http://tycho.usno.navy.mil/cgi-bin/timer.pl')\ntree = html.fromstring(page.content)\nprint(tree.xpath('//html//body//h3//pre/text()')[1])\nIf you live in the D.C. area (like me) the latency might not be too bad...",
                "Using pandas to get the current time, kind of overkilling the problem at hand:pandasimport pandas as pd\nprint(pd.datetime.now())\nprint(pd.datetime.now().date())\nprint(pd.datetime.now().year)\nprint(pd.datetime.now().month)\nprint(pd.datetime.now().day)\nprint(pd.datetime.now().hour)\nprint(pd.datetime.now().minute)\nprint(pd.datetime.now().second)\nprint(pd.datetime.now().microsecond)\nimport pandas as pd\nprint(pd.datetime.now())\nprint(pd.datetime.now().date())\nprint(pd.datetime.now().year)\nprint(pd.datetime.now().month)\nprint(pd.datetime.now().day)\nprint(pd.datetime.now().hour)\nprint(pd.datetime.now().minute)\nprint(pd.datetime.now().second)\nprint(pd.datetime.now().microsecond)\nOutput:2017-09-22 12:44:56.092642\n2017-09-22\n2017\n9\n22\n12\n44\n56\n92693\n2017-09-22 12:44:56.092642\n2017-09-22\n2017\n9\n22\n12\n44\n56\n92693\n",
                "if you are using numpy already then directly you can use numpy.datetime64() \nfunction.import numpy as np\nstr(np.datetime64('now'))\nimport numpy as np\nstr(np.datetime64('now'))\nfor only date:str(np.datetime64('today'))\nstr(np.datetime64('today'))\nor, if you are using pandas already then you can use pandas.to_datetime() functionimport pandas as pd\nstr(pd.to_datetime('now'))\nimport pandas as pd\nstr(pd.to_datetime('now'))\nor,str(pd.to_datetime('today'))\nstr(pd.to_datetime('today'))\n",
                "This is what I ended up going with: >>>from time import strftime\n>>>strftime(\"%m/%d/%Y %H:%M\")\n01/09/2015 13:11\n>>>from time import strftime\n>>>strftime(\"%m/%d/%Y %H:%M\")\n01/09/2015 13:11\nAlso, this table is a necessary reference for choosing the appropriate format codes to get the date formatted just the way you want it (from Python \"datetime\" documentation here).here",
                "datetime.now() returns the current time as a naive datetime object that represents time in the local timezone. That value may be ambiguous e.g., during DST transitions (\"fall back\"). To avoid ambiguity either UTC timezone should be used:datetime.now()from datetime import datetime\n\nutc_time = datetime.utcnow()\nprint(utc_time) # -> 2014-12-22 22:48:59.916417\nfrom datetime import datetime\n\nutc_time = datetime.utcnow()\nprint(utc_time) # -> 2014-12-22 22:48:59.916417\nOr a timezone-aware object that has the corresponding timezone info attached (Python 3.2+):from datetime import datetime, timezone\n\nnow = datetime.now(timezone.utc).astimezone()\nprint(now) # -> 2014-12-23 01:49:25.837541+03:00\nfrom datetime import datetime, timezone\n\nnow = datetime.now(timezone.utc).astimezone()\nprint(now) # -> 2014-12-23 01:49:25.837541+03:00\n",
                "import datetime\ndate_time = datetime.datetime.now()\n\ndate = date_time.date()  # Gives the date\ntime = date_time.time()  # Gives the time\n\nprint date.year, date.month, date.day\nprint time.hour, time.minute, time.second, time.microsecond\nimport datetime\ndate_time = datetime.datetime.now()\n\ndate = date_time.date()  # Gives the date\ntime = date_time.time()  # Gives the time\n\nprint date.year, date.month, date.day\nprint time.hour, time.minute, time.second, time.microsecond\nDo dir(date) or any variables including the package. You can get all the attributes and methods associated with the variable.dir(date)",
                ">>> import datetime, time\n>>> time = time.strftime(\"%H:%M:%S:%MS\", time.localtime())\n>>> print time\n'00:21:38:20S'\n>>> import datetime, time\n>>> time = time.strftime(\"%H:%M:%S:%MS\", time.localtime())\n>>> print time\n'00:21:38:20S'\n",
                "This question doesn't need a new answer just for the sake of it ... a shiny new-ish toy/module, however, is enough justification.  That being the Pendulum library, which appears to do the sort of things which arrow attempted, except without the inherent flaws and bugs which beset arrow.Pendulum libraryFor instance, the answer to the original question:>>> import pendulum\n>>> print(pendulum.now())\n2018-08-14T05:29:28.315802+10:00\n>>> print(pendulum.now('utc'))\n2018-08-13T19:29:35.051023+00:00\n>>> import pendulum\n>>> print(pendulum.now())\n2018-08-14T05:29:28.315802+10:00\n>>> print(pendulum.now('utc'))\n2018-08-13T19:29:35.051023+00:00\nThere's a lot of standards which need addressing, including multiple RFCs and ISOs, to worry about.  Ever get them mixed up; not to worry, take a little look into dir(pendulum.constants) There's a bit more than RFC and ISO formats there, though.dir(pendulum.constants)When we say local, though what do we mean?  Well I mean:>>> print(pendulum.now().timezone_name)\nAustralia/Melbourne\n>>>\n>>> print(pendulum.now().timezone_name)\nAustralia/Melbourne\n>>>\nPresumably most of the rest of you mean somewhere else.And on it goes.  Long story short: Pendulum attempts to do for date and time what requests did for HTTP.  It's worth consideration, particularly for both its ease of use and extensive documentation.",
                "By default, now() function returns output in the YYYY-MM-DD HH:MM:SS:MS format. Use the below sample script to get the current date and time in a Python script and print results on the screen. Create file getDateTime1.py with the below content.now()YYYY-MM-DD HH:MM:SS:MSgetDateTime1.pyimport datetime\n\ncurrentDT = datetime.datetime.now()\nprint (str(currentDT))\nimport datetime\n\ncurrentDT = datetime.datetime.now()\nprint (str(currentDT))\nThe output looks like below:2018-03-01 17:03:46.759624\n2018-03-01 17:03:46.759624\n",
                "Try the arrow module from http://crsmithdev.com/arrow/:http://crsmithdev.com/arrow/import arrow\narrow.now()\nimport arrow\narrow.now()\nOr the UTC version:arrow.utcnow()\narrow.utcnow()\nTo change its output, add .format():arrow.utcnow().format('YYYY-MM-DD HH:mm:ss ZZ')\narrow.utcnow().format('YYYY-MM-DD HH:mm:ss ZZ')\nFor a specific timezone:arrow.now('US/Pacific')\narrow.now('US/Pacific')\nAn hour ago:arrow.utcnow().replace(hours=-1)\narrow.utcnow().replace(hours=-1)\nOr if you want the gist.arrow.get('2013-05-11T21:23:58.970460+00:00').humanize()\n>>> '2 years ago'\narrow.get('2013-05-11T21:23:58.970460+00:00').humanize()\n>>> '2 years ago'\n",
                "Current time of a timezoneCurrent time of a timezonefrom datetime import datetime\nimport pytz\n\ntz_NY = pytz.timezone('America/New_York') \ndatetime_NY = datetime.now(tz_NY)\nprint(\"NY time:\", datetime_NY.strftime(\"%H:%M:%S\"))\n\ntz_London = pytz.timezone('Europe/London')\ndatetime_London = datetime.now(tz_London)\nprint(\"London time:\", datetime_London.strftime(\"%H:%M:%S\"))\n\ntz_India = pytz.timezone('Asia/India')\ndatetime_India = datetime.now(tz_India)\nprint(\"India time:\", datetime_India.strftime(\"%H:%M:%S\"))\n\n#list timezones\npytz.all_timezones\nfrom datetime import datetime\nimport pytz\n\ntz_NY = pytz.timezone('America/New_York') \ndatetime_NY = datetime.now(tz_NY)\nprint(\"NY time:\", datetime_NY.strftime(\"%H:%M:%S\"))\n\ntz_London = pytz.timezone('Europe/London')\ndatetime_London = datetime.now(tz_London)\nprint(\"London time:\", datetime_London.strftime(\"%H:%M:%S\"))\n\ntz_India = pytz.timezone('Asia/India')\ndatetime_India = datetime.now(tz_India)\nprint(\"India time:\", datetime_India.strftime(\"%H:%M:%S\"))\n\n#list timezones\npytz.all_timezones\n",
                "To get exactly 3 decimal points for milliseconds 11:34:23.751 run this:3decimal points11:34:23.751def get_time_str(decimal_points=3):\n        return time.strftime(\"%H:%M:%S\", time.localtime()) + '.%d' % (time.time() % 1 * 10**decimal_points)\ndef get_time_str(decimal_points=3):\n        return time.strftime(\"%H:%M:%S\", time.localtime()) + '.%d' % (time.time() % 1 * 10**decimal_points)\nMore context:More context:I want to get the time with milliseconds. A simple way to get them:import time, datetime\n\nprint(datetime.datetime.now().time())                         # 11:20:08.272239\n\n# Or in a more complicated way\nprint(datetime.datetime.now().time().isoformat())             # 11:20:08.272239\nprint(datetime.datetime.now().time().strftime('%H:%M:%S.%f')) # 11:20:08.272239\n\n# But do not use this:\nprint(time.strftime(\"%H:%M:%S.%f\", time.localtime()), str)    # 11:20:08.%f\nimport time, datetime\n\nprint(datetime.datetime.now().time())                         # 11:20:08.272239\n\n# Or in a more complicated way\nprint(datetime.datetime.now().time().isoformat())             # 11:20:08.272239\nprint(datetime.datetime.now().time().strftime('%H:%M:%S.%f')) # 11:20:08.272239\n\n# But do not use this:\nprint(time.strftime(\"%H:%M:%S.%f\", time.localtime()), str)    # 11:20:08.%f\nBut I want only milliseconds, right? The shortest way to get them:only millisecondsimport time\n\ntime.strftime(\"%H:%M:%S\", time.localtime()) + '.%d' % (time.time() % 1 * 1000)\n# 11:34:23.751\nimport time\n\ntime.strftime(\"%H:%M:%S\", time.localtime()) + '.%d' % (time.time() % 1 * 1000)\n# 11:34:23.751\nAdd or remove zeroes from the last multiplication to adjust number of decimal points, or just:def get_time_str(decimal_points=3):\n    return time.strftime(\"%H:%M:%S\", time.localtime()) + '.%d' % (time.time() % 1 * 10**decimal_points)\ndef get_time_str(decimal_points=3):\n    return time.strftime(\"%H:%M:%S\", time.localtime()) + '.%d' % (time.time() % 1 * 10**decimal_points)\n",
                "If you just want the current timestamp in ms (for example, to measure execution time), you can also use the \"timeit\" module:import timeit\nstart_time = timeit.default_timer()\ndo_stuff_you_want_to_measure()\nend_time = timeit.default_timer()\nprint(\"Elapsed time: {}\".format(end_time - start_time))\nimport timeit\nstart_time = timeit.default_timer()\ndo_stuff_you_want_to_measure()\nend_time = timeit.default_timer()\nprint(\"Elapsed time: {}\".format(end_time - start_time))\n",
                "You can use this function to get the time (unfortunately it doesn't say AM or PM):def gettime():\n    from datetime import datetime\n    return ((str(datetime.now())).split(' ')[1]).split('.')[0]\ndef gettime():\n    from datetime import datetime\n    return ((str(datetime.now())).split(' ')[1]).split('.')[0]\nTo get the hours, minutes, seconds and milliseconds to merge later, you can use these functions:Hour:Hour:def gethour():\n    from datetime import datetime\n    return (((str(datetime.now())).split(' ')[1]).split('.')[0]).split(':')[0]\ndef gethour():\n    from datetime import datetime\n    return (((str(datetime.now())).split(' ')[1]).split('.')[0]).split(':')[0]\nMinute:Minute:def getminute():\n    from datetime import datetime\n    return (((str(datetime.now())).split(' ')[1]).split('.')[0]).split(':')[1]\ndef getminute():\n    from datetime import datetime\n    return (((str(datetime.now())).split(' ')[1]).split('.')[0]).split(':')[1]\nSecond:Second:def getsecond():\n    from datetime import datetime\n    return (((str(datetime.now())).split(' ')[1]).split('.')[0]).split(':')[2]\ndef getsecond():\n    from datetime import datetime\n    return (((str(datetime.now())).split(' ')[1]).split('.')[0]).split(':')[2]\nMillisecond:Millisecond:def getmillisecond():\n    from datetime import datetime\n    return (str(datetime.now())).split('.')[1]\ndef getmillisecond():\n    from datetime import datetime\n    return (str(datetime.now())).split('.')[1]\n",
                "You can  try the followingimport datetime\n\nnow = datetime.datetime.now()\nprint(now)\nimport datetime\n\nnow = datetime.datetime.now()\nprint(now)\norimport datetime\n\nnow = datetime.datetime.now()\nprint(now.strftime(\"%Y-%b-%d, %A %I:%M:%S\"))\nimport datetime\n\nnow = datetime.datetime.now()\nprint(now.strftime(\"%Y-%b-%d, %A %I:%M:%S\"))\n",
                "Because no one has mentioned it yet, and this is something I ran into recently... a pytz timezone's fromutc() method combined with datetime's utcnow() is the best way I've found to get a useful current time (and date) in any timezone.from datetime import datetime\n\nimport pytz\n\n\nJST = pytz.timezone(\"Asia/Tokyo\")\n\n\nlocal_time = JST.fromutc(datetime.utcnow())\nfrom datetime import datetime\n\nimport pytz\n\n\nJST = pytz.timezone(\"Asia/Tokyo\")\n\n\nlocal_time = JST.fromutc(datetime.utcnow())\nIf all you want is the time, you can then get that with local_time.time().local_time.time()",
                "import datetime\n\ntodays_date = datetime.date.today()\nprint(todays_date)\n>>> 2019-10-12\n\n# adding strftime will remove the seconds\ncurrent_time = datetime.datetime.now().strftime('%H:%M')\nprint(current_time)\n>>> 23:38\nimport datetime\n\ntodays_date = datetime.date.today()\nprint(todays_date)\n>>> 2019-10-12\n\n# adding strftime will remove the seconds\ncurrent_time = datetime.datetime.now().strftime('%H:%M')\nprint(current_time)\n>>> 23:38\n",
                "Method1: Getting Current Date and Time from system datetimeMethod1: Getting Current Date and Time from system datetimeThe datetime module supplies classes for manipulating dates and times.datetime\nCode\nCodefrom datetime import datetime,date\n\nprint(\"Date: \"+str(date.today().year)+\"-\"+str(date.today().month)+\"-\"+str(date.today().day))\nprint(\"Year: \"+str(date.today().year))\nprint(\"Month: \"+str(date.today().month))\nprint(\"Day: \"+str(date.today().day)+\"\\n\")\n\nprint(\"Time: \"+str(datetime.today().hour)+\":\"+str(datetime.today().minute)+\":\"+str(datetime.today().second))\nprint(\"Hour: \"+str(datetime.today().hour))\nprint(\"Minute: \"+str(datetime.today().minute))\nprint(\"Second: \"+str(datetime.today().second))\nprint(\"MilliSecond: \"+str(datetime.today().microsecond))\nfrom datetime import datetime,date\n\nprint(\"Date: \"+str(date.today().year)+\"-\"+str(date.today().month)+\"-\"+str(date.today().day))\nprint(\"Year: \"+str(date.today().year))\nprint(\"Month: \"+str(date.today().month))\nprint(\"Day: \"+str(date.today().day)+\"\\n\")\n\nprint(\"Time: \"+str(datetime.today().hour)+\":\"+str(datetime.today().minute)+\":\"+str(datetime.today().second))\nprint(\"Hour: \"+str(datetime.today().hour))\nprint(\"Minute: \"+str(datetime.today().minute))\nprint(\"Second: \"+str(datetime.today().second))\nprint(\"MilliSecond: \"+str(datetime.today().microsecond))\n\nOutput will be like\nOutput will be likeDate: 2020-4-18\nYear: 2020\nMonth: 4\nDay: 18\n\nTime: 19:30:5\nHour: 19\nMinute: 30\nSecond: 5\nMilliSecond: 836071\nDate: 2020-4-18\nYear: 2020\nMonth: 4\nDay: 18\n\nTime: 19:30:5\nHour: 19\nMinute: 30\nSecond: 5\nMilliSecond: 836071\nMethod2: Getting Current Date and Time if Network is availableMethod2: Getting Current Date and Time if Network is availableurllib package helps us to handle the url's that means webpages. Here we collects data from the webpage http://just-the-time.appspot.com/ and parses dateime from the webpage using the package dateparser.urllibhttp://just-the-time.appspot.com/dateimedateparser\nCode\nCodefrom urllib.request import urlopen\nimport dateparser\n\ntime_url = urlopen(u'http://just-the-time.appspot.com/')\ndatetime = time_url.read().decode(\"utf-8\", errors=\"ignore\").split(' ')[:-1]\ndate = datetime[0]\ntime = datetime[1]\n\nprint(\"Date: \"+str(date))\nprint(\"Year: \"+str(date.split('-')[0]))\nprint(\"Month: \"+str(date.split('-')[1]))\nprint(\"Day: \"+str(date.split('-')[2])+'\\n')\n\nprint(\"Time: \"+str(time))\nprint(\"Hour: \"+str(time.split(':')[0]))\nprint(\"Minute: \"+str(time.split(':')[1]))\nprint(\"Second: \"+str(time.split(':')[2]))\nfrom urllib.request import urlopen\nimport dateparser\n\ntime_url = urlopen(u'http://just-the-time.appspot.com/')\ndatetime = time_url.read().decode(\"utf-8\", errors=\"ignore\").split(' ')[:-1]\ndate = datetime[0]\ntime = datetime[1]\n\nprint(\"Date: \"+str(date))\nprint(\"Year: \"+str(date.split('-')[0]))\nprint(\"Month: \"+str(date.split('-')[1]))\nprint(\"Day: \"+str(date.split('-')[2])+'\\n')\n\nprint(\"Time: \"+str(time))\nprint(\"Hour: \"+str(time.split(':')[0]))\nprint(\"Minute: \"+str(time.split(':')[1]))\nprint(\"Second: \"+str(time.split(':')[2]))\n\nOutput will be like\nOutput will be likeDate: 2020-04-18\nYear: 2020\nMonth: 04\nDay: 18\n\nTime: 14:17:10\nHour: 14\nMinute: 17\nSecond: 10\nDate: 2020-04-18\nYear: 2020\nMonth: 04\nDay: 18\n\nTime: 14:17:10\nHour: 14\nMinute: 17\nSecond: 10\nMethod3: Getting Current Date and Time from Local Time of the MachineMethod3: Getting Current Date and Time from Local Time of the MachinePython's time module provides a function for getting local time from the number of seconds elapsed since the epoch called localtime(). ctime() function takes seconds passed since epoch as an argument and returns a string representing local time.timectime()\nCode\nCodefrom time import time, ctime\ndatetime = ctime(time()).split(' ')\n\nprint(\"Date: \"+str(datetime[4])+\"-\"+str(datetime[1])+\"-\"+str(datetime[2]))\nprint(\"Year: \"+str(datetime[4]))\nprint(\"Month: \"+str(datetime[1]))\nprint(\"Day: \"+str(datetime[2]))\nprint(\"Week Day: \"+str(datetime[0])+'\\n')\n\nprint(\"Time: \"+str(datetime[3]))\nprint(\"Hour: \"+str(datetime[3]).split(':')[0])\nprint(\"Minute: \"+str(datetime[3]).split(':')[1])\nprint(\"Second: \"+str(datetime[3]).split(':')[2])\nfrom time import time, ctime\ndatetime = ctime(time()).split(' ')\n\nprint(\"Date: \"+str(datetime[4])+\"-\"+str(datetime[1])+\"-\"+str(datetime[2]))\nprint(\"Year: \"+str(datetime[4]))\nprint(\"Month: \"+str(datetime[1]))\nprint(\"Day: \"+str(datetime[2]))\nprint(\"Week Day: \"+str(datetime[0])+'\\n')\n\nprint(\"Time: \"+str(datetime[3]))\nprint(\"Hour: \"+str(datetime[3]).split(':')[0])\nprint(\"Minute: \"+str(datetime[3]).split(':')[1])\nprint(\"Second: \"+str(datetime[3]).split(':')[2])\n\nOutput will be like\nOutput will be likeDate: 2020-Apr-18\nYear: 2020\nMonth: Apr\nDay: 18\nWeek Day: Sat\n\nTime: 19:30:20\nHour: 19\nMinute: 30\nSecond: 20\nDate: 2020-Apr-18\nYear: 2020\nMonth: Apr\nDay: 18\nWeek Day: Sat\n\nTime: 19:30:20\nHour: 19\nMinute: 30\nSecond: 20\n"
            ]
        },
        {
            "tag": "multiple_exceptions",
            "patterns": [
                "Catch multiple exceptions in one line (except block)",
                "catch multiple exceptions in a line other than the block exception"
            ],
            "responses": [
                "From Python Documentation:Python Documentation\nAn except clause may name multiple exceptions as a parenthesized tuple, for example\nAn except clause may name multiple exceptions as a parenthesized tuple, for exampleexcept (IDontLikeYouException, YouAreBeingMeanException) as e:\n    pass\nexcept (IDontLikeYouException, YouAreBeingMeanException) as e:\n    pass\nOr, for Python 2 only:except (IDontLikeYouException, YouAreBeingMeanException), e:\n    pass\nexcept (IDontLikeYouException, YouAreBeingMeanException), e:\n    pass\nSeparating the exception from the variable with a comma will still work in Python 2.6 and 2.7, but is now deprecated and does not work in Python 3; now you should be using as.as",
                "\nHow do I catch multiple exceptions in one line (except block)\nHow do I catch multiple exceptions in one line (except block)Do this:try:\n    may_raise_specific_errors():\nexcept (SpecificErrorOne, SpecificErrorTwo) as error:\n    handle(error) # might log or have some other default behavior...\ntry:\n    may_raise_specific_errors():\nexcept (SpecificErrorOne, SpecificErrorTwo) as error:\n    handle(error) # might log or have some other default behavior...\nThe parentheses are required due to older syntax that used the commas to assign the error object to a name. The as keyword is used for the assignment. You can use any name for the error object, I prefer error personally.aserrorBest PracticeTo do this in a manner currently and forward compatible with Python, you need to separate the Exceptions with commas and wrap them with parentheses to differentiate from earlier syntax that assigned the exception instance to a variable name by following the Exception type to be caught with a comma. Here's an example of simple usage:import sys\n\ntry:\n    mainstuff()\nexcept (KeyboardInterrupt, EOFError): # the parens are necessary\n    sys.exit(0)\nimport sys\n\ntry:\n    mainstuff()\nexcept (KeyboardInterrupt, EOFError): # the parens are necessary\n    sys.exit(0)\nI'm specifying only these exceptions to avoid hiding bugs, which if I encounter I expect the full stack trace from.This is documented here: https://docs.python.org/tutorial/errors.htmlhttps://docs.python.org/tutorial/errors.htmlYou can assign the exception to a variable, (e is common, but you might prefer a more verbose variable if you have long exception handling or your IDE only highlights selections larger than that, as mine does.) The instance has an args attribute. Here is an example:eimport sys\n\ntry:\n    mainstuff()\nexcept (KeyboardInterrupt, EOFError) as err: \n    print(err)\n    print(err.args)\n    sys.exit(0)\nimport sys\n\ntry:\n    mainstuff()\nexcept (KeyboardInterrupt, EOFError) as err: \n    print(err)\n    print(err.args)\n    sys.exit(0)\nNote that in Python 3, the err object falls out of scope when the except block is concluded.errexceptDeprecatedYou may see code that assigns the error with a comma. This usage, the only form available in Python 2.5 and earlier, is deprecated, and if you wish your code to be forward compatible in Python 3, you should update the syntax to use the new form:import sys\n\ntry:\n    mainstuff()\nexcept (KeyboardInterrupt, EOFError), err: # don't do this in Python 2.6+\n    print err\n    print err.args\n    sys.exit(0)\nimport sys\n\ntry:\n    mainstuff()\nexcept (KeyboardInterrupt, EOFError), err: # don't do this in Python 2.6+\n    print err\n    print err.args\n    sys.exit(0)\nIf you see the comma name assignment in your codebase, and you're using Python 2.5 or higher, switch to the new way of doing it so your code remains compatible when you upgrade.The suppress context managersuppressThe accepted answer is really 4 lines of code, minimum:try:\n    do_something()\nexcept (IDontLikeYouException, YouAreBeingMeanException) as e:\n    pass\ntry:\n    do_something()\nexcept (IDontLikeYouException, YouAreBeingMeanException) as e:\n    pass\nThe try, except, pass lines can be handled in a single line with the suppress context manager, available in Python 3.4:tryexceptpasssuppress context manager, available in Python 3.4from contextlib import suppress\n\nwith suppress(IDontLikeYouException, YouAreBeingMeanException):\n     do_something()\nfrom contextlib import suppress\n\nwith suppress(IDontLikeYouException, YouAreBeingMeanException):\n     do_something()\nSo when you want to pass on certain exceptions, use suppress.passsuppress",
                "From Python documentation -> 8.3 Handling Exceptions:Python documentation -> 8.3 Handling Exceptions\nA try statement may have more than one except clause, to specify\n  handlers for different exceptions. At most one handler will be\n  executed. Handlers only handle exceptions that occur in the\n  corresponding try clause, not in other handlers of the same try\n  statement. An except clause may name multiple exceptions as a\n  parenthesized tuple, for example:\nexcept (RuntimeError, TypeError, NameError):\n    pass\n\nNote that the parentheses around this tuple are required, because\n  except ValueError, e: was the syntax used for what is normally\n  written as except ValueError as e: in modern Python (described\n  below). The old syntax is still supported for backwards compatibility.\n  This means except RuntimeError, TypeError is not equivalent to\n  except (RuntimeError, TypeError): but to except RuntimeError as\nTypeError: which is not what you want.\nA try statement may have more than one except clause, to specify\n  handlers for different exceptions. At most one handler will be\n  executed. Handlers only handle exceptions that occur in the\n  corresponding try clause, not in other handlers of the same try\n  statement. An except clause may name multiple exceptions as a\n  parenthesized tuple, for example:tryexcept (RuntimeError, TypeError, NameError):\n    pass\nexcept (RuntimeError, TypeError, NameError):\n    pass\nNote that the parentheses around this tuple are required, because\n  except ValueError, e: was the syntax used for what is normally\n  written as except ValueError as e: in modern Python (described\n  below). The old syntax is still supported for backwards compatibility.\n  This means except RuntimeError, TypeError is not equivalent to\n  except (RuntimeError, TypeError): but to except RuntimeError as\nTypeError: which is not what you want.ValueError, e:except ValueError as e:except RuntimeError, TypeErrorexcept (RuntimeError, TypeError):except RuntimeError asTypeError:",
                "If you frequently use a large number of exceptions, you can pre-define a tuple, so you don't have to re-type them many times. #This example code is a technique I use in a library that connects with websites to gather data\n\nConnectErrs  = (URLError, SSLError, SocketTimeoutError, BadStatusLine, ConnectionResetError)\n\ndef connect(url, data):\n    #do connection and return some data\n    return(received_data)\n\ndef some_function(var_a, var_b, ...):\n    try: o = connect(url, data)\n    except ConnectErrs as e:\n        #do the recovery stuff\n    blah #do normal stuff you would do if no exception occurred\n#This example code is a technique I use in a library that connects with websites to gather data\n\nConnectErrs  = (URLError, SSLError, SocketTimeoutError, BadStatusLine, ConnectionResetError)\n\ndef connect(url, data):\n    #do connection and return some data\n    return(received_data)\n\ndef some_function(var_a, var_b, ...):\n    try: o = connect(url, data)\n    except ConnectErrs as e:\n        #do the recovery stuff\n    blah #do normal stuff you would do if no exception occurred\nNOTES: NOTES:\nIf you, also, need to catch other exceptions than those in the\npre-defined tuple, you will need to define another except block.  \nIf you just cannot tolerate a global variable, define it in main()\nand pass it around where needed...\nIf you, also, need to catch other exceptions than those in the\npre-defined tuple, you will need to define another except block.  If you, also, need to catch other exceptions than those in the\npre-defined tuple, you will need to define another except block.  If you just cannot tolerate a global variable, define it in main()\nand pass it around where needed...If you just cannot tolerate a global variable, define it in main()\nand pass it around where needed...",
                "One of the way to do this is..try:\n   You do your operations here;\n   ......................\nexcept(Exception1[, Exception2[,...ExceptionN]]]):\n   If there is any exception from the given exception list, \n   then execute this block.\n   ......................\nelse:\n   If there is no exception then execute this block. \ntry:\n   You do your operations here;\n   ......................\nexcept(Exception1[, Exception2[,...ExceptionN]]]):\n   If there is any exception from the given exception list, \n   then execute this block.\n   ......................\nelse:\n   If there is no exception then execute this block. \nand another way is to create method which performs task executed by except block and call it through all of the except block that you write..exceptexcepttry:\n   You do your operations here;\n   ......................\nexcept Exception1:\n    functionname(parameterList)\nexcept Exception2:\n    functionname(parameterList)\nexcept Exception3:\n    functionname(parameterList)\nelse:\n   If there is no exception then execute this block. \n\ndef functionname( parameters ):\n   //your task..\n   return [expression]\ntry:\n   You do your operations here;\n   ......................\nexcept Exception1:\n    functionname(parameterList)\nexcept Exception2:\n    functionname(parameterList)\nexcept Exception3:\n    functionname(parameterList)\nelse:\n   If there is no exception then execute this block. \n\ndef functionname( parameters ):\n   //your task..\n   return [expression]\nI know that second one is not the best way to do this, but i'm just showing number of ways to do this thing.",
                "As of Python 3.11 you can take advantage of the except* clause that is used to handle multiple exceptions.except*PEP-654 introduced a new standard exception type called ExceptionGroup that corresponds to a group of exceptions that are being propagated together. The ExceptionGroup can be handled using a new except* syntax. The * symbol indicates that multiple exceptions can be handled by each except* clause.ExceptionGroupExceptionGroupexcept**except*For example, you can handle multiple exceptionstry:\n    raise ExceptionGroup('Example ExceptionGroup', (\n        TypeError('Example TypeError'),\n        ValueError('Example ValueError'),\n        KeyError('Example KeyError'),\n        AttributeError('Example AttributeError')\n    ))\nexcept* TypeError:\n    ...\nexcept* ValueError as e:\n    ...\nexcept* (KeyError, AttributeError) as e:\n    ...\ntry:\n    raise ExceptionGroup('Example ExceptionGroup', (\n        TypeError('Example TypeError'),\n        ValueError('Example ValueError'),\n        KeyError('Example KeyError'),\n        AttributeError('Example AttributeError')\n    ))\nexcept* TypeError:\n    ...\nexcept* ValueError as e:\n    ...\nexcept* (KeyError, AttributeError) as e:\n    ...\nFor more details see PEP-654.PEP-654"
            ]
        },
        {
            "tag": "contains",
            "patterns": [
                "Does Python have a string 'contains' substring method?",
                "Does Python have a string 'contains' substring method?"
            ],
            "responses": [
                "Use the in operator:in operatorinif \"blah\" not in somestring: \n    continue\nif \"blah\" not in somestring: \n    continue\n",
                "If it's just a substring search you can use string.find(\"substring\").string.find(\"substring\")You do have to be a little careful with find, index, and in though, as they are substring searches. In other words, this:findfindindexindexinins = \"This be a string\"\nif s.find(\"is\") == -1:\n    print(\"No 'is' here!\")\nelse:\n    print(\"Found 'is' in the string.\")\ns = \"This be a string\"\nif s.find(\"is\") == -1:\n    print(\"No 'is' here!\")\nelse:\n    print(\"Found 'is' in the string.\")\nIt would print Found 'is' in the string. Similarly, if \"is\" in s: would evaluate to True. This may or may not be what you want.Found 'is' in the string.if \"is\" in s:True",
                "\nDoes Python have a string contains substring method?\nDoes Python have a string contains substring method?99% of use cases will be covered using the keyword, in, which returns True or False:99%inTrueFalse'substring' in any_string\n'substring' in any_string\nFor the use case of getting the index, use str.find (which returns -1 on failure, and has optional positional arguments):str.findstart = 0\nstop = len(any_string)\nany_string.find('substring', start, stop)\nstart = 0\nstop = len(any_string)\nany_string.find('substring', start, stop)\nor str.index (like find but raises ValueError on failure):str.indexfindstart = 100 \nend = 1000\nany_string.index('substring', start, end)\nstart = 100 \nend = 1000\nany_string.index('substring', start, end)\nExplanationUse the in comparison operator becausein\nthe language intends its usage, and\nother Python programmers will expect you to use it.\nthe language intends its usage, andother Python programmers will expect you to use it.>>> 'foo' in '**foo**'\nTrue\n>>> 'foo' in '**foo**'\nTrue\nThe opposite (complement), which the original question asked for, is not in:not in>>> 'foo' not in '**foo**' # returns False\nFalse\n>>> 'foo' not in '**foo**' # returns False\nFalse\nThis is semantically the same as not 'foo' in '**foo**' but it's much more readable and explicitly provided for in the language as a readability improvement.not 'foo' in '**foo**'Avoid using __contains____contains__The \"contains\" method implements the behavior for in. This example,instr.__contains__('**foo**', 'foo')\nstr.__contains__('**foo**', 'foo')\nreturns True. You could also call this function from the instance of the superstring:True'**foo**'.__contains__('foo')\n'**foo**'.__contains__('foo')\nBut don't. Methods that start with underscores are considered semantically non-public. The only reason to use this is when implementing or extending the in and not in functionality (e.g. if subclassing str):innot instrclass NoisyString(str):\n    def __contains__(self, other):\n        print(f'testing if \"{other}\" in \"{self}\"')\n        return super(NoisyString, self).__contains__(other)\n\nns = NoisyString('a string with a substring inside')\nclass NoisyString(str):\n    def __contains__(self, other):\n        print(f'testing if \"{other}\" in \"{self}\"')\n        return super(NoisyString, self).__contains__(other)\n\nns = NoisyString('a string with a substring inside')\nand now:>>> 'substring' in ns\ntesting if \"substring\" in \"a string with a substring inside\"\nTrue\n>>> 'substring' in ns\ntesting if \"substring\" in \"a string with a substring inside\"\nTrue\nDon't use find and index to test for \"contains\"findindexDon't use the following string methods to test for \"contains\":>>> '**foo**'.index('foo')\n2\n>>> '**foo**'.find('foo')\n2\n\n>>> '**oo**'.find('foo')\n-1\n>>> '**oo**'.index('foo')\n\nTraceback (most recent call last):\n  File \"<pyshell#40>\", line 1, in <module>\n    '**oo**'.index('foo')\nValueError: substring not found\n>>> '**foo**'.index('foo')\n2\n>>> '**foo**'.find('foo')\n2\n\n>>> '**oo**'.find('foo')\n-1\n>>> '**oo**'.index('foo')\n\nTraceback (most recent call last):\n  File \"<pyshell#40>\", line 1, in <module>\n    '**oo**'.index('foo')\nValueError: substring not found\nOther languages may have no methods to directly test for substrings, and so you would have to use these types of methods, but with Python, it is much more efficient to use the in comparison operator.inAlso, these are not drop-in replacements for in. You may have to handle the exception or -1 cases, and if they return 0 (because they found the substring at the beginning) the boolean interpretation is False instead of True.in-10FalseTrueIf you really mean not any_string.startswith(substring) then say it.not any_string.startswith(substring)Performance comparisonsWe can compare various ways of accomplishing the same goal.import timeit\n\ndef in_(s, other):\n    return other in s\n\ndef contains(s, other):\n    return s.__contains__(other)\n\ndef find(s, other):\n    return s.find(other) != -1\n\ndef index(s, other):\n    try:\n        s.index(other)\n    except ValueError:\n        return False\n    else:\n        return True\n\n\n\nperf_dict = {\n'in:True': min(timeit.repeat(lambda: in_('superstring', 'str'))),\n'in:False': min(timeit.repeat(lambda: in_('superstring', 'not'))),\n'__contains__:True': min(timeit.repeat(lambda: contains('superstring', 'str'))),\n'__contains__:False': min(timeit.repeat(lambda: contains('superstring', 'not'))),\n'find:True': min(timeit.repeat(lambda: find('superstring', 'str'))),\n'find:False': min(timeit.repeat(lambda: find('superstring', 'not'))),\n'index:True': min(timeit.repeat(lambda: index('superstring', 'str'))),\n'index:False': min(timeit.repeat(lambda: index('superstring', 'not'))),\n}\nimport timeit\n\ndef in_(s, other):\n    return other in s\n\ndef contains(s, other):\n    return s.__contains__(other)\n\ndef find(s, other):\n    return s.find(other) != -1\n\ndef index(s, other):\n    try:\n        s.index(other)\n    except ValueError:\n        return False\n    else:\n        return True\n\n\n\nperf_dict = {\n'in:True': min(timeit.repeat(lambda: in_('superstring', 'str'))),\n'in:False': min(timeit.repeat(lambda: in_('superstring', 'not'))),\n'__contains__:True': min(timeit.repeat(lambda: contains('superstring', 'str'))),\n'__contains__:False': min(timeit.repeat(lambda: contains('superstring', 'not'))),\n'find:True': min(timeit.repeat(lambda: find('superstring', 'str'))),\n'find:False': min(timeit.repeat(lambda: find('superstring', 'not'))),\n'index:True': min(timeit.repeat(lambda: index('superstring', 'str'))),\n'index:False': min(timeit.repeat(lambda: index('superstring', 'not'))),\n}\nAnd now we see that using in is much faster than the others.\nLess time to do an equivalent operation is better:in>>> perf_dict\n{'in:True': 0.16450627865128808,\n 'in:False': 0.1609668098178645,\n '__contains__:True': 0.24355481654697542,\n '__contains__:False': 0.24382793854783813,\n 'find:True': 0.3067379407923454,\n 'find:False': 0.29860888058124146,\n 'index:True': 0.29647137792585454,\n 'index:False': 0.5502287584545229}\n>>> perf_dict\n{'in:True': 0.16450627865128808,\n 'in:False': 0.1609668098178645,\n '__contains__:True': 0.24355481654697542,\n '__contains__:False': 0.24382793854783813,\n 'find:True': 0.3067379407923454,\n 'find:False': 0.29860888058124146,\n 'index:True': 0.29647137792585454,\n 'index:False': 0.5502287584545229}\nHow can in be faster than __contains__ if in uses __contains__?in__contains__in__contains__This is a fine follow-on question.Let's disassemble functions with the methods of interest:>>> from dis import dis\n>>> dis(lambda: 'a' in 'b')\n  1           0 LOAD_CONST               1 ('a')\n              2 LOAD_CONST               2 ('b')\n              4 COMPARE_OP               6 (in)\n              6 RETURN_VALUE\n>>> dis(lambda: 'b'.__contains__('a'))\n  1           0 LOAD_CONST               1 ('b')\n              2 LOAD_METHOD              0 (__contains__)\n              4 LOAD_CONST               2 ('a')\n              6 CALL_METHOD              1\n              8 RETURN_VALUE\n>>> from dis import dis\n>>> dis(lambda: 'a' in 'b')\n  1           0 LOAD_CONST               1 ('a')\n              2 LOAD_CONST               2 ('b')\n              4 COMPARE_OP               6 (in)\n              6 RETURN_VALUE\n>>> dis(lambda: 'b'.__contains__('a'))\n  1           0 LOAD_CONST               1 ('b')\n              2 LOAD_METHOD              0 (__contains__)\n              4 LOAD_CONST               2 ('a')\n              6 CALL_METHOD              1\n              8 RETURN_VALUE\nso we see that the .__contains__ method has to be separately looked up and then called from the Python virtual machine - this should adequately explain the difference..__contains__",
                "if needle in haystack: is the normal use, as @Michael says -- it relies on the in operator, more readable and faster than a method call.if needle in haystack:ininIf you truly need a method instead of an operator (e.g. to do some weird key= for a very peculiar sort...?), that would be 'haystack'.__contains__.  But since your example is for use in an if, I guess you don't really mean what you say;-).  It's not good form (nor readable, nor efficient) to use special methods directly -- they're meant to be used, instead, through the operators and builtins that delegate to them.key='haystack'.__contains__'haystack'.__contains__if",
                "in Python strings and listsinHere are a few useful examples that speak for themselves concerning the in method:in>>> \"foo\" in \"foobar\"\nTrue\n>>> \"foo\" in \"Foobar\"\nFalse\n>>> \"foo\" in \"Foobar\".lower()\nTrue\n>>> \"foo\".capitalize() in \"Foobar\"\nTrue\n>>> \"foo\" in [\"bar\", \"foo\", \"foobar\"]\nTrue\n>>> \"foo\" in [\"fo\", \"o\", \"foobar\"]\nFalse\n>>> [\"foo\" in a for a in [\"fo\", \"o\", \"foobar\"]]\n[False, False, True]\n>>> \"foo\" in \"foobar\"\nTrue\n>>> \"foo\" in \"Foobar\"\nFalse\n>>> \"foo\" in \"Foobar\".lower()\nTrue\n>>> \"foo\".capitalize() in \"Foobar\"\nTrue\n>>> \"foo\" in [\"bar\", \"foo\", \"foobar\"]\nTrue\n>>> \"foo\" in [\"fo\", \"o\", \"foobar\"]\nFalse\n>>> [\"foo\" in a for a in [\"fo\", \"o\", \"foobar\"]]\n[False, False, True]\nCaveat. Lists are iterables, and the in method acts on iterables, not just strings.inIf you want to compare strings in a more fuzzy way to measure how \"alike\" they are, consider using the Levenshtein packageHere's an answer that shows how it works.Here's an answer that shows how it works.",
                "If you are happy with \"blah\" in somestring but want it to be a function/method call, you can probably do this\"blah\" in somestringimport operator\n\nif not operator.contains(somestring, \"blah\"):\n    continue\nimport operator\n\nif not operator.contains(somestring, \"blah\"):\n    continue\nAll operators in Python can be more or less found in the operator module including in.operator modulein",
                "So apparently there is nothing similar for vector-wise comparison. An obvious Python way to do so would be:names = ['bob', 'john', 'mike']\nany(st in 'bob and john' for st in names) \n>> True\n\nany(st in 'mary and jane' for st in names) \n>> False\nnames = ['bob', 'john', 'mike']\nany(st in 'bob and john' for st in names) \n>> True\n\nany(st in 'mary and jane' for st in names) \n>> False\n",
                "You can use y.count().y.count()It will return the integer value of the number of times a sub string appears in a string.For example:string.count(\"bah\") >> 0\nstring.count(\"Hello\") >> 1\nstring.count(\"bah\") >> 0\nstring.count(\"Hello\") >> 1\n",
                "Here is your answer:if \"insert_char_or_string_here\" in \"insert_string_to_search_here\":\n    #DOSTUFF\nif \"insert_char_or_string_here\" in \"insert_string_to_search_here\":\n    #DOSTUFF\nFor checking if it is false:if not \"insert_char_or_string_here\" in \"insert_string_to_search_here\":\n    #DOSTUFF\nif not \"insert_char_or_string_here\" in \"insert_string_to_search_here\":\n    #DOSTUFF\nOR:if \"insert_char_or_string_here\" not in \"insert_string_to_search_here\":\n    #DOSTUFF\nif \"insert_char_or_string_here\" not in \"insert_string_to_search_here\":\n    #DOSTUFF\n",
                "You can use regular expressions to get the occurrences:>>> import re\n>>> print(re.findall(r'( |t)', to_search_in)) # searches for t or space\n['t', ' ', 't', ' ', ' ']\n>>> import re\n>>> print(re.findall(r'( |t)', to_search_in)) # searches for t or space\n['t', ' ', 't', ' ', ' ']\n"
            ]
        },
        {
            "tag": "bytes_to_string",
            "patterns": [
                "Convert bytes to a string", "How to change bytes into a string"
            ],
            "responses": [
                "Decode the bytes object to produce a string:Decode the bytes objectbytes>>> b\"abcde\".decode(\"utf-8\") \n'abcde'\n>>> b\"abcde\".decode(\"utf-8\") \n'abcde'\nThe above example assumes that the bytes object is in UTF-8, because it is a common encoding. However, you should use the encoding your data is actually in!assumesbytes",
                "Decode the byte string and turn it in to a character (Unicode) string.Python 3:encoding = 'utf-8'\nb'hello'.decode(encoding)\nencoding = 'utf-8'\nb'hello'.decode(encoding)\norstr(b'hello', encoding)\nstr(b'hello', encoding)\nPython 2:encoding = 'utf-8'\n'hello'.decode(encoding)\nencoding = 'utf-8'\n'hello'.decode(encoding)\norunicode('hello', encoding)\nunicode('hello', encoding)\n",
                "This joins together a list of bytes into a string:>>> bytes_data = [112, 52, 52]\n>>> \"\".join(map(chr, bytes_data))\n'p44'\n>>> bytes_data = [112, 52, 52]\n>>> \"\".join(map(chr, bytes_data))\n'p44'\n",
                "If you don't know the encoding, then to read binary input into string in Python 3 and Python 2 compatible way, use the ancient MS-DOS CP437 encoding:CP437PY3K = sys.version_info >= (3, 0)\n\nlines = []\nfor line in stream:\n    if not PY3K:\n        lines.append(line)\n    else:\n        lines.append(line.decode('cp437'))\nPY3K = sys.version_info >= (3, 0)\n\nlines = []\nfor line in stream:\n    if not PY3K:\n        lines.append(line)\n    else:\n        lines.append(line.decode('cp437'))\nBecause encoding is unknown, expect non-English symbols to translate to characters of cp437 (English characters are not translated, because they match in most single byte encodings and UTF-8).cp437Decoding arbitrary binary input to UTF-8 is unsafe, because you may get this:>>> b'\\x00\\x01\\xffsd'.decode('utf-8')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 2: invalid\nstart byte\n>>> b'\\x00\\x01\\xffsd'.decode('utf-8')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nUnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 2: invalid\nstart byte\nThe same applies to latin-1, which was popular (the default?) for Python 2. See the missing points in Codepage Layout - it is where Python chokes with infamous ordinal not in range.latin-1Codepage Layoutordinal not in rangeUPDATE 20150604: There are rumors that Python 3 has the surrogateescape error strategy for encoding stuff into binary data without data loss and crashes, but it needs conversion tests, [binary] -> [str] -> [binary], to validate both performance and reliability.UPDATE 20150604surrogateescape[binary] -> [str] -> [binary]UPDATE 20170116: Thanks to comment by Nearoo - there is also a possibility to slash escape all unknown bytes with backslashreplace error handler. That works only for Python 3, so even with this workaround you will still get inconsistent output from different Python versions:UPDATE 20170116backslashreplacePY3K = sys.version_info >= (3, 0)\n\nlines = []\nfor line in stream:\n    if not PY3K:\n        lines.append(line)\n    else:\n        lines.append(line.decode('utf-8', 'backslashreplace'))\nPY3K = sys.version_info >= (3, 0)\n\nlines = []\nfor line in stream:\n    if not PY3K:\n        lines.append(line)\n    else:\n        lines.append(line.decode('utf-8', 'backslashreplace'))\nSee Python\u2019s Unicode Support for details.Python\u2019s Unicode SupportPython\u2019s Unicode SupportUPDATE 20170119: I decided to implement slash escaping decode that works for both Python\u00a02 and Python\u00a03. It should be slower than the cp437 solution, but it should produce identical results on every Python version.UPDATE 20170119cp437identical results# --- preparation\n\nimport codecs\n\ndef slashescape(err):\n    \"\"\" codecs error handler. err is UnicodeDecode instance. return\n    a tuple with a replacement for the unencodable part of the input\n    and a position where encoding should continue\"\"\"\n    #print err, dir(err), err.start, err.end, err.object[:err.start]\n    thebyte = err.object[err.start:err.end]\n    repl = u'\\\\x'+hex(ord(thebyte))[2:]\n    return (repl, err.end)\n\ncodecs.register_error('slashescape', slashescape)\n\n# --- processing\n\nstream = [b'\\x80abc']\n\nlines = []\nfor line in stream:\n    lines.append(line.decode('utf-8', 'slashescape'))\n# --- preparation\n\nimport codecs\n\ndef slashescape(err):\n    \"\"\" codecs error handler. err is UnicodeDecode instance. return\n    a tuple with a replacement for the unencodable part of the input\n    and a position where encoding should continue\"\"\"\n    #print err, dir(err), err.start, err.end, err.object[:err.start]\n    thebyte = err.object[err.start:err.end]\n    repl = u'\\\\x'+hex(ord(thebyte))[2:]\n    return (repl, err.end)\n\ncodecs.register_error('slashescape', slashescape)\n\n# --- processing\n\nstream = [b'\\x80abc']\n\nlines = []\nfor line in stream:\n    lines.append(line.decode('utf-8', 'slashescape'))\n",
                "In Python 3, the default encoding is \"utf-8\", so you can directly use:In Python 3\"utf-8\"b'hello'.decode()\nb'hello'.decode()\nwhich is equivalent tob'hello'.decode(encoding=\"utf-8\")\nb'hello'.decode(encoding=\"utf-8\")\nOn the other hand, in Python 2, encoding defaults to the default string encoding. Thus, you should use:in Python 2b'hello'.decode(encoding)\nb'hello'.decode(encoding)\nwhere encoding is the encoding you want.encodingNote: support for keyword arguments was added in Python\u00a02.7.Note:Note:",
                "I think you actually want this:>>> from subprocess import *\n>>> command_stdout = Popen(['ls', '-l'], stdout=PIPE).communicate()[0]\n>>> command_text = command_stdout.decode(encoding='windows-1252')\n>>> from subprocess import *\n>>> command_stdout = Popen(['ls', '-l'], stdout=PIPE).communicate()[0]\n>>> command_text = command_stdout.decode(encoding='windows-1252')\nAaron's answer was correct, except that you need to know which encoding to use. And I believe that Windows uses 'windows-1252'. It will only matter if you have some unusual (non-ASCII) characters in your content, but then it will make a difference.whichBy the way, the fact that it does matter is the reason that Python moved to using two different types for binary and text data: it can't convert magically between them, because it doesn't know the encoding unless you tell it! The only way YOU would know is to read the Windows documentation (or read it here).does",
                "Since this question is actually asking about subprocess output, you have more direct approaches available. The most modern would be using subprocess.check_output and passing text=True (Python 3.7+) to automatically decode stdout using the system default coding:subprocesssubprocess.check_outputsubprocess.check_outputtext=Truetext = subprocess.check_output([\"ls\", \"-l\"], text=True)\ntext = subprocess.check_output([\"ls\", \"-l\"], text=True)\nFor Python 3.6, Popen accepts an encoding keyword:Popenencodingencoding>>> from subprocess import Popen, PIPE\n>>> text = Popen(['ls', '-l'], stdout=PIPE, encoding='utf-8').communicate()[0]\n>>> type(text)\nstr\n>>> print(text)\ntotal 0\n-rw-r--r-- 1 wim badger 0 May 31 12:45 some_file.txt\n>>> from subprocess import Popen, PIPE\n>>> text = Popen(['ls', '-l'], stdout=PIPE, encoding='utf-8').communicate()[0]\n>>> type(text)\nstr\n>>> print(text)\ntotal 0\n-rw-r--r-- 1 wim badger 0 May 31 12:45 some_file.txt\nThe general answer to the question in the title, if you're not dealing with subprocess output, is to decode bytes to text:decode>>> b'abcde'.decode()\n'abcde'\n>>> b'abcde'.decode()\n'abcde'\nWith no argument, sys.getdefaultencoding() will be used.  If your data is not sys.getdefaultencoding(), then you must specify the encoding explicitly in the decode call:sys.getdefaultencoding()sys.getdefaultencoding()sys.getdefaultencoding()decodedecode>>> b'caf\\xe9'.decode('cp1250')\n'caf\u00e9'\n>>> b'caf\\xe9'.decode('cp1250')\n'caf\u00e9'\n",
                "Set universal_newlines to True, i.e.command_stdout = Popen(['ls', '-l'], stdout=PIPE, universal_newlines=True).communicate()[0]\ncommand_stdout = Popen(['ls', '-l'], stdout=PIPE, universal_newlines=True).communicate()[0]\n",
                "To interpret a byte sequence as a text, you have to know the\ncorresponding character encoding:unicode_text = bytestring.decode(character_encoding)\nunicode_text = bytestring.decode(character_encoding)\nExample:>>> b'\\xc2\\xb5'.decode('utf-8')\n'\u00b5'\n>>> b'\\xc2\\xb5'.decode('utf-8')\n'\u00b5'\nls command may produce output that can't be interpreted as text. File names\non Unix may be any sequence of bytes except slash b'/' and zero\nb'\\0':lsb'/'b'\\0'>>> open(bytes(range(0x100)).translate(None, b'\\0/'), 'w').close()\n>>> open(bytes(range(0x100)).translate(None, b'\\0/'), 'w').close()\nTrying to decode such byte soup using utf-8 encoding raises UnicodeDecodeError.UnicodeDecodeErrorIt can be worse. The decoding may fail silently and produce mojibake\nif you use a wrong incompatible encoding:mojibake>>> '\u2014'.encode('utf-8').decode('cp1252')\n'\u00e2\u20ac\u201d'\n>>> '\u2014'.encode('utf-8').decode('cp1252')\n'\u00e2\u20ac\u201d'\nThe data is corrupted but your program remains unaware that a failure\nhas occurred.In general, what character encoding to use is not embedded in the byte sequence itself. You have to communicate this info out-of-band. Some outcomes are more likely than others and therefore chardet module exists that can guess the character encoding. A single Python script may use multiple character encodings in different places.chardetguessls output can be converted to a Python string using os.fsdecode()\nfunction that succeeds even for undecodable\nfilenames (it uses\nsys.getfilesystemencoding() and surrogateescape error handler on\nUnix):lsos.fsdecode()undecodable\nfilenamessys.getfilesystemencoding()surrogateescapeimport os\nimport subprocess\n\noutput = os.fsdecode(subprocess.check_output('ls'))\nimport os\nimport subprocess\n\noutput = os.fsdecode(subprocess.check_output('ls'))\nTo get the original bytes, you could use os.fsencode().os.fsencode()If you pass universal_newlines=True parameter then subprocess uses\nlocale.getpreferredencoding(False) to decode bytes e.g., it can be\ncp1252 on Windows.universal_newlines=Truesubprocesslocale.getpreferredencoding(False)cp1252To decode the byte stream on-the-fly,\nio.TextIOWrapper()\ncould be used: example.io.TextIOWrapper()io.TextIOWrapper()exampleDifferent commands may use different character encodings for their\noutput e.g., dir internal command (cmd) may use cp437. To decode its\noutput, you could pass the encoding explicitly (Python 3.6+):dircmdoutput = subprocess.check_output('dir', shell=True, encoding='cp437')\noutput = subprocess.check_output('dir', shell=True, encoding='cp437')\nThe filenames may differ from os.listdir() (which uses Windows\nUnicode API) e.g., '\\xb6' can be substituted with '\\x14'\u2014Python's\ncp437 codec maps b'\\x14' to control character U+0014 instead of\nU+00B6 (\u00b6). To support filenames with arbitrary Unicode characters, see  Decode PowerShell output possibly containing non-ASCII Unicode characters into a Python stringos.listdir()'\\xb6''\\x14'b'\\x14'Decode PowerShell output possibly containing non-ASCII Unicode characters into a Python string",
                "While @Aaron Maenpaa's answer just works, a user recently asked:@Aaron Maenpaa's answerrecently asked\nIs there any more simply way? 'fhand.read().decode(\"ASCII\")' [...] It's so long!\nIs there any more simply way? 'fhand.read().decode(\"ASCII\")' [...] It's so long!You can use:command_stdout.decode()\ncommand_stdout.decode()\ndecode() has a standard argument:decode()standard argument\ncodecs.decode(obj, encoding='utf-8', errors='strict')\ncodecs.decode(obj, encoding='utf-8', errors='strict')codecs.decode(obj, encoding='utf-8', errors='strict')",
                "If you should get the following by trying decode():decode()\nAttributeError: 'str' object has no attribute 'decode'\nAttributeError: 'str' object has no attribute 'decode'You can also specify the encoding type straight in a cast:>>> my_byte_str\nb'Hello World'\n\n>>> str(my_byte_str, 'utf-8')\n'Hello World'\n>>> my_byte_str\nb'Hello World'\n\n>>> str(my_byte_str, 'utf-8')\n'Hello World'\n",
                "Bytesm=b'This is bytes'\nm=b'This is bytes'\nConverting to stringConverting to stringConverting to stringMethod 1m.decode(\"utf-8\")\nm.decode(\"utf-8\")\norm.decode()\nm.decode()\nMethod 2import codecs\ncodecs.decode(m,encoding=\"utf-8\")\nimport codecs\ncodecs.decode(m,encoding=\"utf-8\")\norimport codecs\ncodecs.decode(m)\nimport codecs\ncodecs.decode(m)\nMethod 3str(m,encoding=\"utf-8\")\nstr(m,encoding=\"utf-8\")\norstr(m)[2:-1]\nstr(m)[2:-1]\nResult'This is bytes'\n'This is bytes'\n",
                "If you have had this error:\nutf-8 codec can't decode byte 0x8a,\nutf-8 codec can't decode byte 0x8a,then it is better to use the following code to convert bytes to a string:bytes = b\"abcdefg\"\nstring = bytes.decode(\"utf-8\", \"ignore\") \nbytes = b\"abcdefg\"\nstring = bytes.decode(\"utf-8\", \"ignore\") \n",
                "For Python 3, this is a much safer and Pythonic approach to convert from byte to string:Pythonicbytestringdef byte_to_str(bytes_or_str):\n    if isinstance(bytes_or_str, bytes): # Check if it's in bytes\n        print(bytes_or_str.decode('utf-8'))\n    else:\n        print(\"Object not of byte type\")\n\nbyte_to_str(b'total 0\\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file1\\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file2\\n')\ndef byte_to_str(bytes_or_str):\n    if isinstance(bytes_or_str, bytes): # Check if it's in bytes\n        print(bytes_or_str.decode('utf-8'))\n    else:\n        print(\"Object not of byte type\")\n\nbyte_to_str(b'total 0\\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file1\\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file2\\n')\nOutput:total 0\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file1\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file2\ntotal 0\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file1\n-rw-rw-r-- 1 thomas thomas 0 Mar  3 07:03 file2\n",
                "When working with data from Windows systems (with \\r\\n line endings), my answer is\\r\\nString = Bytes.decode(\"utf-8\").replace(\"\\r\\n\", \"\\n\")\nString = Bytes.decode(\"utf-8\").replace(\"\\r\\n\", \"\\n\")\nWhy? Try this with a multiline Input.txt:Bytes = open(\"Input.txt\", \"rb\").read()\nString = Bytes.decode(\"utf-8\")\nopen(\"Output.txt\", \"w\").write(String)\nBytes = open(\"Input.txt\", \"rb\").read()\nString = Bytes.decode(\"utf-8\")\nopen(\"Output.txt\", \"w\").write(String)\nAll your line endings will be doubled (to \\r\\r\\n), leading to extra empty lines. Python's text-read functions usually normalize line endings so that strings use only \\n. If you receive binary data from a Windows system, Python does not have a chance to do that. Thus,\\r\\r\\n\\nBytes = open(\"Input.txt\", \"rb\").read()\nString = Bytes.decode(\"utf-8\").replace(\"\\r\\n\", \"\\n\")\nopen(\"Output.txt\", \"w\").write(String)\nBytes = open(\"Input.txt\", \"rb\").read()\nString = Bytes.decode(\"utf-8\").replace(\"\\r\\n\", \"\\n\")\nopen(\"Output.txt\", \"w\").write(String)\nwill replicate your original file.",
                "We can decode the bytes object to produce a string using bytes.decode(encoding='utf-8', errors='strict').\nFor documentation see bytes.decode.bytes.decode(encoding='utf-8', errors='strict')bytes.decodebytes.decodePython 3 example:byte_value = b\"abcde\"\nprint(\"Initial value = {}\".format(byte_value))\nprint(\"Initial value type = {}\".format(type(byte_value)))\nstring_value = byte_value.decode(\"utf-8\")\n# utf-8 is used here because it is a very common encoding, but you need to use the encoding your data is actually in.\nprint(\"------------\")\nprint(\"Converted value = {}\".format(string_value))\nprint(\"Converted value type = {}\".format(type(string_value)))\nbyte_value = b\"abcde\"\nprint(\"Initial value = {}\".format(byte_value))\nprint(\"Initial value type = {}\".format(type(byte_value)))\nstring_value = byte_value.decode(\"utf-8\")\n# utf-8 is used here because it is a very common encoding, but you need to use the encoding your data is actually in.\nprint(\"------------\")\nprint(\"Converted value = {}\".format(string_value))\nprint(\"Converted value type = {}\".format(type(string_value)))\nOutput:Initial value = b'abcde'\nInitial value type = <class 'bytes'>\n------------\nConverted value = abcde\nConverted value type = <class 'str'>\nInitial value = b'abcde'\nInitial value type = <class 'bytes'>\n------------\nConverted value = abcde\nConverted value type = <class 'str'>\nNote: In Python 3, by default the encoding type is UTF-8. So, <byte_string>.decode(\"utf-8\") can be also written as <byte_string>.decode()<byte_string>.decode(\"utf-8\")<byte_string>.decode()",
                "For your specific case of \"run a shell command and get its output as text instead of bytes\", on Python 3.7, you should use subprocess.run and pass in text=True (as well as capture_output=True to capture the output)specificsubprocess.runsubprocess.runtext=Truecapture_output=Truecommand_result = subprocess.run([\"ls\", \"-l\"], capture_output=True, text=True)\ncommand_result.stdout  # is a `str` containing your program's stdout\ncommand_result = subprocess.run([\"ls\", \"-l\"], capture_output=True, text=True)\ncommand_result.stdout  # is a `str` containing your program's stdout\ntext used to be called universal_newlines, and was changed (well, aliased) in Python 3.7. If you want to support Python versions before 3.7, pass in universal_newlines=True instead of text=Truetextuniversal_newlinesuniversal_newlines=Truetext=True",
                "From sys \u2014 System-specific parameters and functions:sys \u2014 System-specific parameters and functionssys \u2014 System-specific parameters and functionsTo write or read binary data from/to the standard streams, use the underlying binary buffer. For example, to write bytes to stdout, use sys.stdout.buffer.write(b'abc').sys.stdout.buffer.write(b'abc')",
                "Try this:bytes.fromhex('c3a9').decode('utf-8') \nbytes.fromhex('c3a9').decode('utf-8') \n",
                "Decode with .decode(). This will decode the string. Pass in 'utf-8') as the value in the inside..decode()'utf-8'",
                "def toString(string):    \n    try:\n        return v.decode(\"utf-8\")\n    except ValueError:\n        return string\n\nb = b'97.080.500'\ns = '97.080.500'\nprint(toString(b))\nprint(toString(s))\ndef toString(string):    \n    try:\n        return v.decode(\"utf-8\")\n    except ValueError:\n        return string\n\nb = b'97.080.500'\ns = '97.080.500'\nprint(toString(b))\nprint(toString(s))\n",
                "If you want to convert any bytes, not just string converted to bytes:with open(\"bytesfile\", \"rb\") as infile:\n    str = base64.b85encode(imageFile.read())\n\nwith open(\"bytesfile\", \"rb\") as infile:\n    str2 = json.dumps(list(infile.read()))\nwith open(\"bytesfile\", \"rb\") as infile:\n    str = base64.b85encode(imageFile.read())\n\nwith open(\"bytesfile\", \"rb\") as infile:\n    str2 = json.dumps(list(infile.read()))\nThis is not very efficient, however. It will turn a 2 MB picture into 9 MB.",
                "Try using this one; this function will ignore all the non-character sets (like UTF-8) binaries and return a clean string. It is tested for Python\u00a03.6 and above.def bin2str(text, encoding = 'utf-8'):\n    \"\"\"Converts a binary to Unicode string by removing all non Unicode char\n    text: binary string to work on\n    encoding: output encoding *utf-8\"\"\"\n\n    return text.decode(encoding, 'ignore')\ndef bin2str(text, encoding = 'utf-8'):\n    \"\"\"Converts a binary to Unicode string by removing all non Unicode char\n    text: binary string to work on\n    encoding: output encoding *utf-8\"\"\"\n\n    return text.decode(encoding, 'ignore')\nHere, the function will take the binary and decode it (converts binary data to characters using the Python predefined character set and the ignore argument ignores all non-character set data from your binary and finally returns your desired string value.ignorestringIf you are not sure about the encoding, use sys.getdefaultencoding() to get the default encoding of your device.sys.getdefaultencoding()"
            ]
        },
        {
            "tag": "__repr__",
            "patterns": [
                "What is the difference between __str__ and __repr__?",
                "tell me the difference between repr str and str?",
                "tell me the difference between repr and str?",
                "tell me the difference between str and repr?"
            ],
            "responses": [
                "Alex summarized well but, surprisingly, was too succinct.AlexFirst, let me reiterate the main points in Alex\u2019s post:Alex\u2019s post\nThe default implementation is useless (it\u2019s hard to think of one which wouldn\u2019t be, but yeah)\n__repr__ goal is to be unambiguous\n__str__ goal is to be readable\nContainer\u2019s __str__ uses contained objects\u2019 __repr__\nThe default implementation is useless (it\u2019s hard to think of one which wouldn\u2019t be, but yeah)__repr__ goal is to be unambiguous__repr____str__ goal is to be readable__str__Container\u2019s __str__ uses contained objects\u2019 __repr____str____repr__Default implementation is uselessDefault implementation is uselessThis is mostly a surprise because Python\u2019s defaults tend to be fairly useful. However, in this case, having a default for __repr__ which would act like:__repr__return \"%s(%r)\" % (self.__class__, self.__dict__)\nreturn \"%s(%r)\" % (self.__class__, self.__dict__)\nwould have been too dangerous (for example, too easy to get into infinite recursion if objects reference each other). So Python cops out. Note that there is one default which is true: if __repr__ is defined, and __str__ is not, the object will behave as though __str__=__repr__.__repr____str____str__=__repr__This means, in simple terms: almost every object you implement should have a functional __repr__ that\u2019s usable for understanding the object. Implementing __str__ is optional: do that if you need a \u201cpretty print\u201d functionality (for example, used by a report generator).__repr____str__The goal of __repr__ is to be unambiguousThe goal of __repr__ is to be unambiguous__repr__Let me come right out and say it \u2014 I do not believe in debuggers. I don\u2019t really know how to use any debugger, and have never used one seriously. Furthermore, I believe that the big fault in debuggers is their basic nature \u2014 most failures I debug happened a long long time ago, in a galaxy far far away. This means that I do believe, with religious fervor, in logging. Logging is the lifeblood of any decent fire-and-forget server system. Python makes it easy to log: with maybe some project specific wrappers, all you need is alog(INFO, \"I am in the weird function and a is\", a, \"and b is\", b, \"but I got a null C \u2014 using default\", default_c)\nlog(INFO, \"I am in the weird function and a is\", a, \"and b is\", b, \"but I got a null C \u2014 using default\", default_c)\nBut you have to do the last step \u2014 make sure every object you implement has a useful repr, so code like that can just work. This is why the \u201ceval\u201d thing comes up: if you have enough information so eval(repr(c))==c, that means you know everything there is to know about c. If that\u2019s easy enough, at least in a fuzzy way, do it. If not, make sure you have enough information about c anyway. I usually use an eval-like format: \"MyClass(this=%r,that=%r)\" % (self.this,self.that). It does not mean that you can actually construct MyClass, or that those are the right constructor arguments \u2014 but it is a useful form to express \u201cthis is everything you need to know about this instance\u201d.eval(repr(c))==ccc\"MyClass(this=%r,that=%r)\" % (self.this,self.that)Note: I used %r above, not %s. You always want to use repr() [or %r formatting character, equivalently] inside __repr__ implementation, or you\u2019re defeating the goal of repr. You want to be able to differentiate MyClass(3) and MyClass(\"3\").%r%srepr()%r__repr__MyClass(3)MyClass(\"3\")The goal of __str__ is to be readableThe goal of __str__ is to be readable__str__Specifically, it is not intended to be unambiguous \u2014 notice that str(3)==str(\"3\"). Likewise, if you implement an IP abstraction, having the str of it look like 192.168.1.1 is just fine. When implementing a date/time abstraction, the str can be \"2010/4/12 15:35:22\", etc. The goal is to represent it in a way that a user, not a programmer, would want to read it. Chop off useless digits, pretend to be some other class \u2014 as long is it supports readability, it is an improvement.str(3)==str(\"3\")Container\u2019s __str__ uses contained objects\u2019 __repr__Container\u2019s __str__ uses contained objects\u2019 __repr____str____repr__This seems surprising, doesn\u2019t it? It is a little, but how readable would it be if it used their __str__?__str__[moshe is, 3, hello\nworld, this is a list, oh I don't know, containing just 4 elements]\n[moshe is, 3, hello\nworld, this is a list, oh I don't know, containing just 4 elements]\nNot very. Specifically, the strings in a container would find it way too easy to disturb its string representation. In the face of ambiguity, remember, Python resists the temptation to guess. If you want the above behavior when you\u2019re printing a list, justprint(\"[\" + \", \".join(l) + \"]\")\nprint(\"[\" + \", \".join(l) + \"]\")\n(you can probably also figure out what to do about dictionaries.SummarySummaryImplement __repr__ for any class you implement. This should be second nature. Implement __str__ if you think it would be useful to have a string version which errs on the side of readability.__repr____str__",
                "My rule of thumb:  __repr__ is for developers, __str__ is for customers.__repr____str__",
                "Unless you specifically act to ensure otherwise, most classes don't have helpful results for either:>>> class Sic(object): pass\n... \n>>> print(str(Sic()))\n<__main__.Sic object at 0x8b7d0>\n>>> print(repr(Sic()))\n<__main__.Sic object at 0x8b7d0>\n>>> \n>>> class Sic(object): pass\n... \n>>> print(str(Sic()))\n<__main__.Sic object at 0x8b7d0>\n>>> print(repr(Sic()))\n<__main__.Sic object at 0x8b7d0>\n>>> \nAs you see -- no difference, and no info beyond the class and object's id.  If you only override one of the two...:id>>> class Sic(object): \n...   def __repr__(self): return 'foo'\n... \n>>> print(str(Sic()))\nfoo\n>>> print(repr(Sic()))\nfoo\n>>> class Sic(object):\n...   def __str__(self): return 'foo'\n... \n>>> print(str(Sic()))\nfoo\n>>> print(repr(Sic()))\n<__main__.Sic object at 0x2617f0>\n>>> \n>>> class Sic(object): \n...   def __repr__(self): return 'foo'\n... \n>>> print(str(Sic()))\nfoo\n>>> print(repr(Sic()))\nfoo\n>>> class Sic(object):\n...   def __str__(self): return 'foo'\n... \n>>> print(str(Sic()))\nfoo\n>>> print(repr(Sic()))\n<__main__.Sic object at 0x2617f0>\n>>> \nas you see, if you override __repr__, that's ALSO used for __str__, but not vice versa.__repr____str__Other crucial tidbits to know: __str__ on a built-on container uses the __repr__, NOT the __str__, for the items it contains. And, despite the words on the subject found in typical docs, hardly anybody bothers making the __repr__ of objects be a string that eval may use to build an equal object (it's just too hard, AND not knowing how the relevant module was actually imported makes it actually flat out impossible).__str____repr____str____repr__evalSo, my advice: focus on making __str__ reasonably human-readable, and __repr__ as unambiguous as you possibly can, even if that interferes with the fuzzy unattainable goal of making __repr__'s returned value acceptable as input to __eval__!__str____repr____repr____eval__",
                "__repr__: representation of python object usually eval will convert it back to that object__repr____repr____str__: is whatever you think is that object in text form__str____str__e.g.>>> s=\"\"\"w'o\"w\"\"\"\n>>> repr(s)\n'\\'w\\\\\\'o\"w\\''\n>>> str(s)\n'w\\'o\"w'\n>>> eval(str(s))==s\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<string>\", line 1\n    w'o\"w\n       ^\nSyntaxError: EOL while scanning single-quoted string\n>>> eval(repr(s))==s\nTrue\n>>> s=\"\"\"w'o\"w\"\"\"\n>>> repr(s)\n'\\'w\\\\\\'o\"w\\''\n>>> str(s)\n'w\\'o\"w'\n>>> eval(str(s))==s\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<string>\", line 1\n    w'o\"w\n       ^\nSyntaxError: EOL while scanning single-quoted string\n>>> eval(repr(s))==s\nTrue\n",
                "\nIn short, the goal of __repr__ is to be unambiguous and __str__ is to be\n  readable.\nIn short, the goal of __repr__ is to be unambiguous and __str__ is to be\n  readable.__repr____str__Here is a good example:>>> import datetime\n>>> today = datetime.datetime.now()\n>>> str(today)\n'2012-03-14 09:21:58.130922'\n>>> repr(today)\n'datetime.datetime(2012, 3, 14, 9, 21, 58, 130922)'\n>>> import datetime\n>>> today = datetime.datetime.now()\n>>> str(today)\n'2012-03-14 09:21:58.130922'\n>>> repr(today)\n'datetime.datetime(2012, 3, 14, 9, 21, 58, 130922)'\nRead this documentation for repr:\nrepr(object)\nReturn a string containing a printable representation of an object. This is the same value yielded by conversions (reverse\n  quotes). It is sometimes useful to be able to access this operation as\n  an ordinary function. For many types, this function makes an attempt\n  to return a string that would yield an object with the same value when\n  passed to eval(), otherwise the representation is a string enclosed in\n  angle brackets that contains the name of the type of the object\n  together with additional information often including the name and\n  address of the object. A class can control what this function returns\n  for its instances by defining a __repr__() method.\nrepr(object)repr(object)Return a string containing a printable representation of an object. This is the same value yielded by conversions (reverse\n  quotes). It is sometimes useful to be able to access this operation as\n  an ordinary function. For many types, this function makes an attempt\n  to return a string that would yield an object with the same value when\n  passed to eval(), otherwise the representation is a string enclosed in\n  angle brackets that contains the name of the type of the object\n  together with additional information often including the name and\n  address of the object. A class can control what this function returns\n  for its instances by defining a __repr__() method.eval()__repr__()Here is the documentation for str:\nstr(object='')\nReturn a string containing a nicely printable\n  representation of an object. For strings, this returns the string\n  itself. The difference with repr(object) is that str(object) does not\n  always attempt to return a string that is acceptable to eval(); its\n  goal is to return a printable string. If no argument is given, returns\n  the empty string, ''.\nstr(object='')str(object='')Return a string containing a nicely printable\n  representation of an object. For strings, this returns the string\n  itself. The difference with repr(object) is that str(object) does not\n  always attempt to return a string that is acceptable to eval(); its\n  goal is to return a printable string. If no argument is given, returns\n  the empty string, ''.repr(object)str(object)eval()''",
                "\nWhat is the difference between __str__ and __repr__ in Python?\nWhat is the difference between __str__ and __repr__ in Python?__str____repr____str__ (read as \"dunder (double-underscore) string\") and __repr__ (read as \"dunder-repper\" (for \"representation\")) are both special methods that return strings based on the state of the object.__str____repr____repr__ provides backup behavior if __str__ is missing.__repr____str__So one should first write a __repr__ that allows you to reinstantiate an equivalent object from the string it returns e.g. using eval or by typing it in character-for-character in a Python shell.__repr__evalAt any time later, one can write a __str__ for a user-readable string representation of the instance, when one believes it to be necessary.__str____str____str__If you print an object, or pass it to format, str.format, or str, then if a __str__ method is defined, that method will be called, otherwise, __repr__ will be used.formatstr.formatstr__str____repr____repr____repr__The __repr__ method is called by the builtin function repr and is what is echoed on your python shell when it evaluates an expression that returns an object.__repr__reprSince it provides a backup for __str__, if you can only write one, start with __repr____str____repr__Here's the builtin help on repr:reprrepr(...)\n    repr(object) -> string\n    \n    Return the canonical string representation of the object.\n    For most object types, eval(repr(object)) == object.\nrepr(...)\n    repr(object) -> string\n    \n    Return the canonical string representation of the object.\n    For most object types, eval(repr(object)) == object.\nThat is, for most objects, if you type in what is printed by repr, you should be able to create an equivalent object. But this is not the default implementation.reprBut this is not the default implementation.Default Implementation of __repr____repr__The default object __repr__ is (C Python source) something like:__repr__C Python sourcedef __repr__(self):\n    return '<{0}.{1} object at {2}>'.format(\n      type(self).__module__, type(self).__qualname__, hex(id(self)))\ndef __repr__(self):\n    return '<{0}.{1} object at {2}>'.format(\n      type(self).__module__, type(self).__qualname__, hex(id(self)))\nThat means by default you'll print the module the object is from, the class name, and the hexadecimal representation of its location in memory - for example:<__main__.Foo object at 0x7f80665abdd0>\n<__main__.Foo object at 0x7f80665abdd0>\nThis information isn't very useful, but there's no way to derive how one might accurately create a canonical representation of any given instance, and it's better than nothing, at least telling us how we might uniquely identify it in memory.How can __repr__ be useful?__repr__Let's look at how useful it can be, using the Python shell and datetime objects. First we need to import the datetime module:datetimedatetimeimport datetime\nimport datetime\nIf we call datetime.now in the shell, we'll see everything we need to recreate an equivalent datetime object. This is created by the datetime __repr__:datetime.now__repr__>>> datetime.datetime.now()\ndatetime.datetime(2015, 1, 24, 20, 5, 36, 491180)\n>>> datetime.datetime.now()\ndatetime.datetime(2015, 1, 24, 20, 5, 36, 491180)\nIf we print a datetime object, we see a nice human readable (in fact, ISO) format. This is implemented by datetime's __str__:__str__>>> print(datetime.datetime.now())\n2015-01-24 20:05:44.977951\n>>> print(datetime.datetime.now())\n2015-01-24 20:05:44.977951\nIt is a simple matter to recreate the object we lost because we didn't assign it to a variable by copying and pasting from the __repr__ output, and then printing it, and we get it in the same human readable output as the other object:__repr__>>> the_past = datetime.datetime(2015, 1, 24, 20, 5, 36, 491180)\n>>> print(the_past)\n2015-01-24 20:05:36.491180\n>>> the_past = datetime.datetime(2015, 1, 24, 20, 5, 36, 491180)\n>>> print(the_past)\n2015-01-24 20:05:36.491180\n#How do I implement them?As you're developing, you'll want to be able to reproduce objects in the same state, if possible. This, for example, is how the datetime object defines __repr__ (Python source). It is fairly complex, because of all of the attributes needed to reproduce such an object:__repr__Python sourcedef __repr__(self):\n    \"\"\"Convert to formal string, for repr().\"\"\"\n    L = [self._year, self._month, self._day,  # These are never zero\n         self._hour, self._minute, self._second, self._microsecond]\n    if L[-1] == 0:\n        del L[-1]\n    if L[-1] == 0:\n        del L[-1]\n    s = \"%s.%s(%s)\" % (self.__class__.__module__,\n                       self.__class__.__qualname__,\n                       \", \".join(map(str, L)))\n    if self._tzinfo is not None:\n        assert s[-1:] == \")\"\n        s = s[:-1] + \", tzinfo=%r\" % self._tzinfo + \")\"\n    if self._fold:\n        assert s[-1:] == \")\"\n        s = s[:-1] + \", fold=1)\"\n    return s\ndef __repr__(self):\n    \"\"\"Convert to formal string, for repr().\"\"\"\n    L = [self._year, self._month, self._day,  # These are never zero\n         self._hour, self._minute, self._second, self._microsecond]\n    if L[-1] == 0:\n        del L[-1]\n    if L[-1] == 0:\n        del L[-1]\n    s = \"%s.%s(%s)\" % (self.__class__.__module__,\n                       self.__class__.__qualname__,\n                       \", \".join(map(str, L)))\n    if self._tzinfo is not None:\n        assert s[-1:] == \")\"\n        s = s[:-1] + \", tzinfo=%r\" % self._tzinfo + \")\"\n    if self._fold:\n        assert s[-1:] == \")\"\n        s = s[:-1] + \", fold=1)\"\n    return s\nIf you want your object to have a more human readable representation, you can implement __str__ next. Here's how the datetime object (Python source) implements __str__, which it easily does because it already has a function to display it in ISO format:__str__Python source__str__def __str__(self):\n    \"Convert to string, for str().\"\n    return self.isoformat(sep=' ')\ndef __str__(self):\n    \"Convert to string, for str().\"\n    return self.isoformat(sep=' ')\nSet __repr__ = __str__?__repr__ = __str__This is a critique of another answer here that suggests setting __repr__ = __str__.__repr__ = __str__Setting __repr__ = __str__ is silly - __repr__ is a fallback for __str__ and a __repr__, written for developers usage in debugging, should be written before you write a __str__.__repr__ = __str____repr____str____repr____str__You need a __str__ only when you need a textual representation of the object.__str__ConclusionDefine __repr__ for objects you write so you and other developers have a reproducible example when using it as you develop. Define __str__ when you need a human readable string representation of it.__repr____str__",
                "On page 358 of the book Python scripting for computational science by Hans Petter Langtangen, it clearly states that Python scripting for computational sciencePython scripting for computational science\nThe __repr__ aims at a complete string representation of the object;\nThe __str__ is to return a nice string for printing.\nThe __repr__ aims at a complete string representation of the object;__repr__The __str__ is to return a nice string for printing.__str__So, I prefer to understand them as\nrepr = reproduce\nstr = string (representation)\nrepr = reproducerepr = reproducestr = string (representation)str = string (representation)from the user's point of view\nalthough this is a misunderstanding I made when learning python.A small but good example is also given on the same page as follows:ExampleIn [38]: str('s')\nOut[38]: 's'\n\nIn [39]: repr('s')\nOut[39]: \"'s'\"\n\nIn [40]: eval(str('s'))\nTraceback (most recent call last):\n\n  File \"<ipython-input-40-abd46c0c43e7>\", line 1, in <module>\n    eval(str('s'))\n\n  File \"<string>\", line 1, in <module>\n\nNameError: name 's' is not defined\n\n\nIn [41]: eval(repr('s'))\nOut[41]: 's'\nIn [38]: str('s')\nOut[38]: 's'\n\nIn [39]: repr('s')\nOut[39]: \"'s'\"\n\nIn [40]: eval(str('s'))\nTraceback (most recent call last):\n\n  File \"<ipython-input-40-abd46c0c43e7>\", line 1, in <module>\n    eval(str('s'))\n\n  File \"<string>\", line 1, in <module>\n\nNameError: name 's' is not defined\n\n\nIn [41]: eval(repr('s'))\nOut[41]: 's'\n",
                "Apart from all the answers given, I would like to add few points :-1) __repr__() is invoked when you simply write object's name on interactive python console and press enter.__repr__()2) __str__() is invoked when you use object with print statement.__str__()3) In case, if __str__ is missing, then print and any function using str() invokes __repr__() of object.__str__str()__repr__()4) __str__() of containers, when invoked will execute __repr__() method of its contained elements.__str__()__repr__()5) str() called within __str__() could potentially recurse without a base case, and error on maximum recursion depth.str()__str__()6) __repr__() can call repr() which will attempt to avoid infinite recursion automatically, replacing an already represented object with ....__repr__()repr()...",
                "(2020 entry)(2020 entry)Q: What's the difference between __str__() and __repr__()?Q:__str__()__repr__()TL;DR:\nTL;DR:LONGLONGThis question has been around a long time, and there are a variety of answers of which most are correct (not to mention from several Python community legends[!]). However when it comes down to the nitty-gritty, this question is analogous to asking the difference between the str() and repr() built-in functions. I'm going to describe the differences in my own words (which means I may be \"borrowing\" liberally from Core Python Programming so pls forgive me).str()repr()Core Python ProgrammingCore Python ProgrammingBoth str() and repr() have the same basic job: their goal is to return a string representation of a Python object. What kind of string representation is what differentiates them.Bothstr()repr()kind\nstr() & __str__() return a printable string representation of\nan object... something human-readable/for human consumption\nrepr() & __repr__() return a string representation of an object that is a valid Python expression, an object you can pass to eval() or type into the Python shell without getting an error.\nstr() & __str__() return a printable string representation of\nan object... something human-readable/for human consumptionstr()__str__()printablerepr() & __repr__() return a string representation of an object that is a valid Python expression, an object you can pass to eval() or type into the Python shell without getting an error.repr()__repr__()valid Python expressioneval()For example, let's assign a string to x and an int to y, and simply showing human-readable string versions of each:xinty>>> x, y = 'foo', 123\n>>> str(x), str(y)\n('foo', '123')\n>>> x, y = 'foo', 123\n>>> str(x), str(y)\n('foo', '123')\nCan we take what is inside the quotes in both cases and enter them verbatim into the Python interpreter? Let's give it a try:what is inside the quotes>>> 123\n123\n>>> foo\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'foo' is not defined\n>>> 123\n123\n>>> foo\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'foo' is not defined\nClearly you can for an int but not necessarily for a str. Similarly, while I can pass '123' to eval(), that doesn't work for 'foo':intstr'123'eval()'foo'>>> eval('123')\n123\n>>> eval('foo')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\nNameError: name 'foo' is not defined\n>>> eval('123')\n123\n>>> eval('foo')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<string>\", line 1, in <module>\nNameError: name 'foo' is not defined\nSo this tells you the Python shell just eval()s what you give it. Got it? Now, let's repr() both expressions and see what we get. More specifically, take its output and dump those out in the interpreter (there's a point to this which we'll address afterwards):eval()repr()those>>> repr(x), repr(y)\n(\"'foo'\", '123')\n>>> 123\n123\n>>> 'foo'\n'foo'\n>>> repr(x), repr(y)\n(\"'foo'\", '123')\n>>> 123\n123\n>>> 'foo'\n'foo'\nWow, they both work? That's because 'foo', while a printable string representation of that string, it's not evaluatable, but \"'foo'\" is. 123 is a valid Python int called by either str() or repr(). What happens when we call eval() with these?both'foo'not\"'foo'\"123intstr()repr()eval()>>> eval('123')\n123\n>>> eval(\"'foo'\")\n'foo'\n>>> eval('123')\n123\n>>> eval(\"'foo'\")\n'foo'\nIt works because 123 and 'foo' are valid Python objects. Another key takeaway is that while sometimes both return the same thing (the same string representation), that's not always the case. (And yes, yes, I can go create a variable foo where the eval() works, but that's not the point.)123'foo'fooeval()More factoids about both pairsMore factoids about both pairs\nSometimes, str() and repr() are called implicitly, meaning they're called on behalf of users: when users execute print (Py1/Py2) or call print() (Py3+), even if users don't call str() explicitly, such a call is made on their behalf before the object is displayed.\nIn the Python shell (interactive interpreter), if you enter a variable at the >>> prompt and press RETURN, the interpreter displays the results of repr() implicitly called on that object.\nTo connect str() and repr() to __str__() and __repr__(), realize that calls to the built-in functions, i.e., str(x) or repr(y) result in calling their object's corresponding special methods: x.__str__() or y.__repr()__\nBy implementing __str__() and __repr__() for your Python classes, you overload the built-in functions (str() and repr()), allowing instances of your classes to be passed in to str() and repr(). When such calls are made, they turn around and call the class' __str__() and __repr__() (per #3).\nSometimes, str() and repr() are called implicitly, meaning they're called on behalf of users: when users execute print (Py1/Py2) or call print() (Py3+), even if users don't call str() explicitly, such a call is made on their behalf before the object is displayed.str()repr()implicitlyprintprintprint()str()In the Python shell (interactive interpreter), if you enter a variable at the >>> prompt and press RETURN, the interpreter displays the results of repr() implicitly called on that object.>>>repr()To connect str() and repr() to __str__() and __repr__(), realize that calls to the built-in functions, i.e., str(x) or repr(y) result in calling their object's corresponding special methods: x.__str__() or y.__repr()__str()repr()__str__()__repr__()str(x)repr(y)x.__str__()y.__repr()__By implementing __str__() and __repr__() for your Python classes, you overload the built-in functions (str() and repr()), allowing instances of your classes to be passed in to str() and repr(). When such calls are made, they turn around and call the class' __str__() and __repr__() (per #3).__str__()__repr__()yourstr()repr()str()repr()__str__()__repr__()",
                "To put it simply:__str__ is used in to show a string representation of your object to be read easily by others.__str__to be read easily__repr__ is used to show a string representation of the object.__repr__theLet's say I want to create a Fraction class where the string representation of a fraction is '(1/2)' and the object (Fraction class) is to be represented as 'Fraction (1,2)'FractionSo we can create a simple Fraction class:class Fraction:\n    def __init__(self, num, den):\n        self.__num = num\n        self.__den = den\n\n    def __str__(self):\n        return '(' + str(self.__num) + '/' + str(self.__den) + ')'\n\n    def __repr__(self):\n        return 'Fraction (' + str(self.__num) + ',' + str(self.__den) + ')'\n\n\n\nf = Fraction(1,2)\nprint('I want to represent the Fraction STRING as ' + str(f)) # (1/2)\nprint('I want to represent the Fraction OBJECT as ', repr(f)) # Fraction (1,2)\nclass Fraction:\n    def __init__(self, num, den):\n        self.__num = num\n        self.__den = den\n\n    def __str__(self):\n        return '(' + str(self.__num) + '/' + str(self.__den) + ')'\n\n    def __repr__(self):\n        return 'Fraction (' + str(self.__num) + ',' + str(self.__den) + ')'\n\n\n\nf = Fraction(1,2)\nprint('I want to represent the Fraction STRING as ' + str(f)) # (1/2)\nprint('I want to represent the Fraction OBJECT as ', repr(f)) # Fraction (1,2)\n",
                "From an (An Unofficial) Python Reference Wiki (archive copy) by effbot:an (An Unofficial) Python Reference Wiki (archive copy)__str__ \"computes the \"informal\" string representation of an object. This differs from __repr__ in that it does not have to be a valid Python expression: a more convenient or concise representation may be used instead.\"__str__computes the \"informal\" string representation of an object. This differs from __repr__ in that it does not have to be a valid Python expression: a more convenient or concise representation may be used instead.__repr__",
                "In all honesty, eval(repr(obj)) is never used. If you find yourself using it, you should stop, because eval is dangerous, and strings are a very inefficient way to serialize your objects (use pickle instead).eval(repr(obj))evalpickleTherefore, I would recommend setting __repr__ = __str__. The reason is that str(list) calls repr on the elements (I consider this to be one of the biggest design flaws of Python that was not addressed by Python 3). An actual repr will probably not be very helpful as the output of print([your, objects]).__repr__ = __str__str(list)reprreprprint([your, objects])To qualify this, in my experience, the most useful use case of the repr function is to put a string inside another string (using string formatting). This way, you don't have to worry about escaping quotes or anything. But note that there is no eval happening here.repreval",
                "str - Creates a new string object from the given object.strrepr - Returns the canonical string representation of the object.reprThe differences:str():str():\nmakes object readable\ngenerates output for end-user\nmakes object readablegenerates output for end-userrepr():repr():\nneeds code that reproduces object\ngenerates output for developer\nneeds code that reproduces objectgenerates output for developer",
                "One aspect that is missing in other answers. It's true that in general the pattern is:\nGoal of __str__: human-readable\nGoal of __repr__: unambiguous, possibly machine-readable via eval\nGoal of __str__: human-readable__str__Goal of __repr__: unambiguous, possibly machine-readable via eval__repr__evalUnfortunately, this differentiation is flawed, because the Python REPL and also IPython use __repr__ for printing objects in a REPL console (see related questions for Python and IPython). Thus, projects which are targeted for interactive console work (e.g., Numpy or Pandas) have started to ignore above rules and provide a human-readable __repr__ implementation instead.__repr__PythonIPython__repr__",
                "From the book Fluent Python:Fluent Python\nA basic requirement for a Python object is to provide usable \n       string   representations of itself, one used for debugging and\n       logging, another for presentation to end users. That is why the\n       special methods __repr__ and __str__ exist in the data model.\nA basic requirement for a Python object is to provide usable \n       string   representations of itself, one used for debugging and\n       logging, another for presentation to end users. That is why the\n       special methods __repr__ and __str__ exist in the data model.__repr____str__",
                "__str__ can be invoked on an object by calling str(obj) and should return a human readable string. __str__str(obj)__repr__ can be invoked on an object by calling repr(obj) and should return internal object (object fields/attributes)__repr__repr(obj)This example may help:class C1:pass\n\nclass C2:        \n    def __str__(self):\n        return str(f\"{self.__class__.__name__} class str \")\n\nclass C3:        \n    def __repr__(self):        \n         return str(f\"{self.__class__.__name__} class repr\")\n\nclass C4:        \n    def __str__(self):\n        return str(f\"{self.__class__.__name__} class str \")\n    def __repr__(self):        \n         return str(f\"{self.__class__.__name__} class repr\")\n\n\nci1 = C1()    \nci2 = C2()  \nci3 = C3()  \nci4 = C4()\n\nprint(ci1)       #<__main__.C1 object at 0x0000024C44A80C18>\nprint(str(ci1))  #<__main__.C1 object at 0x0000024C44A80C18>\nprint(repr(ci1)) #<__main__.C1 object at 0x0000024C44A80C18>\nprint(ci2)       #C2 class str\nprint(str(ci2))  #C2 class str\nprint(repr(ci2)) #<__main__.C2 object at 0x0000024C44AE12E8>\nprint(ci3)       #C3 class repr\nprint(str(ci3))  #C3 class repr\nprint(repr(ci3)) #C3 class repr\nprint(ci4)       #C4 class str \nprint(str(ci4))  #C4 class str \nprint(repr(ci4)) #C4 class repr\nclass C1:pass\n\nclass C2:        \n    def __str__(self):\n        return str(f\"{self.__class__.__name__} class str \")\n\nclass C3:        \n    def __repr__(self):        \n         return str(f\"{self.__class__.__name__} class repr\")\n\nclass C4:        \n    def __str__(self):\n        return str(f\"{self.__class__.__name__} class str \")\n    def __repr__(self):        \n         return str(f\"{self.__class__.__name__} class repr\")\n\n\nci1 = C1()    \nci2 = C2()  \nci3 = C3()  \nci4 = C4()\n\nprint(ci1)       #<__main__.C1 object at 0x0000024C44A80C18>\nprint(str(ci1))  #<__main__.C1 object at 0x0000024C44A80C18>\nprint(repr(ci1)) #<__main__.C1 object at 0x0000024C44A80C18>\nprint(ci2)       #C2 class str\nprint(str(ci2))  #C2 class str\nprint(repr(ci2)) #<__main__.C2 object at 0x0000024C44AE12E8>\nprint(ci3)       #C3 class repr\nprint(str(ci3))  #C3 class repr\nprint(repr(ci3)) #C3 class repr\nprint(ci4)       #C4 class str \nprint(str(ci4))  #C4 class str \nprint(repr(ci4)) #C4 class repr\n",
                "You can get some insight from this code:class Foo():\n    def __repr__(self):\n        return(\"repr\")\n    def __str__(self):\n        return(\"str\")\n\nfoo = Foo()\nfoo #repr\nprint(foo) #str\nclass Foo():\n    def __repr__(self):\n        return(\"repr\")\n    def __str__(self):\n        return(\"str\")\n\nfoo = Foo()\nfoo #repr\nprint(foo) #str\n",
                "Excellent answers already cover the difference between __str__ and __repr__, which for me boils down to the former being readable even by an end user, and the latter being as useful as possible to developers. Given that, I find that the default implementation of __repr__ often fails to achieve this goal because it omits information useful to developers.__str____repr____repr__omitsFor this reason, if I have a simple enough __str__, I generally just try to get the best of both worlds with something like:__str__def __repr__(self):\n    return '{0} ({1})'.format(object.__repr__(self), str(self))\ndef __repr__(self):\n    return '{0} ({1})'.format(object.__repr__(self), str(self))\n",
                ">>> print(decimal.Decimal(23) / decimal.Decimal(\"1.05\"))\n21.90476190476190476190476190\n>>> decimal.Decimal(23) / decimal.Decimal(\"1.05\")\nDecimal('21.90476190476190476190476190')\n>>> print(decimal.Decimal(23) / decimal.Decimal(\"1.05\"))\n21.90476190476190476190476190\n>>> decimal.Decimal(23) / decimal.Decimal(\"1.05\")\nDecimal('21.90476190476190476190476190')\nWhen print() is called on the result of decimal.Decimal(23) / decimal.Decimal(\"1.05\") the raw number is printed; this output is in string form which can be achieved with __str__(). If we simply enter the expression we get a decimal.Decimal output \u2014 this output is in representational form which can be achieved with __repr__(). All Python objects have two output forms. String form is designed to be human-readable. The representational form is designed to produce output that if fed to a Python interpreter would (when possible) reproduce the represented object.print()decimal.Decimal(23) / decimal.Decimal(\"1.05\")string form__str__()decimal.Decimalrepresentational form__repr__()",
                "\nOne important thing to keep in mind is that container's __str__ uses contained objects' __repr__.\nOne important thing to keep in mind is that container's __str__ uses contained objects' __repr__.__str____repr__>>> from datetime import datetime\n>>> from decimal import Decimal\n>>> print (Decimal('52'), datetime.now())\n(Decimal('52'), datetime.datetime(2015, 11, 16, 10, 51, 26, 185000))\n>>> str((Decimal('52'), datetime.now()))\n\"(Decimal('52'), datetime.datetime(2015, 11, 16, 10, 52, 22, 176000))\"\n>>> from datetime import datetime\n>>> from decimal import Decimal\n>>> print (Decimal('52'), datetime.now())\n(Decimal('52'), datetime.datetime(2015, 11, 16, 10, 51, 26, 185000))\n>>> str((Decimal('52'), datetime.now()))\n\"(Decimal('52'), datetime.datetime(2015, 11, 16, 10, 52, 22, 176000))\"\nPython favors unambiguity over readability, the __str__ call of a tuple calls the contained objects' __repr__, the \"formal\" representation of an object. Although the formal representation is harder to read than an informal one, it is unambiguous and more robust against bugs.Python favors unambiguity over readability__str__tuple__repr__\"formal\"",
                "In a nutshell:class Demo:\n  def __repr__(self):\n    return 'repr'\n  def __str__(self):\n    return 'str'\n\ndemo = Demo()\nprint(demo) # use __str__, output 'str' to stdout\n\ns = str(demo) # __str__ is used, return 'str'\nr = repr(demo) # __repr__ is used, return 'repr'\n\nimport logging\nlogger = logging.getLogger(logging.INFO)\nlogger.info(demo) # use __str__, output 'str' to stdout\n\nfrom pprint import pprint, pformat\npprint(demo) # use __repr__, output 'repr' to stdout\nresult = pformat(demo) # use __repr__, result is string which value is 'str'\nclass Demo:\n  def __repr__(self):\n    return 'repr'\n  def __str__(self):\n    return 'str'\n\ndemo = Demo()\nprint(demo) # use __str__, output 'str' to stdout\n\ns = str(demo) # __str__ is used, return 'str'\nr = repr(demo) # __repr__ is used, return 'repr'\n\nimport logging\nlogger = logging.getLogger(logging.INFO)\nlogger.info(demo) # use __str__, output 'str' to stdout\n\nfrom pprint import pprint, pformat\npprint(demo) # use __repr__, output 'repr' to stdout\nresult = pformat(demo) # use __repr__, result is string which value is 'str'\n",
                "Understand __str__ and __repr__ intuitively and permanently distinguish them at all.__str____repr____str__ return the string disguised body of a given object for readable of eyes\n__repr__ return the real flesh body of a given object (return itself) for unambiguity to identify.__str____repr__See it in an exampleIn [30]: str(datetime.datetime.now())\nOut[30]: '2017-12-07 15:41:14.002752'\nDisguised in string form\nIn [30]: str(datetime.datetime.now())\nOut[30]: '2017-12-07 15:41:14.002752'\nDisguised in string form\nAs to __repr____repr__In [32]: datetime.datetime.now()\nOut[32]: datetime.datetime(2017, 12, 7, 15, 43, 27, 297769)\nPresence in real body which allows to be manipulated directly.\nIn [32]: datetime.datetime.now()\nOut[32]: datetime.datetime(2017, 12, 7, 15, 43, 27, 297769)\nPresence in real body which allows to be manipulated directly.\nWe can do arithmetic operation on __repr__ results conveniently.__repr__In [33]: datetime.datetime.now()\nOut[33]: datetime.datetime(2017, 12, 7, 15, 47, 9, 741521)\nIn [34]: datetime.datetime(2017, 12, 7, 15, 47, 9, 741521) - datetime.datetime(2\n    ...: 017, 12, 7, 15, 43, 27, 297769)\nOut[34]: datetime.timedelta(0, 222, 443752)\nIn [33]: datetime.datetime.now()\nOut[33]: datetime.datetime(2017, 12, 7, 15, 47, 9, 741521)\nIn [34]: datetime.datetime(2017, 12, 7, 15, 47, 9, 741521) - datetime.datetime(2\n    ...: 017, 12, 7, 15, 43, 27, 297769)\nOut[34]: datetime.timedelta(0, 222, 443752)\nif apply the operation on __str____str__In [35]: '2017-12-07 15:43:14.002752' - '2017-12-07 15:41:14.002752'\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\nIn [35]: '2017-12-07 15:43:14.002752' - '2017-12-07 15:41:14.002752'\nTypeError: unsupported operand type(s) for -: 'str' and 'str'\nReturns nothing but error.Another example.In [36]: str('string_body')\nOut[36]: 'string_body' # in string form\n\nIn [37]: repr('real_body')\nOut[37]: \"'real_body'\" #its real body hide inside\nIn [36]: str('string_body')\nOut[36]: 'string_body' # in string form\n\nIn [37]: repr('real_body')\nOut[37]: \"'real_body'\" #its real body hide inside\nHope this help you build concrete grounds to explore more answers.",
                "\n__str__ must return string object whereas __repr__ can return any python expression.\nIf __str__ implementation is missing then __repr__ function is used as fallback. There is no fallback if __repr__ function implementation is missing.\nIf __repr__ function is returning String representation of the object, we can skip implementation of __str__ function.\n__str__ must return string object whereas __repr__ can return any python expression.__str____repr__If __str__ implementation is missing then __repr__ function is used as fallback. There is no fallback if __repr__ function implementation is missing.__str____repr____repr__If __repr__ function is returning String representation of the object, we can skip implementation of __str__ function.__repr____str__Source: https://www.journaldev.com/22460/python-str-repr-functionshttps://www.journaldev.com/22460/python-str-repr-functions",
                "__repr__ is used everywhere, except by print and str methods (when a __str__is defined !)__repr__printstr__str__",
                "Every object inherits __repr__  from the base class that all objects created.__repr__class Person:\n     pass\n\np=Person()\nclass Person:\n     pass\n\np=Person()\nif you call repr(p) you will get this as default:repr(p) <__main__.Person object at 0x7fb2604f03a0>\n <__main__.Person object at 0x7fb2604f03a0>\nBut if you call str(p) you will get the same output. it is because when __str__ does not exist, Python calls __repr__str(p)__str____repr__Let's implement our own __str____str__class Person:\n    def __init__(self,name,age):\n        self.name=name\n        self.age=age\n    def __repr__(self):\n        print(\"__repr__ called\")\n        return f\"Person(name='{self.name}',age={self.age})\"\n\np=Person(\"ali\",20)\nclass Person:\n    def __init__(self,name,age):\n        self.name=name\n        self.age=age\n    def __repr__(self):\n        print(\"__repr__ called\")\n        return f\"Person(name='{self.name}',age={self.age})\"\n\np=Person(\"ali\",20)\nprint(p) and str(p)will returnprint(p)str(p) __repr__ called\n     Person(name='ali',age=20)\n __repr__ called\n     Person(name='ali',age=20)\nlet's add __str__()__str__()class Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n        \n    def __repr__(self):\n        print('__repr__ called')\n        return f\"Person(name='{self.name}, age=self.age')\"\n    \n    def __str__(self):\n        print('__str__ called')\n        return self.name\n\np=Person(\"ali\",20)\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n        \n    def __repr__(self):\n        print('__repr__ called')\n        return f\"Person(name='{self.name}, age=self.age')\"\n    \n    def __str__(self):\n        print('__str__ called')\n        return self.name\n\np=Person(\"ali\",20)\nif we call print(p) and str(p), it will call __str__() so it will returnprint(p)__str__()__str__ called\nali\n__str__ called\nali\nrepr(p) will returnrepr(p)repr called\n\"Person(name='ali, age=self.age')\"reprLet's omit __repr__ and just implement __str__.__repr____str__class Person:\ndef __init__(self, name, age):\n    self.name = name\n    self.age = age\n\ndef __str__(self):\n    print('__str__ called')\n    return self.name\n\np=Person('ali',20)\nclass Person:\ndef __init__(self, name, age):\n    self.name = name\n    self.age = age\n\ndef __str__(self):\n    print('__str__ called')\n    return self.name\n\np=Person('ali',20)\nprint(p) will look for the __str__ and will return:print(p)__str____str__ called\nali\n__str__ called\nali\nNOTE= if we had __repr__ and __str__ defined, f'name is {p}' would call __str____repr____str__f'name is {p}'__str__",
                "\nProgrammers with prior experience in languages with a toString method tend to implement __str__ and not __repr__.\nIf you only implement one of these special methods in Python, choose __repr__.\nProgrammers with prior experience in languages with a toString method tend to implement __str__ and not __repr__.\nIf you only implement one of these special methods in Python, choose __repr__.toString__str____repr____repr__From Fluent Python book, by Ramalho, Luciano.Fluent Python",
                "Basically __str__ or str() is used for creating output that is human-readable are must be for end-users.\nOn the other hand, repr() or __repr__ mainly returns canonical string representation of objects which serve the purpose of debugging and development helps the programmers.__str__str()repr()__repr__",
                "repr() used when we debug or log.It is used for developers to understand code.\none the other hand str() user for non developer like(QA) or user.class Customer:\n    def __init__(self,name):\n        self.name = name\n    def __repr__(self):\n        return \"Customer('{}')\".format(self.name)\n    def __str__(self):\n        return f\"cunstomer name is {self.name}\"\n\ncus_1 = Customer(\"Thusi\")\nprint(repr(cus_1)) #print(cus_1.__repr__()) \nprint(str(cus_1)) #print(cus_1.__str__())\nclass Customer:\n    def __init__(self,name):\n        self.name = name\n    def __repr__(self):\n        return \"Customer('{}')\".format(self.name)\n    def __str__(self):\n        return f\"cunstomer name is {self.name}\"\n\ncus_1 = Customer(\"Thusi\")\nprint(repr(cus_1)) #print(cus_1.__repr__()) \nprint(str(cus_1)) #print(cus_1.__str__())\n"
            ]
        },
        {
            "tag": "copy_files",
            "patterns": [
                "How to copy files"
            ],
            "responses": [
                "shutil has many methods you can use. One of which is:shutilshutilimport shutil\n\nshutil.copyfile(src, dst)\n\n# 2nd option\nshutil.copy(src, dst)  # dst can be a folder; use shutil.copy2() to preserve timestamp\nimport shutil\n\nshutil.copyfile(src, dst)\n\n# 2nd option\nshutil.copy(src, dst)  # dst can be a folder; use shutil.copy2() to preserve timestamp\n\nCopy the contents of the file named src to a file named dst. Both src and dst need to be the entire filename of the files, including path.\nThe destination location must be writable; otherwise, an IOError exception will be raised.\nIf dst already exists, it will be replaced.\nSpecial files such as character or block devices and pipes cannot be copied with this function.\nWith copy, src and dst are path names given as strs.\nCopy the contents of the file named src to a file named dst. Both src and dst need to be the entire filename of the files, including path.srcdstsrcdstThe destination location must be writable; otherwise, an IOError exception will be raised.IOErrorIf dst already exists, it will be replaced.dstSpecial files such as character or block devices and pipes cannot be copied with this function.With copy, src and dst are path names given as strs.copysrcdststrAnother shutil method to look at is shutil.copy2(). It's similar but preserves more metadata (e.g. time stamps).shutilshutil.copy2()shutil.copy2()If you use os.path operations, use copy rather than copyfile. copyfile will only accept strings.os.pathcopycopyfilecopyfile",
                "\n\n\n\nFunction\nCopiesmetadata\nCopiespermissions\nUses file object\nDestinationmay be directory\n\n\n\n\nshutil.copy\nNo\nYes\nNo\nYes\n\n\nshutil.copyfile\nNo\nNo\nNo\nNo\n\n\nshutil.copy2\nYes\nYes\nNo\nYes\n\n\nshutil.copyfileobj\nNo\nNo\nYes\nNo\n\n\n\n\n\n\nFunction\nCopiesmetadata\nCopiespermissions\nUses file object\nDestinationmay be directory\n\n\n\n\nshutil.copy\nNo\nYes\nNo\nYes\n\n\nshutil.copyfile\nNo\nNo\nNo\nNo\n\n\nshutil.copy2\nYes\nYes\nNo\nYes\n\n\nshutil.copyfileobj\nNo\nNo\nYes\nNo\n\n\n\n\nFunction\nCopiesmetadata\nCopiespermissions\nUses file object\nDestinationmay be directory\n\n\nFunction\nCopiesmetadata\nCopiespermissions\nUses file object\nDestinationmay be directory\nFunctionCopiesmetadataCopiespermissionsUses file objectDestinationmay be directory\n\nshutil.copy\nNo\nYes\nNo\nYes\n\n\nshutil.copyfile\nNo\nNo\nNo\nNo\n\n\nshutil.copy2\nYes\nYes\nNo\nYes\n\n\nshutil.copyfileobj\nNo\nNo\nYes\nNo\n\n\nshutil.copy\nNo\nYes\nNo\nYes\nshutil.copyshutil.copyNoYesNoYes\nshutil.copyfile\nNo\nNo\nNo\nNo\nshutil.copyfileshutil.copyfileNoNoNoNo\nshutil.copy2\nYes\nYes\nNo\nYes\nshutil.copy2shutil.copy2YesYesNoYes\nshutil.copyfileobj\nNo\nNo\nYes\nNo\nshutil.copyfileobjshutil.copyfileobjNoNoYesNo",
                "copy2(src,dst) is often more useful than copyfile(src,dst) because:copy2(src,dst)copy2(src,dst)copyfile(src,dst)copyfile(src,dst)\nit allows dst to be a directory (instead of the complete target filename), in which case the basename of src is used for creating the new file;\nit preserves the original modification and access info (mtime and atime) in the file metadata (however, this comes with a slight overhead).\nit allows dst to be a directory (instead of the complete target filename), in which case the basename of src is used for creating the new file;dstdirectorybasenamesrcit preserves the original modification and access info (mtime and atime) in the file metadata (however, this comes with a slight overhead).Here is a short example:import shutil\nshutil.copy2('/src/dir/file.ext', '/dst/dir/newname.ext') # complete target filename given\nshutil.copy2('/src/file.ext', '/dst/dir') # target filename is /dst/dir/file.ext\nimport shutil\nshutil.copy2('/src/dir/file.ext', '/dst/dir/newname.ext') # complete target filename given\nshutil.copy2('/src/file.ext', '/dst/dir') # target filename is /dst/dir/file.ext\n",
                "In Python, you can copy the files using\nshutil module\nos module\nsubprocess module\nshutil moduleshutilshutilshutilos moduleososossubprocess modulesubprocesssubprocesssubprocessimport os\nimport shutil\nimport subprocess\nimport os\nimport shutil\nimport subprocess\n1) Copying files using shutil moduleshutilshutilshutil.copyfile  signatureshutil.copyfileshutil.copyfileshutil.copyfileshutil.copyfile(src_file, dest_file, *, follow_symlinks=True)\n\n# example    \nshutil.copyfile('source.txt', 'destination.txt')\nshutil.copyfile(src_file, dest_file, *, follow_symlinks=True)\n\n# example    \nshutil.copyfile('source.txt', 'destination.txt')\nshutil.copy  signatureshutil.copyshutil.copyshutil.copyshutil.copy(src_file, dest_file, *, follow_symlinks=True)\n\n# example\nshutil.copy('source.txt', 'destination.txt')\nshutil.copy(src_file, dest_file, *, follow_symlinks=True)\n\n# example\nshutil.copy('source.txt', 'destination.txt')\nshutil.copy2  signatureshutil.copy2shutil.copy2shutil.copy2shutil.copy2(src_file, dest_file, *, follow_symlinks=True)\n\n# example\nshutil.copy2('source.txt', 'destination.txt')  \nshutil.copy2(src_file, dest_file, *, follow_symlinks=True)\n\n# example\nshutil.copy2('source.txt', 'destination.txt')  \nshutil.copyfileobj  signatureshutil.copyfileobjshutil.copyfileobjshutil.copyfileobjshutil.copyfileobj(src_file_object, dest_file_object[, length])\n\n# example\nfile_src = 'source.txt'  \nf_src = open(file_src, 'rb')\n\nfile_dest = 'destination.txt'  \nf_dest = open(file_dest, 'wb')\n\nshutil.copyfileobj(f_src, f_dest)  \nshutil.copyfileobj(src_file_object, dest_file_object[, length])\n\n# example\nfile_src = 'source.txt'  \nf_src = open(file_src, 'rb')\n\nfile_dest = 'destination.txt'  \nf_dest = open(file_dest, 'wb')\n\nshutil.copyfileobj(f_src, f_dest)  \n2) Copying files using os moduleososos.popen  signatureos.popenos.popenos.popenos.popen(cmd[, mode[, bufsize]])\n\n# example\n# In Unix/Linux\nos.popen('cp source.txt destination.txt') \n\n# In Windows\nos.popen('copy source.txt destination.txt')\nos.popen(cmd[, mode[, bufsize]])\n\n# example\n# In Unix/Linux\nos.popen('cp source.txt destination.txt') \n\n# In Windows\nos.popen('copy source.txt destination.txt')\nos.system  signatureos.systemos.systemos.systemos.system(command)\n\n\n# In Linux/Unix\nos.system('cp source.txt destination.txt')  \n\n# In Windows\nos.system('copy source.txt destination.txt')\nos.system(command)\n\n\n# In Linux/Unix\nos.system('cp source.txt destination.txt')  \n\n# In Windows\nos.system('copy source.txt destination.txt')\n3) Copying files using subprocess modulesubprocesssubprocesssubprocess.call  signaturesubprocess.callsubprocess.callsubprocess.callsubprocess.call(args, *, stdin=None, stdout=None, stderr=None, shell=False)\n\n# example (WARNING: setting `shell=True` might be a security-risk)\n# In Linux/Unix\nstatus = subprocess.call('cp source.txt destination.txt', shell=True) \n\n# In Windows\nstatus = subprocess.call('copy source.txt destination.txt', shell=True)\nsubprocess.call(args, *, stdin=None, stdout=None, stderr=None, shell=False)\n\n# example (WARNING: setting `shell=True` might be a security-risk)\n# In Linux/Unix\nstatus = subprocess.call('cp source.txt destination.txt', shell=True) \n\n# In Windows\nstatus = subprocess.call('copy source.txt destination.txt', shell=True)\nsubprocess.check_output  signaturesubprocess.check_outputsubprocess.check_outputsubprocess.check_outputsubprocess.check_output(args, *, stdin=None, stderr=None, shell=False, universal_newlines=False)\n\n# example (WARNING: setting `shell=True` might be a security-risk)\n# In Linux/Unix\nstatus = subprocess.check_output('cp source.txt destination.txt', shell=True)\n\n# In Windows\nstatus = subprocess.check_output('copy source.txt destination.txt', shell=True)\nsubprocess.check_output(args, *, stdin=None, stderr=None, shell=False, universal_newlines=False)\n\n# example (WARNING: setting `shell=True` might be a security-risk)\n# In Linux/Unix\nstatus = subprocess.check_output('cp source.txt destination.txt', shell=True)\n\n# In Windows\nstatus = subprocess.check_output('copy source.txt destination.txt', shell=True)\n",
                "You can use one of the copy functions from the shutil package:shutilshutil\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nFunction              preserves     supports          accepts     copies other\n                      permissions   directory dest.   file obj    metadata  \n\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\u2015\nshutil.copy              \u2714             \u2714                 \u2610           \u2610\nshutil.copy2             \u2714             \u2714                 \u2610           \u2714\nshutil.copyfile          \u2610             \u2610                 \u2610           \u2610\nshutil.copyfileobj       \u2610             \u2610                 \u2714           \u2610\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nshutil.copyshutil.copy2shutil.copyfileshutil.copyfileobjExample:import shutil\nshutil.copy('/etc/hostname', '/var/tmp/testhostname')\nimport shutil\nshutil.copy('/etc/hostname', '/var/tmp/testhostname')\n",
                "Copying a file is a relatively straightforward operation as shown by the examples below, but you should instead use the shutil stdlib module for that.shutil stdlib moduledef copyfileobj_example(source, dest, buffer_size=1024*1024):\n    \"\"\"      \n    Copy a file from source to dest. source and dest\n    must be file-like objects, i.e. any object with a read or\n    write method, like for example StringIO.\n    \"\"\"\n    while True:\n        copy_buffer = source.read(buffer_size)\n        if not copy_buffer:\n            break\n        dest.write(copy_buffer)\ndef copyfileobj_example(source, dest, buffer_size=1024*1024):\n    \"\"\"      \n    Copy a file from source to dest. source and dest\n    must be file-like objects, i.e. any object with a read or\n    write method, like for example StringIO.\n    \"\"\"\n    while True:\n        copy_buffer = source.read(buffer_size)\n        if not copy_buffer:\n            break\n        dest.write(copy_buffer)\nIf you want to copy by filename you could do something like this:def copyfile_example(source, dest):\n    # Beware, this example does not handle any edge cases!\n    with open(source, 'rb') as src, open(dest, 'wb') as dst:\n        copyfileobj_example(src, dst)\ndef copyfile_example(source, dest):\n    # Beware, this example does not handle any edge cases!\n    with open(source, 'rb') as src, open(dest, 'wb') as dst:\n        copyfileobj_example(src, dst)\n",
                "Use the shutil module.shutil modulecopyfile(src, dst)\ncopyfile(src, dst)\nCopy the contents of the file named src to a file named dst. The destination location must be writable; otherwise, an IOError exception will be raised. If dst already exists, it will be replaced. Special files such as character or block devices and pipes cannot be copied with this function. src and dst are path names given as strings.srcdstdstsrcdstTake a look at filesys for all the file and directory handling functions available in standard Python modules.filesys",
                "Directory and File copy example, from Tim Golden's Python Stuff:from Tim Golden's Python StuffPython Stuffimport os\nimport shutil\nimport tempfile\n\nfilename1 = tempfile.mktemp (\".txt\")\nopen (filename1, \"w\").close ()\nfilename2 = filename1 + \".copy\"\nprint filename1, \"=>\", filename2\n\nshutil.copy (filename1, filename2)\n\nif os.path.isfile (filename2): print \"Success\"\n\ndirname1 = tempfile.mktemp (\".dir\")\nos.mkdir (dirname1)\ndirname2 = dirname1 + \".copy\"\nprint dirname1, \"=>\", dirname2\n\nshutil.copytree (dirname1, dirname2)\n\nif os.path.isdir (dirname2): print \"Success\"\nimport os\nimport shutil\nimport tempfile\n\nfilename1 = tempfile.mktemp (\".txt\")\nopen (filename1, \"w\").close ()\nfilename2 = filename1 + \".copy\"\nprint filename1, \"=>\", filename2\n\nshutil.copy (filename1, filename2)\n\nif os.path.isfile (filename2): print \"Success\"\n\ndirname1 = tempfile.mktemp (\".dir\")\nos.mkdir (dirname1)\ndirname2 = dirname1 + \".copy\"\nprint dirname1, \"=>\", dirname2\n\nshutil.copytree (dirname1, dirname2)\n\nif os.path.isdir (dirname2): print \"Success\"\n",
                "For small files and using only Python built-ins, you can use the following one-liner:with open(source, 'rb') as src, open(dest, 'wb') as dst: dst.write(src.read())\nwith open(source, 'rb') as src, open(dest, 'wb') as dst: dst.write(src.read())\nThis is not optimal way for applications where the file is too large or when memory is critical, thus Swati's answer should be preferred.Swati's",
                "Firstly, I made an exhaustive cheat sheet of the shutil methods for your reference.shutilshutil_methods =\n{'copy':['shutil.copyfileobj',\n          'shutil.copyfile',\n          'shutil.copymode',\n          'shutil.copystat',\n          'shutil.copy',\n          'shutil.copy2',\n          'shutil.copytree',],\n 'move':['shutil.rmtree',\n         'shutil.move',],\n 'exception': ['exception shutil.SameFileError',\n                 'exception shutil.Error'],\n 'others':['shutil.disk_usage',\n             'shutil.chown',\n             'shutil.which',\n             'shutil.ignore_patterns',]\n}\nshutil_methods =\n{'copy':['shutil.copyfileobj',\n          'shutil.copyfile',\n          'shutil.copymode',\n          'shutil.copystat',\n          'shutil.copy',\n          'shutil.copy2',\n          'shutil.copytree',],\n 'move':['shutil.rmtree',\n         'shutil.move',],\n 'exception': ['exception shutil.SameFileError',\n                 'exception shutil.Error'],\n 'others':['shutil.disk_usage',\n             'shutil.chown',\n             'shutil.which',\n             'shutil.ignore_patterns',]\n}\nSecondly, explaining methods of copy in examples:\nshutil.copyfileobj(fsrc, fdst[, length]) manipulate opened objects\nIn [3]: src = '~/Documents/Head+First+SQL.pdf'\nIn [4]: dst = '~/desktop'\nIn [5]: shutil.copyfileobj(src, dst)\nAttributeError: 'str' object has no attribute 'read'\n\n# Copy the file object\nIn [7]: with open(src, 'rb') as f1,open(os.path.join(dst,'test.pdf'), 'wb') as f2:\n    ...:      shutil.copyfileobj(f1, f2)\nIn [8]: os.stat(os.path.join(dst,'test.pdf'))\nOut[8]: os.stat_result(st_mode=33188, st_ino=8598319475, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516067347, st_mtime=1516067335, st_ctime=1516067345)\n\n\nshutil.copyfile(src, dst, *, follow_symlinks=True)  Copy and rename\nIn [9]: shutil.copyfile(src, dst)\nIsADirectoryError: [Errno 21] Is a directory: ~/desktop'\n# So dst should be a filename instead of a directory name\n\n\nshutil.copy()  Copy without preseving the metadata\nIn [10]: shutil.copy(src, dst)\nOut[10]: ~/desktop/Head+First+SQL.pdf'\n\n# Check their metadata\nIn [25]: os.stat(src)\nOut[25]: os.stat_result(st_mode=33188, st_ino=597749, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516066425, st_mtime=1493698739, st_ctime=1514871215)\nIn [26]: os.stat(os.path.join(dst, 'Head+First+SQL.pdf'))\nOut[26]: os.stat_result(st_mode=33188, st_ino=8598313736, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516066427, st_mtime=1516066425, st_ctime=1516066425)\n# st_atime,st_mtime,st_ctime changed\n\n\nshutil.copy2()  Copy with preserving the metadata\nIn [30]: shutil.copy2(src, dst)\nOut[30]: ~/desktop/Head+First+SQL.pdf'\nIn [31]: os.stat(src)\nOut[31]: os.stat_result(st_mode=33188, st_ino=597749, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516067055, st_mtime=1493698739, st_ctime=1514871215)\nIn [32]: os.stat(os.path.join(dst, 'Head+First+SQL.pdf'))\nOut[32]: os.stat_result(st_mode=33188, st_ino=8598313736, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516067063, st_mtime=1493698739, st_ctime=1516067055)\n# Preserved st_mtime\n\n\nshutil.copytree()\nRecursively copy an entire directory tree rooted at src, returning the destination directory.\n\nshutil.copyfileobj(fsrc, fdst[, length]) manipulate opened objects\nIn [3]: src = '~/Documents/Head+First+SQL.pdf'\nIn [4]: dst = '~/desktop'\nIn [5]: shutil.copyfileobj(src, dst)\nAttributeError: 'str' object has no attribute 'read'\n\n# Copy the file object\nIn [7]: with open(src, 'rb') as f1,open(os.path.join(dst,'test.pdf'), 'wb') as f2:\n    ...:      shutil.copyfileobj(f1, f2)\nIn [8]: os.stat(os.path.join(dst,'test.pdf'))\nOut[8]: os.stat_result(st_mode=33188, st_ino=8598319475, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516067347, st_mtime=1516067335, st_ctime=1516067345)\n\nshutil.copyfileobj(fsrc, fdst[, length]) manipulate opened objectsshutil.copyfileobj(fsrc, fdst[, length])In [3]: src = '~/Documents/Head+First+SQL.pdf'\nIn [4]: dst = '~/desktop'\nIn [5]: shutil.copyfileobj(src, dst)\nAttributeError: 'str' object has no attribute 'read'\n\n# Copy the file object\nIn [7]: with open(src, 'rb') as f1,open(os.path.join(dst,'test.pdf'), 'wb') as f2:\n    ...:      shutil.copyfileobj(f1, f2)\nIn [8]: os.stat(os.path.join(dst,'test.pdf'))\nOut[8]: os.stat_result(st_mode=33188, st_ino=8598319475, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516067347, st_mtime=1516067335, st_ctime=1516067345)\nIn [3]: src = '~/Documents/Head+First+SQL.pdf'\nIn [4]: dst = '~/desktop'\nIn [5]: shutil.copyfileobj(src, dst)\nAttributeError: 'str' object has no attribute 'read'\n\n# Copy the file object\nIn [7]: with open(src, 'rb') as f1,open(os.path.join(dst,'test.pdf'), 'wb') as f2:\n    ...:      shutil.copyfileobj(f1, f2)\nIn [8]: os.stat(os.path.join(dst,'test.pdf'))\nOut[8]: os.stat_result(st_mode=33188, st_ino=8598319475, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516067347, st_mtime=1516067335, st_ctime=1516067345)\nshutil.copyfile(src, dst, *, follow_symlinks=True)  Copy and rename\nIn [9]: shutil.copyfile(src, dst)\nIsADirectoryError: [Errno 21] Is a directory: ~/desktop'\n# So dst should be a filename instead of a directory name\n\nshutil.copyfile(src, dst, *, follow_symlinks=True)  Copy and renameshutil.copyfile(src, dst, *, follow_symlinks=True)In [9]: shutil.copyfile(src, dst)\nIsADirectoryError: [Errno 21] Is a directory: ~/desktop'\n# So dst should be a filename instead of a directory name\nIn [9]: shutil.copyfile(src, dst)\nIsADirectoryError: [Errno 21] Is a directory: ~/desktop'\n# So dst should be a filename instead of a directory name\nshutil.copy()  Copy without preseving the metadata\nIn [10]: shutil.copy(src, dst)\nOut[10]: ~/desktop/Head+First+SQL.pdf'\n\n# Check their metadata\nIn [25]: os.stat(src)\nOut[25]: os.stat_result(st_mode=33188, st_ino=597749, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516066425, st_mtime=1493698739, st_ctime=1514871215)\nIn [26]: os.stat(os.path.join(dst, 'Head+First+SQL.pdf'))\nOut[26]: os.stat_result(st_mode=33188, st_ino=8598313736, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516066427, st_mtime=1516066425, st_ctime=1516066425)\n# st_atime,st_mtime,st_ctime changed\n\nshutil.copy()  Copy without preseving the metadatashutil.copy()In [10]: shutil.copy(src, dst)\nOut[10]: ~/desktop/Head+First+SQL.pdf'\n\n# Check their metadata\nIn [25]: os.stat(src)\nOut[25]: os.stat_result(st_mode=33188, st_ino=597749, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516066425, st_mtime=1493698739, st_ctime=1514871215)\nIn [26]: os.stat(os.path.join(dst, 'Head+First+SQL.pdf'))\nOut[26]: os.stat_result(st_mode=33188, st_ino=8598313736, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516066427, st_mtime=1516066425, st_ctime=1516066425)\n# st_atime,st_mtime,st_ctime changed\nIn [10]: shutil.copy(src, dst)\nOut[10]: ~/desktop/Head+First+SQL.pdf'\n\n# Check their metadata\nIn [25]: os.stat(src)\nOut[25]: os.stat_result(st_mode=33188, st_ino=597749, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516066425, st_mtime=1493698739, st_ctime=1514871215)\nIn [26]: os.stat(os.path.join(dst, 'Head+First+SQL.pdf'))\nOut[26]: os.stat_result(st_mode=33188, st_ino=8598313736, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516066427, st_mtime=1516066425, st_ctime=1516066425)\n# st_atime,st_mtime,st_ctime changed\nshutil.copy2()  Copy with preserving the metadata\nIn [30]: shutil.copy2(src, dst)\nOut[30]: ~/desktop/Head+First+SQL.pdf'\nIn [31]: os.stat(src)\nOut[31]: os.stat_result(st_mode=33188, st_ino=597749, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516067055, st_mtime=1493698739, st_ctime=1514871215)\nIn [32]: os.stat(os.path.join(dst, 'Head+First+SQL.pdf'))\nOut[32]: os.stat_result(st_mode=33188, st_ino=8598313736, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516067063, st_mtime=1493698739, st_ctime=1516067055)\n# Preserved st_mtime\n\nshutil.copy2()  Copy with preserving the metadatashutil.copy2()In [30]: shutil.copy2(src, dst)\nOut[30]: ~/desktop/Head+First+SQL.pdf'\nIn [31]: os.stat(src)\nOut[31]: os.stat_result(st_mode=33188, st_ino=597749, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516067055, st_mtime=1493698739, st_ctime=1514871215)\nIn [32]: os.stat(os.path.join(dst, 'Head+First+SQL.pdf'))\nOut[32]: os.stat_result(st_mode=33188, st_ino=8598313736, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516067063, st_mtime=1493698739, st_ctime=1516067055)\n# Preserved st_mtime\nIn [30]: shutil.copy2(src, dst)\nOut[30]: ~/desktop/Head+First+SQL.pdf'\nIn [31]: os.stat(src)\nOut[31]: os.stat_result(st_mode=33188, st_ino=597749, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516067055, st_mtime=1493698739, st_ctime=1514871215)\nIn [32]: os.stat(os.path.join(dst, 'Head+First+SQL.pdf'))\nOut[32]: os.stat_result(st_mode=33188, st_ino=8598313736, st_dev=16777220, st_nlink=1, st_uid=501, st_gid=20, st_size=13507926, st_atime=1516067063, st_mtime=1493698739, st_ctime=1516067055)\n# Preserved st_mtime\nshutil.copytree()\nRecursively copy an entire directory tree rooted at src, returning the destination directory.\nshutil.copytree()shutil.copytree()Recursively copy an entire directory tree rooted at src, returning the destination directory.",
                "shutil module offers some high-level operations on files. It supports file copying and removal.shutilfilescopyingremovalRefer to the table below for your use case.\n\n\n\nFunction\nUtilizeFile Object\nPreserve FileMetadata\nPreserve Permissions\nSupports Directory Dest.\n\n\n\n\nshutil.copyfileobj\n\u2714\n\u2179\n\u2179\n\u2179\n\n\nshutil.copyfile\n\u2179\n\u2179\n\u2179\n\u2179\n\n\nshutil.copy2\n\u2179\n\u2714\n\u2714\n\u2714\n\n\nshutil.copy\n\u2179\n\u2179\n\u2714\n\u2714\n\n\n\n\n\n\nFunction\nUtilizeFile Object\nPreserve FileMetadata\nPreserve Permissions\nSupports Directory Dest.\n\n\n\n\nshutil.copyfileobj\n\u2714\n\u2179\n\u2179\n\u2179\n\n\nshutil.copyfile\n\u2179\n\u2179\n\u2179\n\u2179\n\n\nshutil.copy2\n\u2179\n\u2714\n\u2714\n\u2714\n\n\nshutil.copy\n\u2179\n\u2179\n\u2714\n\u2714\n\n\n\n\nFunction\nUtilizeFile Object\nPreserve FileMetadata\nPreserve Permissions\nSupports Directory Dest.\n\n\nFunction\nUtilizeFile Object\nPreserve FileMetadata\nPreserve Permissions\nSupports Directory Dest.\nFunctionUtilizeFile ObjectPreserve FileMetadataPreserve PermissionsSupports Directory Dest.\n\nshutil.copyfileobj\n\u2714\n\u2179\n\u2179\n\u2179\n\n\nshutil.copyfile\n\u2179\n\u2179\n\u2179\n\u2179\n\n\nshutil.copy2\n\u2179\n\u2714\n\u2714\n\u2714\n\n\nshutil.copy\n\u2179\n\u2179\n\u2714\n\u2714\n\n\nshutil.copyfileobj\n\u2714\n\u2179\n\u2179\n\u2179\nshutil.copyfileobjshutil.copyfileobj\u2714\u2179\u2179\u2179\u2179\u2179\u2179\nshutil.copyfile\n\u2179\n\u2179\n\u2179\n\u2179\nshutil.copyfileshutil.copyfile\u2179\u2179\u2179\u2179\u2179\u2179\u2179\u2179\nshutil.copy2\n\u2179\n\u2714\n\u2714\n\u2714\nshutil.copy2shutil.copy2\u2179\u2179\u2714\u2714\u2714\nshutil.copy\n\u2179\n\u2179\n\u2714\n\u2714\nshutil.copyshutil.copy\u2179\u2179\u2179\u2179\u2714\u2714",
                "As of Python 3.5 you can do the following for small files (ie: text files, small jpegs):Python 3.5from pathlib import Path\n\nsource = Path('../path/to/my/file.txt')\ndestination = Path('../path/where/i/want/to/store/it.txt')\ndestination.write_bytes(source.read_bytes())\nfrom pathlib import Path\n\nsource = Path('../path/to/my/file.txt')\ndestination = Path('../path/where/i/want/to/store/it.txt')\ndestination.write_bytes(source.read_bytes())\nwrite_bytes will overwrite whatever was at the destination's locationwrite_bytes",
                "You could use os.system('cp nameoffilegeneratedbyprogram /otherdirectory/').os.system('cp nameoffilegeneratedbyprogram /otherdirectory/')Or as I did it,os.system('cp '+ rawfile + ' rawdata.dat')\nos.system('cp '+ rawfile + ' rawdata.dat')\nwhere rawfile is the name that I had generated inside the program.rawfileThis is a Linux-only solution.",
                "Use subprocess.call to copy the filesubprocess.callfrom subprocess import call\ncall(\"cp -p <file> <file>\", shell=True)\nfrom subprocess import call\ncall(\"cp -p <file> <file>\", shell=True)\n",
                "For large files, I read the file line by line and read each line into an array. Then, once the array reached a certain size, append it to a new file.for line in open(\"file.txt\", \"r\"):\n    list.append(line)\n    if len(list) == 1000000: \n        output.writelines(list)\n        del list[:]\nfor line in open(\"file.txt\", \"r\"):\n    list.append(line)\n    if len(list) == 1000000: \n        output.writelines(list)\n        del list[:]\n",
                "Useopen(destination, 'wb').write(open(source, 'rb').read())\nopen(destination, 'wb').write(open(source, 'rb').read())\nOpen the source file in read mode, and write to the destination file in write mode.",
                "In case you've come this far down. The answer is that you need the entire path and file nameimport os\n\nshutil.copy(os.path.join(old_dir, file), os.path.join(new_dir, file))\nimport os\n\nshutil.copy(os.path.join(old_dir, file), os.path.join(new_dir, file))\n",
                "Here is a simple way to do it, without any module. It's similar to this answer, but has the benefit to also work if it's a big file that doesn't fit in RAM:this answerwith open('sourcefile', 'rb') as f, open('destfile', 'wb') as g:\n    while True:\n        block = f.read(16*1024*1024)  # work by blocks of 16 MB\n        if not block:  # end of file\n            break\n        g.write(block)\nwith open('sourcefile', 'rb') as f, open('destfile', 'wb') as g:\n    while True:\n        block = f.read(16*1024*1024)  # work by blocks of 16 MB\n        if not block:  # end of file\n            break\n        g.write(block)\nSince we're writing a new file, it does not preserve the modification time, etc.\nWe can then use os.utime for this if needed.\nWe can then use os.utime for this if needed.os.utimeos.utime",
                "Similar to the accepted answer, the following code block might come in handy if you also want to make sure to create any (non-existent) folders in the path to the destination.from os import path, makedirs\nfrom shutil import copyfile\nmakedirs(path.dirname(path.abspath(destination_path)), exist_ok=True)\ncopyfile(source_path, destination_path)\nfrom os import path, makedirs\nfrom shutil import copyfile\nmakedirs(path.dirname(path.abspath(destination_path)), exist_ok=True)\ncopyfile(source_path, destination_path)\nAs the accepted answers notes, these lines will overwrite any file which exists at the destination path, so sometimes it might be useful to also add: if not path.exists(destination_path): before this code block.if not path.exists(destination_path):",
                "There are two best ways to copy file in Python.1. We can use the shutil moduleshutilshutilCode Example:import shutil\nshutil.copyfile('/path/to/file', '/path/to/new/file')\nimport shutil\nshutil.copyfile('/path/to/file', '/path/to/new/file')\nThere are other methods available also other than copyfile, like copy, copy2, etc, but copyfile is best in terms of performance,copyfilecopyfile2. We can use the OS moduleOSOSCode Example:import os\nos.system('cp /path/to/file /path/to/new/file')\nimport os\nos.system('cp /path/to/file /path/to/new/file')\nAnother method is by the use of a subprocess, but it is not preferable as it\u2019s one of the call methods and is not secure.",
                "You can use system.systemFor Unix-like systems:import os\n\ncopy_file = lambda src_file, dest: os.system(f\"cp {src_file} {dest}\")\n\ncopy_file(\"./file\", \"../new_dir/file\")\nimport os\n\ncopy_file = lambda src_file, dest: os.system(f\"cp {src_file} {dest}\")\n\ncopy_file(\"./file\", \"../new_dir/file\")\n",
                "You can use os.link to create a hard link to a file:os.linkos.link(source, dest)\nos.link(source, dest)\nThis is not an independent clone, but if you plan to only read (not modify) the new file and its content must remain the same as the original, this will work well. It also has a benefit that if you want to check whether the copy already exists, you can compare the hard links (with os.stat) instead of their content.os.statIn Linux, the command cp with keyscpcp -al\ncp -al\ncreates a hard link. Therefore a hard link may be considered a copy. Sometimes a person would need exactly this behaviour (access to file content from a different place), and not need a separate copy.",
                "Python provides in-built functions for easily copying files using the operating system shell utilities.The Following command is used to copy a file:shutil.copy(src, dst)\nshutil.copy(src, dst)\nThe following command is used to copy a file with metadata information:shutil.copystat(src, dst)\nshutil.copystat(src, dst)\n",
                "Here is an answer utilizing \"shutil.copyfileobj\" and it is highly efficient. I used it in a tool I created some time ago. I didn't write this originally, but I tweaked it a little bit.def copyFile(src, dst, buffer_size=10485760, perserveFileDate=True):\n    '''\n    @param src:    Source File\n    @param dst:    Destination File (not file path)\n    @param buffer_size:    Buffer size to use during copy\n    @param perserveFileDate:    Preserve the original file date\n    '''\n    #    Check to make sure destination directory exists. If it doesn't create the directory\n    dstParent, dstFileName = os.path.split(dst)\n    if(not(os.path.exists(dstParent))):\n        os.makedirs(dstParent)\n\n    # Optimize the buffer for small files\n    buffer_size = min(buffer_size,os.path.getsize(src))\n    if(buffer_size == 0):\n        buffer_size = 1024\n\n    if shutil._samefile(src, dst):\n        raise shutil.Error(\"`%s` and `%s` are the same file\" % (src, dst))\n    for fn in [src, dst]:\n        try:\n            st = os.stat(fn)\n        except OSError:\n            # File most likely does not exist\n            pass\n        else:\n            # XXX What about other special files? (sockets, devices...)\n            if shutil.stat.S_ISFIFO(st.st_mode):\n                raise shutil.SpecialFileError(\"`%s` is a named pipe\" % fn)\n    with open(src, 'rb') as fsrc:\n        with open(dst, 'wb') as fdst:\n            shutil.copyfileobj(fsrc, fdst, buffer_size)\n\n    if(perserveFileDate):\n        shutil.copystat(src, dst)\ndef copyFile(src, dst, buffer_size=10485760, perserveFileDate=True):\n    '''\n    @param src:    Source File\n    @param dst:    Destination File (not file path)\n    @param buffer_size:    Buffer size to use during copy\n    @param perserveFileDate:    Preserve the original file date\n    '''\n    #    Check to make sure destination directory exists. If it doesn't create the directory\n    dstParent, dstFileName = os.path.split(dst)\n    if(not(os.path.exists(dstParent))):\n        os.makedirs(dstParent)\n\n    # Optimize the buffer for small files\n    buffer_size = min(buffer_size,os.path.getsize(src))\n    if(buffer_size == 0):\n        buffer_size = 1024\n\n    if shutil._samefile(src, dst):\n        raise shutil.Error(\"`%s` and `%s` are the same file\" % (src, dst))\n    for fn in [src, dst]:\n        try:\n            st = os.stat(fn)\n        except OSError:\n            # File most likely does not exist\n            pass\n        else:\n            # XXX What about other special files? (sockets, devices...)\n            if shutil.stat.S_ISFIFO(st.st_mode):\n                raise shutil.SpecialFileError(\"`%s` is a named pipe\" % fn)\n    with open(src, 'rb') as fsrc:\n        with open(dst, 'wb') as fdst:\n            shutil.copyfileobj(fsrc, fdst, buffer_size)\n\n    if(perserveFileDate):\n        shutil.copystat(src, dst)\n",
                "shutil.copy(src, dst, *, follow_symlinks=True)"
            ]
        },
        {
            "tag": "list_files",
            "patterns": [
                "How do I list all files of a directory?",
                "how can you list all the files in a directory?",
                "how do i list all of the files in a directory?",
                "how do i get all files in a directory?",
                "how do i list the files in a directory?",
                "what can i do to list all files in a directory?",
                "how do i list all files in a directory?"
            ],
            "responses": [
                "os.listdir() returns everything inside a directory -- including both files and directories.os.listdir()os.listdir()os.listdir()filesdirectoriesos.path's isfile() can be used to only list files:os.pathos.pathos.pathisfile()from os import listdir\nfrom os.path import isfile, join\nonlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\nfrom os import listdir\nfrom os.path import isfile, join\nonlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\nAlternatively, os.walk() yields two lists for each directory it visits -- one for files and one for dirs. If you only want the top directory you can break the first time it yields:os.walk()os.walk()os.walk()yields two listsfilesdirsfrom os import walk\n\nf = []\nfor (dirpath, dirnames, filenames) in walk(mypath):\n    f.extend(filenames)\n    break\nfrom os import walk\n\nf = []\nfor (dirpath, dirnames, filenames) in walk(mypath):\n    f.extend(filenames)\n    break\nor, shorter:from os import walk\n\nfilenames = next(walk(mypath), (None, None, []))[2]  # [] if no file\nfrom os import walk\n\nfilenames = next(walk(mypath), (None, None, []))[2]  # [] if no file\n",
                "I prefer using the glob module, as it does pattern matching and expansion.globglobimport glob\nprint(glob.glob(\"/home/adam/*\"))\nimport glob\nprint(glob.glob(\"/home/adam/*\"))\nIt does pattern matching intuitivelyimport glob\n# All files and directories ending with .txt and that don't begin with a dot:\nprint(glob.glob(\"/home/adam/*.txt\")) \n# All files and directories ending with .txt with depth of 2 folders, ignoring names beginning with a dot:\nprint(glob.glob(\"/home/adam/*/*.txt\")) \nimport glob\n# All files and directories ending with .txt and that don't begin with a dot:\nprint(glob.glob(\"/home/adam/*.txt\")) \n# All files and directories ending with .txt with depth of 2 folders, ignoring names beginning with a dot:\nprint(glob.glob(\"/home/adam/*/*.txt\")) \nIt will return a list with the queried files and directories:['/home/adam/file1.txt', '/home/adam/file2.txt', .... ]\n['/home/adam/file1.txt', '/home/adam/file2.txt', .... ]\nNote that glob ignores files and directories that begin with a dot ., as those are considered hidden files and directories, unless the pattern is something like .*.glob..*Use glob.escape to escape strings that are not meant to be patterns:glob.escapeglob.escapeprint(glob.glob(glob.escape(directory_name) + \"/*.txt\"))\nprint(glob.glob(glob.escape(directory_name) + \"/*.txt\"))\n",
                "list in the current directoryWith listdir in os module you get the files and the folders in the current dirlistdirosimport os\n\narr = os.listdir()\nimport os\n\narr = os.listdir()\nLooking in a directoryarr = os.listdir('c:\\\\files')\narr = os.listdir('c:\\\\files')\nwith glob you can specify a type of file to list like thisglobimport glob\n\ntxtfiles = []\nfor file in glob.glob(\"*.txt\"):\n    txtfiles.append(file)\nimport glob\n\ntxtfiles = []\nfor file in glob.glob(\"*.txt\"):\n    txtfiles.append(file)\normylist = [f for f in glob.glob(\"*.txt\")]\nmylist = [f for f in glob.glob(\"*.txt\")]\nget the full path of only files in the current directoryimport os\nfrom os import listdir\nfrom os.path import isfile, join\n\ncwd = os.getcwd()\nonlyfiles = [os.path.join(cwd, f) for f in os.listdir(cwd) if \nos.path.isfile(os.path.join(cwd, f))]\nprint(onlyfiles) \n\n['G:\\\\getfilesname\\\\getfilesname.py', 'G:\\\\getfilesname\\\\example.txt']\nimport os\nfrom os import listdir\nfrom os.path import isfile, join\n\ncwd = os.getcwd()\nonlyfiles = [os.path.join(cwd, f) for f in os.listdir(cwd) if \nos.path.isfile(os.path.join(cwd, f))]\nprint(onlyfiles) \n\n['G:\\\\getfilesname\\\\getfilesname.py', 'G:\\\\getfilesname\\\\example.txt']\nGetting the full path name with os.path.abspathos.path.abspathYou get the full path in return import os\n files_path = [os.path.abspath(x) for x in os.listdir()]\n print(files_path)\n \n ['F:\\\\documenti\\applications.txt', 'F:\\\\documenti\\collections.txt']\n import os\n files_path = [os.path.abspath(x) for x in os.listdir()]\n print(files_path)\n \n ['F:\\\\documenti\\applications.txt', 'F:\\\\documenti\\collections.txt']\nWalk: going through sub directoriesos.walk returns the root, the directories list and the files list, that is why I unpacked them in r, d, f in the for loop; it, then, looks for other files and directories in the subfolders of the root and so on until there are no subfolders.import os\n\n# Getting the current work directory (cwd)\nthisdir = os.getcwd()\n\n# r=root, d=directories, f = files\nfor r, d, f in os.walk(thisdir):\n    for file in f:\n        if file.endswith(\".docx\"):\n            print(os.path.join(r, file))\nimport os\n\n# Getting the current work directory (cwd)\nthisdir = os.getcwd()\n\n# r=root, d=directories, f = files\nfor r, d, f in os.walk(thisdir):\n    for file in f:\n        if file.endswith(\".docx\"):\n            print(os.path.join(r, file))\nTo go up in the directory tree# Method 1\nx = os.listdir('..')\n\n# Method 2\nx= os.listdir('/')\n# Method 1\nx = os.listdir('..')\n\n# Method 2\nx= os.listdir('/')\nGet files of a particular subdirectory with os.listdir()os.listdir()import os\n\nx = os.listdir(\"./content\")\nimport os\n\nx = os.listdir(\"./content\")\nos.walk('.') - current directory import os\n arr = next(os.walk('.'))[2]\n print(arr)\n \n >>> ['5bs_Turismo1.pdf', '5bs_Turismo1.pptx', 'esperienza.txt']\n import os\n arr = next(os.walk('.'))[2]\n print(arr)\n \n >>> ['5bs_Turismo1.pdf', '5bs_Turismo1.pptx', 'esperienza.txt']\nnext(os.walk('.')) and os.path.join('dir', 'file') import os\n arr = []\n for d,r,f in next(os.walk(\"F:\\\\_python\")):\n     for file in f:\n         arr.append(os.path.join(r,file))\n\n for f in arr:\n     print(files)\n\n>>> F:\\\\_python\\\\dict_class.py\n>>> F:\\\\_python\\\\programmi.txt\n import os\n arr = []\n for d,r,f in next(os.walk(\"F:\\\\_python\")):\n     for file in f:\n         arr.append(os.path.join(r,file))\n\n for f in arr:\n     print(files)\n\n>>> F:\\\\_python\\\\dict_class.py\n>>> F:\\\\_python\\\\programmi.txt\nnext... walk [os.path.join(r,file) for r,d,f in next(os.walk(\"F:\\\\_python\")) for file in f]\n \n >>> ['F:\\\\_python\\\\dict_class.py', 'F:\\\\_python\\\\programmi.txt']\n [os.path.join(r,file) for r,d,f in next(os.walk(\"F:\\\\_python\")) for file in f]\n \n >>> ['F:\\\\_python\\\\dict_class.py', 'F:\\\\_python\\\\programmi.txt']\nos.walkx = [os.path.join(r,file) for r,d,f in os.walk(\"F:\\\\_python\") for file in f]\nprint(x)\n\n>>> ['F:\\\\_python\\\\dict.py', 'F:\\\\_python\\\\progr.txt', 'F:\\\\_python\\\\readl.py']\nx = [os.path.join(r,file) for r,d,f in os.walk(\"F:\\\\_python\") for file in f]\nprint(x)\n\n>>> ['F:\\\\_python\\\\dict.py', 'F:\\\\_python\\\\progr.txt', 'F:\\\\_python\\\\readl.py']\nos.listdir() - get only txt files arr_txt = [x for x in os.listdir() if x.endswith(\".txt\")]\n \n arr_txt = [x for x in os.listdir() if x.endswith(\".txt\")]\n \nUsing glob to get the full path of the filesglobfrom path import path\nfrom glob import glob\n\nx = [path(f).abspath() for f in glob(\"F:\\\\*.txt\")]\nfrom path import path\nfrom glob import glob\n\nx = [path(f).abspath() for f in glob(\"F:\\\\*.txt\")]\nUsing os.path.isfile to avoid directories in the listos.path.isfileimport os.path\nlistOfFiles = [f for f in os.listdir() if os.path.isfile(f)]\nimport os.path\nlistOfFiles = [f for f in os.listdir() if os.path.isfile(f)]\nUsing pathlib from Python 3.4pathlibimport pathlib\n\nflist = []\nfor p in pathlib.Path('.').iterdir():\n    if p.is_file():\n        print(p)\n        flist.append(p)\nimport pathlib\n\nflist = []\nfor p in pathlib.Path('.').iterdir():\n    if p.is_file():\n        print(p)\n        flist.append(p)\nWith list comprehension:list comprehensionflist = [p for p in pathlib.Path('.').iterdir() if p.is_file()]\nflist = [p for p in pathlib.Path('.').iterdir() if p.is_file()]\nUse glob method in pathlib.Path()import pathlib\n\npy = pathlib.Path().glob(\"*.py\")\nimport pathlib\n\npy = pathlib.Path().glob(\"*.py\")\nGet all and only files with os.walk: checks only in the third element returned, i.e. the list of the filesimport os\nx = [i[2] for i in os.walk('.')]\ny=[]\nfor t in x:\n    for f in t:\n        y.append(f)\nimport os\nx = [i[2] for i in os.walk('.')]\ny=[]\nfor t in x:\n    for f in t:\n        y.append(f)\nGet only files with next in a directory: returns only the file in the root folder import os\n x = next(os.walk('F://python'))[2]\n import os\n x = next(os.walk('F://python'))[2]\nGet only directories with next and walk in a directory, because in the [1] element there are the folders only import os\n next(os.walk('F://python'))[1] # for the current dir use ('.')\n \n >>> ['python3','others']\n import os\n next(os.walk('F://python'))[1] # for the current dir use ('.')\n \n >>> ['python3','others']\nGet all the subdir names with walksubdirwalkfor r,d,f in os.walk(\"F:\\\\_python\"):\n    for dirs in d:\n        print(dirs)\nfor r,d,f in os.walk(\"F:\\\\_python\"):\n    for dirs in d:\n        print(dirs)\nos.scandir() from Python 3.5 and greateros.scandir()import os\nx = [f.name for f in os.scandir() if f.is_file()]\n\n# Another example with `scandir` (a little variation from docs.python.org)\n# This one is more efficient than `os.listdir`.\n# In this case, it shows the files only in the current directory\n# where the script is executed.\n\nimport os\nwith os.scandir() as i:\n    for entry in i:\n        if entry.is_file():\n            print(entry.name)\nimport os\nx = [f.name for f in os.scandir() if f.is_file()]\n\n# Another example with `scandir` (a little variation from docs.python.org)\n# This one is more efficient than `os.listdir`.\n# In this case, it shows the files only in the current directory\n# where the script is executed.\n\nimport os\nwith os.scandir() as i:\n    for entry in i:\n        if entry.is_file():\n            print(entry.name)\n",
                "import os\nos.listdir(\"somedirectory\")\nimport os\nos.listdir(\"somedirectory\")\nwill return a list of all files and directories in \"somedirectory\".",
                "A one-line solution to get only list of files (no subdirectories):only list of filesfilenames = next(os.walk(path))[2]\nfilenames = next(os.walk(path))[2]\nor absolute pathnames:paths = [os.path.join(path, fn) for fn in next(os.walk(path))[2]]\npaths = [os.path.join(path, fn) for fn in next(os.walk(path))[2]]\n",
                "Getting Full File Paths From a Directory and All Its SubdirectoriesGetting Full File Paths From a Directory and All Its Subdirectoriesimport os\n\ndef get_filepaths(directory):\n    \"\"\"\n    This function will generate the file names in a directory \n    tree by walking the tree either top-down or bottom-up. For each \n    directory in the tree rooted at directory top (including top itself), \n    it yields a 3-tuple (dirpath, dirnames, filenames).\n    \"\"\"\n    file_paths = []  # List which will store all of the full filepaths.\n\n    # Walk the tree.\n    for root, directories, files in os.walk(directory):\n        for filename in files:\n            # Join the two strings in order to form the full filepath.\n            filepath = os.path.join(root, filename)\n            file_paths.append(filepath)  # Add it to the list.\n\n    return file_paths  # Self-explanatory.\n\n# Run the above function and store its results in a variable.   \nfull_file_paths = get_filepaths(\"/Users/johnny/Desktop/TEST\")\nimport os\n\ndef get_filepaths(directory):\n    \"\"\"\n    This function will generate the file names in a directory \n    tree by walking the tree either top-down or bottom-up. For each \n    directory in the tree rooted at directory top (including top itself), \n    it yields a 3-tuple (dirpath, dirnames, filenames).\n    \"\"\"\n    file_paths = []  # List which will store all of the full filepaths.\n\n    # Walk the tree.\n    for root, directories, files in os.walk(directory):\n        for filename in files:\n            # Join the two strings in order to form the full filepath.\n            filepath = os.path.join(root, filename)\n            file_paths.append(filepath)  # Add it to the list.\n\n    return file_paths  # Self-explanatory.\n\n# Run the above function and store its results in a variable.   \nfull_file_paths = get_filepaths(\"/Users/johnny/Desktop/TEST\")\n\nThe path I provided in the above function contained 3 files\u2014 two of them in the root directory, and another in a subfolder called \"SUBFOLDER.\"  You can now do things like:\nprint full_file_paths which will print the list:\n\n['/Users/johnny/Desktop/TEST/file1.txt', '/Users/johnny/Desktop/TEST/file2.txt', '/Users/johnny/Desktop/TEST/SUBFOLDER/file3.dat']\n\nThe path I provided in the above function contained 3 files\u2014 two of them in the root directory, and another in a subfolder called \"SUBFOLDER.\"  You can now do things like:print full_file_paths which will print the list:\n\n['/Users/johnny/Desktop/TEST/file1.txt', '/Users/johnny/Desktop/TEST/file2.txt', '/Users/johnny/Desktop/TEST/SUBFOLDER/file3.dat']\nprint full_file_paths which will print the list:print full_file_paths\n['/Users/johnny/Desktop/TEST/file1.txt', '/Users/johnny/Desktop/TEST/file2.txt', '/Users/johnny/Desktop/TEST/SUBFOLDER/file3.dat']\n['/Users/johnny/Desktop/TEST/file1.txt', '/Users/johnny/Desktop/TEST/file2.txt', '/Users/johnny/Desktop/TEST/SUBFOLDER/file3.dat']['/Users/johnny/Desktop/TEST/file1.txt', '/Users/johnny/Desktop/TEST/file2.txt', '/Users/johnny/Desktop/TEST/SUBFOLDER/file3.dat']If you'd like, you can open and read the contents, or focus only on files with the extension \".dat\" like in the code below:for f in full_file_paths:\n  if f.endswith(\".dat\"):\n    print f\nfor f in full_file_paths:\n  if f.endswith(\".dat\"):\n    print f\n/Users/johnny/Desktop/TEST/SUBFOLDER/file3.dat/Users/johnny/Desktop/TEST/SUBFOLDER/file3.dat",
                "Since version 3.4 there are builtin iterators for this which are a lot more efficient than os.listdir():iteratorsos.listdir()os.listdir()pathlib: New in version 3.4.pathlibpathlibNew in version 3.4.>>> import pathlib\n>>> [p for p in pathlib.Path('.').iterdir() if p.is_file()]\n>>> import pathlib\n>>> [p for p in pathlib.Path('.').iterdir() if p.is_file()]\nAccording to PEP 428, the aim of the pathlib library is to provide a simple hierarchy of classes to handle filesystem paths and the common operations users do over them.PEP 428pathlibpathlibos.scandir(): New in version 3.5.os.scandir()os.scandir()New in version 3.5.>>> import os\n>>> [entry for entry in os.scandir('.') if entry.is_file()]\n>>> import os\n>>> [entry for entry in os.scandir('.') if entry.is_file()]\nNote that os.walk() uses os.scandir() instead of os.listdir() from version 3.5, and its speed got increased by 2-20 times according to PEP 471.os.walk()os.walk()os.scandir()os.scandir()os.listdir()os.listdir()PEP 471Let me also recommend reading ShadowRanger's comment below.",
                "Preliminary notes\nAlthough there's a clear differentiation between file and directory terms in the question text, some may argue that directories are actually special files\n\nThe statement: \"all files of a directory\" can be interpreted in two ways:\n\nAll direct (or level 1) descendants only\n\nAll descendants in the whole directory tree (including the ones in sub-directories)\n\n\n\nWhen the question was asked, I imagine that Python 2, was the LTS version, however the code samples will be run by Python 3(.5) (I'll keep them as Python 2 compliant as possible; also, any code belonging to Python that I'm going to post, is from v3.5.4 - unless otherwise specified).\nThat has consequences related to another keyword in the question: \"add them into a list\":\n\nIn pre Python 2.2 versions, sequences (iterables) were mostly represented by lists (tuples, sets, ...)\n\nIn Python 2.2, the concept of generator ([Python.Wiki]: Generators) - courtesy of [Python.Docs]: Simple statements - The yield statement) - was introduced. As time passed, generator counterparts started to appear for functions that returned / worked with lists\n\nIn Python 3, generator is the default behavior\n\nNot sure if returning a list is still mandatory (or a generator would do as well), but passing a generator to the list constructor, will create a list out of it (and also consume it). The example below illustrates the differences on [Python.Docs]: Built-in functions - map(function, iterable, *iterables)\n\n\n\n>>> import sys\n>>>\n>>> sys.version\n'2.7.10 (default, Mar  8 2016, 15:02:46) [MSC v.1600 64 bit (AMD64)]'\n>>> m = map(lambda x: x, [1, 2, 3])  # Just a dummy lambda function\n>>> m, type(m)\n([1, 2, 3], <type 'list'>)\n>>> len(m)\n3\n\n\n\n\n>>> import sys\n>>>\n>>> sys.version\n'3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:17:05) [MSC v.1900 64 bit (AMD64)]'\n>>> m = map(lambda x: x, [1, 2, 3])\n>>> m, type(m)\n(<map object at 0x000001B4257342B0>, <class 'map'>)\n>>> len(m)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: object of type 'map' has no len()\n>>> lm0 = list(m)  # Build a list from the generator\n>>> lm0, type(lm0)\n([1, 2, 3], <class 'list'>)\n>>>\n>>> lm1 = list(m)  # Build a list from the same generator\n>>> lm1, type(lm1)  # Empty list now - generator already exhausted\n([], <class 'list'>)\n\n\n\nThe examples will be based on a directory called root_dir with the following structure (this example is for Win, but I'm using the same tree on Nix as well). Note that I'll be reusing the console:\n\n[cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q003207219]> sopr.bat\n### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n\n[prompt]> \n[prompt]> tree /f \"root_dir\"\nFolder PATH listing for volume Work\nVolume serial number is 00000029 3655:6FED\nE:\\WORK\\DEV\\STACKOVERFLOW\\Q003207219\\ROOT_DIR\n\u00a6   file0\n\u00a6   file1\n\u00a6\n+---dir0\n\u00a6   +---dir00\n\u00a6   \u00a6   \u00a6   file000\n\u00a6   \u00a6   \u00a6\n\u00a6   \u00a6   +---dir000\n\u00a6   \u00a6           file0000\n\u00a6   \u00a6\n\u00a6   +---dir01\n\u00a6   \u00a6       file010\n\u00a6   \u00a6       file011\n\u00a6   \u00a6\n\u00a6   +---dir02\n\u00a6       +---dir020\n\u00a6           +---dir0200\n+---dir1\n\u00a6       file10\n\u00a6       file11\n\u00a6       file12\n\u00a6\n+---dir2\n\u00a6   \u00a6   file20\n\u00a6   \u00a6\n\u00a6   +---dir20\n\u00a6           file200\n\u00a6\n+---dir3\n\n\n\nAlthough there's a clear differentiation between file and directory terms in the question text, some may argue that directories are actually special files\nAlthough there's a clear differentiation between file and directory terms in the question text, some may argue that directories are actually special filesfiledirectoryThe statement: \"all files of a directory\" can be interpreted in two ways:\n\nAll direct (or level 1) descendants only\n\nAll descendants in the whole directory tree (including the ones in sub-directories)\n\n\nThe statement: \"all files of a directory\" can be interpreted in two ways:all files of a directory\nAll direct (or level 1) descendants only\n\nAll descendants in the whole directory tree (including the ones in sub-directories)\n\nAll direct (or level 1) descendants only\nAll direct (or level 1) descendants onlydirectonlyAll descendants in the whole directory tree (including the ones in sub-directories)\nAll descendants in the whole directory tree (including the ones in sub-directories)When the question was asked, I imagine that Python 2, was the LTS version, however the code samples will be run by Python 3(.5) (I'll keep them as Python 2 compliant as possible; also, any code belonging to Python that I'm going to post, is from v3.5.4 - unless otherwise specified).\nThat has consequences related to another keyword in the question: \"add them into a list\":\n\nIn pre Python 2.2 versions, sequences (iterables) were mostly represented by lists (tuples, sets, ...)\n\nIn Python 2.2, the concept of generator ([Python.Wiki]: Generators) - courtesy of [Python.Docs]: Simple statements - The yield statement) - was introduced. As time passed, generator counterparts started to appear for functions that returned / worked with lists\n\nIn Python 3, generator is the default behavior\n\nNot sure if returning a list is still mandatory (or a generator would do as well), but passing a generator to the list constructor, will create a list out of it (and also consume it). The example below illustrates the differences on [Python.Docs]: Built-in functions - map(function, iterable, *iterables)\n\n\n\n>>> import sys\n>>>\n>>> sys.version\n'2.7.10 (default, Mar  8 2016, 15:02:46) [MSC v.1600 64 bit (AMD64)]'\n>>> m = map(lambda x: x, [1, 2, 3])  # Just a dummy lambda function\n>>> m, type(m)\n([1, 2, 3], <type 'list'>)\n>>> len(m)\n3\n\n\n\n\n>>> import sys\n>>>\n>>> sys.version\n'3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:17:05) [MSC v.1900 64 bit (AMD64)]'\n>>> m = map(lambda x: x, [1, 2, 3])\n>>> m, type(m)\n(<map object at 0x000001B4257342B0>, <class 'map'>)\n>>> len(m)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: object of type 'map' has no len()\n>>> lm0 = list(m)  # Build a list from the generator\n>>> lm0, type(lm0)\n([1, 2, 3], <class 'list'>)\n>>>\n>>> lm1 = list(m)  # Build a list from the same generator\n>>> lm1, type(lm1)  # Empty list now - generator already exhausted\n([], <class 'list'>)\n\n\nWhen the question was asked, I imagine that Python 2, was the LTS version, however the code samples will be run by Python 3(.5) (I'll keep them as Python 2 compliant as possible; also, any code belonging to Python that I'm going to post, is from v3.5.4 - unless otherwise specified).\nThat has consequences related to another keyword in the question: \"add them into a list\":Python 22LTSPython 3(.5)3.5Python 2Pythonv3.5.4v3.5.4add them into a listlist\nIn pre Python 2.2 versions, sequences (iterables) were mostly represented by lists (tuples, sets, ...)\n\nIn Python 2.2, the concept of generator ([Python.Wiki]: Generators) - courtesy of [Python.Docs]: Simple statements - The yield statement) - was introduced. As time passed, generator counterparts started to appear for functions that returned / worked with lists\n\nIn Python 3, generator is the default behavior\n\nNot sure if returning a list is still mandatory (or a generator would do as well), but passing a generator to the list constructor, will create a list out of it (and also consume it). The example below illustrates the differences on [Python.Docs]: Built-in functions - map(function, iterable, *iterables)\n\nIn pre Python 2.2 versions, sequences (iterables) were mostly represented by lists (tuples, sets, ...)\nIn pre Python 2.2 versions, sequences (iterables) were mostly represented by lists (tuples, sets, ...)Python 2.2In Python 2.2, the concept of generator ([Python.Wiki]: Generators) - courtesy of [Python.Docs]: Simple statements - The yield statement) - was introduced. As time passed, generator counterparts started to appear for functions that returned / worked with lists\nIn Python 2.2, the concept of generator ([Python.Wiki]: Generators) - courtesy of [Python.Docs]: Simple statements - The yield statement) - was introduced. As time passed, generator counterparts started to appear for functions that returned / worked with listsPython 2.2generator[Python.Wiki]: Generators[Python.Docs]: Simple statements - The yield statementIn Python 3, generator is the default behavior\nIn Python 3, generator is the default behaviorPython 3Not sure if returning a list is still mandatory (or a generator would do as well), but passing a generator to the list constructor, will create a list out of it (and also consume it). The example below illustrates the differences on [Python.Docs]: Built-in functions - map(function, iterable, *iterables)\nNot sure if returning a list is still mandatory (or a generator would do as well), but passing a generator to the list constructor, will create a list out of it (and also consume it). The example below illustrates the differences on [Python.Docs]: Built-in functions - map(function, iterable, *iterables)list[Python.Docs]: Built-in functions - map(function, iterable, *iterables)\n>>> import sys\n>>>\n>>> sys.version\n'2.7.10 (default, Mar  8 2016, 15:02:46) [MSC v.1600 64 bit (AMD64)]'\n>>> m = map(lambda x: x, [1, 2, 3])  # Just a dummy lambda function\n>>> m, type(m)\n([1, 2, 3], <type 'list'>)\n>>> len(m)\n3\n\n>>> import sys\n>>>\n>>> sys.version\n'2.7.10 (default, Mar  8 2016, 15:02:46) [MSC v.1600 64 bit (AMD64)]'\n>>> m = map(lambda x: x, [1, 2, 3])  # Just a dummy lambda function\n>>> m, type(m)\n([1, 2, 3], <type 'list'>)\n>>> len(m)\n3\n>>> import sys\n>>>\n>>> sys.version\n'2.7.10 (default, Mar  8 2016, 15:02:46) [MSC v.1600 64 bit (AMD64)]'\n>>> m = map(lambda x: x, [1, 2, 3])  # Just a dummy lambda function\n>>> m, type(m)\n([1, 2, 3], <type 'list'>)\n>>> len(m)\n3\n\n>>> import sys\n>>>\n>>> sys.version\n'3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:17:05) [MSC v.1900 64 bit (AMD64)]'\n>>> m = map(lambda x: x, [1, 2, 3])\n>>> m, type(m)\n(<map object at 0x000001B4257342B0>, <class 'map'>)\n>>> len(m)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: object of type 'map' has no len()\n>>> lm0 = list(m)  # Build a list from the generator\n>>> lm0, type(lm0)\n([1, 2, 3], <class 'list'>)\n>>>\n>>> lm1 = list(m)  # Build a list from the same generator\n>>> lm1, type(lm1)  # Empty list now - generator already exhausted\n([], <class 'list'>)\n\n>>> import sys\n>>>\n>>> sys.version\n'3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:17:05) [MSC v.1900 64 bit (AMD64)]'\n>>> m = map(lambda x: x, [1, 2, 3])\n>>> m, type(m)\n(<map object at 0x000001B4257342B0>, <class 'map'>)\n>>> len(m)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: object of type 'map' has no len()\n>>> lm0 = list(m)  # Build a list from the generator\n>>> lm0, type(lm0)\n([1, 2, 3], <class 'list'>)\n>>>\n>>> lm1 = list(m)  # Build a list from the same generator\n>>> lm1, type(lm1)  # Empty list now - generator already exhausted\n([], <class 'list'>)\n>>> import sys\n>>>\n>>> sys.version\n'3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:17:05) [MSC v.1900 64 bit (AMD64)]'\n>>> m = map(lambda x: x, [1, 2, 3])\n>>> m, type(m)\n(<map object at 0x000001B4257342B0>, <class 'map'>)\n>>> len(m)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: object of type 'map' has no len()\n>>> lm0 = list(m)  # Build a list from the generator\n>>> lm0, type(lm0)\n([1, 2, 3], <class 'list'>)\n>>>\n>>> lm1 = list(m)  # Build a list from the same generator\n>>> lm1, type(lm1)  # Empty list now - generator already exhausted\n([], <class 'list'>)\nThe examples will be based on a directory called root_dir with the following structure (this example is for Win, but I'm using the same tree on Nix as well). Note that I'll be reusing the console:\n\n[cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q003207219]> sopr.bat\n### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n\n[prompt]> \n[prompt]> tree /f \"root_dir\"\nFolder PATH listing for volume Work\nVolume serial number is 00000029 3655:6FED\nE:\\WORK\\DEV\\STACKOVERFLOW\\Q003207219\\ROOT_DIR\n\u00a6   file0\n\u00a6   file1\n\u00a6\n+---dir0\n\u00a6   +---dir00\n\u00a6   \u00a6   \u00a6   file000\n\u00a6   \u00a6   \u00a6\n\u00a6   \u00a6   +---dir000\n\u00a6   \u00a6           file0000\n\u00a6   \u00a6\n\u00a6   +---dir01\n\u00a6   \u00a6       file010\n\u00a6   \u00a6       file011\n\u00a6   \u00a6\n\u00a6   +---dir02\n\u00a6       +---dir020\n\u00a6           +---dir0200\n+---dir1\n\u00a6       file10\n\u00a6       file11\n\u00a6       file12\n\u00a6\n+---dir2\n\u00a6   \u00a6   file20\n\u00a6   \u00a6\n\u00a6   +---dir20\n\u00a6           file200\n\u00a6\n+---dir3\n\n\nThe examples will be based on a directory called root_dir with the following structure (this example is for Win, but I'm using the same tree on Nix as well). Note that I'll be reusing the console:root_dirWinNix\n[cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q003207219]> sopr.bat\n### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n\n[prompt]> \n[prompt]> tree /f \"root_dir\"\nFolder PATH listing for volume Work\nVolume serial number is 00000029 3655:6FED\nE:\\WORK\\DEV\\STACKOVERFLOW\\Q003207219\\ROOT_DIR\n\u00a6   file0\n\u00a6   file1\n\u00a6\n+---dir0\n\u00a6   +---dir00\n\u00a6   \u00a6   \u00a6   file000\n\u00a6   \u00a6   \u00a6\n\u00a6   \u00a6   +---dir000\n\u00a6   \u00a6           file0000\n\u00a6   \u00a6\n\u00a6   +---dir01\n\u00a6   \u00a6       file010\n\u00a6   \u00a6       file011\n\u00a6   \u00a6\n\u00a6   +---dir02\n\u00a6       +---dir020\n\u00a6           +---dir0200\n+---dir1\n\u00a6       file10\n\u00a6       file11\n\u00a6       file12\n\u00a6\n+---dir2\n\u00a6   \u00a6   file20\n\u00a6   \u00a6\n\u00a6   +---dir20\n\u00a6           file200\n\u00a6\n+---dir3\n\n[cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q003207219]> sopr.bat\n### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n\n[prompt]> \n[prompt]> tree /f \"root_dir\"\nFolder PATH listing for volume Work\nVolume serial number is 00000029 3655:6FED\nE:\\WORK\\DEV\\STACKOVERFLOW\\Q003207219\\ROOT_DIR\n\u00a6   file0\n\u00a6   file1\n\u00a6\n+---dir0\n\u00a6   +---dir00\n\u00a6   \u00a6   \u00a6   file000\n\u00a6   \u00a6   \u00a6\n\u00a6   \u00a6   +---dir000\n\u00a6   \u00a6           file0000\n\u00a6   \u00a6\n\u00a6   +---dir01\n\u00a6   \u00a6       file010\n\u00a6   \u00a6       file011\n\u00a6   \u00a6\n\u00a6   +---dir02\n\u00a6       +---dir020\n\u00a6           +---dir0200\n+---dir1\n\u00a6       file10\n\u00a6       file11\n\u00a6       file12\n\u00a6\n+---dir2\n\u00a6   \u00a6   file20\n\u00a6   \u00a6\n\u00a6   +---dir20\n\u00a6           file200\n\u00a6\n+---dir3\n[cfati@CFATI-5510-0:e:\\Work\\Dev\\StackOverflow\\q003207219]> sopr.bat\n### Set shorter prompt to better fit when pasted in StackOverflow (or other) pages ###\n\n[prompt]> \n[prompt]> tree /f \"root_dir\"\nFolder PATH listing for volume Work\nVolume serial number is 00000029 3655:6FED\nE:\\WORK\\DEV\\STACKOVERFLOW\\Q003207219\\ROOT_DIR\n\u00a6   file0\n\u00a6   file1\n\u00a6\n+---dir0\n\u00a6   +---dir00\n\u00a6   \u00a6   \u00a6   file000\n\u00a6   \u00a6   \u00a6\n\u00a6   \u00a6   +---dir000\n\u00a6   \u00a6           file0000\n\u00a6   \u00a6\n\u00a6   +---dir01\n\u00a6   \u00a6       file010\n\u00a6   \u00a6       file011\n\u00a6   \u00a6\n\u00a6   +---dir02\n\u00a6       +---dir020\n\u00a6           +---dir0200\n+---dir1\n\u00a6       file10\n\u00a6       file11\n\u00a6       file12\n\u00a6\n+---dir2\n\u00a6   \u00a6   file20\n\u00a6   \u00a6\n\u00a6   +---dir20\n\u00a6           file200\n\u00a6\n+---dir3\nSolutionsProgrammatic approaches1. [Python.Docs]: os.listdir(path='.')[Python.Docs]: os.listdir(path='.')\nReturn a list containing the names of the entries in the directory given by path. The list is in arbitrary order, and does not include the special entries '.' and '..' ...\nReturn a list containing the names of the entries in the directory given by path. The list is in arbitrary order, and does not include the special entries '.' and '..' ...path'.''..'\n>>> import os\n>>>\n>>> root_dir = \"root_dir\"  # Path relative to current dir (os.getcwd())\n>>>\n>>> os.listdir(root_dir)  # List all the items in root_dir\n['dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> [item for item in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, item))]  # Filter items and only keep files (strip out directories)\n['file0', 'file1']\n\n>>> import os\n>>>\n>>> root_dir = \"root_dir\"  # Path relative to current dir (os.getcwd())\n>>>\n>>> os.listdir(root_dir)  # List all the items in root_dir\n['dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> [item for item in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, item))]  # Filter items and only keep files (strip out directories)\n['file0', 'file1']\n>>> import os\n>>>\n>>> root_dir = \"root_dir\"  # Path relative to current dir (os.getcwd())\n>>>\n>>> os.listdir(root_dir)  # List all the items in root_dir\n['dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> [item for item in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, item))]  # Filter items and only keep files (strip out directories)\n['file0', 'file1']\nA more elaborate example (code_os_listdir.py):code_os_listdir.py#!/usr/bin/env python\n\nimport os\nimport sys\nfrom pprint import pformat as pf\n\n\ndef _get_dir_content(path, include_folders, recursive):\n    entries = os.listdir(path)\n    for entry in entries:\n        entry_with_path = os.path.join(path, entry)\n        if os.path.isdir(entry_with_path):\n            if include_folders:\n                yield entry_with_path\n            if recursive:\n                for sub_entry in _get_dir_content(entry_with_path, include_folders, recursive):\n                    yield sub_entry\n        else:\n            yield entry_with_path\n\n\ndef get_dir_content(path, include_folders=True, recursive=True, prepend_folder_name=True):\n    path_len = len(path) + len(os.path.sep)\n    for item in _get_dir_content(path, include_folders, recursive):\n        yield item if prepend_folder_name else item[path_len:]\n\n\ndef _get_dir_content_old(path, include_folders, recursive):\n    entries = os.listdir(path)\n    ret = list()\n    for entry in entries:\n        entry_with_path = os.path.join(path, entry)\n        if os.path.isdir(entry_with_path):\n            if include_folders:\n                ret.append(entry_with_path)\n            if recursive:\n                ret.extend(_get_dir_content_old(entry_with_path, include_folders, recursive))\n        else:\n            ret.append(entry_with_path)\n    return ret\n\n\ndef get_dir_content_old(path, include_folders=True, recursive=True, prepend_folder_name=True):\n    path_len = len(path) + len(os.path.sep)\n    return [item if prepend_folder_name else item[path_len:] for item in _get_dir_content_old(path, include_folders, recursive)]\n\n\ndef main(*argv):\n    root_dir = \"root_dir\"\n    ret0 = get_dir_content(root_dir, include_folders=True, recursive=True, prepend_folder_name=True)\n    lret0 = list(ret0)\n    print(\"{:} {:d}\\n{:s}\".format(ret0, len(lret0), pf(lret0)))\n    ret1 = get_dir_content_old(root_dir, include_folders=False, recursive=True, prepend_folder_name=False)\n    print(\"\\n{:d}\\n{:s}\".format(len(ret1), pf(ret1)))\n\n\nif __name__ == \"__main__\":\n    print(\"Python {:s} {:03d}bit on {:s}\\n\".format(\" \".join(elem.strip() for elem in sys.version.split(\"\\n\")),\n                                                   64 if sys.maxsize > 0x100000000 else 32, sys.platform))\n    rc = main(*sys.argv[1:])\n    print(\"\\nDone.\\n\")\n    sys.exit(rc)\n#!/usr/bin/env python\n\nimport os\nimport sys\nfrom pprint import pformat as pf\n\n\ndef _get_dir_content(path, include_folders, recursive):\n    entries = os.listdir(path)\n    for entry in entries:\n        entry_with_path = os.path.join(path, entry)\n        if os.path.isdir(entry_with_path):\n            if include_folders:\n                yield entry_with_path\n            if recursive:\n                for sub_entry in _get_dir_content(entry_with_path, include_folders, recursive):\n                    yield sub_entry\n        else:\n            yield entry_with_path\n\n\ndef get_dir_content(path, include_folders=True, recursive=True, prepend_folder_name=True):\n    path_len = len(path) + len(os.path.sep)\n    for item in _get_dir_content(path, include_folders, recursive):\n        yield item if prepend_folder_name else item[path_len:]\n\n\ndef _get_dir_content_old(path, include_folders, recursive):\n    entries = os.listdir(path)\n    ret = list()\n    for entry in entries:\n        entry_with_path = os.path.join(path, entry)\n        if os.path.isdir(entry_with_path):\n            if include_folders:\n                ret.append(entry_with_path)\n            if recursive:\n                ret.extend(_get_dir_content_old(entry_with_path, include_folders, recursive))\n        else:\n            ret.append(entry_with_path)\n    return ret\n\n\ndef get_dir_content_old(path, include_folders=True, recursive=True, prepend_folder_name=True):\n    path_len = len(path) + len(os.path.sep)\n    return [item if prepend_folder_name else item[path_len:] for item in _get_dir_content_old(path, include_folders, recursive)]\n\n\ndef main(*argv):\n    root_dir = \"root_dir\"\n    ret0 = get_dir_content(root_dir, include_folders=True, recursive=True, prepend_folder_name=True)\n    lret0 = list(ret0)\n    print(\"{:} {:d}\\n{:s}\".format(ret0, len(lret0), pf(lret0)))\n    ret1 = get_dir_content_old(root_dir, include_folders=False, recursive=True, prepend_folder_name=False)\n    print(\"\\n{:d}\\n{:s}\".format(len(ret1), pf(ret1)))\n\n\nif __name__ == \"__main__\":\n    print(\"Python {:s} {:03d}bit on {:s}\\n\".format(\" \".join(elem.strip() for elem in sys.version.split(\"\\n\")),\n                                                   64 if sys.maxsize > 0x100000000 else 32, sys.platform))\n    rc = main(*sys.argv[1:])\n    print(\"\\nDone.\\n\")\n    sys.exit(rc)\nNotes:Notes\nThere are two implementations:\n\nOne that uses generators (of course here it seems useless, since I immediately convert the result to a list)\n\nThe classic one (function names ending in _old)\n\n\n\nRecursion is used (to get into subdirectories)\n\nFor each implementation there are two functions:\n\nOne that starts with an underscore (_): \"private\" (should not be called directly) - that does all the work\n\nThe public one (wrapper over previous): it just strips off the initial path (if required) from the returned entries. It's an ugly implementation, but it's the only idea that I could come with at this point\n\n\n\nIn terms of performance, generators are generally a little bit faster (considering both creation and  iteration times), but I didn't test them in recursive functions, and also I am iterating inside the function over inner generators - don't know how performance friendly is that\n\nPlay with the arguments to get different results\n\nThere are two implementations:\n\nOne that uses generators (of course here it seems useless, since I immediately convert the result to a list)\n\nThe classic one (function names ending in _old)\n\n\nThere are two implementations:\nOne that uses generators (of course here it seems useless, since I immediately convert the result to a list)\n\nThe classic one (function names ending in _old)\n\nOne that uses generators (of course here it seems useless, since I immediately convert the result to a list)\nOne that uses generators (of course here it seems useless, since I immediately convert the result to a list)The classic one (function names ending in _old)\nThe classic one (function names ending in _old)_oldRecursion is used (to get into subdirectories)\nRecursion is used (to get into subdirectories)For each implementation there are two functions:\n\nOne that starts with an underscore (_): \"private\" (should not be called directly) - that does all the work\n\nThe public one (wrapper over previous): it just strips off the initial path (if required) from the returned entries. It's an ugly implementation, but it's the only idea that I could come with at this point\n\n\nFor each implementation there are two functions:\nOne that starts with an underscore (_): \"private\" (should not be called directly) - that does all the work\n\nThe public one (wrapper over previous): it just strips off the initial path (if required) from the returned entries. It's an ugly implementation, but it's the only idea that I could come with at this point\n\nOne that starts with an underscore (_): \"private\" (should not be called directly) - that does all the work\nOne that starts with an underscore (_): \"private\" (should not be called directly) - that does all the workunderscore_The public one (wrapper over previous): it just strips off the initial path (if required) from the returned entries. It's an ugly implementation, but it's the only idea that I could come with at this point\nThe public one (wrapper over previous): it just strips off the initial path (if required) from the returned entries. It's an ugly implementation, but it's the only idea that I could come with at this pointIn terms of performance, generators are generally a little bit faster (considering both creation and  iteration times), but I didn't test them in recursive functions, and also I am iterating inside the function over inner generators - don't know how performance friendly is that\nIn terms of performance, generators are generally a little bit faster (considering both creation and  iteration times), but I didn't test them in recursive functions, and also I am iterating inside the function over inner generators - don't know how performance friendly is thatcreationiterationPlay with the arguments to get different results\nPlay with the arguments to get different resultsOutput:Output\n[prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.05.04_test0\\Scripts\\python.exe\" \".\\code_os_listdir.py\"\nPython 3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:17:05) [MSC v.1900 64 bit (AMD64)] 064bit on win32\n\n<generator object get_dir_content at 0x000002C080418F68> 22\n['root_dir\\\\dir0',\n 'root_dir\\\\dir0\\\\dir00',\n 'root_dir\\\\dir0\\\\dir00\\\\dir000',\n 'root_dir\\\\dir0\\\\dir00\\\\dir000\\\\file0000',\n 'root_dir\\\\dir0\\\\dir00\\\\file000',\n 'root_dir\\\\dir0\\\\dir01',\n 'root_dir\\\\dir0\\\\dir01\\\\file010',\n 'root_dir\\\\dir0\\\\dir01\\\\file011',\n 'root_dir\\\\dir0\\\\dir02',\n 'root_dir\\\\dir0\\\\dir02\\\\dir020',\n 'root_dir\\\\dir0\\\\dir02\\\\dir020\\\\dir0200',\n 'root_dir\\\\dir1',\n 'root_dir\\\\dir1\\\\file10',\n 'root_dir\\\\dir1\\\\file11',\n 'root_dir\\\\dir1\\\\file12',\n 'root_dir\\\\dir2',\n 'root_dir\\\\dir2\\\\dir20',\n 'root_dir\\\\dir2\\\\dir20\\\\file200',\n 'root_dir\\\\dir2\\\\file20',\n 'root_dir\\\\dir3',\n 'root_dir\\\\file0',\n 'root_dir\\\\file1']\n\n11\n['dir0\\\\dir00\\\\dir000\\\\file0000',\n 'dir0\\\\dir00\\\\file000',\n 'dir0\\\\dir01\\\\file010',\n 'dir0\\\\dir01\\\\file011',\n 'dir1\\\\file10',\n 'dir1\\\\file11',\n 'dir1\\\\file12',\n 'dir2\\\\dir20\\\\file200',\n 'dir2\\\\file20',\n 'file0',\n 'file1']\n\nDone.\n\n[prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.05.04_test0\\Scripts\\python.exe\" \".\\code_os_listdir.py\"\nPython 3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:17:05) [MSC v.1900 64 bit (AMD64)] 064bit on win32\n\n<generator object get_dir_content at 0x000002C080418F68> 22\n['root_dir\\\\dir0',\n 'root_dir\\\\dir0\\\\dir00',\n 'root_dir\\\\dir0\\\\dir00\\\\dir000',\n 'root_dir\\\\dir0\\\\dir00\\\\dir000\\\\file0000',\n 'root_dir\\\\dir0\\\\dir00\\\\file000',\n 'root_dir\\\\dir0\\\\dir01',\n 'root_dir\\\\dir0\\\\dir01\\\\file010',\n 'root_dir\\\\dir0\\\\dir01\\\\file011',\n 'root_dir\\\\dir0\\\\dir02',\n 'root_dir\\\\dir0\\\\dir02\\\\dir020',\n 'root_dir\\\\dir0\\\\dir02\\\\dir020\\\\dir0200',\n 'root_dir\\\\dir1',\n 'root_dir\\\\dir1\\\\file10',\n 'root_dir\\\\dir1\\\\file11',\n 'root_dir\\\\dir1\\\\file12',\n 'root_dir\\\\dir2',\n 'root_dir\\\\dir2\\\\dir20',\n 'root_dir\\\\dir2\\\\dir20\\\\file200',\n 'root_dir\\\\dir2\\\\file20',\n 'root_dir\\\\dir3',\n 'root_dir\\\\file0',\n 'root_dir\\\\file1']\n\n11\n['dir0\\\\dir00\\\\dir000\\\\file0000',\n 'dir0\\\\dir00\\\\file000',\n 'dir0\\\\dir01\\\\file010',\n 'dir0\\\\dir01\\\\file011',\n 'dir1\\\\file10',\n 'dir1\\\\file11',\n 'dir1\\\\file12',\n 'dir2\\\\dir20\\\\file200',\n 'dir2\\\\file20',\n 'file0',\n 'file1']\n\nDone.\n[prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.05.04_test0\\Scripts\\python.exe\" \".\\code_os_listdir.py\"\nPython 3.5.4 (v3.5.4:3f56838, Aug  8 2017, 02:17:05) [MSC v.1900 64 bit (AMD64)] 064bit on win32\n\n<generator object get_dir_content at 0x000002C080418F68> 22\n['root_dir\\\\dir0',\n 'root_dir\\\\dir0\\\\dir00',\n 'root_dir\\\\dir0\\\\dir00\\\\dir000',\n 'root_dir\\\\dir0\\\\dir00\\\\dir000\\\\file0000',\n 'root_dir\\\\dir0\\\\dir00\\\\file000',\n 'root_dir\\\\dir0\\\\dir01',\n 'root_dir\\\\dir0\\\\dir01\\\\file010',\n 'root_dir\\\\dir0\\\\dir01\\\\file011',\n 'root_dir\\\\dir0\\\\dir02',\n 'root_dir\\\\dir0\\\\dir02\\\\dir020',\n 'root_dir\\\\dir0\\\\dir02\\\\dir020\\\\dir0200',\n 'root_dir\\\\dir1',\n 'root_dir\\\\dir1\\\\file10',\n 'root_dir\\\\dir1\\\\file11',\n 'root_dir\\\\dir1\\\\file12',\n 'root_dir\\\\dir2',\n 'root_dir\\\\dir2\\\\dir20',\n 'root_dir\\\\dir2\\\\dir20\\\\file200',\n 'root_dir\\\\dir2\\\\file20',\n 'root_dir\\\\dir3',\n 'root_dir\\\\file0',\n 'root_dir\\\\file1']\n\n11\n['dir0\\\\dir00\\\\dir000\\\\file0000',\n 'dir0\\\\dir00\\\\file000',\n 'dir0\\\\dir01\\\\file010',\n 'dir0\\\\dir01\\\\file011',\n 'dir1\\\\file10',\n 'dir1\\\\file11',\n 'dir1\\\\file12',\n 'dir2\\\\dir20\\\\file200',\n 'dir2\\\\file20',\n 'file0',\n 'file1']\n\nDone.\n2. [Python.Docs]: os.scandir(path='.')[Python.Docs]: os.scandir(path='.')In Python 3.5+ only, backport: [PyPI]: scandir:Python 3.5+3.5+[PyPI]: scandir\nReturn an iterator of os.DirEntry objects corresponding to the entries in the directory given by path. The entries are yielded in arbitrary order, and the special entries '.' and '..' are not included.\nUsing scandir() instead of listdir() can significantly increase the performance of code that also needs file type or file attribute information, because os.DirEntry objects expose this information if the operating system provides it when scanning a directory. All os.DirEntry methods may perform a system call, but is_dir() and is_file() usually only require a system call for symbolic links; os.DirEntry.stat() always requires a system call on Unix but only requires one for symbolic links on Windows.\nReturn an iterator of os.DirEntry objects corresponding to the entries in the directory given by path. The entries are yielded in arbitrary order, and the special entries '.' and '..' are not included.os.DirEntrypath'.''..'Using scandir() instead of listdir() can significantly increase the performance of code that also needs file type or file attribute information, because os.DirEntry objects expose this information if the operating system provides it when scanning a directory. All os.DirEntry methods may perform a system call, but is_dir() and is_file() usually only require a system call for symbolic links; os.DirEntry.stat() always requires a system call on Unix but only requires one for symbolic links on Windows.scandir()listdir()os.DirEntryos.DirEntryis_dir()is_file()os.DirEntry.stat()\n>>> import os\n>>>\n>>> root_dir = os.path.join(\".\", \"root_dir\")  # Explicitly prepending current directory\n>>> root_dir\n'.\\\\root_dir'\n>>>\n>>> scandir_iterator = os.scandir(root_dir)\n>>> scandir_iterator\n<nt.ScandirIterator object at 0x00000268CF4BC140>\n>>> [item.path for item in scandir_iterator]\n['.\\\\root_dir\\\\dir0', '.\\\\root_dir\\\\dir1', '.\\\\root_dir\\\\dir2', '.\\\\root_dir\\\\dir3', '.\\\\root_dir\\\\file0', '.\\\\root_dir\\\\file1']\n>>>\n>>> [item.path for item in scandir_iterator]  # Will yield an empty list as it was consumed by previous iteration (automatically performed by the list comprehension)\n[]\n>>>\n>>> scandir_iterator = os.scandir(root_dir)  # Reinitialize the generator\n>>> for item in scandir_iterator :\n...     if os.path.isfile(item.path):\n...             print(item.name)\n...\nfile0\nfile1\n\n>>> import os\n>>>\n>>> root_dir = os.path.join(\".\", \"root_dir\")  # Explicitly prepending current directory\n>>> root_dir\n'.\\\\root_dir'\n>>>\n>>> scandir_iterator = os.scandir(root_dir)\n>>> scandir_iterator\n<nt.ScandirIterator object at 0x00000268CF4BC140>\n>>> [item.path for item in scandir_iterator]\n['.\\\\root_dir\\\\dir0', '.\\\\root_dir\\\\dir1', '.\\\\root_dir\\\\dir2', '.\\\\root_dir\\\\dir3', '.\\\\root_dir\\\\file0', '.\\\\root_dir\\\\file1']\n>>>\n>>> [item.path for item in scandir_iterator]  # Will yield an empty list as it was consumed by previous iteration (automatically performed by the list comprehension)\n[]\n>>>\n>>> scandir_iterator = os.scandir(root_dir)  # Reinitialize the generator\n>>> for item in scandir_iterator :\n...     if os.path.isfile(item.path):\n...             print(item.name)\n...\nfile0\nfile1\n>>> import os\n>>>\n>>> root_dir = os.path.join(\".\", \"root_dir\")  # Explicitly prepending current directory\n>>> root_dir\n'.\\\\root_dir'\n>>>\n>>> scandir_iterator = os.scandir(root_dir)\n>>> scandir_iterator\n<nt.ScandirIterator object at 0x00000268CF4BC140>\n>>> [item.path for item in scandir_iterator]\n['.\\\\root_dir\\\\dir0', '.\\\\root_dir\\\\dir1', '.\\\\root_dir\\\\dir2', '.\\\\root_dir\\\\dir3', '.\\\\root_dir\\\\file0', '.\\\\root_dir\\\\file1']\n>>>\n>>> [item.path for item in scandir_iterator]  # Will yield an empty list as it was consumed by previous iteration (automatically performed by the list comprehension)\n[]\n>>>\n>>> scandir_iterator = os.scandir(root_dir)  # Reinitialize the generator\n>>> for item in scandir_iterator :\n...     if os.path.isfile(item.path):\n...             print(item.name)\n...\nfile0\nfile1\nNotes:Notes\nSimilar to os.listdir\n\nBut it's also more flexible (and offers more functionality), more Pythonic (and in some cases, faster)\n\nSimilar to os.listdir\nSimilar to os.listdiros.listdirBut it's also more flexible (and offers more functionality), more Pythonic (and in some cases, faster)\nBut it's also more flexible (and offers more functionality), more Pythonic (and in some cases, faster)Python3. [Python.Docs]: os.walk(top, topdown=True, onerror=None, followlinks=False)[Python.Docs]: os.walk(top, topdown=True, onerror=None, followlinks=False)\nGenerate the file names in a directory tree by walking the tree either top-down or bottom-up. For each directory in the tree rooted at directory top (including top itself), it yields a 3-tuple (dirpath, dirnames, filenames).\nGenerate the file names in a directory tree by walking the tree either top-down or bottom-up. For each directory in the tree rooted at directory top (including top itself), it yields a 3-tuple (dirpath, dirnames, filenames).toptopdirpathdirnamesfilenames\n>>> import os\n>>>\n>>> root_dir = os.path.join(os.getcwd(), \"root_dir\")  # Specify the full path\n>>> root_dir\n'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir'\n>>>\n>>> walk_generator = os.walk(root_dir)\n>>> root_dir_entry = next(walk_generator)  # First entry corresponds to the root dir (passed as an argument)\n>>> root_dir_entry\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir', ['dir0', 'dir1', 'dir2', 'dir3'], ['file0', 'file1'])\n>>>\n>>> root_dir_entry[1] + root_dir_entry[2]  # Display dirs and files (direct descendants) in a single list\n['dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> [os.path.join(root_dir_entry[0], item) for item in root_dir_entry[1] + root_dir_entry[2]]  # Display all the entries in the previous list by their full path\n['E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir1', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir2', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir3', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\file0', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\file1']\n>>>\n>>> for entry in walk_generator:  # Display the rest of the elements (corresponding to every subdir)\n...     print(entry)\n...\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0', ['dir00', 'dir01', 'dir02'], [])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir00', ['dir000'], ['file000'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir00\\\\dir000', [], ['file0000'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir01', [], ['file010', 'file011'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir02', ['dir020'], [])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir02\\\\dir020', ['dir0200'], [])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir02\\\\dir020\\\\dir0200', [], [])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir1', [], ['file10', 'file11', 'file12'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir2', ['dir20'], ['file20'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir2\\\\dir20', [], ['file200'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir3', [], [])\n\n>>> import os\n>>>\n>>> root_dir = os.path.join(os.getcwd(), \"root_dir\")  # Specify the full path\n>>> root_dir\n'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir'\n>>>\n>>> walk_generator = os.walk(root_dir)\n>>> root_dir_entry = next(walk_generator)  # First entry corresponds to the root dir (passed as an argument)\n>>> root_dir_entry\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir', ['dir0', 'dir1', 'dir2', 'dir3'], ['file0', 'file1'])\n>>>\n>>> root_dir_entry[1] + root_dir_entry[2]  # Display dirs and files (direct descendants) in a single list\n['dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> [os.path.join(root_dir_entry[0], item) for item in root_dir_entry[1] + root_dir_entry[2]]  # Display all the entries in the previous list by their full path\n['E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir1', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir2', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir3', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\file0', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\file1']\n>>>\n>>> for entry in walk_generator:  # Display the rest of the elements (corresponding to every subdir)\n...     print(entry)\n...\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0', ['dir00', 'dir01', 'dir02'], [])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir00', ['dir000'], ['file000'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir00\\\\dir000', [], ['file0000'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir01', [], ['file010', 'file011'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir02', ['dir020'], [])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir02\\\\dir020', ['dir0200'], [])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir02\\\\dir020\\\\dir0200', [], [])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir1', [], ['file10', 'file11', 'file12'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir2', ['dir20'], ['file20'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir2\\\\dir20', [], ['file200'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir3', [], [])\n>>> import os\n>>>\n>>> root_dir = os.path.join(os.getcwd(), \"root_dir\")  # Specify the full path\n>>> root_dir\n'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir'\n>>>\n>>> walk_generator = os.walk(root_dir)\n>>> root_dir_entry = next(walk_generator)  # First entry corresponds to the root dir (passed as an argument)\n>>> root_dir_entry\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir', ['dir0', 'dir1', 'dir2', 'dir3'], ['file0', 'file1'])\n>>>\n>>> root_dir_entry[1] + root_dir_entry[2]  # Display dirs and files (direct descendants) in a single list\n['dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> [os.path.join(root_dir_entry[0], item) for item in root_dir_entry[1] + root_dir_entry[2]]  # Display all the entries in the previous list by their full path\n['E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir1', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir2', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir3', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\file0', 'E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\file1']\n>>>\n>>> for entry in walk_generator:  # Display the rest of the elements (corresponding to every subdir)\n...     print(entry)\n...\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0', ['dir00', 'dir01', 'dir02'], [])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir00', ['dir000'], ['file000'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir00\\\\dir000', [], ['file0000'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir01', [], ['file010', 'file011'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir02', ['dir020'], [])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir02\\\\dir020', ['dir0200'], [])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir0\\\\dir02\\\\dir020\\\\dir0200', [], [])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir1', [], ['file10', 'file11', 'file12'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir2', ['dir20'], ['file20'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir2\\\\dir20', [], ['file200'])\n('E:\\\\Work\\\\Dev\\\\StackOverflow\\\\q003207219\\\\root_dir\\\\dir3', [], [])\nNotes:Notes\nUnder the scenes, it uses os.scandir (os.listdir on older (Python) versions)\n\nIt does the heavy lifting by recurring in subfolders\n\nUnder the scenes, it uses os.scandir (os.listdir on older (Python) versions)\nUnder the scenes, it uses os.scandir (os.listdir on older (Python) versions)os.scandiros.listdirPythonIt does the heavy lifting by recurring in subfolders\nIt does the heavy lifting by recurring in subfolders4. [Python.Docs]: glob.glob(pathname, *, root_dir=None, dir_fd=None, recursive=False, include_hidden=False)[Python.Docs]: glob.glob(pathname, *, root_dir=None, dir_fd=None, recursive=False, include_hidden=False)Or glob.iglob:glob.iglobi\nReturn a possibly-empty list of path names that match pathname, which must be a string containing a path specification. pathname can be either absolute (like /usr/src/Python-1.5/Makefile) or relative (like ../../Tools/*/*.gif), and can contain shell-style wildcards. Broken symlinks are included in the results (as in the shell)....Changed in version 3.5: Support for recursive globs using \u201c**\u201d.\nReturn a possibly-empty list of path names that match pathname, which must be a string containing a path specification. pathname can be either absolute (like /usr/src/Python-1.5/Makefile) or relative (like ../../Tools/*/*.gif), and can contain shell-style wildcards. Broken symlinks are included in the results (as in the shell)....Changed in version 3.5: Support for recursive globs using \u201c**\u201d.pathnamepathname/usr/src/Python-1.5/Makefile../../Tools/*/*.gifChanged in version 3.5Changed in version 3.5**\n>>> import glob, os\n>>>\n>>> wildcard_pattern = \"*\"\n>>> root_dir = os.path.join(\"root_dir\", wildcard_pattern)  # Match every file/dir name\n>>> root_dir\n'root_dir\\\\*'\n>>>\n>>> glob_list = glob.glob(root_dir)\n>>> glob_list\n['root_dir\\\\dir0', 'root_dir\\\\dir1', 'root_dir\\\\dir2', 'root_dir\\\\dir3', 'root_dir\\\\file0', 'root_dir\\\\file1']\n>>>\n>>> [item.replace(\"root_dir\" + os.path.sep, \"\") for item in glob_list]  # Strip the dir name and the path separator from begining\n['dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> for entry in glob.iglob(root_dir + \"*\", recursive=True):\n...     print(entry)\n...\nroot_dir\\\nroot_dir\\dir0\nroot_dir\\dir0\\dir00\nroot_dir\\dir0\\dir00\\dir000\nroot_dir\\dir0\\dir00\\dir000\\file0000\nroot_dir\\dir0\\dir00\\file000\nroot_dir\\dir0\\dir01\nroot_dir\\dir0\\dir01\\file010\nroot_dir\\dir0\\dir01\\file011\nroot_dir\\dir0\\dir02\nroot_dir\\dir0\\dir02\\dir020\nroot_dir\\dir0\\dir02\\dir020\\dir0200\nroot_dir\\dir1\nroot_dir\\dir1\\file10\nroot_dir\\dir1\\file11\nroot_dir\\dir1\\file12\nroot_dir\\dir2\nroot_dir\\dir2\\dir20\nroot_dir\\dir2\\dir20\\file200\nroot_dir\\dir2\\file20\nroot_dir\\dir3\nroot_dir\\file0\nroot_dir\\file1\n\n>>> import glob, os\n>>>\n>>> wildcard_pattern = \"*\"\n>>> root_dir = os.path.join(\"root_dir\", wildcard_pattern)  # Match every file/dir name\n>>> root_dir\n'root_dir\\\\*'\n>>>\n>>> glob_list = glob.glob(root_dir)\n>>> glob_list\n['root_dir\\\\dir0', 'root_dir\\\\dir1', 'root_dir\\\\dir2', 'root_dir\\\\dir3', 'root_dir\\\\file0', 'root_dir\\\\file1']\n>>>\n>>> [item.replace(\"root_dir\" + os.path.sep, \"\") for item in glob_list]  # Strip the dir name and the path separator from begining\n['dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> for entry in glob.iglob(root_dir + \"*\", recursive=True):\n...     print(entry)\n...\nroot_dir\\\nroot_dir\\dir0\nroot_dir\\dir0\\dir00\nroot_dir\\dir0\\dir00\\dir000\nroot_dir\\dir0\\dir00\\dir000\\file0000\nroot_dir\\dir0\\dir00\\file000\nroot_dir\\dir0\\dir01\nroot_dir\\dir0\\dir01\\file010\nroot_dir\\dir0\\dir01\\file011\nroot_dir\\dir0\\dir02\nroot_dir\\dir0\\dir02\\dir020\nroot_dir\\dir0\\dir02\\dir020\\dir0200\nroot_dir\\dir1\nroot_dir\\dir1\\file10\nroot_dir\\dir1\\file11\nroot_dir\\dir1\\file12\nroot_dir\\dir2\nroot_dir\\dir2\\dir20\nroot_dir\\dir2\\dir20\\file200\nroot_dir\\dir2\\file20\nroot_dir\\dir3\nroot_dir\\file0\nroot_dir\\file1\n>>> import glob, os\n>>>\n>>> wildcard_pattern = \"*\"\n>>> root_dir = os.path.join(\"root_dir\", wildcard_pattern)  # Match every file/dir name\n>>> root_dir\n'root_dir\\\\*'\n>>>\n>>> glob_list = glob.glob(root_dir)\n>>> glob_list\n['root_dir\\\\dir0', 'root_dir\\\\dir1', 'root_dir\\\\dir2', 'root_dir\\\\dir3', 'root_dir\\\\file0', 'root_dir\\\\file1']\n>>>\n>>> [item.replace(\"root_dir\" + os.path.sep, \"\") for item in glob_list]  # Strip the dir name and the path separator from begining\n['dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> for entry in glob.iglob(root_dir + \"*\", recursive=True):\n...     print(entry)\n...\nroot_dir\\\nroot_dir\\dir0\nroot_dir\\dir0\\dir00\nroot_dir\\dir0\\dir00\\dir000\nroot_dir\\dir0\\dir00\\dir000\\file0000\nroot_dir\\dir0\\dir00\\file000\nroot_dir\\dir0\\dir01\nroot_dir\\dir0\\dir01\\file010\nroot_dir\\dir0\\dir01\\file011\nroot_dir\\dir0\\dir02\nroot_dir\\dir0\\dir02\\dir020\nroot_dir\\dir0\\dir02\\dir020\\dir0200\nroot_dir\\dir1\nroot_dir\\dir1\\file10\nroot_dir\\dir1\\file11\nroot_dir\\dir1\\file12\nroot_dir\\dir2\nroot_dir\\dir2\\dir20\nroot_dir\\dir2\\dir20\\file200\nroot_dir\\dir2\\file20\nroot_dir\\dir3\nroot_dir\\file0\nroot_dir\\file1\nNotes:Notes\nUses os.listdir\n\nFor large trees (especially if recursive is on), iglob is preferred\n\nAllows advanced filtering based on name (due to the wildcard)\n\nUses os.listdir\nUses os.listdiros.listdirFor large trees (especially if recursive is on), iglob is preferred\nFor large trees (especially if recursive is on), iglob is preferredrecursiveiglobAllows advanced filtering based on name (due to the wildcard)\nAllows advanced filtering based on name (due to the wildcard)5. [Python.Docs]: class pathlib.Path(*pathsegments)[Python.Docs]: class pathlib.Path(*pathsegments)Python 3.4+, backport: [PyPI]: pathlib2.Python 3.43.4[PyPI]: pathlib2\n>>> import pathlib\n>>>\n>>> root_dir = \"root_dir\"\n>>> root_dir_instance = pathlib.Path(root_dir)\n>>> root_dir_instance\nWindowsPath('root_dir')\n>>> root_dir_instance.name\n'root_dir'\n>>> root_dir_instance.is_dir()\nTrue\n>>>\n>>> [item.name for item in root_dir_instance.glob(\"*\")]  # Wildcard searching for all direct descendants\n['dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> [os.path.join(item.parent.name, item.name) for item in root_dir_instance.glob(\"*\") if not item.is_dir()]  # Display paths (including parent) for files only\n['root_dir\\\\file0', 'root_dir\\\\file1']\n\n>>> import pathlib\n>>>\n>>> root_dir = \"root_dir\"\n>>> root_dir_instance = pathlib.Path(root_dir)\n>>> root_dir_instance\nWindowsPath('root_dir')\n>>> root_dir_instance.name\n'root_dir'\n>>> root_dir_instance.is_dir()\nTrue\n>>>\n>>> [item.name for item in root_dir_instance.glob(\"*\")]  # Wildcard searching for all direct descendants\n['dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> [os.path.join(item.parent.name, item.name) for item in root_dir_instance.glob(\"*\") if not item.is_dir()]  # Display paths (including parent) for files only\n['root_dir\\\\file0', 'root_dir\\\\file1']\n>>> import pathlib\n>>>\n>>> root_dir = \"root_dir\"\n>>> root_dir_instance = pathlib.Path(root_dir)\n>>> root_dir_instance\nWindowsPath('root_dir')\n>>> root_dir_instance.name\n'root_dir'\n>>> root_dir_instance.is_dir()\nTrue\n>>>\n>>> [item.name for item in root_dir_instance.glob(\"*\")]  # Wildcard searching for all direct descendants\n['dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> [os.path.join(item.parent.name, item.name) for item in root_dir_instance.glob(\"*\") if not item.is_dir()]  # Display paths (including parent) for files only\n['root_dir\\\\file0', 'root_dir\\\\file1']\nNotes:Notes\nThis is one way of achieving our goal\n\nIt's the OOP style of handling paths\n\nOffers lots of functionalities\n\nThis is one way of achieving our goal\nThis is one way of achieving our goaloneIt's the OOP style of handling paths\nIt's the OOP style of handling pathsOOPOffers lots of functionalities\nOffers lots of functionalities6. [Python 2.Docs]: dircache.listdir(path)[Python 2.Docs]: dircache.listdir(path)\nPython 2 only\n\nBut, according to [GitHub]: python/cpython - (2.7) cpython/Lib/dircache.py, it's just a (thin) wrapper over os.listdir with caching\n\nPython 2 only\nPython 2 onlyPython 22onlyBut, according to [GitHub]: python/cpython - (2.7) cpython/Lib/dircache.py, it's just a (thin) wrapper over os.listdir with caching\nBut, according to [GitHub]: python/cpython - (2.7) cpython/Lib/dircache.py, it's just a (thin) wrapper over os.listdir with caching[GitHub]: python/cpython - (2.7) cpython/Lib/dircache.pyos.listdir\ndef listdir(path):\n    \"\"\"List directory contents, using cache.\"\"\"\n    try:\n        cached_mtime, list = cache[path]\n        del cache[path]\n    except KeyError:\n        cached_mtime, list = -1, []\n    mtime = os.stat(path).st_mtime\n    if mtime != cached_mtime:\n        list = os.listdir(path)\n        list.sort()\n    cache[path] = mtime, list\n    return list\n\ndef listdir(path):\n    \"\"\"List directory contents, using cache.\"\"\"\n    try:\n        cached_mtime, list = cache[path]\n        del cache[path]\n    except KeyError:\n        cached_mtime, list = -1, []\n    mtime = os.stat(path).st_mtime\n    if mtime != cached_mtime:\n        list = os.listdir(path)\n        list.sort()\n    cache[path] = mtime, list\n    return list\ndef listdir(path):\n    \"\"\"List directory contents, using cache.\"\"\"\n    try:\n        cached_mtime, list = cache[path]\n        del cache[path]\n    except KeyError:\n        cached_mtime, list = -1, []\n    mtime = os.stat(path).st_mtime\n    if mtime != cached_mtime:\n        list = os.listdir(path)\n        list.sort()\n    cache[path] = mtime, list\n    return list\n7. Native OS APIsOSAPIPOSIX specific:POSIX\n[Man7]: OPENDIR(3)\n\n[Man7]: READDIR(3)\n\n[Man7]: CLOSEDIR(3)\n\n[Man7]: OPENDIR(3)\n[Man7]: OPENDIR(3)[Man7]: OPENDIR(3)[Man7]: READDIR(3)\n[Man7]: READDIR(3)[Man7]: READDIR(3)[Man7]: CLOSEDIR(3)\n[Man7]: CLOSEDIR(3)[Man7]: CLOSEDIR(3)Available via [Python.Docs]: ctypes - A foreign function library for Python:[Python.Docs]: ctypes - A foreign function library for Python\nctypes is a foreign function library for Python. It provides C compatible data types, and allows calling functions in DLLs or shared libraries. It can be used to wrap these libraries in pure Python.\nctypes is a foreign function library for Python. It provides C compatible data types, and allows calling functions in DLLs or shared libraries. It can be used to wrap these libraries in pure Python.ctypesNot directly related, but check [SO]: C function called from Python via ctypes returns incorrect value (@CristiFati's answer) out before working with CTypes.[SO]: C function called from Python via ctypes returns incorrect value (@CristiFati's answer)CTypescode_ctypes.py:code_ctypes.py#!/usr/bin/env python3\n\nimport ctypes as cts\nimport sys\n\n\nDT_DIR = 4\nDT_REG = 8\n\n\nclass NixDirent64(cts.Structure):\n    _fields_ = (\n        (\"d_ino\", cts.c_ulonglong),\n        (\"d_off\", cts.c_longlong),\n        (\"d_reclen\", cts.c_ushort),\n        (\"d_type\", cts.c_ubyte),\n        (\"d_name\", cts.c_char * 256),\n    )\n\nNixDirent64Ptr = cts.POINTER(NixDirent64)\n\n\nlibc = this_process = cts.CDLL(None, use_errno=True)\n\nopendir = libc.opendir\nopendir.argtypes = (cts.c_char_p,)\nopendir.restype = cts.c_void_p\nreaddir = libc.readdir\nreaddir.argtypes = (cts.c_void_p,)\nreaddir.restype = NixDirent64Ptr\nclosedir = libc.closedir\nclosedir.argtypes = (cts.c_void_p,)\n\n\ndef get_dir_content(path):\n    ret = [path, [], []]\n    pdir = opendir(cts.create_string_buffer(path.encode()))\n    if not pdir:\n        print(\"opendir returned NULL (errno: {:d})\".format(cts.get_errno()))\n        return ret\n    cts.set_errno(0)\n    while True:\n        pdirent = readdir(pdir)\n        if not pdirent:\n            break\n        dirent = pdirent.contents\n        name = dirent.d_name.decode()\n        if dirent.d_type & DT_DIR:\n            if name not in (\".\", \"..\"):\n                ret[1].append(name)\n        elif dirent.d_type & DT_REG:\n            ret[2].append(name)\n    if cts.get_errno():\n        print(\"readdir returned NULL (errno: {:d})\".format(cts.get_errno()))\n    closedir(pdir)\n    return ret\n\n\ndef main(*argv):\n    root_dir = \"root_dir\"\n    entries = get_dir_content(root_dir)\n    print(\"Entries:\\n{:}\".format(entries))\n\n\nif __name__ == \"__main__\":\n    print(\"Python {:s} {:03d}bit on {:s}\\n\".format(\" \".join(elem.strip() for elem in sys.version.split(\"\\n\")),\n                                                   64 if sys.maxsize > 0x100000000 else 32, sys.platform))\n    rc = main(*sys.argv[1:])\n    print(\"\\nDone.\\n\")\n    sys.exit(rc)\n#!/usr/bin/env python3\n\nimport ctypes as cts\nimport sys\n\n\nDT_DIR = 4\nDT_REG = 8\n\n\nclass NixDirent64(cts.Structure):\n    _fields_ = (\n        (\"d_ino\", cts.c_ulonglong),\n        (\"d_off\", cts.c_longlong),\n        (\"d_reclen\", cts.c_ushort),\n        (\"d_type\", cts.c_ubyte),\n        (\"d_name\", cts.c_char * 256),\n    )\n\nNixDirent64Ptr = cts.POINTER(NixDirent64)\n\n\nlibc = this_process = cts.CDLL(None, use_errno=True)\n\nopendir = libc.opendir\nopendir.argtypes = (cts.c_char_p,)\nopendir.restype = cts.c_void_p\nreaddir = libc.readdir\nreaddir.argtypes = (cts.c_void_p,)\nreaddir.restype = NixDirent64Ptr\nclosedir = libc.closedir\nclosedir.argtypes = (cts.c_void_p,)\n\n\ndef get_dir_content(path):\n    ret = [path, [], []]\n    pdir = opendir(cts.create_string_buffer(path.encode()))\n    if not pdir:\n        print(\"opendir returned NULL (errno: {:d})\".format(cts.get_errno()))\n        return ret\n    cts.set_errno(0)\n    while True:\n        pdirent = readdir(pdir)\n        if not pdirent:\n            break\n        dirent = pdirent.contents\n        name = dirent.d_name.decode()\n        if dirent.d_type & DT_DIR:\n            if name not in (\".\", \"..\"):\n                ret[1].append(name)\n        elif dirent.d_type & DT_REG:\n            ret[2].append(name)\n    if cts.get_errno():\n        print(\"readdir returned NULL (errno: {:d})\".format(cts.get_errno()))\n    closedir(pdir)\n    return ret\n\n\ndef main(*argv):\n    root_dir = \"root_dir\"\n    entries = get_dir_content(root_dir)\n    print(\"Entries:\\n{:}\".format(entries))\n\n\nif __name__ == \"__main__\":\n    print(\"Python {:s} {:03d}bit on {:s}\\n\".format(\" \".join(elem.strip() for elem in sys.version.split(\"\\n\")),\n                                                   64 if sys.maxsize > 0x100000000 else 32, sys.platform))\n    rc = main(*sys.argv[1:])\n    print(\"\\nDone.\\n\")\n    sys.exit(rc)\nNotes:Notes\nIt loads the three functions from LibC (libc.so - loaded in the current process) and calls them (for more details check [SO]: How do I check whether a file exists without exceptions? (@CristiFati's answer) - last notes from item #4.). That would place this approach very close to the Python / C edge\n\nNixDirent64 is the CTypes representation of struct dirent64 from [Man7]: dirent.h(0P) (so are the DT_ constants) from my Ubuntu OS. On other flavors / versions, the structure definition might differ, and if so, the CTypes alias should be updated, otherwise it will yield Undefined Behavior\n\nIt returns data in the os.walk's format. I didn't bother to make it recursive, but starting from the existing code, that would be a fairly trivial task\n\nEverything is doable on Win as well, the data (libraries, functions, structs, constants, ...) differ\n\nIt loads the three functions from LibC (libc.so - loaded in the current process) and calls them (for more details check [SO]: How do I check whether a file exists without exceptions? (@CristiFati's answer) - last notes from item #4.). That would place this approach very close to the Python / C edge\nIt loads the three functions from LibC (libc.so - loaded in the current process) and calls them (for more details check [SO]: How do I check whether a file exists without exceptions? (@CristiFati's answer) - last notes from item #4.). That would place this approach very close to the Python / C edgeLibClibc.so[SO]: How do I check whether a file exists without exceptions? (@CristiFati's answer)#4.#4.PythonCNixDirent64 is the CTypes representation of struct dirent64 from [Man7]: dirent.h(0P) (so are the DT_ constants) from my Ubuntu OS. On other flavors / versions, the structure definition might differ, and if so, the CTypes alias should be updated, otherwise it will yield Undefined Behavior\nNixDirent64 is the CTypes representation of struct dirent64 from [Man7]: dirent.h(0P) (so are the DT_ constants) from my Ubuntu OS. On other flavors / versions, the structure definition might differ, and if so, the CTypes alias should be updated, otherwise it will yield Undefined BehaviorNixDirent64CTypesstruct dirent64[Man7]: dirent.h(0P)DT_UbuntuOSCTypesUndefined BehaviorUBIt returns data in the os.walk's format. I didn't bother to make it recursive, but starting from the existing code, that would be a fairly trivial task\nIt returns data in the os.walk's format. I didn't bother to make it recursive, but starting from the existing code, that would be a fairly trivial taskos.walkEverything is doable on Win as well, the data (libraries, functions, structs, constants, ...) differ\nEverything is doable on Win as well, the data (libraries, functions, structs, constants, ...) differWinOutput:Output\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q003207219]> python3.5 ./code_ctypes.py\nPython 3.5.10 (default, Jan 15 2022, 19:53:00) [GCC 9.3.0] 064bit on linux\n\nEntries:\n['root_dir', ['dir0', 'dir1', 'dir2', 'dir3'], ['file0', 'file1']]\n\nDone.\n\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q003207219]> python3.5 ./code_ctypes.py\nPython 3.5.10 (default, Jan 15 2022, 19:53:00) [GCC 9.3.0] 064bit on linux\n\nEntries:\n['root_dir', ['dir0', 'dir1', 'dir2', 'dir3'], ['file0', 'file1']]\n\nDone.\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q003207219]> python3.5 ./code_ctypes.py\nPython 3.5.10 (default, Jan 15 2022, 19:53:00) [GCC 9.3.0] 064bit on linux\n\nEntries:\n['root_dir', ['dir0', 'dir1', 'dir2', 'dir3'], ['file0', 'file1']]\n\nDone.\n8. [TimGolden]: win32file.FindFilesW[TimGolden]: win32file.FindFilesWWin specific:Win\nRetrieves a list of matching filenames, using the Windows Unicode API. An interface to the API FindFirstFileW/FindNextFileW/Find close functions.\nRetrieves a list of matching filenames, using the Windows Unicode API. An interface to the API FindFirstFileW/FindNextFileW/Find close functions.\n>>> import os, win32file as wfile, win32con as wcon\n>>>\n>>> root_dir = \"root_dir\"\n>>> root_dir_wildcard = os.path.join(root_dir, \"*\")\n>>> entry_list = wfile.FindFilesW(root_dir_wildcard)\n>>> len(entry_list)  # Don't display the whole content as it's too long\n8\n>>> [entry[-2] for entry in entry_list]  # Only display the entry names\n['.', '..', 'dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> [entry[-2] for entry in entry_list if entry[0] & wcon.FILE_ATTRIBUTE_DIRECTORY and entry[-2] not in (\".\", \"..\")]  # Filter entries and only display dir names (except self and parent)\n['dir0', 'dir1', 'dir2', 'dir3']\n>>>\n>>> [os.path.join(root_dir, entry[-2]) for entry in entry_list if entry[0] & (wcon.FILE_ATTRIBUTE_NORMAL | wcon.FILE_ATTRIBUTE_ARCHIVE)]  # Only display file \"full\" names\n['root_dir\\\\file0', 'root_dir\\\\file1']\n\n>>> import os, win32file as wfile, win32con as wcon\n>>>\n>>> root_dir = \"root_dir\"\n>>> root_dir_wildcard = os.path.join(root_dir, \"*\")\n>>> entry_list = wfile.FindFilesW(root_dir_wildcard)\n>>> len(entry_list)  # Don't display the whole content as it's too long\n8\n>>> [entry[-2] for entry in entry_list]  # Only display the entry names\n['.', '..', 'dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> [entry[-2] for entry in entry_list if entry[0] & wcon.FILE_ATTRIBUTE_DIRECTORY and entry[-2] not in (\".\", \"..\")]  # Filter entries and only display dir names (except self and parent)\n['dir0', 'dir1', 'dir2', 'dir3']\n>>>\n>>> [os.path.join(root_dir, entry[-2]) for entry in entry_list if entry[0] & (wcon.FILE_ATTRIBUTE_NORMAL | wcon.FILE_ATTRIBUTE_ARCHIVE)]  # Only display file \"full\" names\n['root_dir\\\\file0', 'root_dir\\\\file1']\n>>> import os, win32file as wfile, win32con as wcon\n>>>\n>>> root_dir = \"root_dir\"\n>>> root_dir_wildcard = os.path.join(root_dir, \"*\")\n>>> entry_list = wfile.FindFilesW(root_dir_wildcard)\n>>> len(entry_list)  # Don't display the whole content as it's too long\n8\n>>> [entry[-2] for entry in entry_list]  # Only display the entry names\n['.', '..', 'dir0', 'dir1', 'dir2', 'dir3', 'file0', 'file1']\n>>>\n>>> [entry[-2] for entry in entry_list if entry[0] & wcon.FILE_ATTRIBUTE_DIRECTORY and entry[-2] not in (\".\", \"..\")]  # Filter entries and only display dir names (except self and parent)\n['dir0', 'dir1', 'dir2', 'dir3']\n>>>\n>>> [os.path.join(root_dir, entry[-2]) for entry in entry_list if entry[0] & (wcon.FILE_ATTRIBUTE_NORMAL | wcon.FILE_ATTRIBUTE_ARCHIVE)]  # Only display file \"full\" names\n['root_dir\\\\file0', 'root_dir\\\\file1']\nNotes:Notes\nwin32file.FindFilesW is part of [GitHub]: mhammond/pywin32 - Python for Windows (pywin32) Extensions, which is a Python wrapper over WinAPIs\nwin32file.FindFilesW is part of [GitHub]: mhammond/pywin32 - Python for Windows (pywin32) Extensions, which is a Python wrapper over WinAPIswin32file.FindFilesW[GitHub]: mhammond/pywin32 - Python for Windows (pywin32) ExtensionsPythonWinAPI9. Use some (other) 3rd-party package that does the trickrdMost likely, will rely on one (or more) of the above (maybe with slight customizations).Notes:\nCode is meant to be portable (except places that target a specific area - which are marked) or cross:\n\nOS (Nix, Win, )\n\nPython version (2, 3, )\n\n\n\nMultiple path styles (absolute, relatives) were used across the above variants, to illustrate the fact that the \"tools\" used are flexible in this direction\n\nos.listdir and os.scandir use opendir / readdir / closedir ([MS.Learn]: FindFirstFileW function (fileapi.h) / [MS.Learn]: FindNextFileW function (fileapi.h) / [MS.Learn]: FindClose function (fileapi.h)) (via [GitHub]: python/cpython - (main) cpython/Modules/posixmodule.c)\n\nwin32file.FindFilesW uses those (Win specific) functions as well (via [GitHub]: mhammond/pywin32 - (main) pywin32/win32/src/win32file.i)\n\n_get_dir_content (from point #1.) can be implemented using any of these approaches (some will require more work and some less)\n\nSome advanced filtering (instead of just file vs. dir) could be done: e.g. the include_folders argument could be replaced by another one (e.g. filter_func) which would be a function that takes a path as an argument: filter_func=lambda x: True (this doesn't strip out anything) and inside _get_dir_content something like: if not filter_func(entry_with_path): continue (if the function fails for one entry, it will be skipped), but the more complex the code becomes, the longer it will take to execute\n\n\nNota Bene! Since recursion is used, I must mention that I did some tests on my laptop (Win 10 pc064), totally unrelated to this problem, and when the recursion level was reaching values somewhere in the (990 .. 1000) range (recursionlimit - 1000 (default)), I got StackOverflow :). If the directory tree exceeds that limit (I am not an FS expert, so I don't know if that is even possible), that could be a problem.\nI must also mention that I didn't try to increase recursionlimit, but in theory there will always be the possibility for failure, if the dir depth is larger than the highest possible recursionlimit (on that machine).\nCheck [SO]: _csv.Error: field larger than field limit (131072) (@CristiFati's answer) for more details on the topic\n\nCode samples are for demonstrative purposes only. That means that I didn't take into account error handling (I don't think there's any try / except / else / finally block), so the code is not robust (the reason is: to keep it as simple and short as possible). For production, error handling should be added as well\n\nCode is meant to be portable (except places that target a specific area - which are marked) or cross:\n\nOS (Nix, Win, )\n\nPython version (2, 3, )\n\n\nCode is meant to be portable (except places that target a specific area - which are marked) or cross:\nOS (Nix, Win, )\n\nPython version (2, 3, )\n\nOS (Nix, Win, )\nOS (Nix, Win, )OSNixWinPython version (2, 3, )\nPython version (2, 3, )PythonMultiple path styles (absolute, relatives) were used across the above variants, to illustrate the fact that the \"tools\" used are flexible in this direction\nMultiple path styles (absolute, relatives) were used across the above variants, to illustrate the fact that the \"tools\" used are flexible in this directionos.listdir and os.scandir use opendir / readdir / closedir ([MS.Learn]: FindFirstFileW function (fileapi.h) / [MS.Learn]: FindNextFileW function (fileapi.h) / [MS.Learn]: FindClose function (fileapi.h)) (via [GitHub]: python/cpython - (main) cpython/Modules/posixmodule.c)\nos.listdir and os.scandir use opendir / readdir / closedir ([MS.Learn]: FindFirstFileW function (fileapi.h) / [MS.Learn]: FindNextFileW function (fileapi.h) / [MS.Learn]: FindClose function (fileapi.h)) (via [GitHub]: python/cpython - (main) cpython/Modules/posixmodule.c)os.listdiros.scandiropendirreaddirclosedir[MS.Learn]: FindFirstFileW function (fileapi.h)[MS.Learn]: FindNextFileW function (fileapi.h)[MS.Learn]: FindClose function (fileapi.h)[GitHub]: python/cpython - (main) cpython/Modules/posixmodule.cwin32file.FindFilesW uses those (Win specific) functions as well (via [GitHub]: mhammond/pywin32 - (main) pywin32/win32/src/win32file.i)\nwin32file.FindFilesW uses those (Win specific) functions as well (via [GitHub]: mhammond/pywin32 - (main) pywin32/win32/src/win32file.i)win32file.FindFilesWWin[GitHub]: mhammond/pywin32 - (main) pywin32/win32/src/win32file.i_get_dir_content (from point #1.) can be implemented using any of these approaches (some will require more work and some less)\n\nSome advanced filtering (instead of just file vs. dir) could be done: e.g. the include_folders argument could be replaced by another one (e.g. filter_func) which would be a function that takes a path as an argument: filter_func=lambda x: True (this doesn't strip out anything) and inside _get_dir_content something like: if not filter_func(entry_with_path): continue (if the function fails for one entry, it will be skipped), but the more complex the code becomes, the longer it will take to execute\n\n_get_dir_content (from point #1.) can be implemented using any of these approaches (some will require more work and some less)_get_dir_content#1.#1.\nSome advanced filtering (instead of just file vs. dir) could be done: e.g. the include_folders argument could be replaced by another one (e.g. filter_func) which would be a function that takes a path as an argument: filter_func=lambda x: True (this doesn't strip out anything) and inside _get_dir_content something like: if not filter_func(entry_with_path): continue (if the function fails for one entry, it will be skipped), but the more complex the code becomes, the longer it will take to execute\nSome advanced filtering (instead of just file vs. dir) could be done: e.g. the include_folders argument could be replaced by another one (e.g. filter_func) which would be a function that takes a path as an argument: filter_func=lambda x: True (this doesn't strip out anything) and inside _get_dir_content something like: if not filter_func(entry_with_path): continue (if the function fails for one entry, it will be skipped), but the more complex the code becomes, the longer it will take to executevs.dire.g.include_foldersfilter_funcfilter_func=lambda x: True_get_dir_contentif not filter_func(entry_with_path): continueNota Bene! Since recursion is used, I must mention that I did some tests on my laptop (Win 10 pc064), totally unrelated to this problem, and when the recursion level was reaching values somewhere in the (990 .. 1000) range (recursionlimit - 1000 (default)), I got StackOverflow :). If the directory tree exceeds that limit (I am not an FS expert, so I don't know if that is even possible), that could be a problem.\nI must also mention that I didn't try to increase recursionlimit, but in theory there will always be the possibility for failure, if the dir depth is larger than the highest possible recursionlimit (on that machine).\nCheck [SO]: _csv.Error: field larger than field limit (131072) (@CristiFati's answer) for more details on the topic\nNota Bene! Since recursion is used, I must mention that I did some tests on my laptop (Win 10 pc064), totally unrelated to this problem, and when the recursion level was reaching values somewhere in the (990 .. 1000) range (recursionlimit - 1000 (default)), I got StackOverflow :). If the directory tree exceeds that limit (I am not an FS expert, so I don't know if that is even possible), that could be a problem.\nI must also mention that I didn't try to increase recursionlimit, but in theory there will always be the possibility for failure, if the dir depth is larger than the highest possible recursionlimit (on that machine).\nCheck [SO]: _csv.Error: field larger than field limit (131072) (@CristiFati's answer) for more details on the topicNota Bene!Win 10 pc064(990 .. 1000)recursionlimitStackOverflowFSrecursionlimitdirrecursionlimit[SO]: _csv.Error: field larger than field limit (131072) (@CristiFati's answer)Code samples are for demonstrative purposes only. That means that I didn't take into account error handling (I don't think there's any try / except / else / finally block), so the code is not robust (the reason is: to keep it as simple and short as possible). For production, error handling should be added as well\nCode samples are for demonstrative purposes only. That means that I didn't take into account error handling (I don't think there's any try / except / else / finally block), so the code is not robust (the reason is: to keep it as simple and short as possible). For production, error handling should be added as welltrytryexceptexceptelseelsefinallyfinallyOther approaches:1. Use Python only as a wrapperPython\nEverything is done using another technology\n\nThat technology is invoked from Python\n\nThe most famous flavor that I know is what I call the SysAdmin approach:\n\nUse Python (or any programming language for that matter) in order to execute Shell commands (and parse their outputs)\n\nSome consider this a neat hack\n\nI consider it more like a lame workaround (gainarie), as the action per se is performed from Shell (Cmd in this case), and thus doesn't have anything to do with Python\n\nFiltering (grep / findstr) or output formatting could be done on both sides, but I'm not going to insist on it. Also, I deliberately used os.system instead of [Python.Docs]: subprocess - Subprocess management routines (run, check_output, ...)\n\n\n\nEverything is done using another technology\nEverything is done using another technologyThat technology is invoked from Python\nThat technology is invoked from PythonPythonThe most famous flavor that I know is what I call the SysAdmin approach:\n\nUse Python (or any programming language for that matter) in order to execute Shell commands (and parse their outputs)\n\nSome consider this a neat hack\n\nI consider it more like a lame workaround (gainarie), as the action per se is performed from Shell (Cmd in this case), and thus doesn't have anything to do with Python\n\nFiltering (grep / findstr) or output formatting could be done on both sides, but I'm not going to insist on it. Also, I deliberately used os.system instead of [Python.Docs]: subprocess - Subprocess management routines (run, check_output, ...)\n\n\nThe most famous flavor that I know is what I call the SysAdmin approach:SysAdmin\nUse Python (or any programming language for that matter) in order to execute Shell commands (and parse their outputs)\n\nSome consider this a neat hack\n\nI consider it more like a lame workaround (gainarie), as the action per se is performed from Shell (Cmd in this case), and thus doesn't have anything to do with Python\n\nFiltering (grep / findstr) or output formatting could be done on both sides, but I'm not going to insist on it. Also, I deliberately used os.system instead of [Python.Docs]: subprocess - Subprocess management routines (run, check_output, ...)\n\nUse Python (or any programming language for that matter) in order to execute Shell commands (and parse their outputs)\nUse Python (or any programming language for that matter) in order to execute Shell commands (and parse their outputs)PythonShellSome consider this a neat hack\nSome consider this a neat hackI consider it more like a lame workaround (gainarie), as the action per se is performed from Shell (Cmd in this case), and thus doesn't have anything to do with Python\nI consider it more like a lame workaround (gainarie), as the action per se is performed from Shell (Cmd in this case), and thus doesn't have anything to do with Pythongainarieper seShellCmdPythonFiltering (grep / findstr) or output formatting could be done on both sides, but I'm not going to insist on it. Also, I deliberately used os.system instead of [Python.Docs]: subprocess - Subprocess management routines (run, check_output, ...)\nFiltering (grep / findstr) or output formatting could be done on both sides, but I'm not going to insist on it. Also, I deliberately used os.system instead of [Python.Docs]: subprocess - Subprocess management routines (run, check_output, ...)grepfindstros.system[Python.Docs]: subprocess - Subprocess managementruncheck_output\n[prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.05.04_test0\\Scripts\\python.exe\" -c \"import os;os.system(\\\"dir /b root_dir\\\")\"\ndir0\ndir1\ndir2\ndir3\nfile0\nfile1\n\n[prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.05.04_test0\\Scripts\\python.exe\" -c \"import os;os.system(\\\"dir /b root_dir\\\")\"\ndir0\ndir1\ndir2\ndir3\nfile0\nfile1\n[prompt]> \"e:\\Work\\Dev\\VEnvs\\py_pc064_03.05.04_test0\\Scripts\\python.exe\" -c \"import os;os.system(\\\"dir /b root_dir\\\")\"\ndir0\ndir1\ndir2\ndir3\nfile0\nfile1\n\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q003207219]> python3.5 -c \"import os;os.system(\\\"ls root_dir\\\")\"\ndir0  dir1  dir2  dir3  file0  file1\n\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q003207219]> python3.5 -c \"import os;os.system(\\\"ls root_dir\\\")\"\ndir0  dir1  dir2  dir3  file0  file1\n[cfati@cfati-5510-0:/mnt/e/Work/Dev/StackOverflow/q003207219]> python3.5 -c \"import os;os.system(\\\"ls root_dir\\\")\"\ndir0  dir1  dir2  dir3  file0  file1\nIn general, this approach is to be avoided, since if some command output format slightly differs between OS versions / flavors, the parsing code should be adapted as well - not to mention differences between locales.OS",
                "I really liked adamk's answer, suggesting that you use glob(), from the module of the same name. This allows you to have pattern matching with *s.adamk's answerglob()*But as other people pointed out in the comments, glob() can get tripped up over inconsistent slash directions. To help with that, I suggest you use the join() and expanduser() functions in the os.path module, and perhaps the getcwd() function in the os module, as well.glob()join()expanduser()os.pathgetcwd()osAs examples:from glob import glob\n\n# Return everything under C:\\Users\\admin that contains a folder called wlp.\nglob('C:\\Users\\admin\\*\\wlp')\nfrom glob import glob\n\n# Return everything under C:\\Users\\admin that contains a folder called wlp.\nglob('C:\\Users\\admin\\*\\wlp')\nThe above is terrible - the path has been hardcoded and will only ever work on Windows between the drive name and the \\s being hardcoded into the path.\\from glob    import glob\nfrom os.path import join\n\n# Return everything under Users, admin, that contains a folder called wlp.\nglob(join('Users', 'admin', '*', 'wlp'))\nfrom glob    import glob\nfrom os.path import join\n\n# Return everything under Users, admin, that contains a folder called wlp.\nglob(join('Users', 'admin', '*', 'wlp'))\nThe above works better, but it relies on the folder name Users which is often found on Windows and not so often found on other OSs. It also relies on the user having a specific name, admin.Usersadminfrom glob    import glob\nfrom os.path import expanduser, join\n\n# Return everything under the user directory that contains a folder called wlp.\nglob(join(expanduser('~'), '*', 'wlp'))\nfrom glob    import glob\nfrom os.path import expanduser, join\n\n# Return everything under the user directory that contains a folder called wlp.\nglob(join(expanduser('~'), '*', 'wlp'))\nThis works perfectly across all platforms.Another great example that works perfectly across platforms and does something a bit different:from glob    import glob\nfrom os      import getcwd\nfrom os.path import join\n\n# Return everything under the current directory that contains a folder called wlp.\nglob(join(getcwd(), '*', 'wlp'))\nfrom glob    import glob\nfrom os      import getcwd\nfrom os.path import join\n\n# Return everything under the current directory that contains a folder called wlp.\nglob(join(getcwd(), '*', 'wlp'))\nHope these examples help you see the power of a few of the functions you can find in the standard Python library modules.",
                "def list_files(path):\n    # returns a list of names (with extension, without full path) of all files \n    # in folder path\n    files = []\n    for name in os.listdir(path):\n        if os.path.isfile(os.path.join(path, name)):\n            files.append(name)\n    return files \ndef list_files(path):\n    # returns a list of names (with extension, without full path) of all files \n    # in folder path\n    files = []\n    for name in os.listdir(path):\n        if os.path.isfile(os.path.join(path, name)):\n            files.append(name)\n    return files \n",
                "If you are looking for a Python implementation of find, this is a recipe I use rather frequently:findfrom findtools.find_files import (find_files, Match)\n\n# Recursively find all *.sh files in **/usr/bin**\nsh_files_pattern = Match(filetype='f', name='*.sh')\nfound_files = find_files(path='/usr/bin', match=sh_files_pattern)\n\nfor found_file in found_files:\n    print found_file\nfrom findtools.find_files import (find_files, Match)\n\n# Recursively find all *.sh files in **/usr/bin**\nsh_files_pattern = Match(filetype='f', name='*.sh')\nfound_files = find_files(path='/usr/bin', match=sh_files_pattern)\n\nfor found_file in found_files:\n    print found_file\nSo I made a PyPI package out of it and there is also a GitHub repository. I hope that someone finds it potentially useful for this code.packageGitHub repository",
                "For greater results, you can use listdir() method of the os module along with a generator (a generator is a powerful iterator that keeps its state, remember?). The following code works fine with both versions: Python 2 and Python 3.listdir()osHere's a code:import os\n\ndef files(path):  \n    for file in os.listdir(path):\n        if os.path.isfile(os.path.join(path, file)):\n            yield file\n\nfor file in files(\".\"):  \n    print (file)\nimport os\n\ndef files(path):  \n    for file in os.listdir(path):\n        if os.path.isfile(os.path.join(path, file)):\n            yield file\n\nfor file in files(\".\"):  \n    print (file)\nThe listdir() method returns the list of entries for the given directory. The method os.path.isfile() returns True if the given entry is a file. And the yield operator quits the func but keeps its current state, and it returns only the name of the entry detected as a file. All the above allows us to loop over the generator function.listdir()os.path.isfile()Trueyield",
                "Returning a list of absolute filepaths, does not recurse into subdirectoriesReturning a list of absolute filepaths, does not recurse into subdirectoriesL = [os.path.join(os.getcwd(),f) for f in os.listdir('.') if os.path.isfile(os.path.join(os.getcwd(),f))]\nL = [os.path.join(os.getcwd(),f) for f in os.listdir('.') if os.path.isfile(os.path.join(os.getcwd(),f))]\n",
                "A wise teacher told me once that:\nWhen there are several established ways to do something, none of them is good for all cases.\nWhen there are several established ways to do something, none of them is good for all cases.I will thus add a solution for a subset of the problem: quite often, we only want to check whether a file matches a start string and an end string, without going into subdirectories. We would thus like a function that returns a list of filenames, like:subsetfilenames = dir_filter('foo/baz', radical='radical', extension='.txt')\nfilenames = dir_filter('foo/baz', radical='radical', extension='.txt')\nIf you care to first declare two functions, this can be done:def file_filter(filename, radical='', extension=''):\n    \"Check if a filename matches a radical and extension\"\n    if not filename:\n        return False\n    filename = filename.strip()\n    return(filename.startswith(radical) and filename.endswith(extension))\n\ndef dir_filter(dirname='', radical='', extension=''):\n    \"Filter filenames in directory according to radical and extension\"\n    if not dirname:\n        dirname = '.'\n    return [filename for filename in os.listdir(dirname)\n                if file_filter(filename, radical, extension)]\ndef file_filter(filename, radical='', extension=''):\n    \"Check if a filename matches a radical and extension\"\n    if not filename:\n        return False\n    filename = filename.strip()\n    return(filename.startswith(radical) and filename.endswith(extension))\n\ndef dir_filter(dirname='', radical='', extension=''):\n    \"Filter filenames in directory according to radical and extension\"\n    if not dirname:\n        dirname = '.'\n    return [filename for filename in os.listdir(dirname)\n                if file_filter(filename, radical, extension)]\nThis solution could be easily generalized with regular expressions (and you might want to add a pattern argument, if you do not want your patterns to always stick to the start or end of the filename).pattern",
                "import os\nimport os.path\n\n\ndef get_files(target_dir):\n    item_list = os.listdir(target_dir)\n\n    file_list = list()\n    for item in item_list:\n        item_dir = os.path.join(target_dir,item)\n        if os.path.isdir(item_dir):\n            file_list += get_files(item_dir)\n        else:\n            file_list.append(item_dir)\n    return file_list\nimport os\nimport os.path\n\n\ndef get_files(target_dir):\n    item_list = os.listdir(target_dir)\n\n    file_list = list()\n    for item in item_list:\n        item_dir = os.path.join(target_dir,item)\n        if os.path.isdir(item_dir):\n            file_list += get_files(item_dir)\n        else:\n            file_list.append(item_dir)\n    return file_list\nHere I use a recursive structure.",
                "Using generatorsimport os\ndef get_files(search_path):\n     for (dirpath, _, filenames) in os.walk(search_path):\n         for filename in filenames:\n             yield os.path.join(dirpath, filename)\nlist_files = get_files('.')\nfor filename in list_files:\n    print(filename)\nimport os\ndef get_files(search_path):\n     for (dirpath, _, filenames) in os.walk(search_path):\n         for filename in filenames:\n             yield os.path.join(dirpath, filename)\nlist_files = get_files('.')\nfor filename in list_files:\n    print(filename)\n",
                "Another very readable variant for Python 3.4+ is using pathlib.Path.glob:from pathlib import Path\nfolder = '/foo'\n[f for f in Path(folder).glob('*') if f.is_file()]\nfrom pathlib import Path\nfolder = '/foo'\n[f for f in Path(folder).glob('*') if f.is_file()]\nIt is simple to make more specific, e.g. only look for Python source files which are not symbolic links, also in all subdirectories:[f for f in Path(folder).glob('**/*.py') if not f.is_symlink()]\n[f for f in Path(folder).glob('**/*.py') if not f.is_symlink()]\n",
                "For Python 2:pip install rglob\npip install rglob\nThen doimport rglob\nfile_list = rglob.rglob(\"/home/base/dir/\", \"*\")\nprint file_list\nimport rglob\nfile_list = rglob.rglob(\"/home/base/dir/\", \"*\")\nprint file_list\n",
                "Here's my general-purpose function for this.  It returns a list of file paths rather than filenames since I found that to be more useful.  It has a few optional arguments that make it versatile.  For instance, I often use it with arguments like pattern='*.txt' or subfolders=True.pattern='*.txt'subfolders=Trueimport os\nimport fnmatch\n\ndef list_paths(folder='.', pattern='*', case_sensitive=False, subfolders=False):\n    \"\"\"Return a list of the file paths matching the pattern in the specified \n    folder, optionally including files inside subfolders.\n    \"\"\"\n    match = fnmatch.fnmatchcase if case_sensitive else fnmatch.fnmatch\n    walked = os.walk(folder) if subfolders else [next(os.walk(folder))]\n    return [os.path.join(root, f)\n            for root, dirnames, filenames in walked\n            for f in filenames if match(f, pattern)]\nimport os\nimport fnmatch\n\ndef list_paths(folder='.', pattern='*', case_sensitive=False, subfolders=False):\n    \"\"\"Return a list of the file paths matching the pattern in the specified \n    folder, optionally including files inside subfolders.\n    \"\"\"\n    match = fnmatch.fnmatchcase if case_sensitive else fnmatch.fnmatch\n    walked = os.walk(folder) if subfolders else [next(os.walk(folder))]\n    return [os.path.join(root, f)\n            for root, dirnames, filenames in walked\n            for f in filenames if match(f, pattern)]\n",
                "I will provide a sample one liner where sourcepath and file type can be provided as input. The code returns a list of filenames with csv extension. Use . in case all files needs to be returned. This will also recursively scans the subdirectories. .[y for x in os.walk(sourcePath) for y in glob(os.path.join(x[0], '*.csv'))][y for x in os.walk(sourcePath) for y in glob(os.path.join(x[0], '*.csv'))]Modify file extensions and source path as needed. ",
                "dircache is  \"Deprecated since version 2.6: The dircache module has been removed in Python 3.0.\"dircache is  \"Deprecated since version 2.6: The dircache module has been removed in Python 3.0.\"dircacheimport dircache\nlist = dircache.listdir(pathname)\ni = 0\ncheck = len(list[0])\ntemp = []\ncount = len(list)\nwhile count != 0:\n  if len(list[i]) != check:\n     temp.append(list[i-1])\n     check = len(list[i])\n  else:\n    i = i + 1\n    count = count - 1\n\nprint temp\nimport dircache\nlist = dircache.listdir(pathname)\ni = 0\ncheck = len(list[0])\ntemp = []\ncount = len(list)\nwhile count != 0:\n  if len(list[i]) != check:\n     temp.append(list[i-1])\n     check = len(list[i])\n  else:\n    i = i + 1\n    count = count - 1\n\nprint temp\n"
            ]
        },
        {
            "tag": "__init__",
            "patterns": [
                "What is __init__.py for?",
                "what does initpy does?",
                "what does initpy do?"
            ],
            "responses": [
                "It used to be a required part of a package (old, pre-3.3 \"regular package\", not newer 3.3+ \"namespace package\").old, pre-3.3 \"regular package\"newer 3.3+ \"namespace package\"Here's the documentation.Here's the documentation.\nPython defines two types of packages, regular packages and namespace packages. Regular packages are traditional packages as they existed in Python 3.2 and earlier. A regular package is typically implemented as a directory containing an __init__.py file. When a regular package is imported, this __init__.py file is implicitly executed, and the objects it defines are bound to names in the package\u2019s namespace. The __init__.py file can contain the same Python code that any other module can contain, and Python will add some additional attributes to the module when it is imported.\nPython defines two types of packages, regular packages and namespace packages. Regular packages are traditional packages as they existed in Python 3.2 and earlier. A regular package is typically implemented as a directory containing an __init__.py file. When a regular package is imported, this __init__.py file is implicitly executed, and the objects it defines are bound to names in the package\u2019s namespace. The __init__.py file can contain the same Python code that any other module can contain, and Python will add some additional attributes to the module when it is imported.__init__.py__init__.py__init__.pyBut just click the link, it contains an example, more information, and an explanation of namespace packages, the kind of packages without __init__.py.__init__.py",
                "Files named __init__.py are used to mark directories on disk as Python package directories.\nIf you have the files__init__.pymydir/spam/__init__.py\nmydir/spam/module.py\nmydir/spam/__init__.py\nmydir/spam/module.py\nand mydir is on your path, you can import the code in module.py asmydirmodule.pyimport spam.module\nimport spam.module\norfrom spam import module\nfrom spam import module\nIf you remove the __init__.py file, Python will no longer look for submodules inside that directory, so attempts to import the module will fail.__init__.pyThe __init__.py file is usually empty, but can be used to export selected portions of the package under more convenient name, hold convenience functions, etc.\nGiven the example above, the contents of the init module can be accessed as__init__.pyimport spam\nimport spam\nbased on thisthis",
                "In addition to labeling a directory as a Python package and defining __all__, __init__.py allows you to define any variable at the package level. Doing so is often convenient if a package defines something that will be imported frequently, in an API-like fashion. This pattern promotes adherence to the Pythonic \"flat is better than nested\" philosophy.__all____init__.py allows you to define any variable at the package level.__init__.pyAn exampleHere is an example from one of my projects, in which I frequently import a sessionmaker called Session to interact with my database. I wrote a \"database\" package with a few modules:sessionmakerSessiondatabase/\n    __init__.py\n    schema.py\n    insertions.py\n    queries.py\ndatabase/\n    __init__.py\n    schema.py\n    insertions.py\n    queries.py\nMy __init__.py contains the following code:__init__.pyimport os\n\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy import create_engine\n\nengine = create_engine(os.environ['DATABASE_URL'])\nSession = sessionmaker(bind=engine)\nimport os\n\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlalchemy import create_engine\n\nengine = create_engine(os.environ['DATABASE_URL'])\nSession = sessionmaker(bind=engine)\nSince I define Session here, I can start a new session using the syntax below. This code would be the same executed from inside or outside of the \"database\" package directory.Sessionfrom database import Session\nsession = Session()\nfrom database import Session\nsession = Session()\nOf course, this is a small convenience -- the alternative would be to define Session in a new file like \"create_session.py\" in my database package, and start new sessions using:Sessionfrom database.create_session import Session\nsession = Session()\nfrom database.create_session import Session\nsession = Session()\nFurther readingThere is a pretty interesting reddit thread covering appropriate uses of __init__.py here:__init__.pyhttp://www.reddit.com/r/Python/comments/1bbbwk/whats_your_opinion_on_what_to_include_in_init_py/http://www.reddit.com/r/Python/comments/1bbbwk/whats_your_opinion_on_what_to_include_in_init_py/The majority opinion seems to be that __init__.py files should be very thin to avoid violating the \"explicit is better than implicit\" philosophy.__init__.py",
                "There are 2 main reasons for __init__.py__init__.py\nFor convenience: the other users will not need to know your functions' exact location in your package hierarchy (documentation).\nyour_package/\n  __init__.py\n  file1.py\n  file2.py\n    ...\n  fileN.py\n\n# in __init__.py\nfrom .file1 import *\nfrom .file2 import *\n...\nfrom .fileN import *\n\n# in file1.py\ndef add():\n    pass\n\nthen others can call add() by\n from your_package import add\n\nwithout knowing file1's inside functions, like\n from your_package.file1 import add\n\n\nIf you want something to be initialized; for example, logging (which should be put in the top level):\n import logging.config\n logging.config.dictConfig(Your_logging_config)\n\n\nFor convenience: the other users will not need to know your functions' exact location in your package hierarchy (documentation).\nyour_package/\n  __init__.py\n  file1.py\n  file2.py\n    ...\n  fileN.py\n\n# in __init__.py\nfrom .file1 import *\nfrom .file2 import *\n...\nfrom .fileN import *\n\n# in file1.py\ndef add():\n    pass\n\nthen others can call add() by\n from your_package import add\n\nwithout knowing file1's inside functions, like\n from your_package.file1 import add\n\nFor convenience: the other users will not need to know your functions' exact location in your package hierarchy (documentation).documentationyour_package/\n  __init__.py\n  file1.py\n  file2.py\n    ...\n  fileN.py\nyour_package/\n  __init__.py\n  file1.py\n  file2.py\n    ...\n  fileN.py\n# in __init__.py\nfrom .file1 import *\nfrom .file2 import *\n...\nfrom .fileN import *\n# in __init__.py\nfrom .file1 import *\nfrom .file2 import *\n...\nfrom .fileN import *\n# in file1.py\ndef add():\n    pass\n# in file1.py\ndef add():\n    pass\nthen others can call add() by from your_package import add\n from your_package import add\nwithout knowing file1's inside functions, like from your_package.file1 import add\n from your_package.file1 import add\nIf you want something to be initialized; for example, logging (which should be put in the top level):\n import logging.config\n logging.config.dictConfig(Your_logging_config)\n\nIf you want something to be initialized; for example, logging (which should be put in the top level): import logging.config\n logging.config.dictConfig(Your_logging_config)\n import logging.config\n logging.config.dictConfig(Your_logging_config)\n",
                "The __init__.py file makes Python treat directories containing it as modules. __init__.pyFurthermore, this is the first file to be loaded in a module, so you can use it to execute code that you want to run each time a module is loaded, or specify the submodules to be exported.",
                "Since Python 3.3, __init__.py is no longer required to define directories as importable Python packages.__init__.pyCheck PEP 420: Implicit Namespace Packages:PEP 420: Implicit Namespace Packages\nNative support for package directories that don\u2019t require __init__.py marker files and can automatically span multiple path segments (inspired by various third party approaches to namespace packages, as described in PEP 420)\nNative support for package directories that don\u2019t require __init__.py marker files and can automatically span multiple path segments (inspired by various third party approaches to namespace packages, as described in PEP 420)__init__.pyPEP 420Here's the test:  $ mkdir -p /tmp/test_init\n$ touch /tmp/test_init/module.py /tmp/test_init/__init__.py\n$ tree -at /tmp/test_init\n/tmp/test_init\n\u251c\u2500\u2500 module.py\n\u2514\u2500\u2500 __init__.py\n$ python3\n\n>>> import sys\n>>> sys.path.insert(0, '/tmp')\n>>> from test_init import module\n>>> import test_init.module\n\n$ rm -f /tmp/test_init/__init__.py\n$ tree -at /tmp/test_init\n/tmp/test_init\n\u2514\u2500\u2500 module.py\n$ python3\n\n>>> import sys\n>>> sys.path.insert(0, '/tmp')\n>>> from test_init import module\n>>> import test_init.module\n$ mkdir -p /tmp/test_init\n$ touch /tmp/test_init/module.py /tmp/test_init/__init__.py\n$ tree -at /tmp/test_init\n/tmp/test_init\n\u251c\u2500\u2500 module.py\n\u2514\u2500\u2500 __init__.py\n$ python3\n\n>>> import sys\n>>> sys.path.insert(0, '/tmp')\n>>> from test_init import module\n>>> import test_init.module\n\n$ rm -f /tmp/test_init/__init__.py\n$ tree -at /tmp/test_init\n/tmp/test_init\n\u2514\u2500\u2500 module.py\n$ python3\n\n>>> import sys\n>>> sys.path.insert(0, '/tmp')\n>>> from test_init import module\n>>> import test_init.module\nreferences:\nhttps://docs.python.org/3/whatsnew/3.3.html#pep-420-implicit-namespace-packages\nhttps://www.python.org/dev/peps/pep-0420/\nIs __init__.py not required for packages in Python 3? https://docs.python.org/3/whatsnew/3.3.html#pep-420-implicit-namespace-packageshttps://www.python.org/dev/peps/pep-0420/Is __init__.py not required for packages in Python 3?",
                "Although Python works without an __init__.py file you should still include one.__init__.pyIt specifies that the directory should be treated as a package, so therefore include it (even if it is empty).There is also a case where you may actually use an __init__.py file:There is also a case where you may actually use an __init__.py file:__init__.pyImagine you had the following file structure:Imagine you had the following file structure:main_methods \n    |- methods.py\nmain_methods \n    |- methods.py\nAnd methods.py contained this:methods.pydef foo():\n    return 'foo'\ndef foo():\n    return 'foo'\nTo use foo() you would need one of the following:foo()from main_methods.methods import foo # Call with foo()\nfrom main_methods import methods # Call with methods.foo()\nimport main_methods.methods # Call with main_methods.methods.foo()\nfrom main_methods.methods import foo # Call with foo()\nfrom main_methods import methods # Call with methods.foo()\nimport main_methods.methods # Call with main_methods.methods.foo()\nMaybe there you need (or want) to keep methods.py inside main_methods (runtimes/dependencies for example) but you only want to import main_methods.methods.pymain_methodsmain_methodsIf you changed the name of methods.py to __init__.py then you could use foo() by just importing main_methods:methods.py__init__.pyfoo()main_methodsimport main_methods\nprint(main_methods.foo()) # Prints 'foo'\nimport main_methods\nprint(main_methods.foo()) # Prints 'foo'\nThis works because __init__.py is treated as part of the package.__init__.pySome Python packages actually do this.  An example is with JSON, where running import json is actually importing __init__.py from the json package (see the package file structure here):JSONimport json__init__.pyjsonsee the package file structure here\nSource code: Lib/json/__init__.py\nSource code: Lib/json/__init__.pySource code:Lib/json/__init__.py",
                "In Python the definition of package is very simple. Like Java the hierarchical structure and the directory structure are the same. But you have to have __init__.py in a package. I will explain the __init__.py file with the example below:__init__.py__init__.pypackage_x/\n|--  __init__.py\n|--    subPackage_a/\n|------  __init__.py\n|------  module_m1.py\n|--    subPackage_b/\n|------  __init__.py\n|------  module_n1.py\n|------  module_n2.py\n|------  module_n3.py\npackage_x/\n|--  __init__.py\n|--    subPackage_a/\n|------  __init__.py\n|------  module_m1.py\n|--    subPackage_b/\n|------  __init__.py\n|------  module_n1.py\n|------  module_n2.py\n|------  module_n3.py\n__init__.py can be empty, as long as it exists. It indicates that the directory should be regarded as a package. Of course, __init__.py can also set the appropriate content.__init__.py__init__.pyIf we add a function in module_n1:def function_X():\n    print \"function_X in module_n1\"\n    return\ndef function_X():\n    print \"function_X in module_n1\"\n    return\nAfter running:>>>from package_x.subPackage_b.module_n1 import function_X\n>>>function_X()\n\nfunction_X in module_n1 \n>>>from package_x.subPackage_b.module_n1 import function_X\n>>>function_X()\n\nfunction_X in module_n1 \nThen we followed the hierarchy package and called module_n1 the function. We can use __init__.py in subPackage_b like this:__init__.py__all__ = ['module_n2', 'module_n3']\n__all__ = ['module_n2', 'module_n3']\nAfter running: >>>from package_x.subPackage_b import * \n>>>module_n1.function_X()\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named module_n1\n>>>from package_x.subPackage_b import * \n>>>module_n1.function_X()\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named module_n1\nHence using * importing, module package is subject to __init__.py content.__init__.py",
                "__init__.py will treat the directory it is in as a loadable module.__init__.pyFor people who prefer reading code, I put Two-Bit Alchemist's comment here.Two-Bit Alchemist's$ find /tmp/mydir/\n/tmp/mydir/\n/tmp/mydir//spam\n/tmp/mydir//spam/__init__.py\n/tmp/mydir//spam/module.py\n$ cd ~\n$ python\n>>> import sys\n>>> sys.path.insert(0, '/tmp/mydir')\n>>> from spam import module\n>>> module.myfun(3)\n9\n>>> exit()\n$ \n$ rm /tmp/mydir/spam/__init__.py*\n$ \n$ python\n>>> import sys\n>>> sys.path.insert(0, '/tmp/mydir')\n>>> from spam import module\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named spam\n>>> \n$ find /tmp/mydir/\n/tmp/mydir/\n/tmp/mydir//spam\n/tmp/mydir//spam/__init__.py\n/tmp/mydir//spam/module.py\n$ cd ~\n$ python\n>>> import sys\n>>> sys.path.insert(0, '/tmp/mydir')\n>>> from spam import module\n>>> module.myfun(3)\n9\n>>> exit()\n$ \n$ rm /tmp/mydir/spam/__init__.py*\n$ \n$ python\n>>> import sys\n>>> sys.path.insert(0, '/tmp/mydir')\n>>> from spam import module\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nImportError: No module named spam\n>>> \n",
                "It facilitates importing other python files. When you placed this file in a directory (say stuff)containing other py files, then you can do something like import stuff.other.root\\\n    stuff\\\n         other.py\n\n    morestuff\\\n         another.py\nroot\\\n    stuff\\\n         other.py\n\n    morestuff\\\n         another.py\nWithout this __init__.py inside the directory stuff, you couldn't import other.py, because Python doesn't know where the source code for stuff is and unable to recognize it as a package. __init__.py",
                "An __init__.py file makes imports easy. When an __init__.py is present within a package, function a() can be imported from file b.py like so:__init__.py__init__.pya()b.pyfrom b import a\nfrom b import a\nWithout it, however, you can't import directly. You have to amend the system path:import sys\nsys.path.insert(0, 'path/to/b.py')\n\nfrom b import a\nimport sys\nsys.path.insert(0, 'path/to/b.py')\n\nfrom b import a\n",
                "One thing __init__.py allows is converting a module to a package without breaking the API or creating extraneous nested namespaces or private modules*. This helps when I want to extend a namespace.If I have a file util.py containingdef foo():\n    ...\ndef foo():\n    ...\nthen users will access foo withfoofrom util import foo\nfrom util import foo\nIf I then want to add utility functions for database interaction, and I want them to have their own namespace under util, I'll need a new directory**, and to keep API compatibility (so that from util import foo still works), I'll call it util/. I could move util.py into util/ like so,utilfrom util import foocouldutil/\n  __init__.py\n  util.py\n  db.py\nutil/\n  __init__.py\n  util.py\n  db.py\nand in util/__init__.py dofrom util import *\nfrom util import *\nbut this is redundant. Instead of having a util/util.py file, we can just put the util.py contents in __init__.py and the user can nowfrom util import foo\nfrom util.db import check_schema\nfrom util import foo\nfrom util.db import check_schema\nI think this nicely highlights how a util package's __init__.py acts in a similar way to a util moduleutilutil* this is hinted at in the other answers, but I want to highlight it here\n** short of employing import gymnastics. Note it won't work to create a new package with the same name as the file, see this\n** short of employing import gymnastics. Note it won't work to create a new package with the same name as the file, see thisthis",
                "If you're using Python 2 and want to load siblings of your file you can simply add the parent folder of your file to your system paths of the session. It will behave about the same as if your current file was an init file.import os\nimport sys\ndir_path = os.path.dirname(__file__)\nsys.path.insert(0, dir_path)\nimport os\nimport sys\ndir_path = os.path.dirname(__file__)\nsys.path.insert(0, dir_path)\nAfter that regular imports relative to the file's directory will work just fine. E.g.import cheese\nfrom vehicle_parts import *\n# etc.\nimport cheese\nfrom vehicle_parts import *\n# etc.\nGenerally you want to use a proper init.py file instead though, but when dealing with legacy code you might be stuck with f.ex. a library hard-coded to load a particular file and nothing but. For those cases this is an alternative.init",
                "init.py : It is a python file found in a package directory, it is invoked when the package or a module in the package is imported. You can use this to execute package initialization code, i.e. whenever the package is imported the python statements are executed first before the other modules in this folder gets executed. It is similar to main function of c or java program but this exists in the python package module(folder) rather than in the core python file.\nalso it has access to global variables defined in this init.py file as when the module is imported into python file.initinitfor eg.\nI have a init.py file in a folder called pymodlib, this file contains the following statements:initprint(f'Invoking init.py for {name}')\npystructures = ['for_loop', 'while__loop', 'ifCondition']initnamewhen I import this package \"pymodlib\" in the my solution module or notebook or python console:\nthis two statements gets executed while importing.\nSo in the log or console you would see the following output:\n\n\nimport pymodlib\nInvoking init.py for pymodlib\n\n\n\n\nimport pymodlib\nInvoking init.py for pymodlib\n\n\nimport pymodlib\nInvoking init.py for pymodlib\nimport pymodlib\nInvoking init.py for pymodlibinitin the next statement of python console: I can access the global variable:\n\npymodlib.pystructures\nit gives the following output:\n\n\npymodlib.pystructures\nit gives the following output:\npymodlib.pystructures\nit gives the following output:['for_loop', 'while__loop', 'ifCondition']Now from python3.3 onwards the use of this file has been optional to make folder a python module. So you skip from including it in the python module folder."
            ]
        },
        {
            "tag": "sort_dict",
            "patterns": [
                "How do I sort a dictionary by value?",
                "how can you sort a dictionary by value?",
                "what can i do to sort a dictionary by value?",
                "how do i sort a dictionary by value?"
            ],
            "responses": [
                "Python 3.7+ or CPython 3.6Dicts preserve insertion order in Python 3.7+. Same in CPython 3.6, but it's an implementation detail.it's an implementation detail>>> x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n>>> {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}\n{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}\n>>> x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n>>> {k: v for k, v in sorted(x.items(), key=lambda item: item[1])}\n{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}\nor>>> dict(sorted(x.items(), key=lambda item: item[1]))\n{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}\n>>> dict(sorted(x.items(), key=lambda item: item[1]))\n{0: 0, 2: 1, 1: 2, 4: 3, 3: 4}\nOlder PythonIt is not possible to sort a dictionary, only to get a representation of a dictionary that is sorted. Dictionaries are inherently orderless, but other types, such as lists and tuples, are not. So you need an ordered data type to represent sorted values, which will be a list\u2014probably a list of tuples.For instance,import operator\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=operator.itemgetter(1))\nimport operator\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=operator.itemgetter(1))\nsorted_x will be a list of tuples sorted by the second element in each tuple. dict(sorted_x) == x.sorted_xdict(sorted_x) == xAnd for those wishing to sort on keys instead of values:import operator\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=operator.itemgetter(0))\nimport operator\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=operator.itemgetter(0))\nIn Python3 since unpacking is not allowed we can useunpacking is not allowedx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=lambda kv: kv[1])\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=lambda kv: kv[1])\nIf you want the output as a dict, you can use collections.OrderedDict:collections.OrderedDictcollections.OrderedDictimport collections\n\nsorted_dict = collections.OrderedDict(sorted_x)\nimport collections\n\nsorted_dict = collections.OrderedDict(sorted_x)\n",
                "As simple as: sorted(dict1, key=dict1.get)sorted(dict1, key=dict1.get)Well, it is actually possible to do a \"sort by dictionary values\". Recently I had to do that in a Code Golf (Stack Overflow question Code golf: Word frequency chart). Abridged, the problem was of the kind: given a text, count how often each word is encountered and display a list of the top words, sorted by decreasing frequency. Code golf: Word frequency chartCode golf: Word frequency chartIf you construct a dictionary with the words as keys and the number of occurrences of each word as value, simplified here as:from collections import defaultdict\nd = defaultdict(int)\nfor w in text.split():\n    d[w] += 1\nfrom collections import defaultdict\nd = defaultdict(int)\nfor w in text.split():\n    d[w] += 1\nthen you can get a list of the words, ordered by frequency of use with sorted(d, key=d.get) - the sort iterates over the dictionary keys, using the number of word occurrences as a sort key . sorted(d, key=d.get)sorted(d, key=d.get)for w in sorted(d, key=d.get, reverse=True):\n    print(w, d[w])\nfor w in sorted(d, key=d.get, reverse=True):\n    print(w, d[w])\nI am writing this detailed explanation to illustrate what people often mean by \"I can easily sort a dictionary by key, but how do I sort by value\" - and I think the original post was trying to address such an issue. And the solution is to do sort of list of the keys, based on the values, as shown above.",
                "You could use:sorted(d.items(), key=lambda x: x[1])\nsorted(d.items(), key=lambda x: x[1])\nThis will sort the dictionary by the values of each entry within the dictionary from smallest to largest.To sort it in descending order just add reverse=True:reverse=Truesorted(d.items(), key=lambda x: x[1], reverse=True)\nsorted(d.items(), key=lambda x: x[1], reverse=True)\nInput:Input:d = {'one':1,'three':3,'five':5,'two':2,'four':4}\na = sorted(d.items(), key=lambda x: x[1])    \nprint(a)\nd = {'one':1,'three':3,'five':5,'two':2,'four':4}\na = sorted(d.items(), key=lambda x: x[1])    \nprint(a)\nOutput:Output:[('one', 1), ('two', 2), ('three', 3), ('four', 4), ('five', 5)]\n[('one', 1), ('two', 2), ('three', 3), ('four', 4), ('five', 5)]\n",
                "Dicts can't be sorted, but you can build a sorted list from them.A sorted list of dict values:sorted(d.values())\nsorted(d.values())\nA list of (key, value) pairs, sorted by value:from operator import itemgetter\nsorted(d.items(), key=itemgetter(1))\nfrom operator import itemgetter\nsorted(d.items(), key=itemgetter(1))\n",
                "In recent Python 2.7, we have the new OrderedDict type, which remembers the order in which the items were added.OrderedDict>>> d = {\"third\": 3, \"first\": 1, \"fourth\": 4, \"second\": 2}\n\n>>> for k, v in d.items():\n...     print \"%s: %s\" % (k, v)\n...\nsecond: 2\nfourth: 4\nthird: 3\nfirst: 1\n\n>>> d\n{'second': 2, 'fourth': 4, 'third': 3, 'first': 1}\n>>> d = {\"third\": 3, \"first\": 1, \"fourth\": 4, \"second\": 2}\n\n>>> for k, v in d.items():\n...     print \"%s: %s\" % (k, v)\n...\nsecond: 2\nfourth: 4\nthird: 3\nfirst: 1\n\n>>> d\n{'second': 2, 'fourth': 4, 'third': 3, 'first': 1}\nTo make a new ordered dictionary from the original, sorting by the values:>>> from collections import OrderedDict\n>>> d_sorted_by_value = OrderedDict(sorted(d.items(), key=lambda x: x[1]))\n>>> from collections import OrderedDict\n>>> d_sorted_by_value = OrderedDict(sorted(d.items(), key=lambda x: x[1]))\nThe OrderedDict behaves like a normal dict:>>> for k, v in d_sorted_by_value.items():\n...     print \"%s: %s\" % (k, v)\n...\nfirst: 1\nsecond: 2\nthird: 3\nfourth: 4\n\n>>> d_sorted_by_value\nOrderedDict([('first': 1), ('second': 2), ('third': 3), ('fourth': 4)])\n>>> for k, v in d_sorted_by_value.items():\n...     print \"%s: %s\" % (k, v)\n...\nfirst: 1\nsecond: 2\nthird: 3\nfourth: 4\n\n>>> d_sorted_by_value\nOrderedDict([('first': 1), ('second': 2), ('third': 3), ('fourth': 4)])\n",
                "Using Python 3.5Whilst I found the accepted answer useful, I was also surprised that it hasn't been updated to reference OrderedDict from the standard library collections module as a viable, modern alternative - designed to solve exactly this type of problem.OrderedDictOrderedDictcollectionsfrom operator import itemgetter\nfrom collections import OrderedDict\n\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = OrderedDict(sorted(x.items(), key=itemgetter(1)))\n# OrderedDict([(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)])\nfrom operator import itemgetter\nfrom collections import OrderedDict\n\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = OrderedDict(sorted(x.items(), key=itemgetter(1)))\n# OrderedDict([(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)])\nThe official OrderedDict documentation offers a very similar example too, but using a lambda for the sort function:OrderedDictOrderedDict# regular unsorted dictionary\nd = {'banana': 3, 'apple':4, 'pear': 1, 'orange': 2}\n\n# dictionary sorted by value\nOrderedDict(sorted(d.items(), key=lambda t: t[1]))\n# OrderedDict([('pear', 1), ('orange', 2), ('banana', 3), ('apple', 4)])\n# regular unsorted dictionary\nd = {'banana': 3, 'apple':4, 'pear': 1, 'orange': 2}\n\n# dictionary sorted by value\nOrderedDict(sorted(d.items(), key=lambda t: t[1]))\n# OrderedDict([('pear', 1), ('orange', 2), ('banana', 3), ('apple', 4)])\n",
                "Pretty much the same as Hank Gay's answer:Hank Gay's answersorted([(value,key) for (key,value) in mydict.items()])\nsorted([(value,key) for (key,value) in mydict.items()])\nOr optimized slightly as suggested by John Fouhy:sorted((value,key) for (key,value) in mydict.items())\nsorted((value,key) for (key,value) in mydict.items())\n",
                "As of Python 3.6 the built-in dict will be orderedPython 3.6Good news, so the OP's original use case of mapping pairs retrieved from a database with unique string ids as keys and numeric values as values into a built-in Python v3.6+ dict, should now respect the insert order.If say the resulting two column table expressions from a database query like:SELECT a_key, a_value FROM a_table ORDER BY a_value;\nSELECT a_key, a_value FROM a_table ORDER BY a_value;\nwould be stored in two Python tuples, k_seq and v_seq (aligned by numerical index and with the same length of course), then:k_seq = ('foo', 'bar', 'baz')\nv_seq = (0, 1, 42)\nordered_map = dict(zip(k_seq, v_seq))\nk_seq = ('foo', 'bar', 'baz')\nv_seq = (0, 1, 42)\nordered_map = dict(zip(k_seq, v_seq))\nAllow to output later as:for k, v in ordered_map.items():\n    print(k, v)\nfor k, v in ordered_map.items():\n    print(k, v)\nyielding in this case (for the new Python 3.6+ built-in dict!):foo 0\nbar 1\nbaz 42\nfoo 0\nbar 1\nbaz 42\nin the same ordering per value of v.Where in the Python 3.5 install on my machine it currently yields:bar 1\nfoo 0\nbaz 42\nbar 1\nfoo 0\nbaz 42\nDetails:As proposed in 2012 by Raymond Hettinger (cf. mail on python-dev with subject \"More compact dictionaries with faster iteration\") and now (in 2016) announced in a mail by Victor Stinner to python-dev with subject \"Python 3.6 dict becomes compact and gets a private version; and keywords become ordered\" due to the fix/implementation of issue 27350 \"Compact and ordered dict\" in Python 3.6 we will now be able, to use a built-in dict to maintain insert order!!\"More compact dictionaries with faster iteration\"\"Python 3.6 dict becomes compact and gets a private version; and keywords become ordered\"\"Compact and ordered dict\"Hopefully this will lead to a thin layer OrderedDict implementation as a first step. As @JimFasarakis-Hilliard indicated, some see use cases for the OrderedDict type also in the future. I think the Python community at large will carefully inspect, if this will stand the test of time, and what the next steps will be.Time to rethink our coding habits to not miss the possibilities opened by stable ordering of:\nKeyword arguments and\n(intermediate) dict storage\nKeyword arguments and(intermediate) dict storageThe first because it eases dispatch in the implementation of functions and methods in some cases.The second as it encourages to more easily use dicts as intermediate storage in processing pipelines.dictRaymond Hettinger kindly provided documentation explaining \"The Tech Behind Python 3.6 Dictionaries\" - from his San Francisco Python Meetup Group presentation 2016-DEC-08.The Tech Behind Python 3.6 DictionariesAnd maybe quite some Stack Overflow high decorated question and answer pages will receive variants of this information and many high quality answers will require a per version update too.Caveat Emptor (but also see below update 2017-12-15):As @ajcr rightfully notes: \"The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon.\" (from the whatsnew36) not nit picking, but the citation was cut a bit pessimistic ;-). It continues as \" (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5).\"whatsnew36butSo as in some human languages (e.g. German), usage shapes the language, and the will now has been declared ... in whatsnew36.whatsnew36Update 2017-12-15:In a mail to the python-dev list, Guido van Rossum declared:mail to the python-dev list\nMake it so. \"Dict keeps insertion order\" is the ruling. Thanks! \nMake it so. \"Dict keeps insertion order\" is the ruling. Thanks! So, the version 3.6 CPython side-effect of dict insertion ordering is now becoming part of the language spec (and not anymore only an implementation detail). That mail thread also surfaced some distinguishing design goals for collections.OrderedDict as reminded by Raymond Hettinger during discussion.collections.OrderedDict",
                "It can often be very handy to use namedtuple. For example, you have a dictionary of 'name' as keys and 'score' as values and you want to sort on 'score':namedtuplenamedtupleimport collections\nPlayer = collections.namedtuple('Player', 'score name')\nd = {'John':5, 'Alex':10, 'Richard': 7}\nimport collections\nPlayer = collections.namedtuple('Player', 'score name')\nd = {'John':5, 'Alex':10, 'Richard': 7}\nsorting with lowest score first:worst = sorted(Player(v,k) for (k,v) in d.items())\nworst = sorted(Player(v,k) for (k,v) in d.items())\nsorting with highest score first:best = sorted([Player(v,k) for (k,v) in d.items()], reverse=True)\nbest = sorted([Player(v,k) for (k,v) in d.items()], reverse=True)\nNow you can get the name and score of, let's say the second-best player (index=1) very Pythonically like this:player = best[1]\nplayer.name\n    'Richard'\nplayer.score\n    7\nplayer = best[1]\nplayer.name\n    'Richard'\nplayer.score\n    7\n",
                "I had the same problem, and I solved it like this:WantedOutput = sorted(MyDict, key=lambda x : MyDict[x]) \nWantedOutput = sorted(MyDict, key=lambda x : MyDict[x]) \n(People who answer \"It is not possible to sort a dict\" did not read the question! In fact, \"I can sort on the keys, but how can I sort based on the values?\" clearly means that he wants a list of the keys sorted according to the value of their values.)Please notice that the order is not well defined (keys with the same value will be in an arbitrary order in the output list).",
                "If values are numeric you may also use Counter from collections.CounterCountercollectionsfrom collections import Counter\n\nx = {'hello': 1, 'python': 5, 'world': 3}\nc = Counter(x)\nprint(c.most_common())\n\n>> [('python', 5), ('world', 3), ('hello', 1)]    \nfrom collections import Counter\n\nx = {'hello': 1, 'python': 5, 'world': 3}\nc = Counter(x)\nprint(c.most_common())\n\n>> [('python', 5), ('world', 3), ('hello', 1)]    \n",
                "In Python 2.7, simply do:from collections import OrderedDict\n# regular unsorted dictionary\nd = {'banana': 3, 'apple':4, 'pear': 1, 'orange': 2}\n\n# dictionary sorted by key\nOrderedDict(sorted(d.items(), key=lambda t: t[0]))\nOrderedDict([('apple', 4), ('banana', 3), ('orange', 2), ('pear', 1)])\n\n# dictionary sorted by value\nOrderedDict(sorted(d.items(), key=lambda t: t[1]))\nOrderedDict([('pear', 1), ('orange', 2), ('banana', 3), ('apple', 4)])\nfrom collections import OrderedDict\n# regular unsorted dictionary\nd = {'banana': 3, 'apple':4, 'pear': 1, 'orange': 2}\n\n# dictionary sorted by key\nOrderedDict(sorted(d.items(), key=lambda t: t[0]))\nOrderedDict([('apple', 4), ('banana', 3), ('orange', 2), ('pear', 1)])\n\n# dictionary sorted by value\nOrderedDict(sorted(d.items(), key=lambda t: t[1]))\nOrderedDict([('pear', 1), ('orange', 2), ('banana', 3), ('apple', 4)])\ncopy-paste from : http://docs.python.org/dev/library/collections.html#ordereddict-examples-and-recipeshttp://docs.python.org/dev/library/collections.html#ordereddict-examples-and-recipesEnjoy ;-)",
                "Starting from Python 3.6, dict objects are now ordered by insertion order. It's officially in the specifications of Python 3.7.dict>>> words = {\"python\": 2, \"blah\": 4, \"alice\": 3}\n>>> dict(sorted(words.items(), key=lambda x: x[1]))\n{'python': 2, 'alice': 3, 'blah': 4}\n>>> words = {\"python\": 2, \"blah\": 4, \"alice\": 3}\n>>> dict(sorted(words.items(), key=lambda x: x[1]))\n{'python': 2, 'alice': 3, 'blah': 4}\nBefore that, you had to use OrderedDict.OrderedDictPython 3.7 documentation says:Python 3.7 documentation\nChanged in version 3.7: Dictionary order is guaranteed to be insertion\norder. This behavior was implementation detail of CPython from 3.6.\nChanged in version 3.7: Dictionary order is guaranteed to be insertion\norder. This behavior was implementation detail of CPython from 3.6.",
                "This is the code:import operator\norigin_list = [\n    {\"name\": \"foo\", \"rank\": 0, \"rofl\": 20000},\n    {\"name\": \"Silly\", \"rank\": 15, \"rofl\": 1000},\n    {\"name\": \"Baa\", \"rank\": 300, \"rofl\": 20},\n    {\"name\": \"Zoo\", \"rank\": 10, \"rofl\": 200},\n    {\"name\": \"Penguin\", \"rank\": -1, \"rofl\": 10000}\n]\nprint \">> Original >>\"\nfor foo in origin_list:\n    print foo\n\nprint \"\\n>> Rofl sort >>\"\nfor foo in sorted(origin_list, key=operator.itemgetter(\"rofl\")):\n    print foo\n\nprint \"\\n>> Rank sort >>\"\nfor foo in sorted(origin_list, key=operator.itemgetter(\"rank\")):\n    print foo\nimport operator\norigin_list = [\n    {\"name\": \"foo\", \"rank\": 0, \"rofl\": 20000},\n    {\"name\": \"Silly\", \"rank\": 15, \"rofl\": 1000},\n    {\"name\": \"Baa\", \"rank\": 300, \"rofl\": 20},\n    {\"name\": \"Zoo\", \"rank\": 10, \"rofl\": 200},\n    {\"name\": \"Penguin\", \"rank\": -1, \"rofl\": 10000}\n]\nprint \">> Original >>\"\nfor foo in origin_list:\n    print foo\n\nprint \"\\n>> Rofl sort >>\"\nfor foo in sorted(origin_list, key=operator.itemgetter(\"rofl\")):\n    print foo\n\nprint \"\\n>> Rank sort >>\"\nfor foo in sorted(origin_list, key=operator.itemgetter(\"rank\")):\n    print foo\nHere are the results:OriginalOriginal{'name': 'foo', 'rank': 0, 'rofl': 20000}\n{'name': 'Silly', 'rank': 15, 'rofl': 1000}\n{'name': 'Baa', 'rank': 300, 'rofl': 20}\n{'name': 'Zoo', 'rank': 10, 'rofl': 200}\n{'name': 'Penguin', 'rank': -1, 'rofl': 10000}\n{'name': 'foo', 'rank': 0, 'rofl': 20000}\n{'name': 'Silly', 'rank': 15, 'rofl': 1000}\n{'name': 'Baa', 'rank': 300, 'rofl': 20}\n{'name': 'Zoo', 'rank': 10, 'rofl': 200}\n{'name': 'Penguin', 'rank': -1, 'rofl': 10000}\nRoflRofl{'name': 'Baa', 'rank': 300, 'rofl': 20}\n{'name': 'Zoo', 'rank': 10, 'rofl': 200}\n{'name': 'Silly', 'rank': 15, 'rofl': 1000}\n{'name': 'Penguin', 'rank': -1, 'rofl': 10000}\n{'name': 'foo', 'rank': 0, 'rofl': 20000}\n{'name': 'Baa', 'rank': 300, 'rofl': 20}\n{'name': 'Zoo', 'rank': 10, 'rofl': 200}\n{'name': 'Silly', 'rank': 15, 'rofl': 1000}\n{'name': 'Penguin', 'rank': -1, 'rofl': 10000}\n{'name': 'foo', 'rank': 0, 'rofl': 20000}\nRank Rank{'name': 'Penguin', 'rank': -1, 'rofl': 10000}\n{'name': 'foo', 'rank': 0, 'rofl': 20000}\n{'name': 'Zoo', 'rank': 10, 'rofl': 200}\n{'name': 'Silly', 'rank': 15, 'rofl': 1000}\n{'name': 'Baa', 'rank': 300, 'rofl': 20}\n{'name': 'Penguin', 'rank': -1, 'rofl': 10000}\n{'name': 'foo', 'rank': 0, 'rofl': 20000}\n{'name': 'Zoo', 'rank': 10, 'rofl': 200}\n{'name': 'Silly', 'rank': 15, 'rofl': 1000}\n{'name': 'Baa', 'rank': 300, 'rofl': 20}\n",
                "Try the following approach. Let us define a dictionary called mydict with the following data:mydict = {'carl':40,\n          'alan':2,\n          'bob':1,\n          'danny':3}\nmydict = {'carl':40,\n          'alan':2,\n          'bob':1,\n          'danny':3}\nIf one wanted to sort the dictionary by keys, one could do something like:for key in sorted(mydict.iterkeys()):\n    print \"%s: %s\" % (key, mydict[key])\nfor key in sorted(mydict.iterkeys()):\n    print \"%s: %s\" % (key, mydict[key])\nThis should return the following output:alan: 2\nbob: 1\ncarl: 40\ndanny: 3\nalan: 2\nbob: 1\ncarl: 40\ndanny: 3\nOn the other hand, if one wanted to sort a dictionary by value (as is asked in the question), one could do the following:for key, value in sorted(mydict.iteritems(), key=lambda (k,v): (v,k)):\n    print \"%s: %s\" % (key, value)\nfor key, value in sorted(mydict.iteritems(), key=lambda (k,v): (v,k)):\n    print \"%s: %s\" % (key, value)\nThe result of this command (sorting the dictionary by value) should return the following:bob: 1\nalan: 2\ndanny: 3\ncarl: 40\nbob: 1\nalan: 2\ndanny: 3\ncarl: 40\n",
                "You can create an \"inverted index\", alsofrom collections import defaultdict\ninverse= defaultdict( list )\nfor k, v in originalDict.items():\n    inverse[v].append( k )\nfrom collections import defaultdict\ninverse= defaultdict( list )\nfor k, v in originalDict.items():\n    inverse[v].append( k )\nNow your inverse has the values; each value has a list of applicable keys.for k in sorted(inverse):\n    print k, inverse[k]\nfor k in sorted(inverse):\n    print k, inverse[k]\n",
                "You can use the collections.Counter. Note, this will work for both numeric and non-numeric values.collections.Counter>>> x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\n>>> from collections import Counter\n>>> #To sort in reverse order\n>>> Counter(x).most_common()\n[(3, 4), (4, 3), (1, 2), (2, 1), (0, 0)]\n>>> #To sort in ascending order\n>>> Counter(x).most_common()[::-1]\n[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\n>>> #To get a dictionary sorted by values\n>>> from collections import OrderedDict\n>>> OrderedDict(Counter(x).most_common()[::-1])\nOrderedDict([(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)])\n>>> x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\n>>> from collections import Counter\n>>> #To sort in reverse order\n>>> Counter(x).most_common()\n[(3, 4), (4, 3), (1, 2), (2, 1), (0, 0)]\n>>> #To sort in ascending order\n>>> Counter(x).most_common()[::-1]\n[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\n>>> #To get a dictionary sorted by values\n>>> from collections import OrderedDict\n>>> OrderedDict(Counter(x).most_common()[::-1])\nOrderedDict([(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)])\n",
                "The collections solution mentioned in another answer is absolutely superb, because you retain a connection between the key and value which in the case of dictionaries is extremely important.I don't agree with the number one choice presented in another answer, because it throws away the keys.I used the solution mentioned above (code shown below) and retained access to both keys and values and in my case the ordering was on the values, but the importance was the ordering of the keys after ordering the values.from collections import Counter\n\nx = {'hello':1, 'python':5, 'world':3}\nc=Counter(x)\nprint( c.most_common() )\n\n\n>> [('python', 5), ('world', 3), ('hello', 1)]\nfrom collections import Counter\n\nx = {'hello':1, 'python':5, 'world':3}\nc=Counter(x)\nprint( c.most_common() )\n\n\n>> [('python', 5), ('world', 3), ('hello', 1)]\n",
                "You can use a skip dict which is a dictionary that's permanently sorted by value.skip dict>>> data = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n>>> SkipDict(data)\n{0: 0.0, 2: 1.0, 1: 2.0, 4: 3.0, 3: 4.0}\n>>> data = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n>>> SkipDict(data)\n{0: 0.0, 2: 1.0, 1: 2.0, 4: 3.0, 3: 4.0}\nIf you use keys(), values() or items() then you'll iterate in sorted order by value.keys()values()items()It's implemented using the skip list datastructure.skip list",
                "You can also use a custom function that can be passed to parameter key.keydef dict_val(x):\n    return x[1]\n\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=dict_val)\ndef dict_val(x):\n    return x[1]\n\nx = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\nsorted_x = sorted(x.items(), key=dict_val)\n",
                "Of course, remember, you need to use OrderedDict because regular Python dictionaries don't keep the original order. OrderedDictfrom collections import OrderedDict\na = OrderedDict(sorted(originalDict.items(), key=lambda x: x[1]))\nfrom collections import OrderedDict\na = OrderedDict(sorted(originalDict.items(), key=lambda x: x[1]))\nIf you do not have Python 2.7 or higher, the best you can do is iterate over the values in a generator function. (There is an OrderedDict for 2.4 and 2.6  here, but OrderedDictherea) I don't know about how well it works and b) You have to download and install it of course. If you do not have administrative access, then I'm afraid the option's out.)def gen(originalDict):\n    for x, y in sorted(zip(originalDict.keys(), originalDict.values()), key=lambda z: z[1]):\n        yield (x, y)\n    #Yields as a tuple with (key, value). You can iterate with conditional clauses to get what you want. \n\nfor bleh, meh in gen(myDict):\n    if bleh == \"foo\":\n        print(myDict[bleh])\ndef gen(originalDict):\n    for x, y in sorted(zip(originalDict.keys(), originalDict.values()), key=lambda z: z[1]):\n        yield (x, y)\n    #Yields as a tuple with (key, value). You can iterate with conditional clauses to get what you want. \n\nfor bleh, meh in gen(myDict):\n    if bleh == \"foo\":\n        print(myDict[bleh])\nYou can also print out every valuefor bleh, meh in gen(myDict):\n    print(bleh, meh)\nfor bleh, meh in gen(myDict):\n    print(bleh, meh)\nPlease remember to remove the parentheses after print if not using Python 3.0 or above",
                "from django.utils.datastructures import SortedDict\n\ndef sortedDictByKey(self,data):\n    \"\"\"Sorted dictionary order by key\"\"\"\n    sortedDict = SortedDict()\n    if data:\n        if isinstance(data, dict):\n            sortedKey = sorted(data.keys())\n            for k in sortedKey:\n                sortedDict[k] = data[k]\n    return sortedDict\nfrom django.utils.datastructures import SortedDict\n\ndef sortedDictByKey(self,data):\n    \"\"\"Sorted dictionary order by key\"\"\"\n    sortedDict = SortedDict()\n    if data:\n        if isinstance(data, dict):\n            sortedKey = sorted(data.keys())\n            for k in sortedKey:\n                sortedDict[k] = data[k]\n    return sortedDict\n",
                "Here is a solution using zip on d.values() and d.keys().  A few lines down this link (on Dictionary view objects) is:d.values() and d.keys()d.values()d.keys()\nThis allows the creation of (value, key) pairs using zip(): pairs = zip(d.values(), d.keys()).\nThis allows the creation of (value, key) pairs using zip(): pairs = zip(d.values(), d.keys()).So we can do the following:d = {'key1': 874.7, 'key2': 5, 'key3': 8.1}\n\nd_sorted = sorted(zip(d.values(), d.keys()))\n\nprint d_sorted \n# prints: [(5, 'key2'), (8.1, 'key3'), (874.7, 'key1')]\nd = {'key1': 874.7, 'key2': 5, 'key3': 8.1}\n\nd_sorted = sorted(zip(d.values(), d.keys()))\n\nprint d_sorted \n# prints: [(5, 'key2'), (8.1, 'key3'), (874.7, 'key1')]\n",
                "As pointed out by Dilettant, Python 3.6 will now keep the order! I thought I'd share a function I wrote that eases the sorting of an iterable (tuple, list, dict). In the latter case, you can sort either on keys or values, and it can take numeric comparison into account. Only for >= 3.6!As pointed out by Dilettantkeep the orderOnly for >= 3.6!When you try using sorted on an iterable that holds e.g. strings as well as ints, sorted() will fail. Of course you can force string comparison with str(). However, in some cases you want to do actual numeric comparison where 12 is smaller than 20 (which is not the case in string comparison). So I came up with the following. When you want explicit numeric comparison you can use the flag num_as_num which will try to do explicit numeric sorting by trying to convert all values to floats. If that succeeds, it will do numeric sorting, otherwise it'll resort to string comparison.actual1220num_as_numComments for improvement welcome.def sort_iterable(iterable, sort_on=None, reverse=False, num_as_num=False):\n    def _sort(i):\n      # sort by 0 = keys, 1 values, None for lists and tuples\n      try:\n        if num_as_num:\n          if i is None:\n            _sorted = sorted(iterable, key=lambda v: float(v), reverse=reverse)\n          else:\n            _sorted = dict(sorted(iterable.items(), key=lambda v: float(v[i]), reverse=reverse))\n        else:\n          raise TypeError\n      except (TypeError, ValueError):\n        if i is None:\n          _sorted = sorted(iterable, key=lambda v: str(v), reverse=reverse)\n        else:\n          _sorted = dict(sorted(iterable.items(), key=lambda v: str(v[i]), reverse=reverse))\n      \n      return _sorted\n      \n    if isinstance(iterable, list):\n      sorted_list = _sort(None)\n      return sorted_list\n    elif isinstance(iterable, tuple):\n      sorted_list = tuple(_sort(None))\n      return sorted_list\n    elif isinstance(iterable, dict):\n      if sort_on == 'keys':\n        sorted_dict = _sort(0)\n        return sorted_dict\n      elif sort_on == 'values':\n        sorted_dict = _sort(1)\n        return sorted_dict\n      elif sort_on is not None:\n        raise ValueError(f\"Unexpected value {sort_on} for sort_on. When sorting a dict, use key or values\")\n    else:\n      raise TypeError(f\"Unexpected type {type(iterable)} for iterable. Expected a list, tuple, or dict\")\ndef sort_iterable(iterable, sort_on=None, reverse=False, num_as_num=False):\n    def _sort(i):\n      # sort by 0 = keys, 1 values, None for lists and tuples\n      try:\n        if num_as_num:\n          if i is None:\n            _sorted = sorted(iterable, key=lambda v: float(v), reverse=reverse)\n          else:\n            _sorted = dict(sorted(iterable.items(), key=lambda v: float(v[i]), reverse=reverse))\n        else:\n          raise TypeError\n      except (TypeError, ValueError):\n        if i is None:\n          _sorted = sorted(iterable, key=lambda v: str(v), reverse=reverse)\n        else:\n          _sorted = dict(sorted(iterable.items(), key=lambda v: str(v[i]), reverse=reverse))\n      \n      return _sorted\n      \n    if isinstance(iterable, list):\n      sorted_list = _sort(None)\n      return sorted_list\n    elif isinstance(iterable, tuple):\n      sorted_list = tuple(_sort(None))\n      return sorted_list\n    elif isinstance(iterable, dict):\n      if sort_on == 'keys':\n        sorted_dict = _sort(0)\n        return sorted_dict\n      elif sort_on == 'values':\n        sorted_dict = _sort(1)\n        return sorted_dict\n      elif sort_on is not None:\n        raise ValueError(f\"Unexpected value {sort_on} for sort_on. When sorting a dict, use key or values\")\n    else:\n      raise TypeError(f\"Unexpected type {type(iterable)} for iterable. Expected a list, tuple, or dict\")\n",
                "I just learned a relevant skill from Python for Everybody.Python for EverybodyPython for EverybodyYou may use a temporary list to help you to sort the dictionary:# Assume dictionary to be:\nd = {'apple': 500.1, 'banana': 1500.2, 'orange': 1.0, 'pineapple': 789.0}\n\n# Create a temporary list\ntmp = []\n\n# Iterate through the dictionary and append each tuple into the temporary list\nfor key, value in d.items():\n    tmptuple = (value, key)\n    tmp.append(tmptuple)\n\n# Sort the list in ascending order\ntmp = sorted(tmp)\n\nprint (tmp)\n# Assume dictionary to be:\nd = {'apple': 500.1, 'banana': 1500.2, 'orange': 1.0, 'pineapple': 789.0}\n\n# Create a temporary list\ntmp = []\n\n# Iterate through the dictionary and append each tuple into the temporary list\nfor key, value in d.items():\n    tmptuple = (value, key)\n    tmp.append(tmptuple)\n\n# Sort the list in ascending order\ntmp = sorted(tmp)\n\nprint (tmp)\nIf you want to sort the list in descending order, simply change the original sorting line to:tmp = sorted(tmp, reverse=True)\ntmp = sorted(tmp, reverse=True)\nUsing list comprehension, the one-liner would be:# Assuming the dictionary looks like\nd = {'apple': 500.1, 'banana': 1500.2, 'orange': 1.0, 'pineapple': 789.0}\n# One-liner for sorting in ascending order\nprint (sorted([(v, k) for k, v in d.items()]))\n# One-liner for sorting in descending order\nprint (sorted([(v, k) for k, v in d.items()], reverse=True))\n# Assuming the dictionary looks like\nd = {'apple': 500.1, 'banana': 1500.2, 'orange': 1.0, 'pineapple': 789.0}\n# One-liner for sorting in ascending order\nprint (sorted([(v, k) for k, v in d.items()]))\n# One-liner for sorting in descending order\nprint (sorted([(v, k) for k, v in d.items()], reverse=True))\nSample Output:# Ascending order\n[(1.0, 'orange'), (500.1, 'apple'), (789.0, 'pineapple'), (1500.2, 'banana')]\n# Descending order\n[(1500.2, 'banana'), (789.0, 'pineapple'), (500.1, 'apple'), (1.0, 'orange')]\n# Ascending order\n[(1.0, 'orange'), (500.1, 'apple'), (789.0, 'pineapple'), (1500.2, 'banana')]\n# Descending order\n[(1500.2, 'banana'), (789.0, 'pineapple'), (500.1, 'apple'), (1.0, 'orange')]\n",
                "Use ValueSortedDict from dicts:ValueSortedDictdictsfrom dicts.sorteddict import ValueSortedDict\nd = {1: 2, 3: 4, 4:3, 2:1, 0:0}\nsorted_dict = ValueSortedDict(d)\nprint sorted_dict.items() \n\n[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\nfrom dicts.sorteddict import ValueSortedDict\nd = {1: 2, 3: 4, 4:3, 2:1, 0:0}\nsorted_dict = ValueSortedDict(d)\nprint sorted_dict.items() \n\n[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\n",
                "Iterate through a dict and sort it by its values in descending order:$ python --version\nPython 3.2.2\n\n$ cat sort_dict_by_val_desc.py \ndictionary = dict(siis = 1, sana = 2, joka = 3, tuli = 4, aina = 5)\nfor word in sorted(dictionary, key=dictionary.get, reverse=True):\n  print(word, dictionary[word])\n\n$ python sort_dict_by_val_desc.py \naina 5\ntuli 4\njoka 3\nsana 2\nsiis 1\n$ python --version\nPython 3.2.2\n\n$ cat sort_dict_by_val_desc.py \ndictionary = dict(siis = 1, sana = 2, joka = 3, tuli = 4, aina = 5)\nfor word in sorted(dictionary, key=dictionary.get, reverse=True):\n  print(word, dictionary[word])\n\n$ python sort_dict_by_val_desc.py \naina 5\ntuli 4\njoka 3\nsana 2\nsiis 1\n",
                "If your values are integers, and you use Python 2.7 or newer, you can use collections.Counter instead of dict. The most_common method will give you all items, sorted by the value.collections.Countercollections.Counterdictmost_common",
                "This works in 3.1.x:import operator\nslovar_sorted=sorted(slovar.items(), key=operator.itemgetter(1), reverse=True)\nprint(slovar_sorted)\nimport operator\nslovar_sorted=sorted(slovar.items(), key=operator.itemgetter(1), reverse=True)\nprint(slovar_sorted)\n",
                "For the sake of completeness, I am posting a solution using heapq. Note, this method will work for both numeric and non-numeric valuesheapq>>> x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\n>>> x_items = x.items()\n>>> heapq.heapify(x_items)\n>>> #To sort in reverse order\n>>> heapq.nlargest(len(x_items),x_items, operator.itemgetter(1))\n[(3, 4), (4, 3), (1, 2), (2, 1), (0, 0)]\n>>> #To sort in ascending order\n>>> heapq.nsmallest(len(x_items),x_items, operator.itemgetter(1))\n[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\n>>> x = {1: 2, 3: 4, 4:3, 2:1, 0:0}\n>>> x_items = x.items()\n>>> heapq.heapify(x_items)\n>>> #To sort in reverse order\n>>> heapq.nlargest(len(x_items),x_items, operator.itemgetter(1))\n[(3, 4), (4, 3), (1, 2), (2, 1), (0, 0)]\n>>> #To sort in ascending order\n>>> heapq.nsmallest(len(x_items),x_items, operator.itemgetter(1))\n[(0, 0), (2, 1), (1, 2), (4, 3), (3, 4)]\n"
            ]
        },
        {
            "tag": "add_key",
            "patterns": [
                "How can I add new keys to a dictionary?",
                "what can i do to add keys to a dictionary?",
                "how do i add a new key to my dictionary?",
                "how do i add keys to a dictionary?",
                "how can you add new keys to the dictionary?",
                "how do i add the keys to a dictionary?",
                "how do i add new keys in a dictionary?",
                "how do i add new keys to a dictionary?"
            ],
            "responses": [
                "You create a new key/value pair on a dictionary by assigning a value to that keyd = {'key': 'value'}\nprint(d)  # {'key': 'value'}\n\nd['mynewkey'] = 'mynewvalue'\n\nprint(d)  # {'key': 'value', 'mynewkey': 'mynewvalue'}\nd = {'key': 'value'}\nprint(d)  # {'key': 'value'}\n\nd['mynewkey'] = 'mynewvalue'\n\nprint(d)  # {'key': 'value', 'mynewkey': 'mynewvalue'}\nIf the key doesn't exist, it's added and points to that value. If it exists, the current value it points to is overwritten.",
                "I feel like consolidating info about Python dictionaries:Creating an empty dictionarydata = {}\n# OR\ndata = dict()\ndata = {}\n# OR\ndata = dict()\nCreating a dictionary with initial valuesdata = {'a': 1, 'b': 2, 'c': 3}\n# OR\ndata = dict(a=1, b=2, c=3)\n# OR\ndata = {k: v for k, v in (('a', 1), ('b',2), ('c',3))}\ndata = {'a': 1, 'b': 2, 'c': 3}\n# OR\ndata = dict(a=1, b=2, c=3)\n# OR\ndata = {k: v for k, v in (('a', 1), ('b',2), ('c',3))}\nInserting/Updating a single valuedata['a'] = 1  # Updates if 'a' exists, else adds 'a'\n# OR\ndata.update({'a': 1})\n# OR\ndata.update(dict(a=1))\n# OR\ndata.update(a=1)\ndata['a'] = 1  # Updates if 'a' exists, else adds 'a'\n# OR\ndata.update({'a': 1})\n# OR\ndata.update(dict(a=1))\n# OR\ndata.update(a=1)\nInserting/Updating multiple valuesdata.update({'c':3,'d':4})  # Updates 'c' and adds 'd'\ndata.update({'c':3,'d':4})  # Updates 'c' and adds 'd'\nPython\u00a03.9+:The update operator |= now works for dictionaries:update operator|=data |= {'c':3,'d':4}\ndata |= {'c':3,'d':4}\nCreating a merged dictionary without modifying originalsdata3 = {}\ndata3.update(data)  # Modifies data3, not data\ndata3.update(data2)  # Modifies data3, not data2\ndata3 = {}\ndata3.update(data)  # Modifies data3, not data\ndata3.update(data2)  # Modifies data3, not data2\nPython\u00a03.5+:This uses a new feature called dictionary unpacking.dictionary unpackingdata = {**data1, **data2, **data3}\ndata = {**data1, **data2, **data3}\nPython\u00a03.9+:The merge operator | now works for dictionaries:merge operator|data = data1 | {'c':3,'d':4}\ndata = data1 | {'c':3,'d':4}\nDeleting items in dictionarydel data[key]  # Removes specific element in a dictionary\ndata.pop(key)  # Removes the key & returns the value\ndata.clear()  # Clears entire dictionary\ndel data[key]  # Removes specific element in a dictionary\ndata.pop(key)  # Removes the key & returns the value\ndata.clear()  # Clears entire dictionary\nCheck if a key is already in dictionarykey in data\nkey in data\nIterate through pairs in a dictionaryfor key in data: # Iterates just through the keys, ignoring the values\nfor key, value in d.items(): # Iterates through the pairs\nfor key in d.keys(): # Iterates just through key, ignoring the values\nfor value in d.values(): # Iterates just through value, ignoring the keys\nfor key in data: # Iterates just through the keys, ignoring the values\nfor key, value in d.items(): # Iterates through the pairs\nfor key in d.keys(): # Iterates just through key, ignoring the values\nfor value in d.values(): # Iterates just through value, ignoring the keys\nCreate a dictionary from two listsdata = dict(zip(list_with_keys, list_with_values))\ndata = dict(zip(list_with_keys, list_with_values))\n",
                "To add multiple keys simultaneously, use dict.update():dict.update()dict.update()>>> x = {1:2}\n>>> print(x)\n{1: 2}\n\n>>> d = {3:4, 5:6, 7:8}\n>>> x.update(d)\n>>> print(x)\n{1: 2, 3: 4, 5: 6, 7: 8}\n>>> x = {1:2}\n>>> print(x)\n{1: 2}\n\n>>> d = {3:4, 5:6, 7:8}\n>>> x.update(d)\n>>> print(x)\n{1: 2, 3: 4, 5: 6, 7: 8}\nFor adding a single key, the accepted answer has less computational overhead.",
                "\n\"Is it possible to add a key to a Python dictionary after it has been created? It doesn't seem to have an .add() method.\"\n\"Is it possible to add a key to a Python dictionary after it has been created? It doesn't seem to have an .add() method.\"Yes it is possible, and it does have a method that implements this, but you don't want to use it directly.To demonstrate how and how not to use it, let's create an empty dict with the dict literal, {}:{}my_dict = {}\nmy_dict = {}\nBest Practice 1: Subscript notationTo update this dict with a single new key and value, you can use the subscript notation (see Mappings here) that provides for item assignment:the subscript notation (see Mappings here)my_dict['new key'] = 'new value'\nmy_dict['new key'] = 'new value'\nmy_dict is now:my_dict{'new key': 'new value'}\n{'new key': 'new value'}\nBest Practice 2: The update method - 2 waysupdateWe can also update the dict with multiple values efficiently as well using the update method.  We may be unnecessarily creating an extra dict here, so we hope our dict has already been created and came from or was used for another purpose:the update methodupdatedictdictmy_dict.update({'key 2': 'value 2', 'key 3': 'value 3'})\nmy_dict.update({'key 2': 'value 2', 'key 3': 'value 3'})\nmy_dict is now:my_dict{'key 2': 'value 2', 'key 3': 'value 3', 'new key': 'new value'}\n{'key 2': 'value 2', 'key 3': 'value 3', 'new key': 'new value'}\nAnother efficient way of doing this with the update method is with keyword arguments, but since they have to be legitimate python words, you can't have spaces or special symbols or start the name with a number, but many consider this a more readable way to create keys for a dict, and here we certainly avoid creating an extra unnecessary dict:dictmy_dict.update(foo='bar', foo2='baz')\nmy_dict.update(foo='bar', foo2='baz')\nand my_dict is now:my_dict{'key 2': 'value 2', 'key 3': 'value 3', 'new key': 'new value', \n 'foo': 'bar', 'foo2': 'baz'}\n{'key 2': 'value 2', 'key 3': 'value 3', 'new key': 'new value', \n 'foo': 'bar', 'foo2': 'baz'}\nSo now we have covered three Pythonic ways of updating a dict.dictMagic method, __setitem__, and why it should be avoided__setitem__There's another way of updating a dict that you shouldn't use, which uses the __setitem__ method. Here's an example of how one might use the __setitem__ method to add a key-value pair to a dict, and a demonstration of the poor performance of using it:dict__setitem____setitem__dict>>> d = {}\n>>> d.__setitem__('foo', 'bar')\n>>> d\n{'foo': 'bar'}\n\n\n>>> def f():\n...     d = {}\n...     for i in xrange(100):\n...         d['foo'] = i\n... \n>>> def g():\n...     d = {}\n...     for i in xrange(100):\n...         d.__setitem__('foo', i)\n... \n>>> import timeit\n>>> number = 100\n>>> min(timeit.repeat(f, number=number))\n0.0020880699157714844\n>>> min(timeit.repeat(g, number=number))\n0.005071878433227539\n>>> d = {}\n>>> d.__setitem__('foo', 'bar')\n>>> d\n{'foo': 'bar'}\n\n\n>>> def f():\n...     d = {}\n...     for i in xrange(100):\n...         d['foo'] = i\n... \n>>> def g():\n...     d = {}\n...     for i in xrange(100):\n...         d.__setitem__('foo', i)\n... \n>>> import timeit\n>>> number = 100\n>>> min(timeit.repeat(f, number=number))\n0.0020880699157714844\n>>> min(timeit.repeat(g, number=number))\n0.005071878433227539\nSo we see that using the subscript notation is actually much faster than using __setitem__. Doing the Pythonic thing, that is, using the language in the way it was intended to be used, usually is both more readable and computationally efficient.__setitem__",
                "dictionary[key] = value\ndictionary[key] = value\n",
                "If you want to add a dictionary within a dictionary you can do it this way. Example: Add a new entry to your dictionary & sub dictionarydictionary = {}\ndictionary[\"new key\"] = \"some new entry\" # add new dictionary entry\ndictionary[\"dictionary_within_a_dictionary\"] = {} # this is required by python\ndictionary[\"dictionary_within_a_dictionary\"][\"sub_dict\"] = {\"other\" : \"dictionary\"}\nprint (dictionary)\ndictionary = {}\ndictionary[\"new key\"] = \"some new entry\" # add new dictionary entry\ndictionary[\"dictionary_within_a_dictionary\"] = {} # this is required by python\ndictionary[\"dictionary_within_a_dictionary\"][\"sub_dict\"] = {\"other\" : \"dictionary\"}\nprint (dictionary)\nOutput:Output:{'new key': 'some new entry', 'dictionary_within_a_dictionary': {'sub_dict': {'other': 'dictionarly'}}}\n{'new key': 'some new entry', 'dictionary_within_a_dictionary': {'sub_dict': {'other': 'dictionarly'}}}\nNOTE: Python requires that you first add a sub  NOTE:dictionary[\"dictionary_within_a_dictionary\"] = {}\ndictionary[\"dictionary_within_a_dictionary\"] = {}\nbefore adding entries.",
                "The conventional syntax is d[key] = value, but if your keyboard is missing the square bracket keys you could also do:d[key] = valued.__setitem__(key, value)\nd.__setitem__(key, value)\nIn fact, defining __getitem__ and __setitem__ methods is how you can make your own class support the  square bracket syntax. See Dive Into Python, Classes That Act Like Dictionaries.__getitem____setitem__Dive Into Python, Classes That Act Like DictionariesDive Into Python, Classes That Act Like Dictionaries",
                "You can create one:class myDict(dict):\n\n    def __init__(self):\n        self = dict()\n\n    def add(self, key, value):\n        self[key] = value\n\n## example\n\nmyd = myDict()\nmyd.add('apples',6)\nmyd.add('bananas',3)\nprint(myd)\nclass myDict(dict):\n\n    def __init__(self):\n        self = dict()\n\n    def add(self, key, value):\n        self[key] = value\n\n## example\n\nmyd = myDict()\nmyd.add('apples',6)\nmyd.add('bananas',3)\nprint(myd)\nGives:>>> \n{'apples': 6, 'bananas': 3}\n>>> \n{'apples': 6, 'bananas': 3}\n",
                "This popular question addresses functional methods of merging dictionaries a and b.This popular questionfunctionalabHere are some of the more straightforward methods (tested in Python 3)...c = dict( a, **b ) ## see also https://stackoverflow.com/q/2255878\nc = dict( list(a.items()) + list(b.items()) )\nc = dict( i for d in [a,b] for i in d.items() )\nc = dict( a, **b ) ## see also https://stackoverflow.com/q/2255878\nc = dict( list(a.items()) + list(b.items()) )\nc = dict( i for d in [a,b] for i in d.items() )\nNote: The first method above only works if the keys in b are strings.Note: The first method above only works if the keys in b are strings.bTo add or modify a single element, the b dictionary would contain only that one element...To add or modify a single elementbc = dict( a, **{'d':'dog'} ) ## returns a dictionary based on 'a'\nc = dict( a, **{'d':'dog'} ) ## returns a dictionary based on 'a'\nThis is equivalent to...def functional_dict_add( dictionary, key, value ):\n   temp = dictionary.copy()\n   temp[key] = value\n   return temp\n\nc = functional_dict_add( a, 'd', 'dog' )\ndef functional_dict_add( dictionary, key, value ):\n   temp = dictionary.copy()\n   temp[key] = value\n   return temp\n\nc = functional_dict_add( a, 'd', 'dog' )\n",
                "Let's pretend you want to live in the immutable world and do not want to modify the original but want to create a new dict that is the result of adding a new key to the original.notdictIn Python 3.5+ you can do:In Python 3.5+ you can do:params = {'a': 1, 'b': 2}\nnew_params = {**params, **{'c': 3}}\nparams = {'a': 1, 'b': 2}\nnew_params = {**params, **{'c': 3}}\nThe Python 2 equivalent is:The Python 2 equivalent is:params = {'a': 1, 'b': 2}\nnew_params = dict(params, **{'c': 3})\nparams = {'a': 1, 'b': 2}\nnew_params = dict(params, **{'c': 3})\nAfter either of these:params is still equal to {'a': 1, 'b': 2}params{'a': 1, 'b': 2}andnew_params is equal to {'a': 1, 'b': 2, 'c': 3}new_params{'a': 1, 'b': 2, 'c': 3}There will be times when you don't want to modify the original (you only want the result of adding to the original). I find this a refreshing alternative to the following:I find this a refreshing alternative to the following:params = {'a': 1, 'b': 2}\nnew_params = params.copy()\nnew_params['c'] = 3\nparams = {'a': 1, 'b': 2}\nnew_params = params.copy()\nnew_params['c'] = 3\norparams = {'a': 1, 'b': 2}\nnew_params = params.copy()\nnew_params.update({'c': 3})\nparams = {'a': 1, 'b': 2}\nnew_params = params.copy()\nnew_params.update({'c': 3})\nReference: What does `**` mean in the expression `dict(d1, **d2)`?What does `**` mean in the expression `dict(d1, **d2)`?What does `**` mean in the expression `dict(d1, **d2)`?",
                "There is also the strangely named, oddly behaved, and yet still handy dict.setdefault().dict.setdefault()dict.setdefault()Thisvalue = my_dict.setdefault(key, default)\nvalue = my_dict.setdefault(key, default)\nbasically just does this:try:\n    value = my_dict[key]\nexcept KeyError: # key not found\n    value = my_dict[key] = default\ntry:\n    value = my_dict[key]\nexcept KeyError: # key not found\n    value = my_dict[key] = default\nE.g.,>>> mydict = {'a':1, 'b':2, 'c':3}\n>>> mydict.setdefault('d', 4)\n4 # returns new value at mydict['d']\n>>> print(mydict)\n{'a':1, 'b':2, 'c':3, 'd':4} # a new key/value pair was indeed added\n# but see what happens when trying it on an existing key...\n>>> mydict.setdefault('a', 111)\n1 # old value was returned\n>>> print(mydict)\n{'a':1, 'b':2, 'c':3, 'd':4} # existing key was ignored\n>>> mydict = {'a':1, 'b':2, 'c':3}\n>>> mydict.setdefault('d', 4)\n4 # returns new value at mydict['d']\n>>> print(mydict)\n{'a':1, 'b':2, 'c':3, 'd':4} # a new key/value pair was indeed added\n# but see what happens when trying it on an existing key...\n>>> mydict.setdefault('a', 111)\n1 # old value was returned\n>>> print(mydict)\n{'a':1, 'b':2, 'c':3, 'd':4} # existing key was ignored\n",
                "\nThis question has already been answered ad nauseam, but since my\ncomment\ngained a lot of traction, here it is as an answer:\nThis question has already been answered ad nauseam, but since my\ncomment\ngained a lot of traction, here it is as an answer:commentAdding new keys without updating the existing dictIf you are here trying to figure out how to add a key and return a new dictionary (without modifying the existing one), you can do this using the techniques belownewPython >= 3.5new_dict = {**mydict, 'new_key': new_val}\nnew_dict = {**mydict, 'new_key': new_val}\nPython < 3.5new_dict = dict(mydict, new_key=new_val)\nnew_dict = dict(mydict, new_key=new_val)\nNote that with this approach, your key will need to follow the rules of valid identifier names in Python.rules of valid identifier names",
                "If you're not joining two dictionaries, but adding new key-value pairs to a dictionary, then using the subscript notation seems like the best way.import timeit\n\ntimeit.timeit('dictionary = {\"karga\": 1, \"darga\": 2}; dictionary.update({\"aaa\": 123123, \"asd\": 233})')\n>> 0.49582505226135254\n\ntimeit.timeit('dictionary = {\"karga\": 1, \"darga\": 2}; dictionary[\"aaa\"] = 123123; dictionary[\"asd\"] = 233;')\n>> 0.20782899856567383\nimport timeit\n\ntimeit.timeit('dictionary = {\"karga\": 1, \"darga\": 2}; dictionary.update({\"aaa\": 123123, \"asd\": 233})')\n>> 0.49582505226135254\n\ntimeit.timeit('dictionary = {\"karga\": 1, \"darga\": 2}; dictionary[\"aaa\"] = 123123; dictionary[\"asd\"] = 233;')\n>> 0.20782899856567383\nHowever, if you'd like to add, for example, thousands of new key-value pairs, you should consider using the update() method.update()",
                "Here's another way that I didn't see here: >>> foo = dict(a=1,b=2)\n>>> foo\n{'a': 1, 'b': 2}\n>>> goo = dict(c=3,**foo)\n>>> goo\n{'c': 3, 'a': 1, 'b': 2}\n>>> foo = dict(a=1,b=2)\n>>> foo\n{'a': 1, 'b': 2}\n>>> goo = dict(c=3,**foo)\n>>> goo\n{'c': 3, 'a': 1, 'b': 2}\nYou can use the dictionary constructor and implicit expansion to reconstruct a dictionary. Moreover, interestingly, this method can be used to control the positional order during dictionary construction (post Python 3.6). In fact, insertion order is guaranteed for Python 3.7 and above!post Python 3.6In fact, insertion order is guaranteed for Python 3.7 and above!>>> foo = dict(a=1,b=2,c=3,d=4)\n>>> new_dict = {k: v for k, v in list(foo.items())[:2]}\n>>> new_dict\n{'a': 1, 'b': 2}\n>>> new_dict.update(newvalue=99)\n>>> new_dict\n{'a': 1, 'b': 2, 'newvalue': 99}\n>>> new_dict.update({k: v for k, v in list(foo.items())[2:]})\n>>> new_dict\n{'a': 1, 'b': 2, 'newvalue': 99, 'c': 3, 'd': 4}\n>>> \n>>> foo = dict(a=1,b=2,c=3,d=4)\n>>> new_dict = {k: v for k, v in list(foo.items())[:2]}\n>>> new_dict\n{'a': 1, 'b': 2}\n>>> new_dict.update(newvalue=99)\n>>> new_dict\n{'a': 1, 'b': 2, 'newvalue': 99}\n>>> new_dict.update({k: v for k, v in list(foo.items())[2:]})\n>>> new_dict\n{'a': 1, 'b': 2, 'newvalue': 99, 'c': 3, 'd': 4}\n>>> \nThe above is using dictionary comprehension.",
                "First to check whether the key already exists:a={1:2,3:4}\na.get(1)\n2\na.get(5)\nNone\na={1:2,3:4}\na.get(1)\n2\na.get(5)\nNone\nThen you can add the new key and value.",
                "Add a dictionary (key,value) class.(key,value)class myDict(dict):\n\n    def __init__(self):\n        self = dict()\n\n    def add(self, key, value):\n        #self[key] = value # add new key and value overwriting any exiting same key\n        if self.get(key)!=None:\n            print('key', key, 'already used') # report if key already used\n        self.setdefault(key, value) # if key exit do nothing\n\n\n## example\n\nmyd = myDict()\nname = \"fred\"\n\nmyd.add('apples',6)\nprint('\\n', myd)\nmyd.add('bananas',3)\nprint('\\n', myd)\nmyd.add('jack', 7)\nprint('\\n', myd)\nmyd.add(name, myd)\nprint('\\n', myd)\nmyd.add('apples', 23)\nprint('\\n', myd)\nmyd.add(name, 2)\nprint(myd)\nclass myDict(dict):\n\n    def __init__(self):\n        self = dict()\n\n    def add(self, key, value):\n        #self[key] = value # add new key and value overwriting any exiting same key\n        if self.get(key)!=None:\n            print('key', key, 'already used') # report if key already used\n        self.setdefault(key, value) # if key exit do nothing\n\n\n## example\n\nmyd = myDict()\nname = \"fred\"\n\nmyd.add('apples',6)\nprint('\\n', myd)\nmyd.add('bananas',3)\nprint('\\n', myd)\nmyd.add('jack', 7)\nprint('\\n', myd)\nmyd.add(name, myd)\nprint('\\n', myd)\nmyd.add('apples', 23)\nprint('\\n', myd)\nmyd.add(name, 2)\nprint(myd)\n",
                "I think it would also be useful to point out Python's collections module that consists of many useful dictionary subclasses and wrappers that simplify the addition and modification of data types in a dictionary, specifically defaultdict:collectionscollectionscollectionsaddition and modification of data types in a dictionarydefaultdictdefaultdictdefaultdict\ndict subclass that calls a factory function to supply missing values\ndict subclass that calls a factory function to supply missing valuesThis is particularly useful if you are working with dictionaries that always consist of the same data types or structures, for example a dictionary of lists. >>> from collections import defaultdict\n>>> example = defaultdict(int)\n>>> example['key'] += 1\n>>> example['key']\ndefaultdict(<class 'int'>, {'key': 1})\n>>> from collections import defaultdict\n>>> example = defaultdict(int)\n>>> example['key'] += 1\n>>> example['key']\ndefaultdict(<class 'int'>, {'key': 1})\nIf the key does not yet exist, defaultdict assigns the value given (in our case 10) as the initial value to the dictionary (often used inside loops). This operation therefore does two things: it adds a new key to a dictionary (as per question), and assigns the value if the key doesn't yet exist. With the standard dictionary, this would have raised an error as the += operation is trying to access a value that doesn't yet exist:defaultdict10adds a new key to a dictionary (as per question), and assigns the value if the key doesn't yet exist.and+=>>> example = dict()\n>>> example['key'] += 1\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nKeyError: 'key'\n>>> example = dict()\n>>> example['key'] += 1\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nKeyError: 'key'\nWithout the use of defaultdict, the amount of code to add a new element would be much greater and perhaps looks something like: defaultdict# This type of code would often be inside a loop\nif 'key' not in example:\n    example['key'] = 0  # add key and initial value to dict; could also be a list\nexample['key'] += 1  # this is implementing a counter\n# This type of code would often be inside a loop\nif 'key' not in example:\n    example['key'] = 0  # add key and initial value to dict; could also be a list\nexample['key'] += 1  # this is implementing a counter\ndefaultdict can also be used with complex data types such as list and set:defaultdictlistset>>> example = defaultdict(list)\n>>> example['key'].append(1)\n>>> example\ndefaultdict(<class 'list'>, {'key': [1]})\n>>> example = defaultdict(list)\n>>> example['key'].append(1)\n>>> example\ndefaultdict(<class 'list'>, {'key': [1]})\nAdding an element automatically initialises the list.",
                "Adding keys to dictionary without using add        # Inserting/Updating single value\n        # subscript notation method\n        d['mynewkey'] = 'mynewvalue' # Updates if 'a' exists, else adds 'a'\n        # OR\n        d.update({'mynewkey': 'mynewvalue'})\n        # OR\n        d.update(dict('mynewkey'='mynewvalue'))\n        # OR\n        d.update('mynewkey'='mynewvalue')\n        print(d)  # {'key': 'value', 'mynewkey': 'mynewvalue'}\n        # To add/update multiple keys simultaneously, use d.update():\n        x = {3:4, 5:6, 7:8}\n        d.update(x)\n        print(d) # {'key': 'value', 'mynewkey': 'mynewvalue', 3: 4, 5: 6, 7: 8}\n        # update operator |= now works for dictionaries:\n        d |= {'c':3,'d':4}\n        # Assigning new key value pair using dictionary unpacking.\n        data1 = {4:6, 9:10, 17:20}\n        data2 = {20:30, 32:48, 90:100}\n        data3 = { 38:\"value\", 99:\"notvalid\"}\n        d = {**data1, **data2, **data3}\n        # The merge operator | now works for dictionaries:\n        data = data1 | {'c':3,'d':4}\n        # Create a dictionary from two lists\n        data = dict(zip(list_with_keys, list_with_values))\n        # Inserting/Updating single value\n        # subscript notation method\n        d['mynewkey'] = 'mynewvalue' # Updates if 'a' exists, else adds 'a'\n        # OR\n        d.update({'mynewkey': 'mynewvalue'})\n        # OR\n        d.update(dict('mynewkey'='mynewvalue'))\n        # OR\n        d.update('mynewkey'='mynewvalue')\n        print(d)  # {'key': 'value', 'mynewkey': 'mynewvalue'}\n        # To add/update multiple keys simultaneously, use d.update():\n        x = {3:4, 5:6, 7:8}\n        d.update(x)\n        print(d) # {'key': 'value', 'mynewkey': 'mynewvalue', 3: 4, 5: 6, 7: 8}\n        # update operator |= now works for dictionaries:\n        d |= {'c':3,'d':4}\n        # Assigning new key value pair using dictionary unpacking.\n        data1 = {4:6, 9:10, 17:20}\n        data2 = {20:30, 32:48, 90:100}\n        data3 = { 38:\"value\", 99:\"notvalid\"}\n        d = {**data1, **data2, **data3}\n        # The merge operator | now works for dictionaries:\n        data = data1 | {'c':3,'d':4}\n        # Create a dictionary from two lists\n        data = dict(zip(list_with_keys, list_with_values))\n",
                "dico[\"new key\"] = \"value\""
            ]
        },
        {
            "tag": "Least_Astonishment",
            "patterns": [
                "\"Least Astonishment\" and the Mutable Default Argument",
                "\"Least Astonishment\" and the Mutable Default Argument"
            ],
            "responses": [
                "Actually, this is not a design flaw, and it is not because of internals or performance. It comes simply from the fact that functions in Python are first-class objects, and not only a piece of code.As soon as you think of it this way, then it completely makes sense: a function is an object being evaluated on its definition; default parameters are kind of \"member data\" and therefore their state may change from one call to the other - exactly as in any other object.In any case, the effbot (Fredrik Lundh) has a very nice explanation of the reasons for this behavior in Default Parameter Values in Python.\nI found it very clear, and I really suggest reading it for a better knowledge of how function objects work.Default Parameter Values in Python",
                "Suppose you have the following codefruits = (\"apples\", \"bananas\", \"loganberries\")\n\ndef eat(food=fruits):\n    ...\nfruits = (\"apples\", \"bananas\", \"loganberries\")\n\ndef eat(food=fruits):\n    ...\nWhen I see the declaration of eat, the least astonishing thing is to think that if the first parameter is not given, that it will be equal to the tuple (\"apples\", \"bananas\", \"loganberries\")(\"apples\", \"bananas\", \"loganberries\")However, suppose later on in the code, I do something likedef some_random_function():\n    global fruits\n    fruits = (\"blueberries\", \"mangos\")\ndef some_random_function():\n    global fruits\n    fruits = (\"blueberries\", \"mangos\")\nthen if default parameters were bound at function execution rather than function declaration, I would be astonished (in a very bad way) to discover that fruits had been changed. This would be more astonishing IMO than discovering that your foo function above was mutating the list.fooThe real problem lies with mutable variables, and all languages have this problem to some extent. Here's a question: suppose in Java I have the following code:StringBuffer s = new StringBuffer(\"Hello World!\");\nMap<StringBuffer,Integer> counts = new HashMap<StringBuffer,Integer>();\ncounts.put(s, 5);\ns.append(\"!!!!\");\nSystem.out.println( counts.get(s) );  // does this work?\nStringBuffer s = new StringBuffer(\"Hello World!\");\nMap<StringBuffer,Integer> counts = new HashMap<StringBuffer,Integer>();\ncounts.put(s, 5);\ns.append(\"!!!!\");\nSystem.out.println( counts.get(s) );  // does this work?\nNow, does my map use the value of the StringBuffer key when it was placed into the map, or does it store the key by reference? Either way, someone is astonished; either the person who tried to get the object out of the Map using a value identical to the one they put it in with, or the person who can't seem to retrieve their object even though the key they're using is literally the same object that was used to put it into the map (this is actually why Python doesn't allow its mutable built-in data types to be used as dictionary keys).StringBufferMapYour example is a good one of a case where Python newcomers will be surprised and bitten. But I'd argue that if we \"fixed\" this, then that would only create a different situation where they'd be bitten instead, and that one would be even less intuitive. Moreover, this is always the case when dealing with mutable variables; you always run into cases where someone could intuitively expect one or the opposite behavior depending on what code they're writing.I personally like Python's current approach: default function arguments are evaluated when the function is defined and that object is always the default. I suppose they could special-case using an empty list, but that kind of special casing would cause even more astonishment, not to mention be backwards incompatible.",
                "The relevant part of the documentation:documentation\nDefault parameter values are evaluated from left to right when the function definition is executed. This means that the expression is evaluated once, when the function is defined, and that the same \u201cpre-computed\u201d value is used for each call. This is especially important to understand when a default parameter is a mutable object, such as a list or a dictionary: if the function modifies the object (e.g. by appending an item to a list), the default value is in effect modified. This is generally not what was intended. A way around this is to use None as the default, and explicitly test for it in the body of the function, e.g.:\ndef whats_on_the_telly(penguin=None):\n    if penguin is None:\n        penguin = []\n    penguin.append(\"property of the zoo\")\n    return penguin\n\nDefault parameter values are evaluated from left to right when the function definition is executed. This means that the expression is evaluated once, when the function is defined, and that the same \u201cpre-computed\u201d value is used for each call. This is especially important to understand when a default parameter is a mutable object, such as a list or a dictionary: if the function modifies the object (e.g. by appending an item to a list), the default value is in effect modified. This is generally not what was intended. A way around this is to use None as the default, and explicitly test for it in the body of the function, e.g.:Default parameter values are evaluated from left to right when the function definition is executed.Nonedef whats_on_the_telly(penguin=None):\n    if penguin is None:\n        penguin = []\n    penguin.append(\"property of the zoo\")\n    return penguin\ndef whats_on_the_telly(penguin=None):\n    if penguin is None:\n        penguin = []\n    penguin.append(\"property of the zoo\")\n    return penguin\n",
                "I know nothing about the Python interpreter inner workings (and I'm not an expert in compilers and interpreters either) so don't blame me if I propose anything unsensible or impossible.Provided that python objects are mutable I think that this should be taken into account when designing the default arguments stuff.\nWhen you instantiate a list:are mutablea = []\na = []\nyou expect to get a new list referenced by a.newaWhy should the a=[] ina=[]def x(a=[]):\ndef x(a=[]):\ninstantiate a new list on function definition and not on invocation?\nIt's just like you're asking \"if the user doesn't provide the argument then instantiate a new list and use it as if it was produced by the caller\".\nI think this is ambiguous instead:instantiatedef x(a=datetime.datetime.now()):\ndef x(a=datetime.datetime.now()):\nuser, do you want a to default to the datetime corresponding to when you're defining or executing x?\nIn this case, as in the previous one, I'll keep the same behaviour as if the default argument \"assignment\" was the first instruction of the function (datetime.now() called on function invocation).\nOn the other hand, if the user wanted the definition-time mapping he could write:axdatetime.now()b = datetime.datetime.now()\ndef x(a=b):\nb = datetime.datetime.now()\ndef x(a=b):\nI know, I know: that's a closure. Alternatively Python might provide a keyword to force definition-time binding:def x(static a=b):\ndef x(static a=b):\n",
                "Well, the reason is quite simply that bindings are done when code is executed, and the function definition is executed, well... when the functions is defined.Compare this:class BananaBunch:\n    bananas = []\n\n    def addBanana(self, banana):\n        self.bananas.append(banana)\nclass BananaBunch:\n    bananas = []\n\n    def addBanana(self, banana):\n        self.bananas.append(banana)\nThis code suffers from the exact same unexpected happenstance. bananas is a class attribute, and hence, when you add things to it, it's added to all instances of that class. The reason is exactly the same.It's just \"How It Works\", and making it work differently in the function case would probably be complicated, and in the class case likely impossible, or at least slow down object instantiation a lot, as you would have to keep the class code around and execute it when objects are created.Yes, it is unexpected. But once the penny drops, it fits in perfectly with how Python works in general. In fact, it's a good teaching aid, and once you understand why this happens, you'll grok python much better.That said it should feature prominently in any good Python tutorial. Because as you mention, everyone runs into this problem sooner or later.",
                "Why don't you introspect?I'm really surprised no one has performed the insightful introspection offered by Python (2 and 3 apply) on callables. really23Given a simple little function func defined as:func>>> def func(a = []):\n...    a.append(5)\n>>> def func(a = []):\n...    a.append(5)\nWhen Python encounters it, the first thing it will do is compile it in order to create a code object for this function. While this compilation step is done, Python evaluates* and then stores the default arguments (an empty list [] here) in the function object itself. As the top answer mentioned: the list a can now be considered a member of the function func.codePython evaluates* and then stores the default arguments (an empty list [] here) in the function object itselfevaluatesstores[]amemberfuncSo, let's do some introspection, a before and after to examine how the list gets expanded inside the function object. I'm using Python 3.x for this, for Python 2 the same applies (use __defaults__ or func_defaults in Python 2; yes, two names for the same thing).insidePython 3.x__defaults__func_defaultsFunction Before Execution:>>> def func(a = []):\n...     a.append(5)\n...     \n>>> def func(a = []):\n...     a.append(5)\n...     \nAfter Python executes this definition it will take any default parameters specified (a = [] here) and cram them in the __defaults__ attribute for the function object (relevant section: Callables):     a = []cram them in the __defaults__ attribute for the function object__defaults__>>> func.__defaults__\n([],)\n>>> func.__defaults__\n([],)\nO.k, so an empty list as the single entry in __defaults__, just as expected. __defaults__Function After Execution:Let's now execute this function:>>> func()\n>>> func()\nNow, let's see those __defaults__ again: __defaults__>>> func.__defaults__\n([5],)\n>>> func.__defaults__\n([5],)\nAstonished? The value inside the object changes! Consecutive calls to the function will now simply append to that embedded list object:Astonished?list>>> func(); func(); func()\n>>> func.__defaults__\n([5, 5, 5, 5],)\n>>> func(); func(); func()\n>>> func.__defaults__\n([5, 5, 5, 5],)\nSo, there you have it, the reason why this 'flaw' happens, is because default arguments are part of the function object. There's nothing weird going on here, it's all just a bit surprising.'flaw'The common solution to combat this is to use None as the default and then initialize in the function body:Nonedef func(a = None):\n    # or: a = [] if a is None else a\n    if a is None:\n        a = []\ndef func(a = None):\n    # or: a = [] if a is None else a\n    if a is None:\n        a = []\nSince the function body is executed anew each time, you always get a fresh new empty list if no argument was passed for a.aTo further verify that the list in __defaults__ is the same as that used in the function func you can just change your function to return the id of the list a used inside the function body. Then, compare it to the list in __defaults__ (position [0] in __defaults__) and you'll see how these are indeed refering to the same list instance:__defaults__funcida__defaults__[0]__defaults__>>> def func(a = []): \n...     a.append(5)\n...     return id(a)\n>>>\n>>> id(func.__defaults__[0]) == func()\nTrue\n>>> def func(a = []): \n...     a.append(5)\n...     return id(a)\n>>>\n>>> id(func.__defaults__[0]) == func()\nTrue\nAll with the power of introspection! * To verify that Python evaluates the default arguments during compilation of the function, try executing the following:*def bar(a=input('Did you just see me without calling the function?')): \n    pass  # use raw_input in Py2\ndef bar(a=input('Did you just see me without calling the function?')): \n    pass  # use raw_input in Py2\nas you'll notice, input() is called before the process of building the function and binding it to the name bar is made.input()bar",
                "I used to think that creating the objects at runtime would be the better approach.  I'm less certain now, since you do lose some useful features, though it may be worth it regardless simply to prevent newbie confusion.  The disadvantages of doing so are:1. Performance1. Performancedef foo(arg=something_expensive_to_compute())):\n    ...\ndef foo(arg=something_expensive_to_compute())):\n    ...\nIf call-time evaluation is used, then the expensive function is called every time your function is used without an argument.  You'd either pay an expensive price on each call, or need to manually cache the value externally, polluting your namespace and adding verbosity.2. Forcing bound parameters2. Forcing bound parametersA useful trick is to bind parameters of a lambda to the current binding of a variable when the lambda is created.  For example:currentfuncs = [ lambda i=i: i for i in range(10)]\nfuncs = [ lambda i=i: i for i in range(10)]\nThis returns a list of functions that return 0,1,2,3... respectively.  If the behaviour is changed, they will instead bind i to the call-time value of i, so you would get a list of functions that all returned 9.icall-time9The only way to implement this otherwise would be to create a further closure with the i bound, ie:def make_func(i): return lambda: i\nfuncs = [make_func(i) for i in range(10)]\ndef make_func(i): return lambda: i\nfuncs = [make_func(i) for i in range(10)]\n3. Introspection3. IntrospectionConsider the code:def foo(a='test', b=100, c=[]):\n   print a,b,c\ndef foo(a='test', b=100, c=[]):\n   print a,b,c\nWe can get information about the arguments and defaults using the inspect module, which inspect>>> inspect.getargspec(foo)\n(['a', 'b', 'c'], None, None, ('test', 100, []))\n>>> inspect.getargspec(foo)\n(['a', 'b', 'c'], None, None, ('test', 100, []))\nThis information is very useful for things like document generation, metaprogramming, decorators etc.Now, suppose the behaviour of defaults could be changed so that this is the equivalent of:_undefined = object()  # sentinel value\n\ndef foo(a=_undefined, b=_undefined, c=_undefined)\n    if a is _undefined: a='test'\n    if b is _undefined: b=100\n    if c is _undefined: c=[]\n_undefined = object()  # sentinel value\n\ndef foo(a=_undefined, b=_undefined, c=_undefined)\n    if a is _undefined: a='test'\n    if b is _undefined: b=100\n    if c is _undefined: c=[]\nHowever, we've lost the ability to introspect, and see what the default arguments are.  Because the objects haven't been constructed, we can't ever get hold of them without actually calling the function.  The best we could do is to store off the source code and return that as a string.are",
                "5 points in defense of Python\nSimplicity: The behavior is simple in the following sense:\nMost people fall into this trap only once, not several times.\n\nConsistency: Python always passes objects, not names.\nThe default parameter is, obviously, part of the function\nheading (not the function body). It therefore ought to be evaluated\nat module load time (and only at module load time, unless nested), not\nat function call time.\n\nUsefulness: As Frederik Lundh points out in his explanation\nof \"Default Parameter Values in Python\", the\ncurrent behavior can be quite useful for advanced programming.\n(Use sparingly.)\n\nSufficient documentation: In the most basic Python documentation,\nthe tutorial, the issue is loudly announced as\nan \"Important warning\" in the first subsection of Section\n\"More on Defining Functions\".\nThe warning even uses boldface,\nwhich is rarely applied outside of headings.\nRTFM: Read the fine manual.\n\nMeta-learning: Falling into the trap is actually a very\nhelpful moment (at least if you are a reflective learner),\nbecause you will subsequently better understand the point\n\"Consistency\" above and that will\nteach you a great deal about Python.\n\nSimplicity: The behavior is simple in the following sense:\nMost people fall into this trap only once, not several times.\nSimplicity: The behavior is simple in the following sense:\nMost people fall into this trap only once, not several times.SimplicityConsistency: Python always passes objects, not names.\nThe default parameter is, obviously, part of the function\nheading (not the function body). It therefore ought to be evaluated\nat module load time (and only at module load time, unless nested), not\nat function call time.\nConsistency: Python always passes objects, not names.\nThe default parameter is, obviously, part of the function\nheading (not the function body). It therefore ought to be evaluated\nat module load time (and only at module load time, unless nested), not\nat function call time.ConsistencyalwaysUsefulness: As Frederik Lundh points out in his explanation\nof \"Default Parameter Values in Python\", the\ncurrent behavior can be quite useful for advanced programming.\n(Use sparingly.)\nUsefulness: As Frederik Lundh points out in his explanation\nof \"Default Parameter Values in Python\", the\ncurrent behavior can be quite useful for advanced programming.\n(Use sparingly.)Usefulness\"Default Parameter Values in Python\"Sufficient documentation: In the most basic Python documentation,\nthe tutorial, the issue is loudly announced as\nan \"Important warning\" in the first subsection of Section\n\"More on Defining Functions\".\nThe warning even uses boldface,\nwhich is rarely applied outside of headings.\nRTFM: Read the fine manual.\nSufficient documentation: In the most basic Python documentation,\nthe tutorial, the issue is loudly announced as\nan \"Important warning\" in the first subsection of Section\n\"More on Defining Functions\".\nThe warning even uses boldface,\nwhich is rarely applied outside of headings.\nRTFM: Read the fine manual.Sufficient documentation\"Important warning\"first\"More on Defining Functions\"Meta-learning: Falling into the trap is actually a very\nhelpful moment (at least if you are a reflective learner),\nbecause you will subsequently better understand the point\n\"Consistency\" above and that will\nteach you a great deal about Python.\nMeta-learning: Falling into the trap is actually a very\nhelpful moment (at least if you are a reflective learner),\nbecause you will subsequently better understand the point\n\"Consistency\" above and that will\nteach you a great deal about Python.Meta-learning",
                "This behavior is easy explained by:\nfunction (class etc.) declaration is executed only once, creating all default value objects\neverything is passed by reference\nfunction (class etc.) declaration is executed only once, creating all default value objectseverything is passed by referenceSo:def x(a=0, b=[], c=[], d=0):\n    a = a + 1\n    b = b + [1]\n    c.append(1)\n    print a, b, c\ndef x(a=0, b=[], c=[], d=0):\n    a = a + 1\n    b = b + [1]\n    c.append(1)\n    print a, b, c\n\na doesn't change - every assignment call creates new int object - new object is printed\nb doesn't change - new array is build from default value and printed\nc changes - operation is performed on same object - and it is printed\na doesn't change - every assignment call creates new int object - new object is printedab doesn't change - new array is build from default value and printedbc changes - operation is performed on same object - and it is printedc",
                "1)  The so-called problem of \"Mutable Default Argument\" is in general a special example demonstrating that:\n\"All functions with this problem suffer also from similar side effect problem on the actual parameter,\"\nThat is against the rules of functional programming, usually undesiderable and should be fixed both together.suffer also from similar side effect problem on the actual parameterExample:def foo(a=[]):                 # the same problematic function\n    a.append(5)\n    return a\n\n>>> somevar = [1, 2]           # an example without a default parameter\n>>> foo(somevar)\n[1, 2, 5]\n>>> somevar\n[1, 2, 5]                      # usually expected [1, 2]\ndef foo(a=[]):                 # the same problematic function\n    a.append(5)\n    return a\n\n>>> somevar = [1, 2]           # an example without a default parameter\n>>> foo(somevar)\n[1, 2, 5]\n>>> somevar\n[1, 2, 5]                      # usually expected [1, 2]\nSolution:  a copy\nAn absolutely safe solution is to copy or deepcopy the input object first and then to do whatever with the copy.Solutioncopycopycopydeepcopydeepcopydef foo(a=[]):\n    a = a[:]     # a copy\n    a.append(5)\n    return a     # or everything safe by one line: \"return a + [5]\"\ndef foo(a=[]):\n    a = a[:]     # a copy\n    a.append(5)\n    return a     # or everything safe by one line: \"return a + [5]\"\nMany builtin mutable types have a copy method like some_dict.copy() or some_set.copy() or can be copied easy like somelist[:] or list(some_list). Every object can be also copied by copy.copy(any_object) or more thorough by copy.deepcopy() (the latter useful if the mutable object is composed from mutable objects). Some objects are fundamentally based on side effects like \"file\" object and can not be meaningfully reproduced by copy. copyingsome_dict.copy()some_set.copy()somelist[:]list(some_list)copy.copy(any_object)copy.deepcopy()copyingExample problem for a similar SO questiona similar SO questionclass Test(object):            # the original problematic class\n  def __init__(self, var1=[]):\n    self._var1 = var1\n\nsomevar = [1, 2]               # an example without a default parameter\nt1 = Test(somevar)\nt2 = Test(somevar)\nt1._var1.append([1])\nprint somevar                  # [1, 2, [1]] but usually expected [1, 2]\nprint t2._var1                 # [1, 2, [1]] but usually expected [1, 2]\nclass Test(object):            # the original problematic class\n  def __init__(self, var1=[]):\n    self._var1 = var1\n\nsomevar = [1, 2]               # an example without a default parameter\nt1 = Test(somevar)\nt2 = Test(somevar)\nt1._var1.append([1])\nprint somevar                  # [1, 2, [1]] but usually expected [1, 2]\nprint t2._var1                 # [1, 2, [1]] but usually expected [1, 2]\nIt shouldn't be neither saved in any public attribute of an instance returned by this function. (Assuming that private attributes of instance should not be modified from outside of this class or subclasses by convention. i.e. _var1 is a private attribute )publicprivate_var1Conclusion:\nInput parameters objects shouldn't be modified in place (mutated) nor they should not be binded into an object returned by the function. (If we prefere programming without side effects which is strongly recommended. see Wiki about \"side effect\" (The first two paragraphs are relevent in this context.)\n.)Wiki about \"side effect\"2)\nOnly if the side effect on the actual parameter is required but unwanted on the default parameter then the useful solution is def ...(var1=None): if var1 is None: var1 = [] More..def ...(var1=None):if var1 is None:var1 = []More..3) In some cases is the mutable behavior of default parameters useful.the mutable behavior of default parameters useful",
                "What you're asking is why this:def func(a=[], b = 2):\n    pass\ndef func(a=[], b = 2):\n    pass\nisn't internally equivalent to this:def func(a=None, b = None):\n    a_default = lambda: []\n    b_default = lambda: 2\n    def actual_func(a=None, b=None):\n        if a is None: a = a_default()\n        if b is None: b = b_default()\n    return actual_func\nfunc = func()\ndef func(a=None, b = None):\n    a_default = lambda: []\n    b_default = lambda: 2\n    def actual_func(a=None, b=None):\n        if a is None: a = a_default()\n        if b is None: b = b_default()\n    return actual_func\nfunc = func()\nexcept for the case of explicitly calling func(None, None), which we'll ignore.In other words, instead of evaluating default parameters, why not store each of them, and evaluate them when the function is called?One answer is probably right there--it would effectively turn every function with default parameters into a closure.  Even if it's all hidden away in the interpreter and not a full-blown closure, the data's got to be stored somewhere.  It'd be slower and use more memory.",
                "This actually has nothing to do with default values, other than that it often comes up as an unexpected behaviour when you write functions with mutable default values.>>> def foo(a):\n    a.append(5)\n    print a\n\n>>> a  = [5]\n>>> foo(a)\n[5, 5]\n>>> foo(a)\n[5, 5, 5]\n>>> foo(a)\n[5, 5, 5, 5]\n>>> foo(a)\n[5, 5, 5, 5, 5]\n>>> def foo(a):\n    a.append(5)\n    print a\n\n>>> a  = [5]\n>>> foo(a)\n[5, 5]\n>>> foo(a)\n[5, 5, 5]\n>>> foo(a)\n[5, 5, 5, 5]\n>>> foo(a)\n[5, 5, 5, 5, 5]\nNo default values in sight in this code, but you get exactly the same problem.The problem is that foo is modifying a mutable variable passed in from the caller, when the caller doesn't expect this. Code like this would be fine if the function was called something like append_5; then the caller would be calling the function in order to modify the value they pass in, and the behaviour would be expected. But such a function would be very unlikely to take a default argument, and probably wouldn't return the list (since the caller already has a reference to that list; the one it just passed in).foomodifyingappend_5Your original foo, with a default argument, shouldn't be modifying a whether it was explicitly passed in or got the default value. Your code should leave mutable arguments alone unless it is clear from the context/name/documentation that the arguments are supposed to be modified. Using mutable values passed in as arguments as local temporaries is an extremely bad idea, whether we're in Python or not and whether there are default arguments involved or not.fooaIf you need to destructively manipulate a local temporary in the course of computing something, and you need to start your manipulation from an argument value, you need to make a copy.",
                "Python: The Mutable Default ArgumentDefault arguments get evaluated at the time the function is compiled into a function object. When used by the function, multiple times by that function, they are and remain the same object. When they are mutable, when mutated (for example, by adding an element to it) they remain mutated on consecutive calls.They stay mutated because they are the same object each time.Equivalent code:Since the list is bound to the function when the function object is compiled and instantiated, this:def foo(mutable_default_argument=[]): # make a list the default argument\n    \"\"\"function that uses a list\"\"\"\ndef foo(mutable_default_argument=[]): # make a list the default argument\n    \"\"\"function that uses a list\"\"\"\nis almost exactly equivalent to this:_a_list = [] # create a list in the globals\n\ndef foo(mutable_default_argument=_a_list): # make it the default argument\n    \"\"\"function that uses a list\"\"\"\n\ndel _a_list # remove globals name binding\n_a_list = [] # create a list in the globals\n\ndef foo(mutable_default_argument=_a_list): # make it the default argument\n    \"\"\"function that uses a list\"\"\"\n\ndel _a_list # remove globals name binding\nDemonstrationHere's a demonstration - you can verify that they are the same object each time they are referenced by \nseeing that the list is created before the function has finished compiling to a function object,\nobserving that the id is the same each time the list is referenced,\nobserving that the list stays changed when the function that uses it is called a second time,\nobserving the order in which the output is printed from the source (which I conveniently numbered for you):\nseeing that the list is created before the function has finished compiling to a function object,observing that the id is the same each time the list is referenced,observing that the list stays changed when the function that uses it is called a second time,observing the order in which the output is printed from the source (which I conveniently numbered for you):example.pyexample.pyprint('1. Global scope being evaluated')\n\ndef create_list():\n    '''noisily create a list for usage as a kwarg'''\n    l = []\n    print('3. list being created and returned, id: ' + str(id(l)))\n    return l\n\nprint('2. example_function about to be compiled to an object')\n\ndef example_function(default_kwarg1=create_list()):\n    print('appending \"a\" in default default_kwarg1')\n    default_kwarg1.append(\"a\")\n    print('list with id: ' + str(id(default_kwarg1)) + \n          ' - is now: ' + repr(default_kwarg1))\n\nprint('4. example_function compiled: ' + repr(example_function))\n\n\nif __name__ == '__main__':\n    print('5. calling example_function twice!:')\n    example_function()\n    example_function()\nprint('1. Global scope being evaluated')\n\ndef create_list():\n    '''noisily create a list for usage as a kwarg'''\n    l = []\n    print('3. list being created and returned, id: ' + str(id(l)))\n    return l\n\nprint('2. example_function about to be compiled to an object')\n\ndef example_function(default_kwarg1=create_list()):\n    print('appending \"a\" in default default_kwarg1')\n    default_kwarg1.append(\"a\")\n    print('list with id: ' + str(id(default_kwarg1)) + \n          ' - is now: ' + repr(default_kwarg1))\n\nprint('4. example_function compiled: ' + repr(example_function))\n\n\nif __name__ == '__main__':\n    print('5. calling example_function twice!:')\n    example_function()\n    example_function()\nand running it with python example.py:python example.py1. Global scope being evaluated\n2. example_function about to be compiled to an object\n3. list being created and returned, id: 140502758808032\n4. example_function compiled: <function example_function at 0x7fc9590905f0>\n5. calling example_function twice!:\nappending \"a\" in default default_kwarg1\nlist with id: 140502758808032 - is now: ['a']\nappending \"a\" in default default_kwarg1\nlist with id: 140502758808032 - is now: ['a', 'a']\n1. Global scope being evaluated\n2. example_function about to be compiled to an object\n3. list being created and returned, id: 140502758808032\n4. example_function compiled: <function example_function at 0x7fc9590905f0>\n5. calling example_function twice!:\nappending \"a\" in default default_kwarg1\nlist with id: 140502758808032 - is now: ['a']\nappending \"a\" in default default_kwarg1\nlist with id: 140502758808032 - is now: ['a', 'a']\nDoes this violate the principle of \"Least Astonishment\"?This order of execution is frequently confusing to new users of Python. If you understand the Python execution model, then it becomes quite expected. The usual instruction to new Python users:But this is why the usual instruction to new users is to create their default arguments like this instead:def example_function_2(default_kwarg=None):\n    if default_kwarg is None:\n        default_kwarg = []\ndef example_function_2(default_kwarg=None):\n    if default_kwarg is None:\n        default_kwarg = []\nThis uses the None singleton as a sentinel object to tell the function whether or not we've gotten an argument other than the default. If we get no argument, then we actually want to use a new empty list, [], as the default.[]As the tutorial section on control flow says:tutorial section on control flow\nIf you don\u2019t want the default to be shared between subsequent calls,\n  you can write the function like this instead:\ndef f(a, L=None):\n    if L is None:\n        L = []\n    L.append(a)\n    return L\n\nIf you don\u2019t want the default to be shared between subsequent calls,\n  you can write the function like this instead:def f(a, L=None):\n    if L is None:\n        L = []\n    L.append(a)\n    return L\ndef f(a, L=None):\n    if L is None:\n        L = []\n    L.append(a)\n    return L\n",
                "The shortest answer would probably be \"definition is execution\", therefore the whole argument makes no strict sense. As a more contrived example, you may cite this:def a(): return []\n\ndef b(x=a()):\n    print x\ndef a(): return []\n\ndef b(x=a()):\n    print x\nHopefully it's enough to show that not executing the default argument expressions at the execution time of the def statement isn't easy or doesn't make sense, or both.defI agree it's a gotcha when you try to use default constructors, though.",
                "Already busy topic, but from what I read here, the following helped me realizing how it's working internally:def bar(a=[]):\n     print id(a)\n     a = a + [1]\n     print id(a)\n     return a\n\n>>> bar()\n4484370232\n4484524224\n[1]\n>>> bar()\n4484370232\n4484524152\n[1]\n>>> bar()\n4484370232 # Never change, this is 'class property' of the function\n4484523720 # Always a new object \n[1]\n>>> id(bar.func_defaults[0])\n4484370232\ndef bar(a=[]):\n     print id(a)\n     a = a + [1]\n     print id(a)\n     return a\n\n>>> bar()\n4484370232\n4484524224\n[1]\n>>> bar()\n4484370232\n4484524152\n[1]\n>>> bar()\n4484370232 # Never change, this is 'class property' of the function\n4484523720 # Always a new object \n[1]\n>>> id(bar.func_defaults[0])\n4484370232\n",
                "It's a performance optimization.  As a result of this functionality, which of these two function calls do you think is faster?def print_tuple(some_tuple=(1,2,3)):\n    print some_tuple\n\nprint_tuple()        #1\nprint_tuple((1,2,3)) #2\ndef print_tuple(some_tuple=(1,2,3)):\n    print some_tuple\n\nprint_tuple()        #1\nprint_tuple((1,2,3)) #2\nI'll give you a hint.  Here's the disassembly (see http://docs.python.org/library/dis.html):http://docs.python.org/library/dis.html#1#0 LOAD_GLOBAL              0 (print_tuple)\n3 CALL_FUNCTION            0\n6 POP_TOP\n7 LOAD_CONST               0 (None)\n10 RETURN_VALUE\n0 LOAD_GLOBAL              0 (print_tuple)\n3 CALL_FUNCTION            0\n6 POP_TOP\n7 LOAD_CONST               0 (None)\n10 RETURN_VALUE\n#2# 0 LOAD_GLOBAL              0 (print_tuple)\n 3 LOAD_CONST               4 ((1, 2, 3))\n 6 CALL_FUNCTION            1\n 9 POP_TOP\n10 LOAD_CONST               0 (None)\n13 RETURN_VALUE\n 0 LOAD_GLOBAL              0 (print_tuple)\n 3 LOAD_CONST               4 ((1, 2, 3))\n 6 CALL_FUNCTION            1\n 9 POP_TOP\n10 LOAD_CONST               0 (None)\n13 RETURN_VALUE\n\nI doubt the experienced behavior has a practical use (who really used static variables in C, without breeding bugs ?)\nI doubt the experienced behavior has a practical use (who really used static variables in C, without breeding bugs ?)As you can see, there is a performance benefit when using immutable default arguments.  This can make a difference if it's a frequently called function or the default argument takes a long time to construct.  Also, bear in mind that Python isn't C.  In C you have constants that are pretty much free.  In Python you don't have this benefit.is",
                "This behavior is not surprising if you take the following into consideration:\nThe behavior of read-only class attributes upon assignment attempts, and that\nFunctions are objects (explained well in the accepted answer).\nThe behavior of read-only class attributes upon assignment attempts, and thatFunctions are objects (explained well in the accepted answer).The role of (2) has been covered extensively in this thread. (1) is likely the astonishment causing factor, as this behavior is not \"intuitive\" when coming from other languages.(2)(1)(1) is described in the Python tutorial on classes. In an attempt to assign a value to a read-only class attribute:(1)tutorial on classes\n...all variables found outside of the innermost scope are\nread-only (an attempt to write to such a variable will simply create a\nnew local variable in the innermost scope, leaving the identically\nnamed outer variable unchanged).\n...all variables found outside of the innermost scope are\nread-only (an attempt to write to such a variable will simply create a\nnew local variable in the innermost scope, leaving the identically\nnamed outer variable unchanged).an attempt to write to such a variable will simply create a\nnew local variable in the innermost scope, leaving the identically\nnamed outer variable unchangedan attempt to write to such a variable will simply create a\nnew local variable in the innermost scope, leaving the identically\nnamed outer variable unchangedLook back to the original example and consider the above points:def foo(a=[]):\n    a.append(5)\n    return a\ndef foo(a=[]):\n    a.append(5)\n    return a\nHere foo is an object and a is an attribute of foo (available at foo.func_defs[0]). Since a is a list, a is mutable and is thus a read-write attribute of foo. It is initialized to the empty list as specified by the signature when the function is instantiated, and is available for reading and writing as long as the function object exists.fooafoofoo.func_defs[0]aafooCalling foo without overriding a default uses that default's value from foo.func_defs. In this case, foo.func_defs[0] is used for a within function object's code scope. Changes to a change foo.func_defs[0], which is part of the foo object and persists between execution of the code in foo.foofoo.func_defsfoo.func_defs[0]aafoo.func_defs[0]foofooNow, compare this to the example from the documentation on emulating the default argument behavior of other languages, such that the function signature defaults are used every time the function is executed:emulating the default argument behavior of other languagesdef foo(a, L=None):\n    if L is None:\n        L = []\n    L.append(a)\n    return L\ndef foo(a, L=None):\n    if L is None:\n        L = []\n    L.append(a)\n    return L\nTaking (1) and (2) into account, one can see why this accomplishes the desired behavior:(1)(2)\nWhen the foo function object is instantiated, foo.func_defs[0] is set to None, an immutable object.\nWhen the function is executed with defaults (with no parameter specified for L in the function call), foo.func_defs[0] (None) is available in the local scope as L.\nUpon L = [], the assignment cannot succeed at foo.func_defs[0], because that attribute is read-only.\nPer (1), a new local variable also named L is created in the local scope and used for the remainder of the function call. foo.func_defs[0] thus remains unchanged for future invocations of foo.\nWhen the foo function object is instantiated, foo.func_defs[0] is set to None, an immutable object.foofoo.func_defs[0]NoneWhen the function is executed with defaults (with no parameter specified for L in the function call), foo.func_defs[0] (None) is available in the local scope as L.Lfoo.func_defs[0]NoneLUpon L = [], the assignment cannot succeed at foo.func_defs[0], because that attribute is read-only.L = []foo.func_defs[0]Per (1), a new local variable also named L is created in the local scope and used for the remainder of the function call. foo.func_defs[0] thus remains unchanged for future invocations of foo.(1)a new local variable also named L is created in the local scopea new local variable also named L is created in the local scopeLfoo.func_defs[0]foo",
                "A simple workaround using None>>> def bar(b, data=None):\n...     data = data or []\n...     data.append(b)\n...     return data\n... \n>>> bar(3)\n[3]\n>>> bar(3)\n[3]\n>>> bar(3)\n[3]\n>>> bar(3, [34])\n[34, 3]\n>>> bar(3, [34])\n[34, 3]\n>>> def bar(b, data=None):\n...     data = data or []\n...     data.append(b)\n...     return data\n... \n>>> bar(3)\n[3]\n>>> bar(3)\n[3]\n>>> bar(3)\n[3]\n>>> bar(3, [34])\n[34, 3]\n>>> bar(3, [34])\n[34, 3]\n",
                "It may be true that:\nSomeone is using every language/library feature, and\nSwitching the behavior here would be ill-advised, but\nSomeone is using every language/library feature, andSwitching the behavior here would be ill-advised, butit is entirely consistent to hold to both of the features above and still make another point:\nIt is a confusing feature and it is unfortunate in Python.\nIt is a confusing feature and it is unfortunate in Python.The other answers, or at least some of them either make points 1 and 2 but not 3, or make point 3 and downplay points 1 and 2. But all three are true.But all three are true.It may be true that switching horses in midstream here would be asking for significant breakage, and that there could be more problems created by changing Python to intuitively handle Stefano's opening snippet. And it may be true that someone who knew Python internals well could explain a minefield of consequences. However,However,The existing behavior is not Pythonic, and Python is successful because very little about the language violates the principle of least astonishment anywhere near this badly. It is a real problem, whether or not it would be wise to uproot it. It is a design flaw. If you understand the language much better by trying to trace out the behavior, I can say that C++ does all of this and more; you learn a lot by navigating, for instance, subtle pointer errors. But this is not Pythonic: people who care about Python enough to persevere in the face of this behavior are people who are drawn to the language because Python has far fewer surprises than other language. Dabblers and the curious become Pythonistas when they are astonished at how little time it takes to get something working--not because of a design fl--I mean, hidden logic puzzle--that cuts against the intuitions of programmers who are drawn to Python because it Just Works.nearJust Works",
                "I am going to demonstrate an alternative structure to pass a default list value to a function (it works equally well with dictionaries).  As others have extensively commented, the list parameter is bound to the function when it is defined as opposed to when it is executed.  Because lists and dictionaries are mutable, any alteration to this parameter will affect other calls to this function.  As a result, subsequent calls to the function will receive this shared list which may have been altered by any other calls to the function.  Worse yet, two parameters are using this function's shared parameter at the same time oblivious to the changes made by the other.Wrong Method (probably...):Wrong Method (probably...)def foo(list_arg=[5]):\n    return list_arg\n\na = foo()\na.append(6)\n>>> a\n[5, 6]\n\nb = foo()\nb.append(7)\n# The value of 6 appended to variable 'a' is now part of the list held by 'b'.\n>>> b\n[5, 6, 7]  \n\n# Although 'a' is expecting to receive 6 (the last element it appended to the list),\n# it actually receives the last element appended to the shared list.\n# It thus receives the value 7 previously appended by 'b'.\n>>> a.pop()             \n7\ndef foo(list_arg=[5]):\n    return list_arg\n\na = foo()\na.append(6)\n>>> a\n[5, 6]\n\nb = foo()\nb.append(7)\n# The value of 6 appended to variable 'a' is now part of the list held by 'b'.\n>>> b\n[5, 6, 7]  \n\n# Although 'a' is expecting to receive 6 (the last element it appended to the list),\n# it actually receives the last element appended to the shared list.\n# It thus receives the value 7 previously appended by 'b'.\n>>> a.pop()             \n7\nYou can verify that they are one and the same object by using id:id>>> id(a)\n5347866528\n\n>>> id(b)\n5347866528\n>>> id(a)\n5347866528\n\n>>> id(b)\n5347866528\nPer Brett Slatkin's \"Effective Python: 59 Specific Ways to Write Better Python\", Item 20: Use None and Docstrings to specify dynamic default arguments (p. 48)Item 20: Use None and Docstrings to specify dynamic default argumentsNone\nThe convention for achieving the desired result in Python is to\n  provide a default value of None and to document the actual behaviour\n  in the docstring.\nThe convention for achieving the desired result in Python is to\n  provide a default value of None and to document the actual behaviour\n  in the docstring.NoneThis implementation ensures that each call to the function either receives the default list or else the list passed to the function.Preferred Method:Preferred Methoddef foo(list_arg=None):\n   \"\"\"\n   :param list_arg:  A list of input values. \n                     If none provided, used a list with a default value of 5.\n   \"\"\"\n   if not list_arg:\n       list_arg = [5]\n   return list_arg\n\na = foo()\na.append(6)\n>>> a\n[5, 6]\n\nb = foo()\nb.append(7)\n>>> b\n[5, 7]\n\nc = foo([10])\nc.append(11)\n>>> c\n[10, 11]\ndef foo(list_arg=None):\n   \"\"\"\n   :param list_arg:  A list of input values. \n                     If none provided, used a list with a default value of 5.\n   \"\"\"\n   if not list_arg:\n       list_arg = [5]\n   return list_arg\n\na = foo()\na.append(6)\n>>> a\n[5, 6]\n\nb = foo()\nb.append(7)\n>>> b\n[5, 7]\n\nc = foo([10])\nc.append(11)\n>>> c\n[10, 11]\nThere may be legitimate use cases for the 'Wrong Method' whereby the programmer intended the default list parameter to be shared, but this is more likely the exception than the rule.",
                "The solutions here are:\nUse None as your default value (or a nonce object), and switch on that to create your values at runtime; or\nUse a lambda as your default parameter, and call it within a try block to get the default value (this is the sort of thing that lambda abstraction is for).\nUse None as your default value (or a nonce object), and switch on that to create your values at runtime; orNoneobjectUse a lambda as your default parameter, and call it within a try block to get the default value (this is the sort of thing that lambda abstraction is for).lambdaThe second option is nice because users of the function can pass in a callable, which may be already existing (such as a type)type",
                "You can get round this by replacing the object (and therefore the tie with the scope):def foo(a=[]):\n    a = list(a)\n    a.append(5)\n    return a\ndef foo(a=[]):\n    a = list(a)\n    a.append(5)\n    return a\nUgly, but it works.",
                "When we do this:def foo(a=[]):\n    ...\ndef foo(a=[]):\n    ...\n... we assign the argument a to an unnamed list, if the caller does not pass the value of a.aunnamedTo make things simpler for this discussion, let's temporarily give the unnamed list a name. How about pavlo ?pavlodef foo(a=pavlo):\n   ...\ndef foo(a=pavlo):\n   ...\nAt any time, if the caller doesn't tell us what a is, we reuse pavlo.apavloIf pavlo is mutable (modifiable), and foo ends up modifying it, an effect we notice the next time foo is called without specifying a.pavlofoofooaSo this is what you see (Remember, pavlo is initialized to []):pavlo >>> foo()\n [5]\n >>> foo()\n [5]\nNow, pavlo is [5].pavloCalling foo() again modifies pavlo again:foo()pavlo>>> foo()\n[5, 5]\n>>> foo()\n[5, 5]\nSpecifying a when calling foo() ensures pavlo is not touched.afoo()pavlo>>> ivan = [1, 2, 3, 4]\n>>> foo(a=ivan)\n[1, 2, 3, 4, 5]\n>>> ivan\n[1, 2, 3, 4, 5]\n>>> ivan = [1, 2, 3, 4]\n>>> foo(a=ivan)\n[1, 2, 3, 4, 5]\n>>> ivan\n[1, 2, 3, 4, 5]\nSo, pavlo is still [5, 5].pavlo[5, 5]>>> foo()\n[5, 5, 5]\n>>> foo()\n[5, 5, 5]\n",
                "I sometimes exploit this behavior as an alternative to the following pattern:singleton = None\n\ndef use_singleton():\n    global singleton\n\n    if singleton is None:\n        singleton = _make_singleton()\n\n    return singleton.use_me()\nsingleton = None\n\ndef use_singleton():\n    global singleton\n\n    if singleton is None:\n        singleton = _make_singleton()\n\n    return singleton.use_me()\nIf singleton is only used by use_singleton, I like the following pattern as a replacement:singletonuse_singleton# _make_singleton() is called only once when the def is executed\ndef use_singleton(singleton=_make_singleton()):\n    return singleton.use_me()\n# _make_singleton() is called only once when the def is executed\ndef use_singleton(singleton=_make_singleton()):\n    return singleton.use_me()\nI've used this for instantiating client classes that access external resources, and also for creating dicts or lists for memoization.Since I don't think this pattern is well known, I do put a short comment in to guard against future misunderstandings.",
                "Every other answer explains why this is actually a nice and desired behavior, or why you shouldn't be needing this anyway. Mine is for those stubborn ones who want to exercise their right to bend the language to their will, not the other way around.We will \"fix\" this behavior with a decorator that will copy the default value instead of reusing the same instance for each positional argument left at its default value.import inspect\nfrom copy import deepcopy  # copy would fail on deep arguments like nested dicts\n\ndef sanify(function):\n    def wrapper(*a, **kw):\n        # store the default values\n        defaults = inspect.getargspec(function).defaults # for python2\n        # construct a new argument list\n        new_args = []\n        for i, arg in enumerate(defaults):\n            # allow passing positional arguments\n            if i in range(len(a)):\n                new_args.append(a[i])\n            else:\n                # copy the value\n                new_args.append(deepcopy(arg))\n        return function(*new_args, **kw)\n    return wrapper\nimport inspect\nfrom copy import deepcopy  # copy would fail on deep arguments like nested dicts\n\ndef sanify(function):\n    def wrapper(*a, **kw):\n        # store the default values\n        defaults = inspect.getargspec(function).defaults # for python2\n        # construct a new argument list\n        new_args = []\n        for i, arg in enumerate(defaults):\n            # allow passing positional arguments\n            if i in range(len(a)):\n                new_args.append(a[i])\n            else:\n                # copy the value\n                new_args.append(deepcopy(arg))\n        return function(*new_args, **kw)\n    return wrapper\nNow let's redefine our function using this decorator:@sanify\ndef foo(a=[]):\n    a.append(5)\n    return a\n\nfoo() # '[5]'\nfoo() # '[5]' -- as desired\n@sanify\ndef foo(a=[]):\n    a.append(5)\n    return a\n\nfoo() # '[5]'\nfoo() # '[5]' -- as desired\nThis is particularly neat for functions that take multiple arguments. Compare:# the 'correct' approach\ndef bar(a=None, b=None, c=None):\n    if a is None:\n        a = []\n    if b is None:\n        b = []\n    if c is None:\n        c = []\n    # finally do the actual work\n# the 'correct' approach\ndef bar(a=None, b=None, c=None):\n    if a is None:\n        a = []\n    if b is None:\n        b = []\n    if c is None:\n        c = []\n    # finally do the actual work\nwith# the nasty decorator hack\n@sanify\ndef bar(a=[], b=[], c=[]):\n    # wow, works right out of the box!\n# the nasty decorator hack\n@sanify\ndef bar(a=[], b=[], c=[]):\n    # wow, works right out of the box!\nIt's important to note that the above solution breaks if you try to use keyword args, like so:foo(a=[4])\nfoo(a=[4])\nThe decorator could be adjusted to allow for that, but we leave this as an exercise for the reader ;)",
                "This \"bug\" gave me a lot of overtime work hours! But I'm beginning to see a potential use of it (but I would have liked it to be at the execution time, still)I'm gonna give you what I see as a useful example.def example(errors=[]):\n    # statements\n    # Something went wrong\n    mistake = True\n    if mistake:\n        tryToFixIt(errors)\n        # Didn't work.. let's try again\n        tryToFixItAnotherway(errors)\n        # This time it worked\n    return errors\n\ndef tryToFixIt(err):\n    err.append('Attempt to fix it')\n\ndef tryToFixItAnotherway(err):\n    err.append('Attempt to fix it by another way')\n\ndef main():\n    for item in range(2):\n        errors = example()\n    print '\\n'.join(errors)\n\nmain()\ndef example(errors=[]):\n    # statements\n    # Something went wrong\n    mistake = True\n    if mistake:\n        tryToFixIt(errors)\n        # Didn't work.. let's try again\n        tryToFixItAnotherway(errors)\n        # This time it worked\n    return errors\n\ndef tryToFixIt(err):\n    err.append('Attempt to fix it')\n\ndef tryToFixItAnotherway(err):\n    err.append('Attempt to fix it by another way')\n\ndef main():\n    for item in range(2):\n        errors = example()\n    print '\\n'.join(errors)\n\nmain()\nprints the followingAttempt to fix it\nAttempt to fix it by another way\nAttempt to fix it\nAttempt to fix it by another way\nAttempt to fix it\nAttempt to fix it by another way\nAttempt to fix it\nAttempt to fix it by another way\n",
                "This is not a design flaw. Anyone who trips over this is doing something wrong.This is not a design flawThere are 3 cases I see where you might run into this problem:\nYou intend to modify the argument as a side effect of the function. In this case it never makes sense to have a default argument. The only exception is when you're abusing the argument list to have function attributes, e.g. cache={}, and you wouldn't be expected to call the function with an actual argument at all.\nYou intend to leave the argument unmodified, but you accidentally did modify it. That's a bug, fix it.\nYou intend to modify the argument for use inside the function, but didn't expect the modification to be viewable outside of the function. In that case you need to make a copy of the argument, whether it was the default or not! Python is not a call-by-value language so it doesn't make the copy for you, you need to be explicit about it.\nYou intend to modify the argument as a side effect of the function. In this case it never makes sense to have a default argument. The only exception is when you're abusing the argument list to have function attributes, e.g. cache={}, and you wouldn't be expected to call the function with an actual argument at all.never makes sensecache={}You intend to leave the argument unmodified, but you accidentally did modify it. That's a bug, fix it.didYou intend to modify the argument for use inside the function, but didn't expect the modification to be viewable outside of the function. In that case you need to make a copy of the argument, whether it was the default or not! Python is not a call-by-value language so it doesn't make the copy for you, you need to be explicit about it.copyThe example in the question could fall into category 1 or 3. It's odd that it both modifies the passed list and returns it; you should pick one or the other.",
                "Just change the function to be:def notastonishinganymore(a = []): \n    '''The name is just a joke :)'''\n    a = a[:]\n    a.append(5)\n    return a\ndef notastonishinganymore(a = []): \n    '''The name is just a joke :)'''\n    a = a[:]\n    a.append(5)\n    return a\n",
                "TLDR: Define-time defaults are consistent and strictly more expressive.Defining a function affects two scopes: the defining scope containing the function, and the execution  scope contained by the function. While it is pretty clear how blocks map to scopes, the question is where def <name>(<args=defaults>): belongs to:containingcontained bydef <name>(<args=defaults>):...                           # defining scope\ndef name(parameter=default):  # ???\n    ...                       # execution scope\n...                           # defining scope\ndef name(parameter=default):  # ???\n    ...                       # execution scope\nThe def name part must evaluate in the defining scope - we want name to be available there, after all. Evaluating the function only inside itself would make it inaccessible.def namemustnameSince parameter is a constant name, we can \"evaluate\" it at the same time as def name. This also has the advantage it produces the function with a known signature as name(parameter=...):, instead of a bare name(...):.parameterdef namename(parameter=...):name(...):Now, when to evaluate default?defaultConsistency already says \"at definition\": everything else of def <name>(<args=defaults>): is best evaluated at definition as well. Delaying parts of it would be the astonishing choice.def <name>(<args=defaults>):The two choices are not equivalent, either: If default is evaluated at definition time, it can still affect execution time. If default is evaluated at execution time, it cannot affect definition time. Choosing \"at definition\" allows expressing both cases, while choosing \"at execution\" can express only one:defaultcan stilldefaultcannotdef name(parameter=defined):  # set default at definition time\n    ...\n\ndef name(parameter=default):     # delay default until execution time\n    parameter = default if parameter is None else parameter\n    ...\ndef name(parameter=defined):  # set default at definition time\n    ...\n\ndef name(parameter=default):     # delay default until execution time\n    parameter = default if parameter is None else parameter\n    ...\n",
                "Yes, this is  a design flaw in PythonI've read all the other answers and I'm not convinced. This design does violate the principle of least astonishment.The defaults could have been designed to be evaluated when the function is called, rather than when the function is defined. This is how Javascript does it:\n\nfunction foo(a=[]) {\n  a.push(5);\n  return a;\n}\nconsole.log(foo()); // [5]\nconsole.log(foo()); // [5]\nconsole.log(foo()); // [5]\n\n\n\n\nfunction foo(a=[]) {\n  a.push(5);\n  return a;\n}\nconsole.log(foo()); // [5]\nconsole.log(foo()); // [5]\nconsole.log(foo()); // [5]\n\n\nfunction foo(a=[]) {\n  a.push(5);\n  return a;\n}\nconsole.log(foo()); // [5]\nconsole.log(foo()); // [5]\nconsole.log(foo()); // [5]\nfunction foo(a=[]) {\n  a.push(5);\n  return a;\n}\nconsole.log(foo()); // [5]\nconsole.log(foo()); // [5]\nconsole.log(foo()); // [5]function foo(a=[]) {\n  a.push(5);\n  return a;\n}\nconsole.log(foo()); // [5]\nconsole.log(foo()); // [5]\nconsole.log(foo()); // [5]As further evidence that this is a design flaw, Python core developers are currently discussing introducing new syntax to fix this problem. See this article: Late-bound argument defaults for Python.Late-bound argument defaults for PythonFor even more evidence that this a design flaw, if you Google \"Python gotchas\", this design is mentioned as a gotcha, usually the first gotcha in the list, in the first 9 Google results (1, 2, 3, 4, 5, 6, 7, 8, 9). In contrast, if you Google \"Javascript gotchas\", the behaviour of default arguments in Javascript is not mentioned as a gotcha even once.123456789Gotchas, by definition, violate the principle of least astonishment. They astonish. Given there are superiour designs for the behaviour of default argument values, the inescapable conclusion is that Python's behaviour here represents a design flaw."
            ]
        },
        {
            "tag": "concatenate",
            "patterns": [
                "How do I concatenate two lists in Python?",
                "how do you concatenate lists in python?",
                "how do you concatenate two lists using python?",
                "how do i concatenate two lists using python?",
                "how do i concatenate two lists in python?"
            ],
            "responses": [
                "Use the + operator to combine the lists:+listone = [1, 2, 3]\nlisttwo = [4, 5, 6]\n\njoinedlist = listone + listtwo\nlistone = [1, 2, 3]\nlisttwo = [4, 5, 6]\n\njoinedlist = listone + listtwo\nOutput:>>> joinedlist\n[1, 2, 3, 4, 5, 6]\n>>> joinedlist\n[1, 2, 3, 4, 5, 6]\n",
                "Python >= 3.5 alternative: [*l1, *l2][*l1, *l2]Another alternative has been introduced via the acceptance of PEP 448 which deserves mentioning.PEP 448The PEP, titled Additional Unpacking Generalizations, generally reduced some syntactic restrictions when using the starred * expression in Python; with it, joining two lists (applies to any iterable) can now also be done with:Additional Unpacking GeneralizationsAdditional Unpacking Generalizations*>>> l1 = [1, 2, 3]\n>>> l2 = [4, 5, 6]\n>>> joined_list = [*l1, *l2]  # unpack both iterables in a list literal\n>>> print(joined_list)\n[1, 2, 3, 4, 5, 6]\n>>> l1 = [1, 2, 3]\n>>> l2 = [4, 5, 6]\n>>> joined_list = [*l1, *l2]  # unpack both iterables in a list literal\n>>> print(joined_list)\n[1, 2, 3, 4, 5, 6]\nThis functionality was defined for Python 3.5, but it hasn't been backported to previous versions in the 3.x family. In unsupported versions a SyntaxError is going to be raised.was definedSyntaxErrorAs with the other approaches, this too creates as shallow copy of the elements in the corresponding lists.creates as shallow copyThe upside to this approach is that you really don't need lists in order to perform it; anything that is iterable will do. As stated in the PEP:upside\nThis is also useful as a more readable way of summing iterables into a\nlist, such as my_list + list(my_tuple) + list(my_range) which is now\nequivalent to just [*my_list, *my_tuple, *my_range].\nThis is also useful as a more readable way of summing iterables into a\nlist, such as my_list + list(my_tuple) + list(my_range) which is now\nequivalent to just [*my_list, *my_tuple, *my_range].my_list + list(my_tuple) + list(my_range)[*my_list, *my_tuple, *my_range]So while addition with + would raise a TypeError due to type mismatch:+TypeErrorl = [1, 2, 3]\nr = range(4, 7)\nres = l + r\nl = [1, 2, 3]\nr = range(4, 7)\nres = l + r\nThe following won't:res = [*l, *r]\nres = [*l, *r]\nbecause it will first unpack the contents of the iterables and then simply create a list from the contents.list",
                "It's also possible to create a generator that simply iterates over the items in both lists using itertools.chain(). This allows you to chain lists (or any iterable) together for processing without copying the items to a new list:itertools.chain()itertools.chain()import itertools\nfor item in itertools.chain(listone, listtwo):\n    # Do something with each list item\nimport itertools\nfor item in itertools.chain(listone, listtwo):\n    # Do something with each list item\n",
                "You could also use the list.extend() method in order to add a list to the end of another one:list.extend()list.extend()listlistone = [1,2,3]\nlisttwo = [4,5,6]\n\nlistone.extend(listtwo)\nlistone = [1,2,3]\nlisttwo = [4,5,6]\n\nlistone.extend(listtwo)\nIf you want to keep the original list intact, you can create a new list object, and extend both lists to it:listextendmergedlist = []\nmergedlist.extend(listone)\nmergedlist.extend(listtwo)\nmergedlist = []\nmergedlist.extend(listone)\nmergedlist.extend(listtwo)\n",
                "\nHow do I concatenate two lists in Python?\nHow do I concatenate two lists in Python?How do I concatenate two lists in Python?As of 3.9, these are the most popular stdlib methods for concatenating two (or more) lists in Python.\n\n\n\n\nVersion Restrictions\nIn-Place?\nGeneralize to N lists?\n\n\n\n\na+b\n-\nNo\nsum([a, b, c], [])1\n\n\nlist(chain(a,b))2\n>=2.3\nNo\nlist(chain(a, b, c))\n\n\n[*a, *b]3\n>=3.5\nNo\n[*a, *b, *c]\n\n\na += b\n-\nYes\nNo\n\n\na.extend(b)\n-\nYes\nNo\n\n\n\n\n\n\n\nVersion Restrictions\nIn-Place?\nGeneralize to N lists?\n\n\n\n\na+b\n-\nNo\nsum([a, b, c], [])1\n\n\nlist(chain(a,b))2\n>=2.3\nNo\nlist(chain(a, b, c))\n\n\n[*a, *b]3\n>=3.5\nNo\n[*a, *b, *c]\n\n\na += b\n-\nYes\nNo\n\n\na.extend(b)\n-\nYes\nNo\n\n\n\n\n\nVersion Restrictions\nIn-Place?\nGeneralize to N lists?\n\n\n\nVersion Restrictions\nIn-Place?\nGeneralize to N lists?\nVersion RestrictionsIn-Place?Generalize to N lists?\n\na+b\n-\nNo\nsum([a, b, c], [])1\n\n\nlist(chain(a,b))2\n>=2.3\nNo\nlist(chain(a, b, c))\n\n\n[*a, *b]3\n>=3.5\nNo\n[*a, *b, *c]\n\n\na += b\n-\nYes\nNo\n\n\na.extend(b)\n-\nYes\nNo\n\n\na+b\n-\nNo\nsum([a, b, c], [])1\na+ba+b-Nosum([a, b, c], [])1sum([a, b, c], [])1\nlist(chain(a,b))2\n>=2.3\nNo\nlist(chain(a, b, c))\nlist(chain(a,b))2list(chain(a,b))2>=2.3Nolist(chain(a, b, c))list(chain(a, b, c))\n[*a, *b]3\n>=3.5\nNo\n[*a, *b, *c]\n[*a, *b]3[*a, *b]3>=3.5No[*a, *b, *c][*a, *b, *c]\na += b\n-\nYes\nNo\na += ba += b-YesNo\na.extend(b)\n-\nYes\nNo\na.extend(b)a.extend(b)-YesNo\nFootnotes\n\nThis is a slick solution because of its succinctness. But sum performs concatenation in a pairwise fashion, which means this is a\nquadratic operation as memory has to be allocated for each step. DO\nNOT USE if your lists are large.\n\nSee chain\nand\nchain.from_iterable\nfrom the docs. You will need to from itertools import chain first.\nConcatenation is linear in memory, so this is the best in terms of\nperformance and version compatibility. chain.from_iterable was introduced in 2.6.\n\nThis method uses Additional Unpacking Generalizations (PEP 448), but cannot\ngeneralize to N lists unless you manually unpack each one yourself.\n\na += b and a.extend(b) are more or less equivalent for all practical purposes. += when called on a list will internally call\nlist.__iadd__, which extends the first list by the second.\n\n\nFootnotesFootnotes\nThis is a slick solution because of its succinctness. But sum performs concatenation in a pairwise fashion, which means this is a\nquadratic operation as memory has to be allocated for each step. DO\nNOT USE if your lists are large.\n\nSee chain\nand\nchain.from_iterable\nfrom the docs. You will need to from itertools import chain first.\nConcatenation is linear in memory, so this is the best in terms of\nperformance and version compatibility. chain.from_iterable was introduced in 2.6.\n\nThis method uses Additional Unpacking Generalizations (PEP 448), but cannot\ngeneralize to N lists unless you manually unpack each one yourself.\n\na += b and a.extend(b) are more or less equivalent for all practical purposes. += when called on a list will internally call\nlist.__iadd__, which extends the first list by the second.\n\nThis is a slick solution because of its succinctness. But sum performs concatenation in a pairwise fashion, which means this is a\nquadratic operation as memory has to be allocated for each step. DO\nNOT USE if your lists are large.\nThis is a slick solution because of its succinctness. But sum performs concatenation in a pairwise fashion, which means this is a\nquadratic operation as memory has to be allocated for each step. DO\nNOT USE if your lists are large.sumSee chain\nand\nchain.from_iterable\nfrom the docs. You will need to from itertools import chain first.\nConcatenation is linear in memory, so this is the best in terms of\nperformance and version compatibility. chain.from_iterable was introduced in 2.6.\nSee chain\nand\nchain.from_iterable\nfrom the docs. You will need to from itertools import chain first.\nConcatenation is linear in memory, so this is the best in terms of\nperformance and version compatibility. chain.from_iterable was introduced in 2.6.chainchainchain.from_iterablechain.from_iterablefrom itertools import chainchain.from_iterableThis method uses Additional Unpacking Generalizations (PEP 448), but cannot\ngeneralize to N lists unless you manually unpack each one yourself.\nThis method uses Additional Unpacking Generalizations (PEP 448), but cannot\ngeneralize to N lists unless you manually unpack each one yourself.Additional Unpacking Generalizations (PEP 448)a += b and a.extend(b) are more or less equivalent for all practical purposes. += when called on a list will internally call\nlist.__iadd__, which extends the first list by the second.\na += b and a.extend(b) are more or less equivalent for all practical purposes. += when called on a list will internally call\nlist.__iadd__, which extends the first list by the second.a += ba.extend(b)+=list.__iadd__Performance2-List Concatenation12-List Concatenation1There's not much difference between these methods but that makes sense given they all have the same order of complexity (linear). There's no particular reason to prefer one over the other except as a matter of style.N-List ConcatenationN-List ConcatenationPlots have been generated using the perfplot module. Code, for your reference.perfplotCode, for your reference.1. The iadd (+=) and extend methods operate in-place, so a copy has to be generated each time before testing. To keep things fair, all methods have a pre-copy step for the left-hand list which can be ignored.1. The iadd (+=) and extend methods operate in-place, so a copy has to be generated each time before testing. To keep things fair, all methods have a pre-copy step for the left-hand list which can be ignored.iadd+=extendComments on Other Solutions\nDO NOT USE THE DUNDER METHOD list.__add__ directly in any way, shape or form. In fact, stay clear of dunder methods, and use the operators and operator functions like they were designed for. Python has careful semantics baked into these which are more complicated than just calling the dunder directly. Here is an example. So, to summarise, a.__add__(b) => BAD; a + b => GOOD.\n\nSome answers here offer reduce(operator.add, [a, b]) for pairwise concatenation -- this is the same as sum([a, b], []) only more wordy.\n\nAny method that uses set will drop duplicates and lose ordering. Use with caution.\n\nfor i in b: a.append(i) is more wordy, and slower than a.extend(b), which is single function call and more idiomatic. append is slower because of the semantics with which memory is allocated and grown for lists. See here for a similar discussion.\n\nheapq.merge will work, but its use case is for merging sorted lists in linear time. Using it in any other situation is an anti-pattern.\n\nyielding list elements from a function is an acceptable method, but chain does this faster and better (it has a code path in C, so it is fast).\n\noperator.add(a, b) is an acceptable functional equivalent to a + b. It's use cases are mainly for dynamic method dispatch. Otherwise, prefer a + b which is shorter and more readable, in my opinion. YMMV.\n\nDO NOT USE THE DUNDER METHOD list.__add__ directly in any way, shape or form. In fact, stay clear of dunder methods, and use the operators and operator functions like they were designed for. Python has careful semantics baked into these which are more complicated than just calling the dunder directly. Here is an example. So, to summarise, a.__add__(b) => BAD; a + b => GOOD.\nDO NOT USE THE DUNDER METHOD list.__add__ directly in any way, shape or form. In fact, stay clear of dunder methods, and use the operators and operator functions like they were designed for. Python has careful semantics baked into these which are more complicated than just calling the dunder directly. Here is an example. So, to summarise, a.__add__(b) => BAD; a + b => GOOD.list.__add__operatoran examplea.__add__(b)a + bSome answers here offer reduce(operator.add, [a, b]) for pairwise concatenation -- this is the same as sum([a, b], []) only more wordy.\nSome answers here offer reduce(operator.add, [a, b]) for pairwise concatenation -- this is the same as sum([a, b], []) only more wordy.reduce(operator.add, [a, b])sum([a, b], [])Any method that uses set will drop duplicates and lose ordering. Use with caution.\nAny method that uses set will drop duplicates and lose ordering. Use with caution.setfor i in b: a.append(i) is more wordy, and slower than a.extend(b), which is single function call and more idiomatic. append is slower because of the semantics with which memory is allocated and grown for lists. See here for a similar discussion.\nfor i in b: a.append(i) is more wordy, and slower than a.extend(b), which is single function call and more idiomatic. append is slower because of the semantics with which memory is allocated and grown for lists. See here for a similar discussion.for i in b: a.append(i)a.extend(b)appendhereheapq.merge will work, but its use case is for merging sorted lists in linear time. Using it in any other situation is an anti-pattern.\nheapq.merge will work, but its use case is for merging sorted lists in linear time. Using it in any other situation is an anti-pattern.heapq.mergeyielding list elements from a function is an acceptable method, but chain does this faster and better (it has a code path in C, so it is fast).\nyielding list elements from a function is an acceptable method, but chain does this faster and better (it has a code path in C, so it is fast).yieldchainoperator.add(a, b) is an acceptable functional equivalent to a + b. It's use cases are mainly for dynamic method dispatch. Otherwise, prefer a + b which is shorter and more readable, in my opinion. YMMV.\noperator.add(a, b) is an acceptable functional equivalent to a + b. It's use cases are mainly for dynamic method dispatch. Otherwise, prefer a + b which is shorter and more readable, in my opinion. YMMV.operator.add(a, b)a + ba + bin my opinion",
                "You can use sets to obtain merged list of unique valuesmergedlist = list(set(listone + listtwo))\nmergedlist = list(set(listone + listtwo))\n",
                "This is quite simple, and I think it was even shown in the tutorial:the tutorial>>> listone = [1,2,3]\n>>> listtwo = [4,5,6]\n>>>\n>>> listone + listtwo\n[1, 2, 3, 4, 5, 6]\n>>> listone = [1,2,3]\n>>> listtwo = [4,5,6]\n>>>\n>>> listone + listtwo\n[1, 2, 3, 4, 5, 6]\n",
                "This question directly asks about joining two lists. However it's pretty high in search even when you are looking for a way of joining many lists (including the case when you joining zero lists).I think the best option is to use list comprehensions:>>> a = [[1,2,3], [4,5,6], [7,8,9]]\n>>> [x for xs in a for x in xs]\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n>>> a = [[1,2,3], [4,5,6], [7,8,9]]\n>>> [x for xs in a for x in xs]\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\nYou can create generators as well:>>> map(str, (x for xs in a for x in xs))\n['1', '2', '3', '4', '5', '6', '7', '8', '9']\n>>> map(str, (x for xs in a for x in xs))\n['1', '2', '3', '4', '5', '6', '7', '8', '9']\nOld AnswerOld AnswerConsider this more generic approach:a = [[1,2,3], [4,5,6], [7,8,9]]\nreduce(lambda c, x: c + x, a, [])\na = [[1,2,3], [4,5,6], [7,8,9]]\nreduce(lambda c, x: c + x, a, [])\nWill output:[1, 2, 3, 4, 5, 6, 7, 8, 9]\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\nNote, this also works correctly when a is [] or [[1,2,3]].a[][[1,2,3]]However, this can be done more efficiently with itertools:itertoolsa = [[1,2,3], [4,5,6], [7,8,9]]\nlist(itertools.chain(*a))\na = [[1,2,3], [4,5,6], [7,8,9]]\nlist(itertools.chain(*a))\nIf you don't need a list, but just an iterable, omit list().listlist()UpdateUpdateAlternative suggested by Patrick Collins in the comments could also work for you:sum(a, [])\nsum(a, [])\n",
                "You could simply use the + or += operator as follows:++=a = [1, 2, 3]\nb = [4, 5, 6]\n\nc = a + b\na = [1, 2, 3]\nb = [4, 5, 6]\n\nc = a + b\nOr:c = []\na = [1, 2, 3]\nb = [4, 5, 6]\n\nc += (a + b)\nc = []\na = [1, 2, 3]\nb = [4, 5, 6]\n\nc += (a + b)\nAlso, if you want the values in the merged list to be unique you can do:c = list(set(a + b))\nc = list(set(a + b))\n",
                "It's worth noting that the itertools.chain function accepts variable number of arguments:itertools.chain>>> l1 = ['a']; l2 = ['b', 'c']; l3 = ['d', 'e', 'f']\n>>> [i for i in itertools.chain(l1, l2)]\n['a', 'b', 'c']\n>>> [i for i in itertools.chain(l1, l2, l3)]\n['a', 'b', 'c', 'd', 'e', 'f']\n>>> l1 = ['a']; l2 = ['b', 'c']; l3 = ['d', 'e', 'f']\n>>> [i for i in itertools.chain(l1, l2)]\n['a', 'b', 'c']\n>>> [i for i in itertools.chain(l1, l2, l3)]\n['a', 'b', 'c', 'd', 'e', 'f']\nIf an iterable (tuple, list, generator, etc.) is the input, the from_iterable class method may be used:from_iterable>>> il = [['a'], ['b', 'c'], ['d', 'e', 'f']]\n>>> [i for i in itertools.chain.from_iterable(il)]\n['a', 'b', 'c', 'd', 'e', 'f']\n>>> il = [['a'], ['b', 'c'], ['d', 'e', 'f']]\n>>> [i for i in itertools.chain.from_iterable(il)]\n['a', 'b', 'c', 'd', 'e', 'f']\n",
                "For cases with a low number of lists you can simply add the lists together or use in-place unpacking (available in Python-3.5+):In [1]: listone = [1, 2, 3] \n   ...: listtwo = [4, 5, 6]                                                                                                                                                                                 \n\nIn [2]: listone + listtwo                                                                                                                                                                                   \nOut[2]: [1, 2, 3, 4, 5, 6]\n                                                                                                                                                                                     \nIn [3]: [*listone, *listtwo]                                                                                                                                                                                \nOut[3]: [1, 2, 3, 4, 5, 6]\nIn [1]: listone = [1, 2, 3] \n   ...: listtwo = [4, 5, 6]                                                                                                                                                                                 \n\nIn [2]: listone + listtwo                                                                                                                                                                                   \nOut[2]: [1, 2, 3, 4, 5, 6]\n                                                                                                                                                                                     \nIn [3]: [*listone, *listtwo]                                                                                                                                                                                \nOut[3]: [1, 2, 3, 4, 5, 6]\nAs a more general way for cases with more number of lists you can use chain.from_iterable()1 function from itertools module. Also, based on this answer this function is the best; or at least a very good way for flatting a nested list as well.chain.from_iterable()1itertoolsthis answerthis>>> l=[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n>>> import itertools\n>>> list(itertools.chain.from_iterable(l))\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n>>> l=[[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n>>> import itertools\n>>> list(itertools.chain.from_iterable(l))\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n1. Note that `chain.from_iterable()` is available in Python 2.6 and later. In other versions, use `chain(*l)`.\n",
                "With Python 3.3+ you can use yield from:yield fromlistone = [1,2,3]\nlisttwo = [4,5,6]\n\ndef merge(l1, l2):\n    yield from l1\n    yield from l2\n\n>>> list(merge(listone, listtwo))\n[1, 2, 3, 4, 5, 6]\nlistone = [1,2,3]\nlisttwo = [4,5,6]\n\ndef merge(l1, l2):\n    yield from l1\n    yield from l2\n\n>>> list(merge(listone, listtwo))\n[1, 2, 3, 4, 5, 6]\nOr, if you want to support an arbitrary number of iterators:def merge(*iters):\n    for it in iters:\n        yield from it\n\n>>> list(merge(listone, listtwo, 'abcd', [20, 21, 22]))\n[1, 2, 3, 4, 5, 6, 'a', 'b', 'c', 'd', 20, 21, 22]\ndef merge(*iters):\n    for it in iters:\n        yield from it\n\n>>> list(merge(listone, listtwo, 'abcd', [20, 21, 22]))\n[1, 2, 3, 4, 5, 6, 'a', 'b', 'c', 'd', 20, 21, 22]\n",
                "If you want to merge the two lists in sorted form, you can use the merge function from the heapq library.mergeheapqfrom heapq import merge\n\na = [1, 2, 4]\nb = [2, 4, 6, 7]\n\nprint list(merge(a, b))\nfrom heapq import merge\n\na = [1, 2, 4]\nb = [2, 4, 6, 7]\n\nprint list(merge(a, b))\n",
                "If you can't use the plus operator (+),  you can use the operator import:+operatorimport operator\n\nlistone = [1,2,3]\nlisttwo = [4,5,6]\n\nresult = operator.add(listone, listtwo)\nprint(result)\n\n>>> [1, 2, 3, 4, 5, 6]\nimport operator\n\nlistone = [1,2,3]\nlisttwo = [4,5,6]\n\nresult = operator.add(listone, listtwo)\nprint(result)\n\n>>> [1, 2, 3, 4, 5, 6]\nAlternatively, you could also use the __add__ dunder function:__add__dunderlistone = [1,2,3]\nlisttwo = [4,5,6]\n\nresult = list.__add__(listone, listtwo)\nprint(result)\n\n>>> [1, 2, 3, 4, 5, 6]\nlistone = [1,2,3]\nlisttwo = [4,5,6]\n\nresult = list.__add__(listone, listtwo)\nprint(result)\n\n>>> [1, 2, 3, 4, 5, 6]\n",
                "If you need to merge two ordered lists with complicated sorting rules, you might have to roll it yourself like in the following code (using a simple sorting rule for readability :-) ).list1 = [1,2,5]\nlist2 = [2,3,4]\nnewlist = []\n\nwhile list1 and list2:\n    if list1[0] == list2[0]:\n        newlist.append(list1.pop(0))\n        list2.pop(0)\n    elif list1[0] < list2[0]:\n        newlist.append(list1.pop(0))\n    else:\n        newlist.append(list2.pop(0))\n\nif list1:\n    newlist.extend(list1)\nif list2:\n    newlist.extend(list2)\n\nassert(newlist == [1, 2, 3, 4, 5])\nlist1 = [1,2,5]\nlist2 = [2,3,4]\nnewlist = []\n\nwhile list1 and list2:\n    if list1[0] == list2[0]:\n        newlist.append(list1.pop(0))\n        list2.pop(0)\n    elif list1[0] < list2[0]:\n        newlist.append(list1.pop(0))\n    else:\n        newlist.append(list2.pop(0))\n\nif list1:\n    newlist.extend(list1)\nif list2:\n    newlist.extend(list2)\n\nassert(newlist == [1, 2, 3, 4, 5])\n",
                "If you are using NumPy, you can concatenate two arrays of compatible dimensions with this command:numpy.concatenate([a,b])\nnumpy.concatenate([a,b])\n",
                "Use a simple list comprehension:joined_list = [item for list_ in [list_one, list_two] for item in list_]\njoined_list = [item for list_ in [list_one, list_two] for item in list_]\nIt has all the advantages of the newest approach of using Additional Unpacking Generalizations - i.e. you can concatenate an arbitrary number of different iterables (for example, lists, tuples, ranges, and generators) that way - and it's not limited to Python 3.5 or later.Additional Unpacking Generalizations",
                "Another way:Another way:>>> listone = [1, 2, 3]\n>>> listtwo = [4, 5, 6]\n>>> joinedlist = [*listone, *listtwo]\n>>> joinedlist\n[1, 2, 3, 4, 5, 6]\n>>> \n>>> listone = [1, 2, 3]\n>>> listtwo = [4, 5, 6]\n>>> joinedlist = [*listone, *listtwo]\n>>> joinedlist\n[1, 2, 3, 4, 5, 6]\n>>> \n",
                "list(set(listone) | set(listtwo))\nlist(set(listone) | set(listtwo))\nThe above code does not preserve order and removes duplicates from each list (but not from the concatenated list).",
                "As already pointed out by many, itertools.chain() is the way to go if one needs to apply exactly the same treatment to both lists. In my case, I had a label and a flag which were different from one list to the other, so I needed something slightly more complex. As it turns out, behind the scenes itertools.chain() simply does the following:itertools.chain()exactly the same treatmentitertools.chain()for it in iterables:\n    for element in it:\n        yield element\nfor it in iterables:\n    for element in it:\n        yield element\n(see https://docs.python.org/2/library/itertools.html), so I took inspiration from here and wrote something along these lines:https://docs.python.org/2/library/itertools.htmlfor iterable, header, flag in ( (newList, 'New', ''), (modList, 'Modified', '-f')):\n    print header + ':'\n    for path in iterable:\n        [...]\n        command = 'cp -r' if os.path.isdir(srcPath) else 'cp'\n        print >> SCRIPT , command, flag, srcPath, mergedDirPath\n        [...]\nfor iterable, header, flag in ( (newList, 'New', ''), (modList, 'Modified', '-f')):\n    print header + ':'\n    for path in iterable:\n        [...]\n        command = 'cp -r' if os.path.isdir(srcPath) else 'cp'\n        print >> SCRIPT , command, flag, srcPath, mergedDirPath\n        [...]\nThe main points to understand here are that lists are just a special case of iterable, which are objects like any other; and that for ... in loops in python can work with tuple variables, so it is simple to loop on multiple variables at the same time. for ... in",
                "You could use the append() method defined on list objects: append()listmergedlist =[]\nfor elem in listone:\n    mergedlist.append(elem)\nfor elem in listtwo:\n    mergedlist.append(elem)\nmergedlist =[]\nfor elem in listone:\n    mergedlist.append(elem)\nfor elem in listtwo:\n    mergedlist.append(elem)\n",
                " a = [1, 2, 3]\n b = [4, 5, 6]\n     \n c = a + b\n print(c)\n a = [1, 2, 3]\n b = [4, 5, 6]\n     \n c = a + b\n print(c)\nOutput>>> [1, 2, 3, 4, 5, 6]\n>>> [1, 2, 3, 4, 5, 6]\nIn the above code, the \"+\" operator is used to concatenate the two lists into a single list.Another solution a = [1, 2, 3]\n b = [4, 5, 6]\n c = [] # Empty list in which we are going to append the values of list (a) and (b)\n\n for i in a:\n     c.append(i)\n for j in b:\n     c.append(j)\n\n print(c)\n a = [1, 2, 3]\n b = [4, 5, 6]\n c = [] # Empty list in which we are going to append the values of list (a) and (b)\n\n for i in a:\n     c.append(i)\n for j in b:\n     c.append(j)\n\n print(c)\nOutput>>> [1, 2, 3, 4, 5, 6]\n>>> [1, 2, 3, 4, 5, 6]\n",
                "I recommend three methods to concatenate the list, but the first method is most recommended,# Easiest and least complexity method <= recommended\n\nlistone = [1, 2, 3]\nlisttwo = [4, 5, 6]\n\nnewlist = listone + listtwo\nprint(newlist)\n\n# Second-easiest method\nnewlist = listone.copy()\nnewlist.extend(listtwo)\nprint(newlist)\n# Easiest and least complexity method <= recommended\n\nlistone = [1, 2, 3]\nlisttwo = [4, 5, 6]\n\nnewlist = listone + listtwo\nprint(newlist)\n\n# Second-easiest method\nnewlist = listone.copy()\nnewlist.extend(listtwo)\nprint(newlist)\nIn the second method, I assign newlist to a copy of the listone, because I don't want to change listone.newlistlistonelistone# Third method\nnewlist = listone.copy()\nfor j in listtwo:\n    newlist.append(j)\n\nprint(newlist)\n# Third method\nnewlist = listone.copy()\nfor j in listtwo:\n    newlist.append(j)\n\nprint(newlist)\nThis is not a good way to concatenate lists because we are using a for loop to concatenate the lists. So time complexity is much higher than with the other two methods.for",
                "The most common method used to concatenate lists are the plus operator and the built-in method append, for example:plus operatorappendlist = [1,2]\n\nlist = list + [3]\n# list = [1,2,3]\n\nlist.append(3)\n# list = [1,2,3]\n\nlist.append([3,4])\n# list = [1,2,[3,4]]\nlist = [1,2]\n\nlist = list + [3]\n# list = [1,2,3]\n\nlist.append(3)\n# list = [1,2,3]\n\nlist.append([3,4])\n# list = [1,2,[3,4]]\nFor most of the cases, this will work, but the append function will not extend a list if one was added. Because that is not expected, you can use another method called extend. It should work with structures:appendextendlist = [1,2]\nlist.extend([3,4])\n# list = [1,2,3,4]\nlist = [1,2]\nlist.extend([3,4])\n# list = [1,2,3,4]\n",
                "A really concise way to combine a list of lists islist_of_lists = [[1,2,3], [4,5,6], [7,8,9]]\nreduce(list.__add__, list_of_lists)\nlist_of_lists = [[1,2,3], [4,5,6], [7,8,9]]\nreduce(list.__add__, list_of_lists)\nwhich gives us[1, 2, 3, 4, 5, 6, 7, 8, 9]\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
                "So there are two easy ways.\nUsing +: It creates a new list from provided lists\nUsing +: It creates a new list from provided listsUsing ++Example:In [1]: a = [1, 2, 3]\n\nIn [2]: b = [4, 5, 6]\n\nIn [3]: a + b\nOut[3]: [1, 2, 3, 4, 5, 6]\n\nIn [4]: %timeit a + b\n10000000 loops, best of 3: 126 ns per loop\nIn [1]: a = [1, 2, 3]\n\nIn [2]: b = [4, 5, 6]\n\nIn [3]: a + b\nOut[3]: [1, 2, 3, 4, 5, 6]\n\nIn [4]: %timeit a + b\n10000000 loops, best of 3: 126 ns per loop\n\nUsing extend: It appends new list to existing list. That means it does not create a separate list.\nUsing extend: It appends new list to existing list. That means it does not create a separate list.Using extendExample:In [1]: a = [1, 2, 3]\n\nIn [2]: b = [4, 5, 6]\n\nIn [3]: %timeit a.extend(b)\n10000000 loops, best of 3: 91.1 ns per loop\nIn [1]: a = [1, 2, 3]\n\nIn [2]: b = [4, 5, 6]\n\nIn [3]: %timeit a.extend(b)\n10000000 loops, best of 3: 91.1 ns per loop\nThus we see that out of two of most popular methods, extend is efficient.extend",
                "You could also just use sum.sum>>> a = [1, 2, 3]\n>>> b = [4, 5, 6]\n>>> sum([a, b], [])\n[1, 2, 3, 4, 5, 6]\n>>>\n>>> a = [1, 2, 3]\n>>> b = [4, 5, 6]\n>>> sum([a, b], [])\n[1, 2, 3, 4, 5, 6]\n>>>\nThis works for any length and any element type of list:>>> a = ['a', 'b', 'c', 'd']\n>>> b = [1, 2, 3, 4]\n>>> c = [1, 2]\n>>> sum([a, b, c], [])\n['a', 'b', 'c', 'd', 1, 2, 3, 4, 1, 2]\n>>>\n>>> a = ['a', 'b', 'c', 'd']\n>>> b = [1, 2, 3, 4]\n>>> c = [1, 2]\n>>> sum([a, b, c], [])\n['a', 'b', 'c', 'd', 1, 2, 3, 4, 1, 2]\n>>>\nThe reason I add [], is because the start argument is set to 0 by default, so it loops through the list and adds to start, but 0 + [1, 2, 3] would give an error, so if we set the start to []. It would add to [], and [] + [1, 2, 3] would work as expected.[]start0start0 + [1, 2, 3]start[][][] + [1, 2, 3]",
                "I assume you want one of the two methods:Keep duplicate elementsKeep duplicate elementsIt is very easy. Just concatenate like a string:def concat_list(l1,l2):\n    l3 = l1+l2\n    return l3\ndef concat_list(l1,l2):\n    l3 = l1+l2\n    return l3\nNext, if you want to eliminate duplicate elementsNext, if you want to eliminate duplicate elementsdef concat_list(l1,l2):\n   l3 = []\n   for i in [l1,l2]:\n     for j in i:\n       if j not in l3:\n         # Check if element exists in final list, if no then add element to list\n         l3.append(j)\n   return l3\ndef concat_list(l1,l2):\n   l3 = []\n   for i in [l1,l2]:\n     for j in i:\n       if j not in l3:\n         # Check if element exists in final list, if no then add element to list\n         l3.append(j)\n   return l3\n",
                "The solutions provided are for a single list. In case there are lists within a list and the merging of corresponding lists is required, the \"+\" operation through a for loop does the work.fora = [[1,2,3], [4,5,6]]\n\nb = [[0,1,2], [7,8,9]]\n\nfor i in range(len(a)):\n    cc.append(a[i] + b[i])\na = [[1,2,3], [4,5,6]]\n\nb = [[0,1,2], [7,8,9]]\n\nfor i in range(len(a)):\n    cc.append(a[i] + b[i])\nOutput: [[1, 2, 3, 0, 1, 2], [4, 5, 6, 7, 8, 9]]",
                "All the possible ways to join lists that I could findimport itertools\n\nA = [1,3,5,7,9] + [2,4,6,8,10]\n\nB = [1,3,5,7,9]\nB.append([2,4,6,8,10])\n\nC = [1,3,5,7,9]\nC.extend([2,4,6,8,10])\n\nD = list(zip([1,3,5,7,9],[2,4,6,8,10]))\nE = [1,3,5,7,9]+[2,4,6,8,10]\nF = list(set([1,3,5,7,9] + [2,4,6,8,10]))\n\nG = []\nfor a in itertools.chain([1,3,5,7,9], [2,4,6,8,10]):\n    G.append(a)\n\n\nprint(\"A: \" + str(A))\nprint(\"B: \" + str(B))\nprint(\"C: \" + str(C))\nprint(\"D: \" + str(D))\nprint(\"E: \" + str(E))\nprint(\"F: \" + str(F))\nprint(\"G: \" + str(G))\nimport itertools\n\nA = [1,3,5,7,9] + [2,4,6,8,10]\n\nB = [1,3,5,7,9]\nB.append([2,4,6,8,10])\n\nC = [1,3,5,7,9]\nC.extend([2,4,6,8,10])\n\nD = list(zip([1,3,5,7,9],[2,4,6,8,10]))\nE = [1,3,5,7,9]+[2,4,6,8,10]\nF = list(set([1,3,5,7,9] + [2,4,6,8,10]))\n\nG = []\nfor a in itertools.chain([1,3,5,7,9], [2,4,6,8,10]):\n    G.append(a)\n\n\nprint(\"A: \" + str(A))\nprint(\"B: \" + str(B))\nprint(\"C: \" + str(C))\nprint(\"D: \" + str(D))\nprint(\"E: \" + str(E))\nprint(\"F: \" + str(F))\nprint(\"G: \" + str(G))\nOutputA: [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\nB: [1, 3, 5, 7, 9, [2, 4, 6, 8, 10]]\nC: [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\nD: [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)]\nE: [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\nF: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nG: [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\nA: [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\nB: [1, 3, 5, 7, 9, [2, 4, 6, 8, 10]]\nC: [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\nD: [(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)]\nE: [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\nF: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nG: [1, 3, 5, 7, 9, 2, 4, 6, 8, 10]\n"
            ]
        },
        {
            "tag": "select_rows",
            "patterns": [
                "How do I select rows from a DataFrame based on column values?",
                "how do i select columns from a dataframe?",
                "how do i select rows from the dataframe based on the values of column values?",
                "how do i select a row in a dataframe based on a column?",
                "how do i choose a row from dataframes based on column values?",
                "how do i select rows in a dataframe based on columns?",
                "how can you select rows from a dataframe based on the columns values?",
                "how can i select rows from a dataframe based on column values?",
                "how do i select rows from a dataframe based on column values?"
            ],
            "responses": [
                "To select rows whose column value equals a scalar, some_value, use ==:some_value==df.loc[df['column_name'] == some_value]\ndf.loc[df['column_name'] == some_value]\nTo select rows whose column value is in an iterable, some_values, use isin:some_valuesisindf.loc[df['column_name'].isin(some_values)]\ndf.loc[df['column_name'].isin(some_values)]\nCombine multiple conditions with &: &df.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]\ndf.loc[(df['column_name'] >= A) & (df['column_name'] <= B)]\nNote the parentheses. Due to Python's operator precedence rules, & binds more tightly than <= and >=. Thus, the parentheses in the last example are necessary. Without the parentheses operator precedence rules&<=>=df['column_name'] >= A & df['column_name'] <= B\ndf['column_name'] >= A & df['column_name'] <= B\nis parsed as df['column_name'] >= (A & df['column_name']) <= B\ndf['column_name'] >= (A & df['column_name']) <= B\nwhich results in a Truth value of a Series is ambiguous error.Truth value of a Series is ambiguous errorTo select rows whose column value does not equal some_value, use !=:does not equalsome_value!=df.loc[df['column_name'] != some_value]\ndf.loc[df['column_name'] != some_value]\nisin returns a boolean Series, so to select rows whose value is not in some_values, negate the boolean Series using ~:isinnotsome_values~df.loc[~df['column_name'].isin(some_values)]\ndf.loc[~df['column_name'].isin(some_values)]\nFor example,import pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split(),\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n#      A      B  C   D\n# 0  foo    one  0   0\n# 1  bar    one  1   2\n# 2  foo    two  2   4\n# 3  bar  three  3   6\n# 4  foo    two  4   8\n# 5  bar    two  5  10\n# 6  foo    one  6  12\n# 7  foo  three  7  14\n\nprint(df.loc[df['A'] == 'foo'])\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split(),\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\nprint(df)\n#      A      B  C   D\n# 0  foo    one  0   0\n# 1  bar    one  1   2\n# 2  foo    two  2   4\n# 3  bar  three  3   6\n# 4  foo    two  4   8\n# 5  bar    two  5  10\n# 6  foo    one  6  12\n# 7  foo  three  7  14\n\nprint(df.loc[df['A'] == 'foo'])\nyields     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\nIf you have multiple values you want to include, put them in a\nlist (or more generally, any iterable) and use isin:isinprint(df.loc[df['B'].isin(['one','three'])])\nprint(df.loc[df['B'].isin(['one','three'])])\nyields     A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\n     A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\nNote, however, that if you wish to do this many times, it is more efficient to\nmake an index first, and then use df.loc:df.locdf = df.set_index(['B'])\nprint(df.loc['one'])\ndf = df.set_index(['B'])\nprint(df.loc['one'])\nyields       A  C   D\nB              \none  foo  0   0\none  bar  1   2\none  foo  6  12\n       A  C   D\nB              \none  foo  0   0\none  bar  1   2\none  foo  6  12\nor, to include multiple values from the index use df.index.isin:df.index.isindf.loc[df.index.isin(['one','two'])]\ndf.loc[df.index.isin(['one','two'])]\nyields       A  C   D\nB              \none  foo  0   0\none  bar  1   2\ntwo  foo  2   4\ntwo  foo  4   8\ntwo  bar  5  10\none  foo  6  12\n       A  C   D\nB              \none  foo  0   0\none  bar  1   2\ntwo  foo  2   4\ntwo  foo  4   8\ntwo  bar  5  10\none  foo  6  12\n",
                "There are several ways to select rows from a Pandas dataframe:\nBoolean indexing (df[df['col'] == value] )\nPositional indexing (df.iloc[...])\nLabel indexing (df.xs(...))\ndf.query(...) API\nBoolean indexing (df[df['col'] == value] )Boolean indexing (df[df['col'] == value] )df[df['col'] == valuePositional indexing (df.iloc[...])Positional indexing (df.iloc[...])df.iloc[...]Label indexing (df.xs(...))Label indexing (df.xs(...))df.xs(...)df.query(...) APIdf.query(...) APIdf.query(...)Below I show you examples of each, with advice when to use certain techniques. Assume our criterion is column 'A' == 'foo''A''foo'(Note on performance: For each base type, we can keep things simple by using the Pandas API or we can venture outside the API, usually into NumPy, and speed things up.)\nSetup\nThe first thing we'll need is to identify a condition that will act as our criterion for selecting rows. We'll start with the OP's case column_name == some_value, and include some other common use cases.\nBorrowing from @unutbu:\nimport pandas as pd, numpy as np\n\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split(),\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\n\n\n1. Boolean indexing\n... Boolean indexing requires finding the true value of each row's 'A' column being equal to 'foo', then using those truth values to identify which rows to keep.  Typically, we'd name this series, an array of truth values, mask.  We'll do so here as well.\nmask = df['A'] == 'foo'\n\nWe can then use this mask to slice or index the data frame\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\nThis is one of the simplest ways to accomplish this task and if performance or intuitiveness isn't an issue, this should be your chosen method.  However, if performance is a concern, then you might want to consider an alternative way of creating the mask.\n\n2. Positional indexing\nPositional indexing (df.iloc[...]) has its use cases, but this isn't one of them.  In order to identify where to slice, we first need to perform the same boolean analysis we did above.  This leaves us performing one extra step to accomplish the same task.\nmask = df['A'] == 'foo'\npos = np.flatnonzero(mask)\ndf.iloc[pos]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n3. Label indexing\nLabel indexing can be very handy, but in this case, we are again doing more work for no benefit\ndf.set_index('A', append=True, drop=False).xs('foo', level=1)\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n4. df.query() API\npd.DataFrame.query is a very elegant/intuitive way to perform this task, but is often slower. However, if you pay attention to the timings below, for large data, the query is very efficient. More so than the standard approach and of similar magnitude as my best suggestion.\ndf.query('A == \"foo\"')\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n\nMy preference is to use the Boolean mask\nActual improvements can be made by modifying how we create our Boolean mask.\nmask alternative 1\nUse the underlying NumPy array and forgo the overhead of creating another pd.Series\nmask = df['A'].values == 'foo'\n\nI'll show more complete time tests at the end, but just take a look at the performance gains we get using the sample data frame.  First, we look at the difference in creating the mask\n%timeit mask = df['A'].values == 'foo'\n%timeit mask = df['A'] == 'foo'\n\n5.84 \u00b5s \u00b1 195 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n166 \u00b5s \u00b1 4.45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nEvaluating the mask with the NumPy array is ~ 30 times faster.  This is partly due to NumPy evaluation often being faster. It is also partly due to the lack of overhead necessary to build an index and a corresponding pd.Series object.\nNext, we'll look at the timing for slicing with one mask versus the other.\nmask = df['A'].values == 'foo'\n%timeit df[mask]\nmask = df['A'] == 'foo'\n%timeit df[mask]\n\n219 \u00b5s \u00b1 12.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n239 \u00b5s \u00b1 7.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nThe performance gains aren't as pronounced.  We'll see if this holds up over more robust testing.\n\nmask alternative 2\nWe could have reconstructed the data frame as well.  There is a big caveat when reconstructing a dataframe\u2014you must take care of the dtypes when doing so!\nInstead of df[mask] we will do this\npd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\n\nIf the data frame is of mixed type, which our example is, then when we get df.values the resulting array is of dtype object and consequently, all columns of the new data frame will be of dtype object.  Thus requiring the astype(df.dtypes) and killing any potential performance gains.\n%timeit df[m]\n%timeit pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\n\n216 \u00b5s \u00b1 10.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.43 ms \u00b1 39.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nHowever, if the data frame is not of mixed type, this is a very useful way to do it.\nGiven\nnp.random.seed([3,1415])\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\n\nd1\n\n   A  B  C  D  E\n0  0  2  7  3  8\n1  7  0  6  8  6\n2  0  2  0  4  9\n3  7  3  2  4  3\n4  3  6  7  7  4\n5  5  3  7  5  9\n6  8  7  6  4  7\n7  6  2  6  6  5\n8  2  8  7  5  8\n9  4  7  6  1  5\n\n\n%%timeit\nmask = d1['A'].values == 7\nd1[mask]\n\n179 \u00b5s \u00b1 8.73 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nVersus\n%%timeit\nmask = d1['A'].values == 7\npd.DataFrame(d1.values[mask], d1.index[mask], d1.columns)\n\n87 \u00b5s \u00b1 5.12 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nWe cut the time in half.\n\nmask alternative 3\n@unutbu also shows us how to use pd.Series.isin to account for each element of df['A'] being in a set of values.  This evaluates to the same thing if our set of values is a set of one value, namely 'foo'.  But it also generalizes to include larger sets of values if needed.  Turns out, this is still pretty fast even though it is a more general solution.  The only real loss is in intuitiveness for those not familiar with the concept.\nmask = df['A'].isin(['foo'])\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\nHowever, as before, we can utilize NumPy to improve performance while sacrificing virtually nothing. We'll use np.in1d\nmask = np.in1d(df['A'].values, ['foo'])\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n\nTiming\nI'll include other concepts mentioned in other posts as well for reference.\nCode Below\nEach column in this table represents a different length data frame over which we test each function. Each column shows relative time taken, with the fastest function given a base index of 1.0.\nres.div(res.min())\n\n                         10        30        100       300       1000      3000      10000     30000\nmask_standard         2.156872  1.850663  2.034149  2.166312  2.164541  3.090372  2.981326  3.131151\nmask_standard_loc     1.879035  1.782366  1.988823  2.338112  2.361391  3.036131  2.998112  2.990103\nmask_with_values      1.010166  1.000000  1.005113  1.026363  1.028698  1.293741  1.007824  1.016919\nmask_with_values_loc  1.196843  1.300228  1.000000  1.000000  1.038989  1.219233  1.037020  1.000000\nquery                 4.997304  4.765554  5.934096  4.500559  2.997924  2.397013  1.680447  1.398190\nxs_label              4.124597  4.272363  5.596152  4.295331  4.676591  5.710680  6.032809  8.950255\nmask_with_isin        1.674055  1.679935  1.847972  1.724183  1.345111  1.405231  1.253554  1.264760\nmask_with_in1d        1.000000  1.083807  1.220493  1.101929  1.000000  1.000000  1.000000  1.144175\n\nYou'll notice that the fastest times seem to be shared between mask_with_values and mask_with_in1d.\nres.T.plot(loglog=True)\n\n\nFunctions\ndef mask_standard(df):\n    mask = df['A'] == 'foo'\n    return df[mask]\n\ndef mask_standard_loc(df):\n    mask = df['A'] == 'foo'\n    return df.loc[mask]\n\ndef mask_with_values(df):\n    mask = df['A'].values == 'foo'\n    return df[mask]\n\ndef mask_with_values_loc(df):\n    mask = df['A'].values == 'foo'\n    return df.loc[mask]\n\ndef query(df):\n    return df.query('A == \"foo\"')\n\ndef xs_label(df):\n    return df.set_index('A', append=True, drop=False).xs('foo', level=-1)\n\ndef mask_with_isin(df):\n    mask = df['A'].isin(['foo'])\n    return df[mask]\n\ndef mask_with_in1d(df):\n    mask = np.in1d(df['A'].values, ['foo'])\n    return df[mask]\n\n\nTesting\nres = pd.DataFrame(\n    index=[\n        'mask_standard', 'mask_standard_loc', 'mask_with_values', 'mask_with_values_loc',\n        'query', 'xs_label', 'mask_with_isin', 'mask_with_in1d'\n    ],\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    dtype=float\n)\n\nfor j in res.columns:\n    d = pd.concat([df] * j, ignore_index=True)\n    for i in res.index:a\n        stmt = '{}(d)'.format(i)\n        setp = 'from __main__ import d, {}'.format(i)\n        res.at[i, j] = timeit(stmt, setp, number=50)\n\n\nSpecial Timing\nLooking at the special case when we have a single non-object dtype for the entire data frame.\nCode Below\nspec.div(spec.min())\n\n                     10        30        100       300       1000      3000      10000     30000\nmask_with_values  1.009030  1.000000  1.194276  1.000000  1.236892  1.095343  1.000000  1.000000\nmask_with_in1d    1.104638  1.094524  1.156930  1.072094  1.000000  1.000000  1.040043  1.027100\nreconstruct       1.000000  1.142838  1.000000  1.355440  1.650270  2.222181  2.294913  3.406735\n\nTurns out, reconstruction isn't worth it past a few hundred rows.\nspec.T.plot(loglog=True)\n\n\nFunctions\nnp.random.seed([3,1415])\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\n\ndef mask_with_values(df):\n    mask = df['A'].values == 'foo'\n    return df[mask]\n\ndef mask_with_in1d(df):\n    mask = np.in1d(df['A'].values, ['foo'])\n    return df[mask]\n\ndef reconstruct(df):\n    v = df.values\n    mask = np.in1d(df['A'].values, ['foo'])\n    return pd.DataFrame(v[mask], df.index[mask], df.columns)\n\nspec = pd.DataFrame(\n    index=['mask_with_values', 'mask_with_in1d', 'reconstruct'],\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    dtype=float\n)\n\nTesting\nfor j in spec.columns:\n    d = pd.concat([df] * j, ignore_index=True)\n    for i in spec.index:\n        stmt = '{}(d)'.format(i)\n        setp = 'from __main__ import d, {}'.format(i)\n        spec.at[i, j] = timeit(stmt, setp, number=50)\n\nSetupSetupThe first thing we'll need is to identify a condition that will act as our criterion for selecting rows. We'll start with the OP's case column_name == some_value, and include some other common use cases.column_name == some_valueBorrowing from @unutbu:import pandas as pd, numpy as np\n\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split(),\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\nimport pandas as pd, numpy as np\n\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split(),\n                   'C': np.arange(8), 'D': np.arange(8) * 2})\n\n1. Boolean indexing\n... Boolean indexing requires finding the true value of each row's 'A' column being equal to 'foo', then using those truth values to identify which rows to keep.  Typically, we'd name this series, an array of truth values, mask.  We'll do so here as well.\nmask = df['A'] == 'foo'\n\nWe can then use this mask to slice or index the data frame\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\nThis is one of the simplest ways to accomplish this task and if performance or intuitiveness isn't an issue, this should be your chosen method.  However, if performance is a concern, then you might want to consider an alternative way of creating the mask.\n\n2. Positional indexing\nPositional indexing (df.iloc[...]) has its use cases, but this isn't one of them.  In order to identify where to slice, we first need to perform the same boolean analysis we did above.  This leaves us performing one extra step to accomplish the same task.\nmask = df['A'] == 'foo'\npos = np.flatnonzero(mask)\ndf.iloc[pos]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n3. Label indexing\nLabel indexing can be very handy, but in this case, we are again doing more work for no benefit\ndf.set_index('A', append=True, drop=False).xs('foo', level=1)\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n4. df.query() API\npd.DataFrame.query is a very elegant/intuitive way to perform this task, but is often slower. However, if you pay attention to the timings below, for large data, the query is very efficient. More so than the standard approach and of similar magnitude as my best suggestion.\ndf.query('A == \"foo\"')\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n\nMy preference is to use the Boolean mask\nActual improvements can be made by modifying how we create our Boolean mask.\nmask alternative 1\nUse the underlying NumPy array and forgo the overhead of creating another pd.Series\nmask = df['A'].values == 'foo'\n\nI'll show more complete time tests at the end, but just take a look at the performance gains we get using the sample data frame.  First, we look at the difference in creating the mask\n%timeit mask = df['A'].values == 'foo'\n%timeit mask = df['A'] == 'foo'\n\n5.84 \u00b5s \u00b1 195 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n166 \u00b5s \u00b1 4.45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nEvaluating the mask with the NumPy array is ~ 30 times faster.  This is partly due to NumPy evaluation often being faster. It is also partly due to the lack of overhead necessary to build an index and a corresponding pd.Series object.\nNext, we'll look at the timing for slicing with one mask versus the other.\nmask = df['A'].values == 'foo'\n%timeit df[mask]\nmask = df['A'] == 'foo'\n%timeit df[mask]\n\n219 \u00b5s \u00b1 12.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n239 \u00b5s \u00b1 7.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nThe performance gains aren't as pronounced.  We'll see if this holds up over more robust testing.\n\nmask alternative 2\nWe could have reconstructed the data frame as well.  There is a big caveat when reconstructing a dataframe\u2014you must take care of the dtypes when doing so!\nInstead of df[mask] we will do this\npd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\n\nIf the data frame is of mixed type, which our example is, then when we get df.values the resulting array is of dtype object and consequently, all columns of the new data frame will be of dtype object.  Thus requiring the astype(df.dtypes) and killing any potential performance gains.\n%timeit df[m]\n%timeit pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\n\n216 \u00b5s \u00b1 10.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.43 ms \u00b1 39.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nHowever, if the data frame is not of mixed type, this is a very useful way to do it.\nGiven\nnp.random.seed([3,1415])\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\n\nd1\n\n   A  B  C  D  E\n0  0  2  7  3  8\n1  7  0  6  8  6\n2  0  2  0  4  9\n3  7  3  2  4  3\n4  3  6  7  7  4\n5  5  3  7  5  9\n6  8  7  6  4  7\n7  6  2  6  6  5\n8  2  8  7  5  8\n9  4  7  6  1  5\n\n\n%%timeit\nmask = d1['A'].values == 7\nd1[mask]\n\n179 \u00b5s \u00b1 8.73 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nVersus\n%%timeit\nmask = d1['A'].values == 7\npd.DataFrame(d1.values[mask], d1.index[mask], d1.columns)\n\n87 \u00b5s \u00b1 5.12 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nWe cut the time in half.\n\nmask alternative 3\n@unutbu also shows us how to use pd.Series.isin to account for each element of df['A'] being in a set of values.  This evaluates to the same thing if our set of values is a set of one value, namely 'foo'.  But it also generalizes to include larger sets of values if needed.  Turns out, this is still pretty fast even though it is a more general solution.  The only real loss is in intuitiveness for those not familiar with the concept.\nmask = df['A'].isin(['foo'])\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\nHowever, as before, we can utilize NumPy to improve performance while sacrificing virtually nothing. We'll use np.in1d\nmask = np.in1d(df['A'].values, ['foo'])\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n\nTiming\nI'll include other concepts mentioned in other posts as well for reference.\nCode Below\nEach column in this table represents a different length data frame over which we test each function. Each column shows relative time taken, with the fastest function given a base index of 1.0.\nres.div(res.min())\n\n                         10        30        100       300       1000      3000      10000     30000\nmask_standard         2.156872  1.850663  2.034149  2.166312  2.164541  3.090372  2.981326  3.131151\nmask_standard_loc     1.879035  1.782366  1.988823  2.338112  2.361391  3.036131  2.998112  2.990103\nmask_with_values      1.010166  1.000000  1.005113  1.026363  1.028698  1.293741  1.007824  1.016919\nmask_with_values_loc  1.196843  1.300228  1.000000  1.000000  1.038989  1.219233  1.037020  1.000000\nquery                 4.997304  4.765554  5.934096  4.500559  2.997924  2.397013  1.680447  1.398190\nxs_label              4.124597  4.272363  5.596152  4.295331  4.676591  5.710680  6.032809  8.950255\nmask_with_isin        1.674055  1.679935  1.847972  1.724183  1.345111  1.405231  1.253554  1.264760\nmask_with_in1d        1.000000  1.083807  1.220493  1.101929  1.000000  1.000000  1.000000  1.144175\n\nYou'll notice that the fastest times seem to be shared between mask_with_values and mask_with_in1d.\nres.T.plot(loglog=True)\n\n\nFunctions\ndef mask_standard(df):\n    mask = df['A'] == 'foo'\n    return df[mask]\n\ndef mask_standard_loc(df):\n    mask = df['A'] == 'foo'\n    return df.loc[mask]\n\ndef mask_with_values(df):\n    mask = df['A'].values == 'foo'\n    return df[mask]\n\ndef mask_with_values_loc(df):\n    mask = df['A'].values == 'foo'\n    return df.loc[mask]\n\ndef query(df):\n    return df.query('A == \"foo\"')\n\ndef xs_label(df):\n    return df.set_index('A', append=True, drop=False).xs('foo', level=-1)\n\ndef mask_with_isin(df):\n    mask = df['A'].isin(['foo'])\n    return df[mask]\n\ndef mask_with_in1d(df):\n    mask = np.in1d(df['A'].values, ['foo'])\n    return df[mask]\n\n\nTesting\nres = pd.DataFrame(\n    index=[\n        'mask_standard', 'mask_standard_loc', 'mask_with_values', 'mask_with_values_loc',\n        'query', 'xs_label', 'mask_with_isin', 'mask_with_in1d'\n    ],\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    dtype=float\n)\n\nfor j in res.columns:\n    d = pd.concat([df] * j, ignore_index=True)\n    for i in res.index:a\n        stmt = '{}(d)'.format(i)\n        setp = 'from __main__ import d, {}'.format(i)\n        res.at[i, j] = timeit(stmt, setp, number=50)\n\n\nSpecial Timing\nLooking at the special case when we have a single non-object dtype for the entire data frame.\nCode Below\nspec.div(spec.min())\n\n                     10        30        100       300       1000      3000      10000     30000\nmask_with_values  1.009030  1.000000  1.194276  1.000000  1.236892  1.095343  1.000000  1.000000\nmask_with_in1d    1.104638  1.094524  1.156930  1.072094  1.000000  1.000000  1.040043  1.027100\nreconstruct       1.000000  1.142838  1.000000  1.355440  1.650270  2.222181  2.294913  3.406735\n\nTurns out, reconstruction isn't worth it past a few hundred rows.\nspec.T.plot(loglog=True)\n\n\nFunctions\nnp.random.seed([3,1415])\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\n\ndef mask_with_values(df):\n    mask = df['A'].values == 'foo'\n    return df[mask]\n\ndef mask_with_in1d(df):\n    mask = np.in1d(df['A'].values, ['foo'])\n    return df[mask]\n\ndef reconstruct(df):\n    v = df.values\n    mask = np.in1d(df['A'].values, ['foo'])\n    return pd.DataFrame(v[mask], df.index[mask], df.columns)\n\nspec = pd.DataFrame(\n    index=['mask_with_values', 'mask_with_in1d', 'reconstruct'],\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    dtype=float\n)\n\nTesting\nfor j in spec.columns:\n    d = pd.concat([df] * j, ignore_index=True)\n    for i in spec.index:\n        stmt = '{}(d)'.format(i)\n        setp = 'from __main__ import d, {}'.format(i)\n        spec.at[i, j] = timeit(stmt, setp, number=50)\n\n1. Boolean indexing1. Boolean indexing... Boolean indexing requires finding the true value of each row's 'A' column being equal to 'foo', then using those truth values to identify which rows to keep.  Typically, we'd name this series, an array of truth values, mask.  We'll do so here as well.'A''foo'maskmask = df['A'] == 'foo'\nmask = df['A'] == 'foo'\nWe can then use this mask to slice or index the data framedf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\nThis is one of the simplest ways to accomplish this task and if performance or intuitiveness isn't an issue, this should be your chosen method.  However, if performance is a concern, then you might want to consider an alternative way of creating the mask.mask\n2. Positional indexing\nPositional indexing (df.iloc[...]) has its use cases, but this isn't one of them.  In order to identify where to slice, we first need to perform the same boolean analysis we did above.  This leaves us performing one extra step to accomplish the same task.\nmask = df['A'] == 'foo'\npos = np.flatnonzero(mask)\ndf.iloc[pos]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n3. Label indexing\nLabel indexing can be very handy, but in this case, we are again doing more work for no benefit\ndf.set_index('A', append=True, drop=False).xs('foo', level=1)\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n4. df.query() API\npd.DataFrame.query is a very elegant/intuitive way to perform this task, but is often slower. However, if you pay attention to the timings below, for large data, the query is very efficient. More so than the standard approach and of similar magnitude as my best suggestion.\ndf.query('A == \"foo\"')\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n\nMy preference is to use the Boolean mask\nActual improvements can be made by modifying how we create our Boolean mask.\nmask alternative 1\nUse the underlying NumPy array and forgo the overhead of creating another pd.Series\nmask = df['A'].values == 'foo'\n\nI'll show more complete time tests at the end, but just take a look at the performance gains we get using the sample data frame.  First, we look at the difference in creating the mask\n%timeit mask = df['A'].values == 'foo'\n%timeit mask = df['A'] == 'foo'\n\n5.84 \u00b5s \u00b1 195 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n166 \u00b5s \u00b1 4.45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nEvaluating the mask with the NumPy array is ~ 30 times faster.  This is partly due to NumPy evaluation often being faster. It is also partly due to the lack of overhead necessary to build an index and a corresponding pd.Series object.\nNext, we'll look at the timing for slicing with one mask versus the other.\nmask = df['A'].values == 'foo'\n%timeit df[mask]\nmask = df['A'] == 'foo'\n%timeit df[mask]\n\n219 \u00b5s \u00b1 12.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n239 \u00b5s \u00b1 7.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nThe performance gains aren't as pronounced.  We'll see if this holds up over more robust testing.\n\nmask alternative 2\nWe could have reconstructed the data frame as well.  There is a big caveat when reconstructing a dataframe\u2014you must take care of the dtypes when doing so!\nInstead of df[mask] we will do this\npd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\n\nIf the data frame is of mixed type, which our example is, then when we get df.values the resulting array is of dtype object and consequently, all columns of the new data frame will be of dtype object.  Thus requiring the astype(df.dtypes) and killing any potential performance gains.\n%timeit df[m]\n%timeit pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\n\n216 \u00b5s \u00b1 10.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.43 ms \u00b1 39.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nHowever, if the data frame is not of mixed type, this is a very useful way to do it.\nGiven\nnp.random.seed([3,1415])\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\n\nd1\n\n   A  B  C  D  E\n0  0  2  7  3  8\n1  7  0  6  8  6\n2  0  2  0  4  9\n3  7  3  2  4  3\n4  3  6  7  7  4\n5  5  3  7  5  9\n6  8  7  6  4  7\n7  6  2  6  6  5\n8  2  8  7  5  8\n9  4  7  6  1  5\n\n\n%%timeit\nmask = d1['A'].values == 7\nd1[mask]\n\n179 \u00b5s \u00b1 8.73 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nVersus\n%%timeit\nmask = d1['A'].values == 7\npd.DataFrame(d1.values[mask], d1.index[mask], d1.columns)\n\n87 \u00b5s \u00b1 5.12 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nWe cut the time in half.\n\nmask alternative 3\n@unutbu also shows us how to use pd.Series.isin to account for each element of df['A'] being in a set of values.  This evaluates to the same thing if our set of values is a set of one value, namely 'foo'.  But it also generalizes to include larger sets of values if needed.  Turns out, this is still pretty fast even though it is a more general solution.  The only real loss is in intuitiveness for those not familiar with the concept.\nmask = df['A'].isin(['foo'])\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\nHowever, as before, we can utilize NumPy to improve performance while sacrificing virtually nothing. We'll use np.in1d\nmask = np.in1d(df['A'].values, ['foo'])\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n\nTiming\nI'll include other concepts mentioned in other posts as well for reference.\nCode Below\nEach column in this table represents a different length data frame over which we test each function. Each column shows relative time taken, with the fastest function given a base index of 1.0.\nres.div(res.min())\n\n                         10        30        100       300       1000      3000      10000     30000\nmask_standard         2.156872  1.850663  2.034149  2.166312  2.164541  3.090372  2.981326  3.131151\nmask_standard_loc     1.879035  1.782366  1.988823  2.338112  2.361391  3.036131  2.998112  2.990103\nmask_with_values      1.010166  1.000000  1.005113  1.026363  1.028698  1.293741  1.007824  1.016919\nmask_with_values_loc  1.196843  1.300228  1.000000  1.000000  1.038989  1.219233  1.037020  1.000000\nquery                 4.997304  4.765554  5.934096  4.500559  2.997924  2.397013  1.680447  1.398190\nxs_label              4.124597  4.272363  5.596152  4.295331  4.676591  5.710680  6.032809  8.950255\nmask_with_isin        1.674055  1.679935  1.847972  1.724183  1.345111  1.405231  1.253554  1.264760\nmask_with_in1d        1.000000  1.083807  1.220493  1.101929  1.000000  1.000000  1.000000  1.144175\n\nYou'll notice that the fastest times seem to be shared between mask_with_values and mask_with_in1d.\nres.T.plot(loglog=True)\n\n\nFunctions\ndef mask_standard(df):\n    mask = df['A'] == 'foo'\n    return df[mask]\n\ndef mask_standard_loc(df):\n    mask = df['A'] == 'foo'\n    return df.loc[mask]\n\ndef mask_with_values(df):\n    mask = df['A'].values == 'foo'\n    return df[mask]\n\ndef mask_with_values_loc(df):\n    mask = df['A'].values == 'foo'\n    return df.loc[mask]\n\ndef query(df):\n    return df.query('A == \"foo\"')\n\ndef xs_label(df):\n    return df.set_index('A', append=True, drop=False).xs('foo', level=-1)\n\ndef mask_with_isin(df):\n    mask = df['A'].isin(['foo'])\n    return df[mask]\n\ndef mask_with_in1d(df):\n    mask = np.in1d(df['A'].values, ['foo'])\n    return df[mask]\n\n\nTesting\nres = pd.DataFrame(\n    index=[\n        'mask_standard', 'mask_standard_loc', 'mask_with_values', 'mask_with_values_loc',\n        'query', 'xs_label', 'mask_with_isin', 'mask_with_in1d'\n    ],\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    dtype=float\n)\n\nfor j in res.columns:\n    d = pd.concat([df] * j, ignore_index=True)\n    for i in res.index:a\n        stmt = '{}(d)'.format(i)\n        setp = 'from __main__ import d, {}'.format(i)\n        res.at[i, j] = timeit(stmt, setp, number=50)\n\n\nSpecial Timing\nLooking at the special case when we have a single non-object dtype for the entire data frame.\nCode Below\nspec.div(spec.min())\n\n                     10        30        100       300       1000      3000      10000     30000\nmask_with_values  1.009030  1.000000  1.194276  1.000000  1.236892  1.095343  1.000000  1.000000\nmask_with_in1d    1.104638  1.094524  1.156930  1.072094  1.000000  1.000000  1.040043  1.027100\nreconstruct       1.000000  1.142838  1.000000  1.355440  1.650270  2.222181  2.294913  3.406735\n\nTurns out, reconstruction isn't worth it past a few hundred rows.\nspec.T.plot(loglog=True)\n\n\nFunctions\nnp.random.seed([3,1415])\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\n\ndef mask_with_values(df):\n    mask = df['A'].values == 'foo'\n    return df[mask]\n\ndef mask_with_in1d(df):\n    mask = np.in1d(df['A'].values, ['foo'])\n    return df[mask]\n\ndef reconstruct(df):\n    v = df.values\n    mask = np.in1d(df['A'].values, ['foo'])\n    return pd.DataFrame(v[mask], df.index[mask], df.columns)\n\nspec = pd.DataFrame(\n    index=['mask_with_values', 'mask_with_in1d', 'reconstruct'],\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    dtype=float\n)\n\nTesting\nfor j in spec.columns:\n    d = pd.concat([df] * j, ignore_index=True)\n    for i in spec.index:\n        stmt = '{}(d)'.format(i)\n        setp = 'from __main__ import d, {}'.format(i)\n        spec.at[i, j] = timeit(stmt, setp, number=50)\n\n2. Positional indexing2. Positional indexingPositional indexing (df.iloc[...]) has its use cases, but this isn't one of them.  In order to identify where to slice, we first need to perform the same boolean analysis we did above.  This leaves us performing one extra step to accomplish the same task.df.iloc[...]mask = df['A'] == 'foo'\npos = np.flatnonzero(mask)\ndf.iloc[pos]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\nmask = df['A'] == 'foo'\npos = np.flatnonzero(mask)\ndf.iloc[pos]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n3. Label indexing3. Label indexingLabel indexing can be very handy, but in this case, we are again doing more work for no benefitLabeldf.set_index('A', append=True, drop=False).xs('foo', level=1)\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\ndf.set_index('A', append=True, drop=False).xs('foo', level=1)\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n4. df.query() API4. df.query() APIdf.query()pd.DataFrame.query is a very elegant/intuitive way to perform this task, but is often slower. However, if you pay attention to the timings below, for large data, the query is very efficient. More so than the standard approach and of similar magnitude as my best suggestion.pd.DataFrame.querypd.DataFrame.queryHoweverdf.query('A == \"foo\"')\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\ndf.query('A == \"foo\"')\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\nMy preference is to use the Boolean mask\nActual improvements can be made by modifying how we create our Boolean mask.\nmask alternative 1\nUse the underlying NumPy array and forgo the overhead of creating another pd.Series\nmask = df['A'].values == 'foo'\n\nI'll show more complete time tests at the end, but just take a look at the performance gains we get using the sample data frame.  First, we look at the difference in creating the mask\n%timeit mask = df['A'].values == 'foo'\n%timeit mask = df['A'] == 'foo'\n\n5.84 \u00b5s \u00b1 195 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n166 \u00b5s \u00b1 4.45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nEvaluating the mask with the NumPy array is ~ 30 times faster.  This is partly due to NumPy evaluation often being faster. It is also partly due to the lack of overhead necessary to build an index and a corresponding pd.Series object.\nNext, we'll look at the timing for slicing with one mask versus the other.\nmask = df['A'].values == 'foo'\n%timeit df[mask]\nmask = df['A'] == 'foo'\n%timeit df[mask]\n\n219 \u00b5s \u00b1 12.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n239 \u00b5s \u00b1 7.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nThe performance gains aren't as pronounced.  We'll see if this holds up over more robust testing.\n\nmask alternative 2\nWe could have reconstructed the data frame as well.  There is a big caveat when reconstructing a dataframe\u2014you must take care of the dtypes when doing so!\nInstead of df[mask] we will do this\npd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\n\nIf the data frame is of mixed type, which our example is, then when we get df.values the resulting array is of dtype object and consequently, all columns of the new data frame will be of dtype object.  Thus requiring the astype(df.dtypes) and killing any potential performance gains.\n%timeit df[m]\n%timeit pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\n\n216 \u00b5s \u00b1 10.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.43 ms \u00b1 39.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n\nHowever, if the data frame is not of mixed type, this is a very useful way to do it.\nGiven\nnp.random.seed([3,1415])\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\n\nd1\n\n   A  B  C  D  E\n0  0  2  7  3  8\n1  7  0  6  8  6\n2  0  2  0  4  9\n3  7  3  2  4  3\n4  3  6  7  7  4\n5  5  3  7  5  9\n6  8  7  6  4  7\n7  6  2  6  6  5\n8  2  8  7  5  8\n9  4  7  6  1  5\n\n\n%%timeit\nmask = d1['A'].values == 7\nd1[mask]\n\n179 \u00b5s \u00b1 8.73 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nVersus\n%%timeit\nmask = d1['A'].values == 7\npd.DataFrame(d1.values[mask], d1.index[mask], d1.columns)\n\n87 \u00b5s \u00b1 5.12 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\nWe cut the time in half.\n\nmask alternative 3\n@unutbu also shows us how to use pd.Series.isin to account for each element of df['A'] being in a set of values.  This evaluates to the same thing if our set of values is a set of one value, namely 'foo'.  But it also generalizes to include larger sets of values if needed.  Turns out, this is still pretty fast even though it is a more general solution.  The only real loss is in intuitiveness for those not familiar with the concept.\nmask = df['A'].isin(['foo'])\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\nHowever, as before, we can utilize NumPy to improve performance while sacrificing virtually nothing. We'll use np.in1d\nmask = np.in1d(df['A'].values, ['foo'])\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\n\n\nTiming\nI'll include other concepts mentioned in other posts as well for reference.\nCode Below\nEach column in this table represents a different length data frame over which we test each function. Each column shows relative time taken, with the fastest function given a base index of 1.0.\nres.div(res.min())\n\n                         10        30        100       300       1000      3000      10000     30000\nmask_standard         2.156872  1.850663  2.034149  2.166312  2.164541  3.090372  2.981326  3.131151\nmask_standard_loc     1.879035  1.782366  1.988823  2.338112  2.361391  3.036131  2.998112  2.990103\nmask_with_values      1.010166  1.000000  1.005113  1.026363  1.028698  1.293741  1.007824  1.016919\nmask_with_values_loc  1.196843  1.300228  1.000000  1.000000  1.038989  1.219233  1.037020  1.000000\nquery                 4.997304  4.765554  5.934096  4.500559  2.997924  2.397013  1.680447  1.398190\nxs_label              4.124597  4.272363  5.596152  4.295331  4.676591  5.710680  6.032809  8.950255\nmask_with_isin        1.674055  1.679935  1.847972  1.724183  1.345111  1.405231  1.253554  1.264760\nmask_with_in1d        1.000000  1.083807  1.220493  1.101929  1.000000  1.000000  1.000000  1.144175\n\nYou'll notice that the fastest times seem to be shared between mask_with_values and mask_with_in1d.\nres.T.plot(loglog=True)\n\n\nFunctions\ndef mask_standard(df):\n    mask = df['A'] == 'foo'\n    return df[mask]\n\ndef mask_standard_loc(df):\n    mask = df['A'] == 'foo'\n    return df.loc[mask]\n\ndef mask_with_values(df):\n    mask = df['A'].values == 'foo'\n    return df[mask]\n\ndef mask_with_values_loc(df):\n    mask = df['A'].values == 'foo'\n    return df.loc[mask]\n\ndef query(df):\n    return df.query('A == \"foo\"')\n\ndef xs_label(df):\n    return df.set_index('A', append=True, drop=False).xs('foo', level=-1)\n\ndef mask_with_isin(df):\n    mask = df['A'].isin(['foo'])\n    return df[mask]\n\ndef mask_with_in1d(df):\n    mask = np.in1d(df['A'].values, ['foo'])\n    return df[mask]\n\n\nTesting\nres = pd.DataFrame(\n    index=[\n        'mask_standard', 'mask_standard_loc', 'mask_with_values', 'mask_with_values_loc',\n        'query', 'xs_label', 'mask_with_isin', 'mask_with_in1d'\n    ],\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    dtype=float\n)\n\nfor j in res.columns:\n    d = pd.concat([df] * j, ignore_index=True)\n    for i in res.index:a\n        stmt = '{}(d)'.format(i)\n        setp = 'from __main__ import d, {}'.format(i)\n        res.at[i, j] = timeit(stmt, setp, number=50)\n\n\nSpecial Timing\nLooking at the special case when we have a single non-object dtype for the entire data frame.\nCode Below\nspec.div(spec.min())\n\n                     10        30        100       300       1000      3000      10000     30000\nmask_with_values  1.009030  1.000000  1.194276  1.000000  1.236892  1.095343  1.000000  1.000000\nmask_with_in1d    1.104638  1.094524  1.156930  1.072094  1.000000  1.000000  1.040043  1.027100\nreconstruct       1.000000  1.142838  1.000000  1.355440  1.650270  2.222181  2.294913  3.406735\n\nTurns out, reconstruction isn't worth it past a few hundred rows.\nspec.T.plot(loglog=True)\n\n\nFunctions\nnp.random.seed([3,1415])\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\n\ndef mask_with_values(df):\n    mask = df['A'].values == 'foo'\n    return df[mask]\n\ndef mask_with_in1d(df):\n    mask = np.in1d(df['A'].values, ['foo'])\n    return df[mask]\n\ndef reconstruct(df):\n    v = df.values\n    mask = np.in1d(df['A'].values, ['foo'])\n    return pd.DataFrame(v[mask], df.index[mask], df.columns)\n\nspec = pd.DataFrame(\n    index=['mask_with_values', 'mask_with_in1d', 'reconstruct'],\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    dtype=float\n)\n\nTesting\nfor j in spec.columns:\n    d = pd.concat([df] * j, ignore_index=True)\n    for i in spec.index:\n        stmt = '{}(d)'.format(i)\n        setp = 'from __main__ import d, {}'.format(i)\n        spec.at[i, j] = timeit(stmt, setp, number=50)\n\nMy preference is to use the Boolean maskBooleanmaskActual improvements can be made by modifying how we create our Boolean mask.Booleanmaskmask alternative 1\nUse the underlying NumPy array and forgo the overhead of creating another pd.Seriesmask alternative 1maskUse the underlying NumPy array and forgo the overhead of creating another pd.Seriespd.Seriesmask = df['A'].values == 'foo'\nmask = df['A'].values == 'foo'\nI'll show more complete time tests at the end, but just take a look at the performance gains we get using the sample data frame.  First, we look at the difference in creating the maskmask%timeit mask = df['A'].values == 'foo'\n%timeit mask = df['A'] == 'foo'\n\n5.84 \u00b5s \u00b1 195 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n166 \u00b5s \u00b1 4.45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n%timeit mask = df['A'].values == 'foo'\n%timeit mask = df['A'] == 'foo'\n\n5.84 \u00b5s \u00b1 195 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n166 \u00b5s \u00b1 4.45 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nEvaluating the mask with the NumPy array is ~ 30 times faster.  This is partly due to NumPy evaluation often being faster. It is also partly due to the lack of overhead necessary to build an index and a corresponding pd.Series object.maskpd.SeriesNext, we'll look at the timing for slicing with one mask versus the other.maskmask = df['A'].values == 'foo'\n%timeit df[mask]\nmask = df['A'] == 'foo'\n%timeit df[mask]\n\n219 \u00b5s \u00b1 12.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n239 \u00b5s \u00b1 7.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\nmask = df['A'].values == 'foo'\n%timeit df[mask]\nmask = df['A'] == 'foo'\n%timeit df[mask]\n\n219 \u00b5s \u00b1 12.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n239 \u00b5s \u00b1 7.03 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\nThe performance gains aren't as pronounced.  We'll see if this holds up over more robust testing.mask alternative 2\nWe could have reconstructed the data frame as well.  There is a big caveat when reconstructing a dataframe\u2014you must take care of the dtypes when doing so!mask alternative 2maskdtypesInstead of df[mask] we will do thisdf[mask]pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\npd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\nIf the data frame is of mixed type, which our example is, then when we get df.values the resulting array is of dtype object and consequently, all columns of the new data frame will be of dtype object.  Thus requiring the astype(df.dtypes) and killing any potential performance gains.df.valuesdtypeobjectdtypeobjectastype(df.dtypes)%timeit df[m]\n%timeit pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\n\n216 \u00b5s \u00b1 10.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.43 ms \u00b1 39.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n%timeit df[m]\n%timeit pd.DataFrame(df.values[mask], df.index[mask], df.columns).astype(df.dtypes)\n\n216 \u00b5s \u00b1 10.4 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n1.43 ms \u00b1 39.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\nHowever, if the data frame is not of mixed type, this is a very useful way to do it.Givennp.random.seed([3,1415])\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\n\nd1\n\n   A  B  C  D  E\n0  0  2  7  3  8\n1  7  0  6  8  6\n2  0  2  0  4  9\n3  7  3  2  4  3\n4  3  6  7  7  4\n5  5  3  7  5  9\n6  8  7  6  4  7\n7  6  2  6  6  5\n8  2  8  7  5  8\n9  4  7  6  1  5\nnp.random.seed([3,1415])\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\n\nd1\n\n   A  B  C  D  E\n0  0  2  7  3  8\n1  7  0  6  8  6\n2  0  2  0  4  9\n3  7  3  2  4  3\n4  3  6  7  7  4\n5  5  3  7  5  9\n6  8  7  6  4  7\n7  6  2  6  6  5\n8  2  8  7  5  8\n9  4  7  6  1  5\n%%timeit\nmask = d1['A'].values == 7\nd1[mask]\n\n179 \u00b5s \u00b1 8.73 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n%%timeit\nmask = d1['A'].values == 7\nd1[mask]\n\n179 \u00b5s \u00b1 8.73 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nVersus%%timeit\nmask = d1['A'].values == 7\npd.DataFrame(d1.values[mask], d1.index[mask], d1.columns)\n\n87 \u00b5s \u00b1 5.12 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n%%timeit\nmask = d1['A'].values == 7\npd.DataFrame(d1.values[mask], d1.index[mask], d1.columns)\n\n87 \u00b5s \u00b1 5.12 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nWe cut the time in half.mask alternative 3mask alternative 3mask@unutbu also shows us how to use pd.Series.isin to account for each element of df['A'] being in a set of values.  This evaluates to the same thing if our set of values is a set of one value, namely 'foo'.  But it also generalizes to include larger sets of values if needed.  Turns out, this is still pretty fast even though it is a more general solution.  The only real loss is in intuitiveness for those not familiar with the concept.pd.Series.isindf['A']'foo'mask = df['A'].isin(['foo'])\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\nmask = df['A'].isin(['foo'])\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\nHowever, as before, we can utilize NumPy to improve performance while sacrificing virtually nothing. We'll use np.in1dnp.in1dmask = np.in1d(df['A'].values, ['foo'])\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\nmask = np.in1d(df['A'].values, ['foo'])\ndf[mask]\n\n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\nTimingTimingI'll include other concepts mentioned in other posts as well for reference.Code BelowCode BelowEach column in this table represents a different length data frame over which we test each function. Each column shows relative time taken, with the fastest function given a base index of 1.0.column1.0res.div(res.min())\n\n                         10        30        100       300       1000      3000      10000     30000\nmask_standard         2.156872  1.850663  2.034149  2.166312  2.164541  3.090372  2.981326  3.131151\nmask_standard_loc     1.879035  1.782366  1.988823  2.338112  2.361391  3.036131  2.998112  2.990103\nmask_with_values      1.010166  1.000000  1.005113  1.026363  1.028698  1.293741  1.007824  1.016919\nmask_with_values_loc  1.196843  1.300228  1.000000  1.000000  1.038989  1.219233  1.037020  1.000000\nquery                 4.997304  4.765554  5.934096  4.500559  2.997924  2.397013  1.680447  1.398190\nxs_label              4.124597  4.272363  5.596152  4.295331  4.676591  5.710680  6.032809  8.950255\nmask_with_isin        1.674055  1.679935  1.847972  1.724183  1.345111  1.405231  1.253554  1.264760\nmask_with_in1d        1.000000  1.083807  1.220493  1.101929  1.000000  1.000000  1.000000  1.144175\nres.div(res.min())\n\n                         10        30        100       300       1000      3000      10000     30000\nmask_standard         2.156872  1.850663  2.034149  2.166312  2.164541  3.090372  2.981326  3.131151\nmask_standard_loc     1.879035  1.782366  1.988823  2.338112  2.361391  3.036131  2.998112  2.990103\nmask_with_values      1.010166  1.000000  1.005113  1.026363  1.028698  1.293741  1.007824  1.016919\nmask_with_values_loc  1.196843  1.300228  1.000000  1.000000  1.038989  1.219233  1.037020  1.000000\nquery                 4.997304  4.765554  5.934096  4.500559  2.997924  2.397013  1.680447  1.398190\nxs_label              4.124597  4.272363  5.596152  4.295331  4.676591  5.710680  6.032809  8.950255\nmask_with_isin        1.674055  1.679935  1.847972  1.724183  1.345111  1.405231  1.253554  1.264760\nmask_with_in1d        1.000000  1.083807  1.220493  1.101929  1.000000  1.000000  1.000000  1.144175\nYou'll notice that the fastest times seem to be shared between mask_with_values and mask_with_in1d.mask_with_valuesmask_with_in1dres.T.plot(loglog=True)\nres.T.plot(loglog=True)\nFunctionsFunctionsdef mask_standard(df):\n    mask = df['A'] == 'foo'\n    return df[mask]\n\ndef mask_standard_loc(df):\n    mask = df['A'] == 'foo'\n    return df.loc[mask]\n\ndef mask_with_values(df):\n    mask = df['A'].values == 'foo'\n    return df[mask]\n\ndef mask_with_values_loc(df):\n    mask = df['A'].values == 'foo'\n    return df.loc[mask]\n\ndef query(df):\n    return df.query('A == \"foo\"')\n\ndef xs_label(df):\n    return df.set_index('A', append=True, drop=False).xs('foo', level=-1)\n\ndef mask_with_isin(df):\n    mask = df['A'].isin(['foo'])\n    return df[mask]\n\ndef mask_with_in1d(df):\n    mask = np.in1d(df['A'].values, ['foo'])\n    return df[mask]\ndef mask_standard(df):\n    mask = df['A'] == 'foo'\n    return df[mask]\n\ndef mask_standard_loc(df):\n    mask = df['A'] == 'foo'\n    return df.loc[mask]\n\ndef mask_with_values(df):\n    mask = df['A'].values == 'foo'\n    return df[mask]\n\ndef mask_with_values_loc(df):\n    mask = df['A'].values == 'foo'\n    return df.loc[mask]\n\ndef query(df):\n    return df.query('A == \"foo\"')\n\ndef xs_label(df):\n    return df.set_index('A', append=True, drop=False).xs('foo', level=-1)\n\ndef mask_with_isin(df):\n    mask = df['A'].isin(['foo'])\n    return df[mask]\n\ndef mask_with_in1d(df):\n    mask = np.in1d(df['A'].values, ['foo'])\n    return df[mask]\nTestingTestingres = pd.DataFrame(\n    index=[\n        'mask_standard', 'mask_standard_loc', 'mask_with_values', 'mask_with_values_loc',\n        'query', 'xs_label', 'mask_with_isin', 'mask_with_in1d'\n    ],\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    dtype=float\n)\n\nfor j in res.columns:\n    d = pd.concat([df] * j, ignore_index=True)\n    for i in res.index:a\n        stmt = '{}(d)'.format(i)\n        setp = 'from __main__ import d, {}'.format(i)\n        res.at[i, j] = timeit(stmt, setp, number=50)\nres = pd.DataFrame(\n    index=[\n        'mask_standard', 'mask_standard_loc', 'mask_with_values', 'mask_with_values_loc',\n        'query', 'xs_label', 'mask_with_isin', 'mask_with_in1d'\n    ],\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    dtype=float\n)\n\nfor j in res.columns:\n    d = pd.concat([df] * j, ignore_index=True)\n    for i in res.index:a\n        stmt = '{}(d)'.format(i)\n        setp = 'from __main__ import d, {}'.format(i)\n        res.at[i, j] = timeit(stmt, setp, number=50)\nSpecial TimingSpecial TimingLooking at the special case when we have a single non-object dtype for the entire data frame.dtypeCode BelowCode Belowspec.div(spec.min())\n\n                     10        30        100       300       1000      3000      10000     30000\nmask_with_values  1.009030  1.000000  1.194276  1.000000  1.236892  1.095343  1.000000  1.000000\nmask_with_in1d    1.104638  1.094524  1.156930  1.072094  1.000000  1.000000  1.040043  1.027100\nreconstruct       1.000000  1.142838  1.000000  1.355440  1.650270  2.222181  2.294913  3.406735\nspec.div(spec.min())\n\n                     10        30        100       300       1000      3000      10000     30000\nmask_with_values  1.009030  1.000000  1.194276  1.000000  1.236892  1.095343  1.000000  1.000000\nmask_with_in1d    1.104638  1.094524  1.156930  1.072094  1.000000  1.000000  1.040043  1.027100\nreconstruct       1.000000  1.142838  1.000000  1.355440  1.650270  2.222181  2.294913  3.406735\nTurns out, reconstruction isn't worth it past a few hundred rows.spec.T.plot(loglog=True)\nspec.T.plot(loglog=True)\nFunctionsFunctionsnp.random.seed([3,1415])\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\n\ndef mask_with_values(df):\n    mask = df['A'].values == 'foo'\n    return df[mask]\n\ndef mask_with_in1d(df):\n    mask = np.in1d(df['A'].values, ['foo'])\n    return df[mask]\n\ndef reconstruct(df):\n    v = df.values\n    mask = np.in1d(df['A'].values, ['foo'])\n    return pd.DataFrame(v[mask], df.index[mask], df.columns)\n\nspec = pd.DataFrame(\n    index=['mask_with_values', 'mask_with_in1d', 'reconstruct'],\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    dtype=float\n)\nnp.random.seed([3,1415])\nd1 = pd.DataFrame(np.random.randint(10, size=(10, 5)), columns=list('ABCDE'))\n\ndef mask_with_values(df):\n    mask = df['A'].values == 'foo'\n    return df[mask]\n\ndef mask_with_in1d(df):\n    mask = np.in1d(df['A'].values, ['foo'])\n    return df[mask]\n\ndef reconstruct(df):\n    v = df.values\n    mask = np.in1d(df['A'].values, ['foo'])\n    return pd.DataFrame(v[mask], df.index[mask], df.columns)\n\nspec = pd.DataFrame(\n    index=['mask_with_values', 'mask_with_in1d', 'reconstruct'],\n    columns=[10, 30, 100, 300, 1000, 3000, 10000, 30000],\n    dtype=float\n)\nTestingTestingfor j in spec.columns:\n    d = pd.concat([df] * j, ignore_index=True)\n    for i in spec.index:\n        stmt = '{}(d)'.format(i)\n        setp = 'from __main__ import d, {}'.format(i)\n        spec.at[i, j] = timeit(stmt, setp, number=50)\nfor j in spec.columns:\n    d = pd.concat([df] * j, ignore_index=True)\n    for i in spec.index:\n        stmt = '{}(d)'.format(i)\n        setp = 'from __main__ import d, {}'.format(i)\n        spec.at[i, j] = timeit(stmt, setp, number=50)\n",
                "tl;drThe Pandas equivalent toselect * from table where column_name = some_value\nselect * from table where column_name = some_value\nistable[table.column_name == some_value]\ntable[table.column_name == some_value]\nMultiple conditions:table[(table.column_name == some_value) | (table.column_name2 == some_value2)]\ntable[(table.column_name == some_value) | (table.column_name2 == some_value2)]\nortable.query('column_name == some_value | column_name2 == some_value2')\ntable.query('column_name == some_value | column_name2 == some_value2')\nCode exampleimport pandas as pd\n\n# Create data set\nd = {'foo':[100, 111, 222],\n     'bar':[333, 444, 555]}\ndf = pd.DataFrame(d)\n\n# Full dataframe:\ndf\n\n# Shows:\n#    bar   foo\n# 0  333   100\n# 1  444   111\n# 2  555   222\n\n# Output only the row(s) in df where foo is 222:\ndf[df.foo == 222]\n\n# Shows:\n#    bar  foo\n# 2  555  222\nimport pandas as pd\n\n# Create data set\nd = {'foo':[100, 111, 222],\n     'bar':[333, 444, 555]}\ndf = pd.DataFrame(d)\n\n# Full dataframe:\ndf\n\n# Shows:\n#    bar   foo\n# 0  333   100\n# 1  444   111\n# 2  555   222\n\n# Output only the row(s) in df where foo is 222:\ndf[df.foo == 222]\n\n# Shows:\n#    bar  foo\n# 2  555  222\nIn the above code it is the line df[df.foo == 222] that gives the rows based on the column value, 222 in this case.df[df.foo == 222]222Multiple conditions are also possible:df[(df.foo == 222) | (df.bar == 444)]\n#    bar  foo\n# 1  444  111\n# 2  555  222\ndf[(df.foo == 222) | (df.bar == 444)]\n#    bar  foo\n# 1  444  111\n# 2  555  222\nBut at that point I would recommend using the query function, since it's less verbose and yields the same result:querydf.query('foo == 222 | bar == 444')\ndf.query('foo == 222 | bar == 444')\n",
                "I find the syntax of the previous answers to be redundant and difficult to remember. Pandas introduced the query() method in v0.13 and I much prefer it. For your question, you could do df.query('col == val').query()df.query('col == val')Reproduced from The query() Method (Experimental):The query() Method (Experimental)The query() Method (Experimental)In [167]: n = 10\n\nIn [168]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\n\nIn [169]: df\nOut[169]:\n          a         b         c\n0  0.687704  0.582314  0.281645\n1  0.250846  0.610021  0.420121\n2  0.624328  0.401816  0.932146\n3  0.011763  0.022921  0.244186\n4  0.590198  0.325680  0.890392\n5  0.598892  0.296424  0.007312\n6  0.634625  0.803069  0.123872\n7  0.924168  0.325076  0.303746\n8  0.116822  0.364564  0.454607\n9  0.986142  0.751953  0.561512\n\n# pure python\nIn [170]: df[(df.a < df.b) & (df.b < df.c)]\nOut[170]:\n          a         b         c\n3  0.011763  0.022921  0.244186\n8  0.116822  0.364564  0.454607\n\n# query\nIn [171]: df.query('(a < b) & (b < c)')\nOut[171]:\n          a         b         c\n3  0.011763  0.022921  0.244186\n8  0.116822  0.364564  0.454607\nIn [167]: n = 10\n\nIn [168]: df = pd.DataFrame(np.random.rand(n, 3), columns=list('abc'))\n\nIn [169]: df\nOut[169]:\n          a         b         c\n0  0.687704  0.582314  0.281645\n1  0.250846  0.610021  0.420121\n2  0.624328  0.401816  0.932146\n3  0.011763  0.022921  0.244186\n4  0.590198  0.325680  0.890392\n5  0.598892  0.296424  0.007312\n6  0.634625  0.803069  0.123872\n7  0.924168  0.325076  0.303746\n8  0.116822  0.364564  0.454607\n9  0.986142  0.751953  0.561512\n\n# pure python\nIn [170]: df[(df.a < df.b) & (df.b < df.c)]\nOut[170]:\n          a         b         c\n3  0.011763  0.022921  0.244186\n8  0.116822  0.364564  0.454607\n\n# query\nIn [171]: df.query('(a < b) & (b < c)')\nOut[171]:\n          a         b         c\n3  0.011763  0.022921  0.244186\n8  0.116822  0.364564  0.454607\nYou can also access variables in the environment by prepending an @.@exclude = ('red', 'orange')\ndf.query('color not in @exclude')\nexclude = ('red', 'orange')\ndf.query('color not in @exclude')\n",
                "More flexibility using .query with pandas >= 0.25.0:.querySince pandas >= 0.25.0 we can use the query method to filter dataframes with pandas methods and even column names which have spaces. Normally the spaces in column names would give an error, but now we can solve that using a backtick (`) - see GitHub:queryGitHub# Example dataframe\ndf = pd.DataFrame({'Sender email':['ex@example.com', \"reply@shop.com\", \"buy@shop.com\"]})\n\n     Sender email\n0  ex@example.com\n1  reply@shop.com\n2    buy@shop.com\n# Example dataframe\ndf = pd.DataFrame({'Sender email':['ex@example.com', \"reply@shop.com\", \"buy@shop.com\"]})\n\n     Sender email\n0  ex@example.com\n1  reply@shop.com\n2    buy@shop.com\nUsing .query with method str.endswith:.querystr.endswithdf.query('`Sender email`.str.endswith(\"@shop.com\")')\ndf.query('`Sender email`.str.endswith(\"@shop.com\")')\nOutputOutput     Sender email\n1  reply@shop.com\n2    buy@shop.com\n     Sender email\n1  reply@shop.com\n2    buy@shop.com\nAlso we can use local variables by prefixing it with an @ in our query:@domain = 'shop.com'\ndf.query('`Sender email`.str.endswith(@domain)')\ndomain = 'shop.com'\ndf.query('`Sender email`.str.endswith(@domain)')\nOutputOutput     Sender email\n1  reply@shop.com\n2    buy@shop.com\n     Sender email\n1  reply@shop.com\n2    buy@shop.com\n",
                "For selecting only specific columns out of multiple columns for a given value in Pandas:select col_name1, col_name2 from table where column_name = some_value.\nselect col_name1, col_name2 from table where column_name = some_value.\nOptions loc:loclocdf.loc[df['column_name'] == some_value, [col_name1, col_name2]]\ndf.loc[df['column_name'] == some_value, [col_name1, col_name2]]\nor query:queryquerydf.query('column_name == some_value')[[col_name1, col_name2]]\ndf.query('column_name == some_value')[[col_name1, col_name2]]\n",
                "In newer versions of Pandas, inspired by the documentation (Viewing data):Viewing dataViewing datadf[df[\"colume_name\"] == some_value] #Scalar, True/False..\n\ndf[df[\"colume_name\"] == \"some_value\"] #String\ndf[df[\"colume_name\"] == some_value] #Scalar, True/False..\n\ndf[df[\"colume_name\"] == \"some_value\"] #String\nCombine multiple conditions by putting the clause in parentheses, (), and combining them with & and | (and/or). Like this:()&|df[(df[\"colume_name\"] == \"some_value1\") & (pd[pd[\"colume_name\"] == \"some_value2\"])]\ndf[(df[\"colume_name\"] == \"some_value1\") & (pd[pd[\"colume_name\"] == \"some_value2\"])]\nOther filterspandas.notna(df[\"colume_name\"]) == True # Not NaN\ndf['colume_name'].str.contains(\"text\") # Search for \"text\"\ndf['colume_name'].str.lower().str.contains(\"text\") # Search for \"text\", after converting  to lowercase\npandas.notna(df[\"colume_name\"]) == True # Not NaN\ndf['colume_name'].str.contains(\"text\") # Search for \"text\"\ndf['colume_name'].str.lower().str.contains(\"text\") # Search for \"text\", after converting  to lowercase\n",
                "Faster results can be achieved using numpy.where. numpy.whereFor example, with unubtu's setup -unubtu's setupIn [76]: df.iloc[np.where(df.A.values=='foo')]\nOut[76]: \n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\nIn [76]: df.iloc[np.where(df.A.values=='foo')]\nOut[76]: \n     A      B  C   D\n0  foo    one  0   0\n2  foo    two  2   4\n4  foo    two  4   8\n6  foo    one  6  12\n7  foo  three  7  14\nTiming comparisons:In [68]: %timeit df.iloc[np.where(df.A.values=='foo')]  # fastest\n1000 loops, best of 3: 380 \u00b5s per loop\n\nIn [69]: %timeit df.loc[df['A'] == 'foo']\n1000 loops, best of 3: 745 \u00b5s per loop\n\nIn [71]: %timeit df.loc[df['A'].isin(['foo'])]\n1000 loops, best of 3: 562 \u00b5s per loop\n\nIn [72]: %timeit df[df.A=='foo']\n1000 loops, best of 3: 796 \u00b5s per loop\n\nIn [74]: %timeit df.query('(A==\"foo\")')  # slowest\n1000 loops, best of 3: 1.71 ms per loop\nIn [68]: %timeit df.iloc[np.where(df.A.values=='foo')]  # fastest\n1000 loops, best of 3: 380 \u00b5s per loop\n\nIn [69]: %timeit df.loc[df['A'] == 'foo']\n1000 loops, best of 3: 745 \u00b5s per loop\n\nIn [71]: %timeit df.loc[df['A'].isin(['foo'])]\n1000 loops, best of 3: 562 \u00b5s per loop\n\nIn [72]: %timeit df[df.A=='foo']\n1000 loops, best of 3: 796 \u00b5s per loop\n\nIn [74]: %timeit df.query('(A==\"foo\")')  # slowest\n1000 loops, best of 3: 1.71 ms per loop\n",
                "Here is a simple example  from pandas import DataFrame\n\n# Create data set\nd = {'Revenue':[100,111,222], \n     'Cost':[333,444,555]}\ndf = DataFrame(d)\n\n\n# mask = Return True when the value in column \"Revenue\" is equal to 111\nmask = df['Revenue'] == 111\n\nprint mask\n\n# Result:\n# 0    False\n# 1     True\n# 2    False\n# Name: Revenue, dtype: bool\n\n\n# Select * FROM df WHERE Revenue = 111\ndf[mask]\n\n# Result:\n#    Cost    Revenue\n# 1  444     111\nfrom pandas import DataFrame\n\n# Create data set\nd = {'Revenue':[100,111,222], \n     'Cost':[333,444,555]}\ndf = DataFrame(d)\n\n\n# mask = Return True when the value in column \"Revenue\" is equal to 111\nmask = df['Revenue'] == 111\n\nprint mask\n\n# Result:\n# 0    False\n# 1     True\n# 2    False\n# Name: Revenue, dtype: bool\n\n\n# Select * FROM df WHERE Revenue = 111\ndf[mask]\n\n# Result:\n#    Cost    Revenue\n# 1  444     111\n",
                "To add: You can also do df.groupby('column_name').get_group('column_desired_value').reset_index() to make a new data frame with specified column having a particular value. E.g.,df.groupby('column_name').get_group('column_desired_value').reset_index()import pandas as pd\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split()})\nprint(\"Original dataframe:\")\nprint(df)\n\nb_is_two_dataframe = pd.DataFrame(df.groupby('B').get_group('two').reset_index()).drop('index', axis = 1) \n#NOTE: the final drop is to remove the extra index column returned by groupby object\nprint('Sub dataframe where B is two:')\nprint(b_is_two_dataframe)\nimport pandas as pd\ndf = pd.DataFrame({'A': 'foo bar foo bar foo bar foo foo'.split(),\n                   'B': 'one one two three two two one three'.split()})\nprint(\"Original dataframe:\")\nprint(df)\n\nb_is_two_dataframe = pd.DataFrame(df.groupby('B').get_group('two').reset_index()).drop('index', axis = 1) \n#NOTE: the final drop is to remove the extra index column returned by groupby object\nprint('Sub dataframe where B is two:')\nprint(b_is_two_dataframe)\nRunning this gives:Original dataframe:\n     A      B\n0  foo    one\n1  bar    one\n2  foo    two\n3  bar  three\n4  foo    two\n5  bar    two\n6  foo    one\n7  foo  three\nSub dataframe where B is two:\n     A    B\n0  foo  two\n1  foo  two\n2  bar  two\nOriginal dataframe:\n     A      B\n0  foo    one\n1  bar    one\n2  foo    two\n3  bar  three\n4  foo    two\n5  bar    two\n6  foo    one\n7  foo  three\nSub dataframe where B is two:\n     A    B\n0  foo  two\n1  foo  two\n2  bar  two\n",
                "You can also use .apply:df.apply(lambda row: row[df['B'].isin(['one','three'])])\ndf.apply(lambda row: row[df['B'].isin(['one','three'])])\nIt actually works row-wise (i.e., applies the function to each row).The output is    A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\n   A      B  C   D\n0  foo    one  0   0\n1  bar    one  1   2\n3  bar  three  3   6\n6  foo    one  6  12\n7  foo  three  7  14\nThe results is the same as using as mentioned by @unutbudf[[df['B'].isin(['one','three'])]]\ndf[[df['B'].isin(['one','three'])]]\n",
                "If you want to make query to your dataframe repeatedly and speed is important to you, the best thing is to convert your dataframe to dictionary and then by doing this you can make query thousands of times faster.my_df = df.set_index(column_name)\nmy_dict = my_df.to_dict('index')\nmy_df = df.set_index(column_name)\nmy_dict = my_df.to_dict('index')\nAfter make my_dict dictionary you can go through:if some_value in my_dict.keys():\n   my_result = my_dict[some_value]\nif some_value in my_dict.keys():\n   my_result = my_dict[some_value]\nIf you have duplicated values in column_name you can't make a dictionary. but you can use:my_result = my_df.loc[some_value]\nmy_result = my_df.loc[some_value]\n",
                "SQL statements on DataFrames to select rows using DuckDBWith DuckDB we can query pandas DataFrames with SQL statements, in a highly performant way.DuckDBhighly performant waySince the question is How do I select rows from a DataFrame based on column values?, and the example in the question is a SQL query, this answer looks logical in this topic.How do I select rows from a DataFrame based on column values?Example:ExampleIn [1]: import duckdb\n\nIn [2]: import pandas as pd\n\nIn [3]: con = duckdb.connect()\n\nIn [4]: df = pd.DataFrame({\"A\": range(11), \"B\": range(11, 22)})\n\nIn [5]: df\nOut[5]:\n     A   B\n0    0  11\n1    1  12\n2    2  13\n3    3  14\n4    4  15\n5    5  16\n6    6  17\n7    7  18\n8    8  19\n9    9  20\n10  10  21\n\nIn [6]: results = con.execute(\"SELECT * FROM df where A > 2\").df()\n\nIn [7]: results\nOut[7]:\n    A   B\n0   3  14\n1   4  15\n2   5  16\n3   6  17\n4   7  18\n5   8  19\n6   9  20\n7  10  21\nIn [1]: import duckdb\n\nIn [2]: import pandas as pd\n\nIn [3]: con = duckdb.connect()\n\nIn [4]: df = pd.DataFrame({\"A\": range(11), \"B\": range(11, 22)})\n\nIn [5]: df\nOut[5]:\n     A   B\n0    0  11\n1    1  12\n2    2  13\n3    3  14\n4    4  15\n5    5  16\n6    6  17\n7    7  18\n8    8  19\n9    9  20\n10  10  21\n\nIn [6]: results = con.execute(\"SELECT * FROM df where A > 2\").df()\n\nIn [7]: results\nOut[7]:\n    A   B\n0   3  14\n1   4  15\n2   5  16\n3   6  17\n4   7  18\n5   8  19\n6   9  20\n7  10  21\n",
                "1. Use f-strings inside query() callsquery()If the column name used to filter your dataframe comes from a local variable, f-strings may be useful. For example,col = 'A'\ndf.query(f\"{col} == 'foo'\")\ncol = 'A'\ndf.query(f\"{col} == 'foo'\")\nIn fact, f-strings can be used for the query variable as well (except for datetime):col = 'A'\nmy_var = 'foo'\ndf.query(f\"{col} == '{my_var}'\") # if my_var is a string\n\nmy_num = 1\ndf.query(f\"{col} == {my_num}\") # if my_var is a number\n\nmy_date = '2022-12-10'\ndf.query(f\"{col} == @my_date\") # must use @ for datetime though\ncol = 'A'\nmy_var = 'foo'\ndf.query(f\"{col} == '{my_var}'\") # if my_var is a string\n\nmy_num = 1\ndf.query(f\"{col} == {my_num}\") # if my_var is a number\n\nmy_date = '2022-12-10'\ndf.query(f\"{col} == @my_date\") # must use @ for datetime though\n2. Install numexpr to speed up query() callsnumexprquery()The pandas documentation recommends installing numexpr to speed up numeric calculation when using query(). Use pip install numexpr (or conda, sudo etc. depending on your environment) to install it.recommends installing numexprquery()pip install numexprcondasudoFor larger dataframes (where performance actually matters), df.query() with numexpr engine performs much faster than df[mask]. In particular, it performs better for the following cases.df.query()numexprdf[mask]Logical and/or comparison operators on columns of stringsLogical and/or comparison operators on columns of stringsIf a column of strings are compared to some other string(s) and matching rows are to be selected, even for a single comparison operation, query() performs faster than df[mask]. For example, for a dataframe with 80k rows, it's 30% faster1 and for a dataframe with 800k rows, it's 60% faster.2query()df[mask]12df[df.A == 'foo']\ndf.query(\"A == 'foo'\")  # <--- performs 30%-60% faster\ndf[df.A == 'foo']\ndf.query(\"A == 'foo'\")  # <--- performs 30%-60% faster\nThis gap increases as the number of operations increases (if 4 comparisons are chained df.query() is 2-2.3 times faster than df[mask])1,2 and/or the dataframe length increases.2df.query()df[mask]1,22Multiple operations on numeric columnsMultiple operations on numeric columnsIf multiple arithmetic, logical or comparison operations need to be computed to create a boolean mask to filter df, query() performs faster. For example, for a frame with 80k rows, it's 20% faster1 and for a frame with 800k rows, it's 2 times faster.2dfquery()12df[(df.B % 5) **2 < 0.1]\ndf.query(\"(B % 5) **2 < 0.1\")  # <--- performs 20%-100% faster.\ndf[(df.B % 5) **2 < 0.1]\ndf.query(\"(B % 5) **2 < 0.1\")  # <--- performs 20%-100% faster.\nThis gap in performance increases as the number of operations increases and/or the dataframe length increases.22The following plot shows how the methods perform as the dataframe length increases.333. Call pandas methods inside query()query()Numexpr currently supports only logical (&, |, ~), comparison (==, >, <, >=, <=, !=) and basic arithmetic operators (+, -, *, /, **, %).Numexprcurrently supports&|~==><>=<=!=+-*/**%For example, it doesn't support integer division (//). However, calling the equivalent pandas method (floordiv()) works.//floordiv()df.query('B.floordiv(2) <= 3')  # or \ndf.query('B.floordiv(2).le(3)')\n\n# for pandas < 1.4, need `.values`\ndf.query('B.floordiv(2).values <= 3')\ndf.query('B.floordiv(2) <= 3')  # or \ndf.query('B.floordiv(2).le(3)')\n\n# for pandas < 1.4, need `.values`\ndf.query('B.floordiv(2).values <= 3')\n1 Benchmark code using a frame with 80k rows1import numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo baz foo bar foo foo'.split()*10000, \n                   'B': np.random.rand(80000)})\n\n%timeit df[df.A == 'foo']\n# 8.5 ms \u00b1 104.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n%timeit df.query(\"A == 'foo'\")\n# 6.36 ms \u00b1 95.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n%timeit df[((df.A == 'foo') & (df.A != 'bar')) | ((df.A != 'baz') & (df.A != 'buz'))]\n# 29 ms \u00b1 554 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n%timeit df.query(\"A == 'foo' & A != 'bar' | A != 'baz' & A != 'buz'\")\n# 16 ms \u00b1 339 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n\n%timeit df[(df.B % 5) **2 < 0.1]\n# 5.35 ms \u00b1 37.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n%timeit df.query(\"(B % 5) **2 < 0.1\")\n# 4.37 ms \u00b1 46.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\nimport numpy as np\ndf = pd.DataFrame({'A': 'foo bar foo baz foo bar foo foo'.split()*10000, \n                   'B': np.random.rand(80000)})\n\n%timeit df[df.A == 'foo']\n# 8.5 ms \u00b1 104.5 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n%timeit df.query(\"A == 'foo'\")\n# 6.36 ms \u00b1 95.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\n%timeit df[((df.A == 'foo') & (df.A != 'bar')) | ((df.A != 'baz') & (df.A != 'buz'))]\n# 29 ms \u00b1 554 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n%timeit df.query(\"A == 'foo' & A != 'bar' | A != 'baz' & A != 'buz'\")\n# 16 ms \u00b1 339 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n\n%timeit df[(df.B % 5) **2 < 0.1]\n# 5.35 ms \u00b1 37.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n%timeit df.query(\"(B % 5) **2 < 0.1\")\n# 4.37 ms \u00b1 46.3 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n2 Benchmark code using a frame with 800k rows2df = pd.DataFrame({'A': 'foo bar foo baz foo bar foo foo'.split()*100000, \n                   'B': np.random.rand(800000)})\n\n%timeit df[df.A == 'foo']\n# 87.9 ms \u00b1 873 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n%timeit df.query(\"A == 'foo'\")\n# 54.4 ms \u00b1 726 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n\n%timeit df[((df.A == 'foo') & (df.A != 'bar')) | ((df.A != 'baz') & (df.A != 'buz'))]\n# 310 ms \u00b1 3.4 ms per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n%timeit df.query(\"A == 'foo' & A != 'bar' | A != 'baz' & A != 'buz'\")\n# 132 ms \u00b1 2.43 ms per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n\n%timeit df[(df.B % 5) **2 < 0.1]\n# 54 ms \u00b1 488 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n%timeit df.query(\"(B % 5) **2 < 0.1\")\n# 26.3 ms \u00b1 320 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\ndf = pd.DataFrame({'A': 'foo bar foo baz foo bar foo foo'.split()*100000, \n                   'B': np.random.rand(800000)})\n\n%timeit df[df.A == 'foo']\n# 87.9 ms \u00b1 873 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n%timeit df.query(\"A == 'foo'\")\n# 54.4 ms \u00b1 726 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n\n%timeit df[((df.A == 'foo') & (df.A != 'bar')) | ((df.A != 'baz') & (df.A != 'buz'))]\n# 310 ms \u00b1 3.4 ms per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n%timeit df.query(\"A == 'foo' & A != 'bar' | A != 'baz' & A != 'buz'\")\n# 132 ms \u00b1 2.43 ms per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n\n%timeit df[(df.B % 5) **2 < 0.1]\n# 54 ms \u00b1 488 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n%timeit df.query(\"(B % 5) **2 < 0.1\")\n# 26.3 ms \u00b1 320 \u00b5s per loop (mean \u00b1 std. dev. of 10 runs, 100 loops each)\n3: Code used to produce the performance graphs of the two methods for strings and numbers.3from perfplot import plot\nconstructor = lambda n: pd.DataFrame({'A': 'foo bar foo baz foo bar foo foo'.split()*n, 'B': np.random.rand(8*n)})\nplot(\n    setup=constructor,\n    kernels=[lambda df: df[(df.B%5)**2<0.1], lambda df: df.query(\"(B%5)**2<0.1\")],\n    labels= ['df[(df.B % 5) **2 < 0.1]', 'df.query(\"(B % 5) **2 < 0.1\")'],\n    n_range=[2**k for k in range(4, 24)],\n    xlabel='Rows in DataFrame',\n    title='Multiple mathematical operations on numbers',\n    equality_check=pd.DataFrame.equals);\nplot(\n    setup=constructor,\n    kernels=[lambda df: df[df.A == 'foo'], lambda df: df.query(\"A == 'foo'\")],\n    labels= [\"df[df.A == 'foo']\", \"\"\"df.query(\"A == 'foo'\")\"\"\"],\n    n_range=[2**k for k in range(4, 24)],\n    xlabel='Rows in DataFrame',\n    title='Comparison operation on strings',\n    equality_check=pd.DataFrame.equals);\nfrom perfplot import plot\nconstructor = lambda n: pd.DataFrame({'A': 'foo bar foo baz foo bar foo foo'.split()*n, 'B': np.random.rand(8*n)})\nplot(\n    setup=constructor,\n    kernels=[lambda df: df[(df.B%5)**2<0.1], lambda df: df.query(\"(B%5)**2<0.1\")],\n    labels= ['df[(df.B % 5) **2 < 0.1]', 'df.query(\"(B % 5) **2 < 0.1\")'],\n    n_range=[2**k for k in range(4, 24)],\n    xlabel='Rows in DataFrame',\n    title='Multiple mathematical operations on numbers',\n    equality_check=pd.DataFrame.equals);\nplot(\n    setup=constructor,\n    kernels=[lambda df: df[df.A == 'foo'], lambda df: df.query(\"A == 'foo'\")],\n    labels= [\"df[df.A == 'foo']\", \"\"\"df.query(\"A == 'foo'\")\"\"\"],\n    n_range=[2**k for k in range(4, 24)],\n    xlabel='Rows in DataFrame',\n    title='Comparison operation on strings',\n    equality_check=pd.DataFrame.equals);\n",
                "You can use loc (square brackets) with a function:loc# Series\ns = pd.Series([1, 2, 3, 4]) \ns.loc[lambda x: x > 1]\n# s[lambda x: x > 1]\n# Series\ns = pd.Series([1, 2, 3, 4]) \ns.loc[lambda x: x > 1]\n# s[lambda x: x > 1]\nOutput:1    2\n2    3\n3    4\ndtype: int64\n1    2\n2    3\n3    4\ndtype: int64\nor# DataFrame\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [10, 20, 30]})\ndf.loc[lambda x: x['A'] > 1]\n# df[lambda x: x['A'] > 1]\n# DataFrame\ndf = pd.DataFrame({'A': [1, 2, 3], 'B': [10, 20, 30]})\ndf.loc[lambda x: x['A'] > 1]\n# df[lambda x: x['A'] > 1]\nOutput:   A   B\n1  2  20\n2  3  30\n   A   B\n1  2  20\n2  3  30\nThe advantage of this method is that you can chain selection with previous operations. For example:df.mul(2).loc[lambda x: x['A'] > 3, 'B']\n# (df * 2).loc[lambda x: x['A'] > 3, 'B']\ndf.mul(2).loc[lambda x: x['A'] > 3, 'B']\n# (df * 2).loc[lambda x: x['A'] > 3, 'B']\nvsdf_temp = df * 2\ndf_temp.loc[df_temp['A'] > 3, 'B']\ndf_temp = df * 2\ndf_temp.loc[df_temp['A'] > 3, 'B']\nOutput:1    40\n2    60\nName: B, dtype: int64\n1    40\n2    60\nName: B, dtype: int64\n",
                "Great answers. Only, when the size of the dataframe approaches million rows, many of the methods tend to take ages when using df[df['col']==val]. I wanted to have all possible values of \"another_column\" that correspond to specific values in \"some_column\" (in this case in a dictionary). This worked and fast.size of the dataframe approaches million rowsdf[df['col']==val]s=datetime.datetime.now()\n\nmy_dict={}\n\nfor i, my_key in enumerate(df['some_column'].values): \n    if i%100==0:\n        print(i)  # to see the progress\n    if my_key not in my_dict.keys():\n        my_dict[my_key]={}\n        my_dict[my_key]['values']=[df.iloc[i]['another_column']]\n    else:\n        my_dict[my_key]['values'].append(df.iloc[i]['another_column'])\n        \ne=datetime.datetime.now()\n\nprint('operation took '+str(e-s)+' seconds')```\n\ns=datetime.datetime.now()\n\nmy_dict={}\n\nfor i, my_key in enumerate(df['some_column'].values): \n    if i%100==0:\n        print(i)  # to see the progress\n    if my_key not in my_dict.keys():\n        my_dict[my_key]={}\n        my_dict[my_key]['values']=[df.iloc[i]['another_column']]\n    else:\n        my_dict[my_key]['values'].append(df.iloc[i]['another_column'])\n        \ne=datetime.datetime.now()\n\nprint('operation took '+str(e-s)+' seconds')```\n\n"
            ]
        },
        {
            "tag": "check_empty",
            "patterns": [
                "How do I check if a list is empty?",
                "how do i know a list is empty or just a blank list?",
                "tell me the best way to see if a list is empty?",
                "how do i know if a list is empty?",
                "how can you check if a list is empty?",
                "how do you check if a list is empty?",
                "what can i do to check if a list is empty?",
                "how do i check if a list is empty?"
            ],
            "responses": [
                "if not a:\n    print(\"List is empty\")\nif not a:\n    print(\"List is empty\")\nUsing the implicit booleanness of the empty list is quite Pythonic.implicit booleannesslist",
                "The Pythonic way to do it is from the PEP 8 style guide.PEP 8 style guide\nFor sequences, (strings, lists, tuples), use the fact that empty sequences are false:\n# Correct:\nif not seq:\nif seq:\n\n# Wrong:\nif len(seq):\nif not len(seq):\n\nFor sequences, (strings, lists, tuples), use the fact that empty sequences are false:# Correct:\nif not seq:\nif seq:\n\n# Wrong:\nif len(seq):\nif not len(seq):\n# Correct:\nif not seq:\nif seq:\n\n# Wrong:\nif len(seq):\nif not len(seq):\n",
                "I prefer it explicitly:if len(li) == 0:\n    print('the list is empty')\nif len(li) == 0:\n    print('the list is empty')\nThis way it's 100% clear that li is a sequence (list) and we want to test its size. My problem with if not li: ... is that it gives the false impression that li is a boolean variable.liif not li: ...li",
                "This is the first google hit for \"python test empty array\" and similar queries, and other people are generalizing the question beyond just lists, so here's a caveat for a different type of sequence that a lot of people use.This is the first google hit for \"python test empty array\" and similar queries, and other people are generalizing the question beyond just lists, so here's a caveat for a different type of sequence that a lot of people use.Other methods don't work for NumPy arraysYou need to be careful with NumPy arrays, because other methods that work fine for lists or other standard containers fail for NumPy arrays.  I explain why below, but in short, the preferred method is to use size.listpreferred methodsizeThe \"pythonic\" way doesn't work: Part 1The \"pythonic\" way fails with NumPy arrays because NumPy tries to cast the array to an array of bools, and if x tries to evaluate all of those bools at once for some kind of aggregate truth value.  But this doesn't make any sense, so you get a ValueError:boolif xboolValueError>>> x = numpy.array([0,1])\n>>> if x: print(\"x\")\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\n>>> x = numpy.array([0,1])\n>>> if x: print(\"x\")\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\nThe \"pythonic\" way doesn't work: Part 2But at least the case above tells you that it failed.  If you happen to have a NumPy array with exactly one element, the if statement will \"work\", in the sense that you don't get an error.  However, if that one element happens to be 0 (or 0.0, or False, ...), the if statement will incorrectly result in False:if00.0FalseifFalse>>> x = numpy.array([0,])\n>>> if x: print(\"x\")\n... else: print(\"No x\")\nNo x\n>>> x = numpy.array([0,])\n>>> if x: print(\"x\")\n... else: print(\"No x\")\nNo x\nBut clearly x exists and is not empty!  This result is not what you wanted.xUsing len can give unexpected resultslenFor example,len( numpy.zeros((1,0)) )\nlen( numpy.zeros((1,0)) )\nreturns 1, even though the array has zero elements.The numpythonic wayAs explained in the SciPy FAQ, the correct method in all cases where you know you have a NumPy array is to use if x.size:SciPy FAQif x.size>>> x = numpy.array([0,1])\n>>> if x.size: print(\"x\")\nx\n\n>>> x = numpy.array([0,])\n>>> if x.size: print(\"x\")\n... else: print(\"No x\")\nx\n\n>>> x = numpy.zeros((1,0))\n>>> if x.size: print(\"x\")\n... else: print(\"No x\")\nNo x\n>>> x = numpy.array([0,1])\n>>> if x.size: print(\"x\")\nx\n\n>>> x = numpy.array([0,])\n>>> if x.size: print(\"x\")\n... else: print(\"No x\")\nx\n\n>>> x = numpy.zeros((1,0))\n>>> if x.size: print(\"x\")\n... else: print(\"No x\")\nNo x\nIf you're not sure whether it might be a list, a NumPy array, or something else, you could combine this approach with the answer @dubiousjim gives to make sure the right test is used for each type.  Not very \"pythonic\", but it turns out that NumPy intentionally broke pythonicity in at least this sense.listthe answer @dubiousjim givesIf you need to do more than just check if the input is empty, and you're using other NumPy features like indexing or math operations, it's probably more efficient (and certainly more common) to force the input to be a NumPy array.  There are a few nice functions for doing this quickly \u2014\u00a0most importantly numpy.asarray.  This takes your input, does nothing if it's already an array, or wraps your input into an array if it's a list, tuple, etc., and optionally converts it to your chosen dtype.  So it's very quick whenever it can be, and it ensures that you just get to assume the input is a NumPy array.  We usually even just use the same name, as the conversion to an array won't make it back outside of the current scope:to benumpy.asarraynumpy.asarraydtypescopex = numpy.asarray(x, dtype=numpy.double)\nx = numpy.asarray(x, dtype=numpy.double)\nThis will make the x.size check work in all cases I see on this page.x.size",
                "\nBest way to check if a list is empty\nFor example, if passed the following:\na = []\n\nHow do I check to see if a is empty?\nBest way to check if a list is emptyFor example, if passed the following:a = []\na = []\nHow do I check to see if a is empty?Short Answer:Place the list in a boolean context (for example, with an if or while statement). It will test False if it is empty, and True otherwise. For example:ifwhileFalseTrueif not a:                           # do this!\n    print('a is an empty list')\nif not a:                           # do this!\n    print('a is an empty list')\nPEP 8PEP 8, the official Python style guide for Python code in Python's standard library, asserts:PEP 8\nFor sequences, (strings, lists, tuples), use the fact that empty sequences are false.\nYes: if not seq:\n     if seq:\n\nNo: if len(seq):\n    if not len(seq):\n\nFor sequences, (strings, lists, tuples), use the fact that empty sequences are false.Yes: if not seq:\n     if seq:\n\nNo: if len(seq):\n    if not len(seq):\nYes: if not seq:\n     if seq:\n\nNo: if len(seq):\n    if not len(seq):\nWe should expect that standard library code should be as performant and correct as possible. But why is that the case, and why do we need this guidance?ExplanationI frequently see code like this from experienced programmers new to Python:if len(a) == 0:                     # Don't do this!\n    print('a is an empty list')\nif len(a) == 0:                     # Don't do this!\n    print('a is an empty list')\nAnd users of lazy languages may be tempted to do this:if a == []:                         # Don't do this!\n    print('a is an empty list')\nif a == []:                         # Don't do this!\n    print('a is an empty list')\nThese are correct in their respective other languages. And this is even semantically correct in Python. But we consider it un-Pythonic because Python supports these semantics directly in the list object's interface via boolean coercion.From the docs (and note specifically the inclusion of the empty list, []):docs[]\nBy default, an object is considered true unless its class defines\n  either a __bool__() method that returns False or a __len__() method\n  that returns zero, when called with the object. Here are most of the built-in objects considered false:\n\nconstants defined to be false: None and False.\nzero of any numeric type: 0, 0.0, 0j, Decimal(0), Fraction(0, 1)\nempty sequences and collections: '', (), [], {}, set(), range(0)\n\nBy default, an object is considered true unless its class defines\n  either a __bool__() method that returns False or a __len__() method\n  that returns zero, when called with the object. Here are most of the built-in objects considered false:__bool__()False__len__()\nconstants defined to be false: None and False.\nzero of any numeric type: 0, 0.0, 0j, Decimal(0), Fraction(0, 1)\nempty sequences and collections: '', (), [], {}, set(), range(0)\nconstants defined to be false: None and False.NoneFalsezero of any numeric type: 0, 0.0, 0j, Decimal(0), Fraction(0, 1)00.00jDecimal(0)Fraction(0, 1)empty sequences and collections: '', (), [], {}, set(), range(0)''()[]{}set()range(0)And the datamodel documentation:\nobject.__bool__(self)\nCalled to implement truth value testing and the built-in operation bool(); should return False or True. When this method is not defined,\n  __len__() is called, if it is defined, and the object is considered true if its result is nonzero. If a class defines neither __len__()\n  nor __bool__(), all its instances are considered true.\nobject.__bool__(self)object.__bool__(self)object.__bool__(self)Called to implement truth value testing and the built-in operation bool(); should return False or True. When this method is not defined,\n  __len__() is called, if it is defined, and the object is considered true if its result is nonzero. If a class defines neither __len__()\n  nor __bool__(), all its instances are considered true.bool()FalseTrue__len__()__len__()__bool__()and \nobject.__len__(self)\nCalled to implement the built-in function len(). Should return the length of the object, an integer >= 0. Also, an object that doesn\u2019t define a __bool__() method and whose __len__() method returns zero is considered to be false in a Boolean context.\nobject.__len__(self)object.__len__(self)object.__len__(self)Called to implement the built-in function len(). Should return the length of the object, an integer >= 0. Also, an object that doesn\u2019t define a __bool__() method and whose __len__() method returns zero is considered to be false in a Boolean context.len()__bool__()__len__()So instead of this:if len(a) == 0:                     # Don't do this!\n    print('a is an empty list')\nif len(a) == 0:                     # Don't do this!\n    print('a is an empty list')\nor this:if a == []:                     # Don't do this!\n    print('a is an empty list')\nif a == []:                     # Don't do this!\n    print('a is an empty list')\nDo this:if not a:\n    print('a is an empty list')\nif not a:\n    print('a is an empty list')\nDoing what's Pythonic usually pays off in performance:Does it pay off? (Note that less time to perform an equivalent operation is better:)>>> import timeit\n>>> min(timeit.repeat(lambda: len([]) == 0, repeat=100))\n0.13775854044661884\n>>> min(timeit.repeat(lambda: [] == [], repeat=100))\n0.0984637276455409\n>>> min(timeit.repeat(lambda: not [], repeat=100))\n0.07878462291455435\n>>> import timeit\n>>> min(timeit.repeat(lambda: len([]) == 0, repeat=100))\n0.13775854044661884\n>>> min(timeit.repeat(lambda: [] == [], repeat=100))\n0.0984637276455409\n>>> min(timeit.repeat(lambda: not [], repeat=100))\n0.07878462291455435\nFor scale, here's the cost of calling the function and constructing and returning an empty list, which you might subtract from the costs of the emptiness checks used above:>>> min(timeit.repeat(lambda: [], repeat=100))\n0.07074015751817342\n>>> min(timeit.repeat(lambda: [], repeat=100))\n0.07074015751817342\nWe see that either checking for length with the builtin function len compared to 0 or checking against an empty list is much less performant than using the builtin syntax of the language as documented.eitherlen0ormuchWhy?For the len(a) == 0 check:len(a) == 0First Python has to check the globals to see if len is shadowed. lenThen it must call the function, load 0, and do the equality comparison in Python (instead of with C):0>>> import dis\n>>> dis.dis(lambda: len([]) == 0)\n  1           0 LOAD_GLOBAL              0 (len)\n              2 BUILD_LIST               0\n              4 CALL_FUNCTION            1\n              6 LOAD_CONST               1 (0)\n              8 COMPARE_OP               2 (==)\n             10 RETURN_VALUE\n>>> import dis\n>>> dis.dis(lambda: len([]) == 0)\n  1           0 LOAD_GLOBAL              0 (len)\n              2 BUILD_LIST               0\n              4 CALL_FUNCTION            1\n              6 LOAD_CONST               1 (0)\n              8 COMPARE_OP               2 (==)\n             10 RETURN_VALUE\nAnd for the [] == [] it has to build an unnecessary list and then, again, do the comparison operation in Python's virtual machine (as opposed to C)[] == []>>> dis.dis(lambda: [] == [])\n  1           0 BUILD_LIST               0\n              2 BUILD_LIST               0\n              4 COMPARE_OP               2 (==)\n              6 RETURN_VALUE\n>>> dis.dis(lambda: [] == [])\n  1           0 BUILD_LIST               0\n              2 BUILD_LIST               0\n              4 COMPARE_OP               2 (==)\n              6 RETURN_VALUE\nThe \"Pythonic\" way is a much simpler and faster check since the length of the list is cached in the object instance header:>>> dis.dis(lambda: not [])\n  1           0 BUILD_LIST               0\n              2 UNARY_NOT\n              4 RETURN_VALUE\n>>> dis.dis(lambda: not [])\n  1           0 BUILD_LIST               0\n              2 UNARY_NOT\n              4 RETURN_VALUE\nEvidence from the C source and documentation\nPyVarObject\nThis is an extension of PyObject that adds the ob_size field. This is only used for objects that have some notion of length. This type does not often appear in the Python/C API. It corresponds to the fields defined by the expansion of the PyObject_VAR_HEAD macro.\nPyVarObjectPyVarObjectPyVarObjectThis is an extension of PyObject that adds the ob_size field. This is only used for objects that have some notion of length. This type does not often appear in the Python/C API. It corresponds to the fields defined by the expansion of the PyObject_VAR_HEAD macro.PyObjectob_sizePyObject_VAR_HEADFrom the c source in Include/listobject.h:Include/listobject.htypedef struct {\n    PyObject_VAR_HEAD\n    /* Vector of pointers to list elements.  list[0] is ob_item[0], etc. */\n    PyObject **ob_item;\n\n    /* ob_item contains space for 'allocated' elements.  The number\n     * currently in use is ob_size.\n     * Invariants:\n     *     0 <= ob_size <= allocated\n     *     len(list) == ob_size\ntypedef struct {\n    PyObject_VAR_HEAD\n    /* Vector of pointers to list elements.  list[0] is ob_item[0], etc. */\n    PyObject **ob_item;\n\n    /* ob_item contains space for 'allocated' elements.  The number\n     * currently in use is ob_size.\n     * Invariants:\n     *     0 <= ob_size <= allocated\n     *     len(list) == ob_size\nResponse to comments:\nI would point out that this is also true for the non-empty case though its pretty ugly as with l=[] then %timeit len(l) != 0 90.6 ns \u00b1 8.3 ns, %timeit l != [] 55.6 ns \u00b1 3.09, %timeit not not l 38.5 ns \u00b1 0.372. But there is no way anyone is going to enjoy not not l despite triple the speed. It looks ridiculous. But the speed wins out\n  I suppose the problem is testing with timeit since just if l: is sufficient but surprisingly %timeit bool(l) yields 101 ns \u00b1 2.64 ns. Interesting there is no way to coerce to bool without this penalty. %timeit l is useless since no conversion would occur.\nI would point out that this is also true for the non-empty case though its pretty ugly as with l=[] then %timeit len(l) != 0 90.6 ns \u00b1 8.3 ns, %timeit l != [] 55.6 ns \u00b1 3.09, %timeit not not l 38.5 ns \u00b1 0.372. But there is no way anyone is going to enjoy not not l despite triple the speed. It looks ridiculous. But the speed wins out\n  I suppose the problem is testing with timeit since just if l: is sufficient but surprisingly %timeit bool(l) yields 101 ns \u00b1 2.64 ns. Interesting there is no way to coerce to bool without this penalty. %timeit l is useless since no conversion would occur.l=[]%timeit len(l) != 0%timeit l != []%timeit not not lnot not lif l:%timeit bool(l)%timeit lIPython magic, %timeit, is not entirely useless here:%timeitIn [1]: l = []                                                                  \n\nIn [2]: %timeit l                                                               \n20 ns \u00b1 0.155 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000000 loops each)\n\nIn [3]: %timeit not l                                                           \n24.4 ns \u00b1 1.58 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\nIn [4]: %timeit not not l                                                       \n30.1 ns \u00b1 2.16 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\nIn [1]: l = []                                                                  \n\nIn [2]: %timeit l                                                               \n20 ns \u00b1 0.155 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000000 loops each)\n\nIn [3]: %timeit not l                                                           \n24.4 ns \u00b1 1.58 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\nIn [4]: %timeit not not l                                                       \n30.1 ns \u00b1 2.16 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\nWe can see there's a bit of linear cost for each additional not here. We want to see the costs, ceteris paribus, that is, all else equal - where all else is minimized as far as possible:notceteris paribusIn [5]: %timeit if l: pass                                                      \n22.6 ns \u00b1 0.963 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\nIn [6]: %timeit if not l: pass                                                  \n24.4 ns \u00b1 0.796 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\nIn [7]: %timeit if not not l: pass                                              \n23.4 ns \u00b1 0.793 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\nIn [5]: %timeit if l: pass                                                      \n22.6 ns \u00b1 0.963 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\nIn [6]: %timeit if not l: pass                                                  \n24.4 ns \u00b1 0.796 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\nIn [7]: %timeit if not not l: pass                                              \n23.4 ns \u00b1 0.793 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\nNow let's look at the case for an unempty list:In [8]: l = [1]                                                                 \n\nIn [9]: %timeit if l: pass                                                      \n23.7 ns \u00b1 1.06 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\nIn [10]: %timeit if not l: pass                                                 \n23.6 ns \u00b1 1.64 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\nIn [11]: %timeit if not not l: pass                                             \n26.3 ns \u00b1 1 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\nIn [8]: l = [1]                                                                 \n\nIn [9]: %timeit if l: pass                                                      \n23.7 ns \u00b1 1.06 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\nIn [10]: %timeit if not l: pass                                                 \n23.6 ns \u00b1 1.64 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\n\nIn [11]: %timeit if not not l: pass                                             \n26.3 ns \u00b1 1 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000000 loops each)\nWhat we can see here is that it makes little difference whether you pass in an actual bool to the condition check or the list itself, and if anything, giving the list, as is, is faster.boolPython is written in C; it uses its logic at the C level. Anything you write in Python will be slower. And it will likely be orders of magnitude slower unless you're using the mechanisms built into Python directly.",
                "An empty list is itself considered false in true value testing (see python documentation):python documentationa = []\nif a:\n     print(\"not empty\")\na = []\nif a:\n     print(\"not empty\")\nTo Daren Thomas's answer:Daren Thomas's answer\nEDIT: Another point against testing\nthe empty list as False: What about\npolymorphism? You shouldn't depend on\na list being a list. It should just\nquack like a duck - how are you going\nto get your duckCollection to quack\n''False'' when it has no elements?\nEDIT: Another point against testing\nthe empty list as False: What about\npolymorphism? You shouldn't depend on\na list being a list. It should just\nquack like a duck - how are you going\nto get your duckCollection to quack\n''False'' when it has no elements?Your duckCollection should implement __nonzero__ or __len__ so the if a: will work without problems.__nonzero____len__",
                "Patrick's (accepted) answer is right: if not a: is the right way to do it. Harley Holcombe's answer is right that this is in the PEP 8 style guide. But what none of the answers explain is why it's a good idea to follow the idiom\u2014even if you personally find it's not explicit enough or confusing to Ruby users or whatever.Patrick's (accepted) answerif not a:Harley Holcombe's answerPython code, and the Python community, has very strong idioms. Following those idioms makes your code easier to read for anyone experienced in Python. And when you violate those idioms, that's a strong signal.It's true that if not a: doesn't distinguish empty lists from None, or numeric 0, or empty tuples, or empty user-created collection types, or empty user-created not-quite-collection types, or single-element NumPy array acting as scalars with falsey values, etc. And sometimes it's important to be explicit about that. And in that case, you know what you want to be explicit about, so you can test for exactly that. For example, if not a and a is not None: means \"anything falsey except None\", while if len(a) != 0: means \"only empty sequences\u2014and anything besides a sequence is an error here\", and so on. Besides testing for exactly what you want to test, this also signals to the reader that this test is important.if not a:Nonewhatif not a and a is not None:if len(a) != 0:But when you don't have anything to be explicit about, anything other than if not a: is misleading the reader. You're signaling something as important when it isn't. (You may also be making the code less flexible, or slower, or whatever, but that's all less important.) And if you habitually mislead the reader like this, then when you do need to make a distinction, it's going to pass unnoticed because you've been \"crying wolf\" all over your code.if not a:habituallydo",
                "Why check at all?No one seems to have addressed questioning your need to test the list in the first place.  Because you provided no additional context, I can imagine that you may not need to do this check in the first place, but are unfamiliar with list processing in Python.needI would argue that the most Pythonic way is to not check at all, but rather to just process the list.  That way it will do the right thing whether empty or full.most Pythonica = []\n\nfor item in a:\n    # <Do something with item>\n\n# <The rest of code>\na = []\n\nfor item in a:\n    # <Do something with item>\n\n# <The rest of code>\nThis has the benefit of handling any contents of a, while not requiring a specific check for emptiness.  If a is empty, the dependent block will not execute and the interpreter will fall through to the next line.aaIf you do actually need to check the array for emptiness:a = []\n\nif not a:\n    # <React to empty list>\n\n# <The rest of code>\na = []\n\nif not a:\n    # <React to empty list>\n\n# <The rest of code>\nis sufficient.",
                "len() is an O(1) operation for Python lists, strings, dicts, and sets. Python internally keeps track of the number of elements in these containers.len() is an O(1) operationlen()JavaScript has a similar notion of truthy/falsy.has a similar notion of truthy/falsy",
                "I had written:if isinstance(a, (list, some, other, types, i, accept)) and not a:\n    do_stuff\nif isinstance(a, (list, some, other, types, i, accept)) and not a:\n    do_stuff\nwhich was voted -1. I'm not sure if that's because readers objected to the strategy or thought the answer wasn't helpful as presented. I'll pretend it was the latter, since---whatever counts as \"pythonic\"---this is the correct strategy. Unless you've already ruled out, or are prepared to handle cases where a is, for example, False, you need a test more restrictive than just if not a:. You could use something like this:aFalseif not a:if isinstance(a, numpy.ndarray) and not a.size:\n    do_stuff\nelif isinstance(a, collections.Sized) and not a:\n    do_stuff\nif isinstance(a, numpy.ndarray) and not a.size:\n    do_stuff\nelif isinstance(a, collections.Sized) and not a:\n    do_stuff\nthe first test is in response to @Mike's answer, above. The third line could also be replaced with:elif isinstance(a, (list, tuple)) and not a:\nelif isinstance(a, (list, tuple)) and not a:\nif you only want to accept instances of particular types (and their subtypes), or with:elif isinstance(a, (list, tuple)) and not len(a):\nelif isinstance(a, (list, tuple)) and not len(a):\nYou can get away without the explicit type check, but only if the surrounding context already assures you that a is a value of the types you're prepared to handle, or if you're sure that types you're not prepared to handle are going to raise errors (e.g., a TypeError if you call len on a value for which it's undefined) that you're prepared to handle. In general, the \"pythonic\" conventions seem to go this last way. Squeeze it like a duck and let it raise a DuckError if it doesn't know how to quack. You still have to think about what type assumptions you're making, though, and whether the cases you're not prepared to handle properly really are going to error out in the right places. The Numpy arrays are a good example where just blindly relying on len or the boolean typecast may not do precisely what you're expecting.aTypeErrorlenthinklen",
                "From documentation on truth value testing:documentationAll values other than what is listed here are considered TrueTrue\nNone\nFalse\nzero of any numeric type, for example, 0, 0.0, 0j.\nany empty sequence, for example, '', (), [].\nany empty mapping, for example, {}.\ninstances of user-defined classes, if the class defines a __bool__() or __len__() method, when that method returns the integer zero or bool value False.\nNoneNoneFalseFalsezero of any numeric type, for example, 0, 0.0, 0j.00.00jany empty sequence, for example, '', (), [].''()[]any empty mapping, for example, {}.{}instances of user-defined classes, if the class defines a __bool__() or __len__() method, when that method returns the integer zero or bool value False.__bool__()__len__()FalseAs can be seen, empty list [] is falsy, so doing what would be done to a boolean value sounds most efficient:[]falsyif not a:\n    print('\"a\" is empty!')\nif not a:\n    print('\"a\" is empty!')\n",
                "I prefer the following:if a == []:\n   print \"The list is empty.\"\nif a == []:\n   print \"The list is empty.\"\n",
                "Here are a few ways you can check if a list is empty:a = [] #the list\na = [] #the list\n1) The pretty simple pythonic way:1)if not a:\n    print(\"a is empty\")\nif not a:\n    print(\"a is empty\")\nIn Python, empty containers such as lists,tuples,sets,dicts,variables etc are seen as False. One could simply treat the list as a predicate (returning a Boolean value). And  a True value would indicate that it's non-empty.empty containersFalsereturning a Boolean valueTrue2) A much explicit way: using the len() to find the length and check if it equals to 0:2)len()0if len(a) == 0:\n    print(\"a is empty\")\nif len(a) == 0:\n    print(\"a is empty\")\n3) Or comparing it to an anonymous empty list:3)if a == []:\n    print(\"a is empty\")\nif a == []:\n    print(\"a is empty\")\n4) Another yet silly way to do is using exception and iter():4)sillyexceptioniter()try:\n    next(iter(a))\n    # list has elements\nexcept StopIteration:\n    print(\"Error: a is empty\")\ntry:\n    next(iter(a))\n    # list has elements\nexcept StopIteration:\n    print(\"Error: a is empty\")\n",
                "Method 1 (preferred):if not a:\n   print (\"Empty\")\nif not a:\n   print (\"Empty\")\nMethod 2:if len(a) == 0:\n   print(\"Empty\")\nif len(a) == 0:\n   print(\"Empty\")\nMethod 3:if a == []:\n  print (\"Empty\")\nif a == []:\n  print (\"Empty\")\n",
                "You can even try using bool() like this. Although it is less readable surely it's a concise way to perform this.bool()    a = [1,2,3];\n    print bool(a); # it will return True\n    a = [];\n    print bool(a); # it will return False\n    a = [1,2,3];\n    print bool(a); # it will return True\n    a = [];\n    print bool(a); # it will return False\nI love this way for the checking list is empty or not.Very handy and useful.",
                "def list_test (L):\n    if   L is None  : print('list is None')\n    elif not L      : print('list is empty')\n    else: print('list has %d elements' % len(L))\n\nlist_test(None)\nlist_test([])\nlist_test([1,2,3])\ndef list_test (L):\n    if   L is None  : print('list is None')\n    elif not L      : print('list is empty')\n    else: print('list has %d elements' % len(L))\n\nlist_test(None)\nlist_test([])\nlist_test([1,2,3])\nIt is sometimes good to test for None and for emptiness separately as those are two different states. The code above produces the following output:Nonelist is None \nlist is empty \nlist has 3 elements\nlist is None \nlist is empty \nlist has 3 elements\nAlthough it's worth nothing that None is falsy. So if you don't want to separate test for None-ness, you don't have to do that. NoneNonedef list_test2 (L):\n    if not L      : print('list is empty')\n    else: print('list has %d elements' % len(L))\n\nlist_test2(None)\nlist_test2([])\nlist_test2([1,2,3])\ndef list_test2 (L):\n    if not L      : print('list is empty')\n    else: print('list has %d elements' % len(L))\n\nlist_test2(None)\nlist_test2([])\nlist_test2([1,2,3])\nproduces expectedlist is empty\nlist is empty\nlist has 3 elements\nlist is empty\nlist is empty\nlist has 3 elements\n",
                "To check whether a list is empty or not you can use two following ways. But remember, we should avoid the way of explicitly checking for a type of sequence (it's a less Pythonic way):less Pythonicdef enquiry(list1):\n    return len(list1) == 0\n\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\n\nlist1 = []\n\nif enquiry(list1):\n    print(\"The list isn't empty\")\nelse:\n    print(\"The list is Empty\")\n\n# Result: \"The list is Empty\".\ndef enquiry(list1):\n    return len(list1) == 0\n\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\n\nlist1 = []\n\nif enquiry(list1):\n    print(\"The list isn't empty\")\nelse:\n    print(\"The list is Empty\")\n\n# Result: \"The list is Empty\".\nThe second way is a more Pythonic one. This method is an implicit way of checking and much more preferable than the previous one.more Pythonicdef enquiry(list1):\n    return not list1\n\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\n\nlist1 = []\n\nif enquiry(list1):\n    print(\"The list is Empty\")\nelse:\n    print(\"The list isn't empty\")\n\n# Result: \"The list is Empty\"\ndef enquiry(list1):\n    return not list1\n\n# \u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\u2013\n\nlist1 = []\n\nif enquiry(list1):\n    print(\"The list is Empty\")\nelse:\n    print(\"The list isn't empty\")\n\n# Result: \"The list is Empty\"\n",
                "Many answers have been given, and a lot of them are pretty good. I just wanted to add that the checknot a\nnot a\nwill also pass for None and other types of empty structures. If you truly want to check for an empty list, you can do this:Noneif isinstance(a, list) and len(a)==0:\n    print(\"Received an empty list\")\nif isinstance(a, list) and len(a)==0:\n    print(\"Received an empty list\")\n",
                "If you want to check if a list is empty:l = []\nif l:\n    # do your stuff.\nl = []\nif l:\n    # do your stuff.\nIf you want to check whether all the values in list is empty. However it will be True for an empty list:Truel = [\"\", False, 0, '', [], {}, ()]\nif all(bool(x) for x in l):\n    # do your stuff.\nl = [\"\", False, 0, '', [], {}, ()]\nif all(bool(x) for x in l):\n    # do your stuff.\nIf you want to use both cases together:def empty_list(lst):\n    if len(lst) == 0:\n        return False\n    else:\n        return all(bool(x) for x in l)\ndef empty_list(lst):\n    if len(lst) == 0:\n        return False\n    else:\n        return all(bool(x) for x in l)\nNow you can use:if empty_list(lst):\n    # do your stuff.\nif empty_list(lst):\n    # do your stuff.\n",
                "print('not empty' if a else 'empty')\nprint('not empty' if a else 'empty')\na little more practical:a.pop() if a else None\na.pop() if a else None\nand the shortest version:if a: a.pop() \nif a: a.pop() \n",
                "We could use a simple if else:if elseitem_list=[]\nif len(item_list) == 0:\n    print(\"list is empty\")\nelse:\n    print(\"list is not empty\")\nitem_list=[]\nif len(item_list) == 0:\n    print(\"list is empty\")\nelse:\n    print(\"list is not empty\")\n",
                "Being inspired by dubiousjim's solution, I propose to use an additional general check of whether is it something iterable:dubiousjim's solutionimport collections\ndef is_empty(a):\n    return not a and isinstance(a, collections.Iterable)\nimport collections\ndef is_empty(a):\n    return not a and isinstance(a, collections.Iterable)\nNote: a string is considered to be iterable\u2014add and not isinstance(a,(str,unicode)) if you want the empty string to be excludedand not isinstance(a,(str,unicode))Test:>>> is_empty('sss')\nFalse\n>>> is_empty(555)\nFalse\n>>> is_empty(0)\nFalse\n>>> is_empty('')\nTrue\n>>> is_empty([3])\nFalse\n>>> is_empty([])\nTrue\n>>> is_empty({})\nTrue\n>>> is_empty(())\nTrue\n>>> is_empty('sss')\nFalse\n>>> is_empty(555)\nFalse\n>>> is_empty(0)\nFalse\n>>> is_empty('')\nTrue\n>>> is_empty([3])\nFalse\n>>> is_empty([])\nTrue\n>>> is_empty({})\nTrue\n>>> is_empty(())\nTrue\n",
                "Simply use is_empty() or make function like:- def is_empty(any_structure):\n    if any_structure:\n        print('Structure is not empty.')\n        return True\n    else:\n        print('Structure is empty.')\n        return False  \ndef is_empty(any_structure):\n    if any_structure:\n        print('Structure is not empty.')\n        return True\n    else:\n        print('Structure is empty.')\n        return False  \nIt can be used for any data_structure like a list,tuples, dictionary and many more. By these, you can call it many times using just is_empty(any_structure). is_empty(any_structure)",
                "Simple way is checking the length is equal zero.if len(a) == 0:\n    print(\"a is empty\")\nif len(a) == 0:\n    print(\"a is empty\")\n",
                "From python3 onwards you can usea == []\na == []\nto check if the list is emptyEDIT : This works with python2.7 too.. I am not sure why there are so many complicated answers.\nIt's pretty clear and straightforward",
                "The truth value of an empty list is False whereas for a non-empty list it is True.FalseTrue",
                "What brought me here is a special use-case: I actually wanted a function to tell me if a list is empty or not. I wanted to avoid writing my own function or using a lambda-expression here (because it seemed like it should be simple enough):functionfoo = itertools.takewhile(is_not_empty, (f(x) for x in itertools.count(1)))\nfoo = itertools.takewhile(is_not_empty, (f(x) for x in itertools.count(1)))\nAnd, of course, there is a very natural way to do it:foo = itertools.takewhile(bool, (f(x) for x in itertools.count(1)))\nfoo = itertools.takewhile(bool, (f(x) for x in itertools.count(1)))\nOf course, do not use bool in if (i.e., if bool(L):) because it's implied. But, for the cases when \"is not empty\" is explicitly needed as a function, bool is the best choice.notboolifif bool(L):bool"
            ]
        },
        {
            "tag": "asterisk",
            "patterns": [
                "What does ** (double star/asterisk) and * (star/asterisk) do for parameters?",
                "how do double star  asterisk and star  asterisk compare?",
                "how does double star and asterisk work for parameters?",
                "what do double star and asterisk do for parameters?",
                "what does double starsasterisk and starasterisk do for parameters?",
                "what does double starasterisk and starasterisk do for parameters?",
                "*args", "**kwargs", "args", "kwargs"
            ],
            "responses": [
                "The *args and **kwargs is a common idiom to allow arbitrary number of arguments to functions as described in the section more on defining functions in the Python documentation.*args**kwargsmore on defining functionsThe *args will give you all function parameters as a tuple:*argsas a tupledef foo(*args):\n    for a in args:\n        print(a)        \n\nfoo(1)\n# 1\n\nfoo(1,2,3)\n# 1\n# 2\n# 3\ndef foo(*args):\n    for a in args:\n        print(a)        \n\nfoo(1)\n# 1\n\nfoo(1,2,3)\n# 1\n# 2\n# 3\nThe **kwargs will give you all\nkeyword arguments except for those corresponding to a formal parameter as a dictionary.**kwargskeyword argumentsdef bar(**kwargs):\n    for a in kwargs:\n        print(a, kwargs[a])  \n\nbar(name='one', age=27)\n# name one\n# age 27\ndef bar(**kwargs):\n    for a in kwargs:\n        print(a, kwargs[a])  \n\nbar(name='one', age=27)\n# name one\n# age 27\nBoth idioms can be mixed with normal arguments to allow a set of fixed and some variable arguments:def foo(kind, *args, **kwargs):\n   pass\ndef foo(kind, *args, **kwargs):\n   pass\nIt is also possible to use this the other way around:def foo(a, b, c):\n    print(a, b, c)\n\nobj = {'b':10, 'c':'lee'}\n\nfoo(100,**obj)\n# 100 10 lee\ndef foo(a, b, c):\n    print(a, b, c)\n\nobj = {'b':10, 'c':'lee'}\n\nfoo(100,**obj)\n# 100 10 lee\nAnother usage of the *l idiom is to unpack argument lists when calling a function.*lunpack argument listsdef foo(bar, lee):\n    print(bar, lee)\n\nl = [1,2]\n\nfoo(*l)\n# 1 2\ndef foo(bar, lee):\n    print(bar, lee)\n\nl = [1,2]\n\nfoo(*l)\n# 1 2\nIn Python 3 it is possible to use *l on the left side of an assignment (Extended Iterable Unpacking), though it gives a list instead of a tuple in this context:*lExtended Iterable Unpackingfirst, *rest = [1,2,3,4]\nfirst, *l, last = [1,2,3,4]\nfirst, *rest = [1,2,3,4]\nfirst, *l, last = [1,2,3,4]\nAlso Python 3 adds new semantic (refer PEP 3102):PEP 3102def func(arg1, arg2, arg3, *, kwarg1, kwarg2):\n    pass\ndef func(arg1, arg2, arg3, *, kwarg1, kwarg2):\n    pass\nFor example the following works in python 3 but not python 2:>>> x = [1, 2]\n>>> [*x]\n[1, 2]\n>>> [*x, 3, 4]\n[1, 2, 3, 4]\n\n>>> x = {1:1, 2:2}\n>>> x\n{1: 1, 2: 2}\n>>> {**x, 3:3, 4:4}\n{1: 1, 2: 2, 3: 3, 4: 4}\n>>> x = [1, 2]\n>>> [*x]\n[1, 2]\n>>> [*x, 3, 4]\n[1, 2, 3, 4]\n\n>>> x = {1:1, 2:2}\n>>> x\n{1: 1, 2: 2}\n>>> {**x, 3:3, 4:4}\n{1: 1, 2: 2, 3: 3, 4: 4}\nSuch function accepts only 3 positional arguments, and everything after * can only be passed as keyword arguments.*Note:\nA Python dict, semantically used for keyword argument passing, are arbitrarily ordered. However, in Python 3.6, keyword arguments are guaranteed to remember insertion order.\n\"The order of elements in **kwargs now corresponds to the order in which keyword arguments were passed to the function.\" - What\u2019s New In Python 3.6\nIn fact, all dicts in CPython 3.6 will remember insertion order as an implementation detail, this becomes standard in Python 3.7.\nA Python dict, semantically used for keyword argument passing, are arbitrarily ordered. However, in Python 3.6, keyword arguments are guaranteed to remember insertion order.dict\"The order of elements in **kwargs now corresponds to the order in which keyword arguments were passed to the function.\" - What\u2019s New In Python 3.6**kwargsWhat\u2019s New In Python 3.6In fact, all dicts in CPython 3.6 will remember insertion order as an implementation detail, this becomes standard in Python 3.7.",
                "It's also worth noting that you can use * and ** when calling functions as well. This is a shortcut that allows you to pass multiple arguments to a function directly using either a list/tuple or a dictionary. For example, if you have the following function:***def foo(x,y,z):\n    print(\"x=\" + str(x))\n    print(\"y=\" + str(y))\n    print(\"z=\" + str(z))\ndef foo(x,y,z):\n    print(\"x=\" + str(x))\n    print(\"y=\" + str(y))\n    print(\"z=\" + str(z))\nYou can do things like:>>> mylist = [1,2,3]\n>>> foo(*mylist)\nx=1\ny=2\nz=3\n\n>>> mydict = {'x':1,'y':2,'z':3}\n>>> foo(**mydict)\nx=1\ny=2\nz=3\n\n>>> mytuple = (1, 2, 3)\n>>> foo(*mytuple)\nx=1\ny=2\nz=3\n>>> mylist = [1,2,3]\n>>> foo(*mylist)\nx=1\ny=2\nz=3\n\n>>> mydict = {'x':1,'y':2,'z':3}\n>>> foo(**mydict)\nx=1\ny=2\nz=3\n\n>>> mytuple = (1, 2, 3)\n>>> foo(*mytuple)\nx=1\ny=2\nz=3\nNote: The keys in mydict have to be named exactly like the parameters of function foo. Otherwise it will throw a TypeError:mydictfooTypeError>>> mydict = {'x':1,'y':2,'z':3,'badnews':9}\n>>> foo(**mydict)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: foo() got an unexpected keyword argument 'badnews'\n>>> mydict = {'x':1,'y':2,'z':3,'badnews':9}\n>>> foo(**mydict)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: foo() got an unexpected keyword argument 'badnews'\n",
                "The single * means that there can be any number of extra positional arguments. foo() can be invoked like foo(1,2,3,4,5). In the body of foo() param2 is a sequence containing 2-5.foo()foo(1,2,3,4,5)The double ** means there can be any number of extra named parameters. bar() can be invoked like bar(1, a=2, b=3). In the body of bar() param2 is a dictionary containing {'a':2, 'b':3 }bar()bar(1, a=2, b=3)With the following code:def foo(param1, *param2):\n    print(param1)\n    print(param2)\n\ndef bar(param1, **param2):\n    print(param1)\n    print(param2)\n\nfoo(1,2,3,4,5)\nbar(1,a=2,b=3)\ndef foo(param1, *param2):\n    print(param1)\n    print(param2)\n\ndef bar(param1, **param2):\n    print(param1)\n    print(param2)\n\nfoo(1,2,3,4,5)\nbar(1,a=2,b=3)\nthe output is1\n(2, 3, 4, 5)\n1\n{'a': 2, 'b': 3}\n1\n(2, 3, 4, 5)\n1\n{'a': 2, 'b': 3}\n",
                "\nWhat does ** (double star) and * (star) do for parameters?\nWhat does ** (double star) and * (star) do for parameters?***They allow for functions to be defined to accept and for users to pass any number of arguments, positional (*) and keyword (**).functions to be defined to acceptusers to pass***Defining Functions*args allows for any number of optional positional arguments (parameters), which will be assigned to a tuple named args.*argsargs**kwargs allows for any number of optional keyword arguments (parameters), which will be in a dict named kwargs.**kwargskwargsYou can (and should) choose any appropriate name, but if the intention is for the arguments to be of non-specific semantics, args and kwargs are standard names.argskwargsExpansion, Passing any number of argumentsYou can also use *args and **kwargs to pass in parameters from lists (or any iterable) and dicts (or any mapping), respectively.*args**kwargsThe function recieving the parameters does not have to know that they are being expanded.For example, Python 2's xrange does not explicitly expect *args, but since it takes 3 integers as arguments:*args>>> x = xrange(3) # create our *args - an iterable of 3 integers\n>>> xrange(*x)    # expand here\nxrange(0, 2, 2)\n>>> x = xrange(3) # create our *args - an iterable of 3 integers\n>>> xrange(*x)    # expand here\nxrange(0, 2, 2)\nAs another example, we can use dict expansion in str.format:str.format>>> foo = 'FOO'\n>>> bar = 'BAR'\n>>> 'this is foo, {foo} and bar, {bar}'.format(**locals())\n'this is foo, FOO and bar, BAR'\n>>> foo = 'FOO'\n>>> bar = 'BAR'\n>>> 'this is foo, {foo} and bar, {bar}'.format(**locals())\n'this is foo, FOO and bar, BAR'\nNew in Python 3: Defining functions with keyword only argumentsYou can have keyword only arguments after the *args - for example, here, kwarg2 must be given as a keyword argument - not positionally:keyword only arguments*argskwarg2def foo(arg, kwarg=None, *args, kwarg2=None, **kwargs): \n    return arg, kwarg, args, kwarg2, kwargs\ndef foo(arg, kwarg=None, *args, kwarg2=None, **kwargs): \n    return arg, kwarg, args, kwarg2, kwargs\nUsage:>>> foo(1,2,3,4,5,kwarg2='kwarg2', bar='bar', baz='baz')\n(1, 2, (3, 4, 5), 'kwarg2', {'bar': 'bar', 'baz': 'baz'})\n>>> foo(1,2,3,4,5,kwarg2='kwarg2', bar='bar', baz='baz')\n(1, 2, (3, 4, 5), 'kwarg2', {'bar': 'bar', 'baz': 'baz'})\nAlso, * can be used by itself  to indicate that keyword only arguments follow, without allowing for unlimited positional arguments.*def foo(arg, kwarg=None, *, kwarg2=None, **kwargs): \n    return arg, kwarg, kwarg2, kwargs\ndef foo(arg, kwarg=None, *, kwarg2=None, **kwargs): \n    return arg, kwarg, kwarg2, kwargs\nHere, kwarg2 again must be an explicitly named, keyword argument:kwarg2>>> foo(1,2,kwarg2='kwarg2', foo='foo', bar='bar')\n(1, 2, 'kwarg2', {'foo': 'foo', 'bar': 'bar'})\n>>> foo(1,2,kwarg2='kwarg2', foo='foo', bar='bar')\n(1, 2, 'kwarg2', {'foo': 'foo', 'bar': 'bar'})\nAnd we can no longer accept unlimited positional arguments because we don't have *args*:*args*>>> foo(1,2,3,4,5, kwarg2='kwarg2', foo='foo', bar='bar')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: foo() takes from 1 to 2 positional arguments \n    but 5 positional arguments (and 1 keyword-only argument) were given\n>>> foo(1,2,3,4,5, kwarg2='kwarg2', foo='foo', bar='bar')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: foo() takes from 1 to 2 positional arguments \n    but 5 positional arguments (and 1 keyword-only argument) were given\nAgain, more simply, here we require kwarg to be given by name, not positionally:kwargdef bar(*, kwarg=None): \n    return kwarg\ndef bar(*, kwarg=None): \n    return kwarg\nIn this example, we see that if we try to pass kwarg positionally, we get an error:kwarg>>> bar('kwarg')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: bar() takes 0 positional arguments but 1 was given\n>>> bar('kwarg')\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nTypeError: bar() takes 0 positional arguments but 1 was given\nWe must explicitly pass the kwarg parameter as a keyword argument.kwarg>>> bar(kwarg='kwarg')\n'kwarg'\n>>> bar(kwarg='kwarg')\n'kwarg'\nPython 2 compatible demos*args (typically said \"star-args\") and **kwargs (stars can be implied by saying \"kwargs\", but be explicit with \"double-star kwargs\") are common idioms of Python for using the * and ** notation. These specific variable names aren't required (e.g. you could use *foos and **bars), but a departure from convention is likely to enrage your fellow Python coders.*args**kwargs****foos**barsWe typically use these when we don't know what our function is going to receive or how many arguments we may be passing, and sometimes even when naming every variable separately would get very messy and redundant (but this is a case where usually explicit is better than implicit).Example 1Example 1The following function describes how they can be used, and demonstrates behavior. Note the named b argument will be consumed by the second positional argument before :bdef foo(a, b=10, *args, **kwargs):\n    '''\n    this function takes required argument a, not required keyword argument b\n    and any number of unknown positional arguments and keyword arguments after\n    '''\n    print('a is a required argument, and its value is {0}'.format(a))\n    print('b not required, its default value is 10, actual value: {0}'.format(b))\n    # we can inspect the unknown arguments we were passed:\n    #  - args:\n    print('args is of type {0} and length {1}'.format(type(args), len(args)))\n    for arg in args:\n        print('unknown arg: {0}'.format(arg))\n    #  - kwargs:\n    print('kwargs is of type {0} and length {1}'.format(type(kwargs),\n                                                        len(kwargs)))\n    for kw, arg in kwargs.items():\n        print('unknown kwarg - kw: {0}, arg: {1}'.format(kw, arg))\n    # But we don't have to know anything about them \n    # to pass them to other functions.\n    print('Args or kwargs can be passed without knowing what they are.')\n    # max can take two or more positional args: max(a, b, c...)\n    print('e.g. max(a, b, *args) \\n{0}'.format(\n      max(a, b, *args))) \n    kweg = 'dict({0})'.format( # named args same as unknown kwargs\n      ', '.join('{k}={v}'.format(k=k, v=v) \n                             for k, v in sorted(kwargs.items())))\n    print('e.g. dict(**kwargs) (same as {kweg}) returns: \\n{0}'.format(\n      dict(**kwargs), kweg=kweg))\ndef foo(a, b=10, *args, **kwargs):\n    '''\n    this function takes required argument a, not required keyword argument b\n    and any number of unknown positional arguments and keyword arguments after\n    '''\n    print('a is a required argument, and its value is {0}'.format(a))\n    print('b not required, its default value is 10, actual value: {0}'.format(b))\n    # we can inspect the unknown arguments we were passed:\n    #  - args:\n    print('args is of type {0} and length {1}'.format(type(args), len(args)))\n    for arg in args:\n        print('unknown arg: {0}'.format(arg))\n    #  - kwargs:\n    print('kwargs is of type {0} and length {1}'.format(type(kwargs),\n                                                        len(kwargs)))\n    for kw, arg in kwargs.items():\n        print('unknown kwarg - kw: {0}, arg: {1}'.format(kw, arg))\n    # But we don't have to know anything about them \n    # to pass them to other functions.\n    print('Args or kwargs can be passed without knowing what they are.')\n    # max can take two or more positional args: max(a, b, c...)\n    print('e.g. max(a, b, *args) \\n{0}'.format(\n      max(a, b, *args))) \n    kweg = 'dict({0})'.format( # named args same as unknown kwargs\n      ', '.join('{k}={v}'.format(k=k, v=v) \n                             for k, v in sorted(kwargs.items())))\n    print('e.g. dict(**kwargs) (same as {kweg}) returns: \\n{0}'.format(\n      dict(**kwargs), kweg=kweg))\nWe can check the online help for the function's signature, with help(foo), which tells ushelp(foo)foo(a, b=10, *args, **kwargs)\nfoo(a, b=10, *args, **kwargs)\nLet's call this function with foo(1, 2, 3, 4, e=5, f=6, g=7)foo(1, 2, 3, 4, e=5, f=6, g=7)which prints:a is a required argument, and its value is 1\nb not required, its default value is 10, actual value: 2\nargs is of type <type 'tuple'> and length 2\nunknown arg: 3\nunknown arg: 4\nkwargs is of type <type 'dict'> and length 3\nunknown kwarg - kw: e, arg: 5\nunknown kwarg - kw: g, arg: 7\nunknown kwarg - kw: f, arg: 6\nArgs or kwargs can be passed without knowing what they are.\ne.g. max(a, b, *args) \n4\ne.g. dict(**kwargs) (same as dict(e=5, f=6, g=7)) returns: \n{'e': 5, 'g': 7, 'f': 6}\na is a required argument, and its value is 1\nb not required, its default value is 10, actual value: 2\nargs is of type <type 'tuple'> and length 2\nunknown arg: 3\nunknown arg: 4\nkwargs is of type <type 'dict'> and length 3\nunknown kwarg - kw: e, arg: 5\nunknown kwarg - kw: g, arg: 7\nunknown kwarg - kw: f, arg: 6\nArgs or kwargs can be passed without knowing what they are.\ne.g. max(a, b, *args) \n4\ne.g. dict(**kwargs) (same as dict(e=5, f=6, g=7)) returns: \n{'e': 5, 'g': 7, 'f': 6}\nExample 2Example 2We can also call it using another function, into which we just provide a:adef bar(a):\n    b, c, d, e, f = 2, 3, 4, 5, 6\n    # dumping every local variable into foo as a keyword argument \n    # by expanding the locals dict:\n    foo(**locals()) \ndef bar(a):\n    b, c, d, e, f = 2, 3, 4, 5, 6\n    # dumping every local variable into foo as a keyword argument \n    # by expanding the locals dict:\n    foo(**locals()) \nbar(100) prints:bar(100)a is a required argument, and its value is 100\nb not required, its default value is 10, actual value: 2\nargs is of type <type 'tuple'> and length 0\nkwargs is of type <type 'dict'> and length 4\nunknown kwarg - kw: c, arg: 3\nunknown kwarg - kw: e, arg: 5\nunknown kwarg - kw: d, arg: 4\nunknown kwarg - kw: f, arg: 6\nArgs or kwargs can be passed without knowing what they are.\ne.g. max(a, b, *args) \n100\ne.g. dict(**kwargs) (same as dict(c=3, d=4, e=5, f=6)) returns: \n{'c': 3, 'e': 5, 'd': 4, 'f': 6}\na is a required argument, and its value is 100\nb not required, its default value is 10, actual value: 2\nargs is of type <type 'tuple'> and length 0\nkwargs is of type <type 'dict'> and length 4\nunknown kwarg - kw: c, arg: 3\nunknown kwarg - kw: e, arg: 5\nunknown kwarg - kw: d, arg: 4\nunknown kwarg - kw: f, arg: 6\nArgs or kwargs can be passed without knowing what they are.\ne.g. max(a, b, *args) \n100\ne.g. dict(**kwargs) (same as dict(c=3, d=4, e=5, f=6)) returns: \n{'c': 3, 'e': 5, 'd': 4, 'f': 6}\nExample 3: practical usage in decoratorsExample 3: practical usage in decoratorsOK, so maybe we're not seeing the utility yet. So imagine you have several functions with redundant code before and/or after the differentiating code. The following named functions are just pseudo-code for illustrative purposes.def foo(a, b, c, d=0, e=100):\n    # imagine this is much more code than a simple function call\n    preprocess() \n    differentiating_process_foo(a,b,c,d,e)\n    # imagine this is much more code than a simple function call\n    postprocess()\n\ndef bar(a, b, c=None, d=0, e=100, f=None):\n    preprocess()\n    differentiating_process_bar(a,b,c,d,e,f)\n    postprocess()\n\ndef baz(a, b, c, d, e, f):\n    ... and so on\ndef foo(a, b, c, d=0, e=100):\n    # imagine this is much more code than a simple function call\n    preprocess() \n    differentiating_process_foo(a,b,c,d,e)\n    # imagine this is much more code than a simple function call\n    postprocess()\n\ndef bar(a, b, c=None, d=0, e=100, f=None):\n    preprocess()\n    differentiating_process_bar(a,b,c,d,e,f)\n    postprocess()\n\ndef baz(a, b, c, d, e, f):\n    ... and so on\nWe might be able to handle this differently, but we can certainly extract the redundancy with a decorator, and so our below example demonstrates how *args and **kwargs can be very useful:*args**kwargsdef decorator(function):\n    '''function to wrap other functions with a pre- and postprocess'''\n    @functools.wraps(function) # applies module, name, and docstring to wrapper\n    def wrapper(*args, **kwargs):\n        # again, imagine this is complicated, but we only write it once!\n        preprocess()\n        function(*args, **kwargs)\n        postprocess()\n    return wrapper\ndef decorator(function):\n    '''function to wrap other functions with a pre- and postprocess'''\n    @functools.wraps(function) # applies module, name, and docstring to wrapper\n    def wrapper(*args, **kwargs):\n        # again, imagine this is complicated, but we only write it once!\n        preprocess()\n        function(*args, **kwargs)\n        postprocess()\n    return wrapper\nAnd now every wrapped function can be written much more succinctly, as we've factored out the redundancy:@decorator\ndef foo(a, b, c, d=0, e=100):\n    differentiating_process_foo(a,b,c,d,e)\n\n@decorator\ndef bar(a, b, c=None, d=0, e=100, f=None):\n    differentiating_process_bar(a,b,c,d,e,f)\n\n@decorator\ndef baz(a, b, c=None, d=0, e=100, f=None, g=None):\n    differentiating_process_baz(a,b,c,d,e,f, g)\n\n@decorator\ndef quux(a, b, c=None, d=0, e=100, f=None, g=None, h=None):\n    differentiating_process_quux(a,b,c,d,e,f,g,h)\n@decorator\ndef foo(a, b, c, d=0, e=100):\n    differentiating_process_foo(a,b,c,d,e)\n\n@decorator\ndef bar(a, b, c=None, d=0, e=100, f=None):\n    differentiating_process_bar(a,b,c,d,e,f)\n\n@decorator\ndef baz(a, b, c=None, d=0, e=100, f=None, g=None):\n    differentiating_process_baz(a,b,c,d,e,f, g)\n\n@decorator\ndef quux(a, b, c=None, d=0, e=100, f=None, g=None, h=None):\n    differentiating_process_quux(a,b,c,d,e,f,g,h)\nAnd by factoring out our code, which *args and **kwargs allows us to do, we reduce lines of code, improve readability and maintainability, and have sole canonical locations for the logic in our program. If we need to change any part of this structure, we have one place in which to make each change.*args**kwargs",
                "Let us first understand what are positional arguments and keyword arguments.\nBelow is an example of function definition with Positional arguments.Positional arguments.def test(a,b,c):\n     print(a)\n     print(b)\n     print(c)\n\ntest(1,2,3)\n#output:\n1\n2\n3\ndef test(a,b,c):\n     print(a)\n     print(b)\n     print(c)\n\ntest(1,2,3)\n#output:\n1\n2\n3\nSo this is a function definition with positional arguments.\nYou can call it with keyword/named arguments as well:def test(a,b,c):\n     print(a)\n     print(b)\n     print(c)\n\ntest(a=1,b=2,c=3)\n#output:\n1\n2\n3\ndef test(a,b,c):\n     print(a)\n     print(b)\n     print(c)\n\ntest(a=1,b=2,c=3)\n#output:\n1\n2\n3\nNow let us study an example of function definition with keyword arguments:keyword argumentsdef test(a=0,b=0,c=0):\n     print(a)\n     print(b)\n     print(c)\n     print('-------------------------')\n\ntest(a=1,b=2,c=3)\n#output :\n1\n2\n3\n-------------------------\ndef test(a=0,b=0,c=0):\n     print(a)\n     print(b)\n     print(c)\n     print('-------------------------')\n\ntest(a=1,b=2,c=3)\n#output :\n1\n2\n3\n-------------------------\nYou can call this function with positional arguments as well:def test(a=0,b=0,c=0):\n    print(a)\n    print(b)\n    print(c)\n    print('-------------------------')\n\ntest(1,2,3)\n# output :\n1\n2\n3\n---------------------------------\ndef test(a=0,b=0,c=0):\n    print(a)\n    print(b)\n    print(c)\n    print('-------------------------')\n\ntest(1,2,3)\n# output :\n1\n2\n3\n---------------------------------\nSo we now know function definitions with positional as well as keyword arguments.Now let us study the '*' operator and '**' operator.Please note these operators can be used in 2 areas:a) function callfunction callb) function definitionfunction definitionThe use of '*' operator and '**' operator in function call. function call.Let us get straight to an example and then discuss it.def sum(a,b):  #receive args from function calls as sum(1,2) or sum(a=1,b=2)\n    print(a+b)\n\nmy_tuple = (1,2)\nmy_list = [1,2]\nmy_dict = {'a':1,'b':2}\n\n# Let us unpack data structure of list or tuple or dict into arguments with help of '*' operator\nsum(*my_tuple)   # becomes same as sum(1,2) after unpacking my_tuple with '*'\nsum(*my_list)    # becomes same as sum(1,2) after unpacking my_list with  '*'\nsum(**my_dict)   # becomes same as sum(a=1,b=2) after unpacking by '**' \n\n# output is 3 in all three calls to sum function.\ndef sum(a,b):  #receive args from function calls as sum(1,2) or sum(a=1,b=2)\n    print(a+b)\n\nmy_tuple = (1,2)\nmy_list = [1,2]\nmy_dict = {'a':1,'b':2}\n\n# Let us unpack data structure of list or tuple or dict into arguments with help of '*' operator\nsum(*my_tuple)   # becomes same as sum(1,2) after unpacking my_tuple with '*'\nsum(*my_list)    # becomes same as sum(1,2) after unpacking my_list with  '*'\nsum(**my_dict)   # becomes same as sum(a=1,b=2) after unpacking by '**' \n\n# output is 3 in all three calls to sum function.\nSo remember when the '*' or '**' operator is used in a function call -function call'*' operator unpacks data structure such as a list or tuple  into arguments needed by function definition.'**' operator unpacks a dictionary into arguments needed by function definition.Now let us study the '*' operator use in function definition.\nExample:function definitiondef sum(*args): #pack the received positional args into data structure of tuple. after applying '*' - def sum((1,2,3,4))\n    sum = 0\n    for a in args:\n        sum+=a\n    print(sum)\n\nsum(1,2,3,4)  #positional args sent to function sum\n#output:\n10\ndef sum(*args): #pack the received positional args into data structure of tuple. after applying '*' - def sum((1,2,3,4))\n    sum = 0\n    for a in args:\n        sum+=a\n    print(sum)\n\nsum(1,2,3,4)  #positional args sent to function sum\n#output:\n10\nIn function definition the '*' operator packs the received arguments into a tuple.definitionNow let us see an example of '**' used in function definition:def sum(**args): #pack keyword args into datastructure of dict after applying '**' - def sum({a:1,b:2,c:3,d:4})\n    sum=0\n    for k,v in args.items():\n        sum+=v\n    print(sum)\n\nsum(a=1,b=2,c=3,d=4) #positional args sent to function sum\ndef sum(**args): #pack keyword args into datastructure of dict after applying '**' - def sum({a:1,b:2,c:3,d:4})\n    sum=0\n    for k,v in args.items():\n        sum+=v\n    print(sum)\n\nsum(a=1,b=2,c=3,d=4) #positional args sent to function sum\nIn function definition The '**' operator packs the received arguments into a dictionary.definitionSo remember:In a function call the '*' unpacks data structure of tuple or list into positional or keyword arguments to be received by function definition.function callunpacksIn a function call the '**' unpacks data structure of dictionary into positional or keyword arguments to be received by function definition.function callunpacksIn a function definition the '*' packs positional arguments into a tuple.function definitionpacksIn a function definition the '**' packs keyword arguments into a dictionary.function definitionpacks",
                "This table is handy for using * and ** in function construction and function call:***constructioncall            In function construction         In function call\n=======================================================================\n          |  def f(*args):                 |  def f(a, b):\n*args     |      for arg in args:          |      return a + b\n          |          print(arg)            |  args = (1, 2)\n          |  f(1, 2)                       |  f(*args)\n----------|--------------------------------|---------------------------\n          |  def f(a, b):                  |  def f(a, b):\n**kwargs  |      return a + b              |      return a + b\n          |  def g(**kwargs):              |  kwargs = dict(a=1, b=2)\n          |      return f(**kwargs)        |  f(**kwargs)\n          |  g(a=1, b=2)                   |\n-----------------------------------------------------------------------\n            In function construction         In function call\n=======================================================================\n          |  def f(*args):                 |  def f(a, b):\n*args     |      for arg in args:          |      return a + b\n          |          print(arg)            |  args = (1, 2)\n          |  f(1, 2)                       |  f(*args)\n----------|--------------------------------|---------------------------\n          |  def f(a, b):                  |  def f(a, b):\n**kwargs  |      return a + b              |      return a + b\n          |  def g(**kwargs):              |  kwargs = dict(a=1, b=2)\n          |      return f(**kwargs)        |  f(**kwargs)\n          |  g(a=1, b=2)                   |\n-----------------------------------------------------------------------\nThis really just serves to summarize Lorin Hochstein's answer but I find it helpful.answerRelatedly: uses for the star/splat operators have been expanded in Python 3expanded",
                "* and ** have special usage in the function argument list. *\nimplies that the argument is a list and ** implies that the argument\nis a dictionary. This allows functions to take arbitrary number of\narguments******",
                "TL;DRTL;DRBelow are 6 different use cases for * and ** in python programming:***\nTo accept any number of positional arguments using *args: def foo(*args): pass, here foo accepts any number of positional arguments, i. e., the following calls are valid foo(1), foo(1, 'bar')\nTo accept any number of keyword arguments using **kwargs: def foo(**kwargs): pass, here 'foo' accepts any number of keyword arguments, i. e., the following calls are valid foo(name='Tom'), foo(name='Tom', age=33)\nTo accept any number of positional and keyword arguments using *args, **kwargs: def foo(*args, **kwargs): pass, here foo accepts any number of positional and keyword arguments, i. e., the following calls are valid foo(1,name='Tom'), foo(1, 'bar', name='Tom', age=33)\nTo enforce keyword only arguments using *: def foo(pos1, pos2, *, kwarg1): pass, here * means that foo only accept keyword arguments after pos2, hence foo(1, 2, 3) raises TypeError but foo(1, 2, kwarg1=3) is ok.\nTo express no further interest in more positional arguments using *_ (Note: this is a convention only): def foo(bar, baz, *_): pass means (by convention) foo only uses bar and baz arguments in its working and will ignore others.\nTo express no further interest in more keyword arguments using **_ (Note: this is a convention only): def foo(bar, baz, **_): pass means (by convention) foo only uses bar and baz arguments in its working and will ignore others.\nTo accept any number of positional arguments using *args: def foo(*args): pass, here foo accepts any number of positional arguments, i. e., the following calls are valid foo(1), foo(1, 'bar')To accept any number of positional arguments using *args:*argsdef foo(*args): passfoofoo(1)foo(1, 'bar')To accept any number of keyword arguments using **kwargs: def foo(**kwargs): pass, here 'foo' accepts any number of keyword arguments, i. e., the following calls are valid foo(name='Tom'), foo(name='Tom', age=33)To accept any number of keyword arguments using **kwargs:**kwargsdef foo(**kwargs): passfoo(name='Tom')foo(name='Tom', age=33)To accept any number of positional and keyword arguments using *args, **kwargs: def foo(*args, **kwargs): pass, here foo accepts any number of positional and keyword arguments, i. e., the following calls are valid foo(1,name='Tom'), foo(1, 'bar', name='Tom', age=33)To accept any number of positional and keyword arguments using *args, **kwargs:*args, **kwargsdef foo(*args, **kwargs): passfoofoo(1,name='Tom')foo(1, 'bar', name='Tom', age=33)To enforce keyword only arguments using *: def foo(pos1, pos2, *, kwarg1): pass, here * means that foo only accept keyword arguments after pos2, hence foo(1, 2, 3) raises TypeError but foo(1, 2, kwarg1=3) is ok.To enforce keyword only arguments using *:*def foo(pos1, pos2, *, kwarg1): pass*foo(1, 2, 3)foo(1, 2, kwarg1=3)To express no further interest in more positional arguments using *_ (Note: this is a convention only): def foo(bar, baz, *_): pass means (by convention) foo only uses bar and baz arguments in its working and will ignore others.To express no further interest in more positional arguments using *_ (Note: this is a convention only):*_def foo(bar, baz, *_): passfoobarbazTo express no further interest in more keyword arguments using **_ (Note: this is a convention only): def foo(bar, baz, **_): pass means (by convention) foo only uses bar and baz arguments in its working and will ignore others.To express no further interest in more keyword arguments using **_ (Note: this is a convention only):**_def foo(bar, baz, **_): passfoobarbazBONUS: From python 3.8 onward, one can use / in function definition to enforce  positional only parameters. In the following example, parameters a and b are positional-only, while c or d can be positional or keyword, and e or f are required to be keywords:BONUS:/positional-onlydef f(a, b, /, c, d, *, e, f):\n    pass\ndef f(a, b, /, c, d, *, e, f):\n    pass\nBONUS 2: THIS ANSWER to the same question also brings a new perspective, where it shares what does * and ** means in a function call, functions signature, for loops, etc.BONUS 2THIS ANSWER***function callfunctions signaturefor loops",
                "For those of you who learn by examples!\nThe purpose of *  is to give you the ability to define a function that can take an arbitrary number of arguments provided as a list (e.g. f(*myList) ).\nThe purpose of ** is to give you the ability to feed a function's arguments by providing a dictionary (e.g. f(**{'x' : 1, 'y' : 2}) ).\nThe purpose of *  is to give you the ability to define a function that can take an arbitrary number of arguments provided as a list (e.g. f(*myList) ).*f(*myList)The purpose of ** is to give you the ability to feed a function's arguments by providing a dictionary (e.g. f(**{'x' : 1, 'y' : 2}) ).**f(**{'x' : 1, 'y' : 2})Let us show this by defining a function that takes two normal variables x, y, and can accept more arguments as myArgs, and can accept even more arguments as myKW. Later, we will show how to feed y using myArgDict.xymyArgsmyKWymyArgDictdef f(x, y, *myArgs, **myKW):\n    print(\"# x      = {}\".format(x))\n    print(\"# y      = {}\".format(y))\n    print(\"# myArgs = {}\".format(myArgs))\n    print(\"# myKW   = {}\".format(myKW))\n    print(\"# ----------------------------------------------------------------------\")\n\n# Define a list for demonstration purposes\nmyList    = [\"Left\", \"Right\", \"Up\", \"Down\"]\n# Define a dictionary for demonstration purposes\nmyDict    = {\"Wubba\": \"lubba\", \"Dub\": \"dub\"}\n# Define a dictionary to feed y\nmyArgDict = {'y': \"Why?\", 'y0': \"Why not?\", \"q\": \"Here is a cue!\"}\n\n# The 1st elem of myList feeds y\nf(\"myEx\", *myList, **myDict)\n# x      = myEx\n# y      = Left\n# myArgs = ('Right', 'Up', 'Down')\n# myKW   = {'Wubba': 'lubba', 'Dub': 'dub'}\n# ----------------------------------------------------------------------\n\n# y is matched and fed first\n# The rest of myArgDict becomes additional arguments feeding myKW\nf(\"myEx\", **myArgDict)\n# x      = myEx\n# y      = Why?\n# myArgs = ()\n# myKW   = {'y0': 'Why not?', 'q': 'Here is a cue!'}\n# ----------------------------------------------------------------------\n\n# The rest of myArgDict becomes additional arguments feeding myArgs\nf(\"myEx\", *myArgDict)\n# x      = myEx\n# y      = y\n# myArgs = ('y0', 'q')\n# myKW   = {}\n# ----------------------------------------------------------------------\n\n# Feed extra arguments manually and append even more from my list\nf(\"myEx\", 4, 42, 420, *myList, *myDict, **myDict)\n# x      = myEx\n# y      = 4\n# myArgs = (42, 420, 'Left', 'Right', 'Up', 'Down', 'Wubba', 'Dub')\n# myKW   = {'Wubba': 'lubba', 'Dub': 'dub'}\n# ----------------------------------------------------------------------\n\n# Without the stars, the entire provided list and dict become x, and y:\nf(myList, myDict)\n# x      = ['Left', 'Right', 'Up', 'Down']\n# y      = {'Wubba': 'lubba', 'Dub': 'dub'}\n# myArgs = ()\n# myKW   = {}\n# ----------------------------------------------------------------------\ndef f(x, y, *myArgs, **myKW):\n    print(\"# x      = {}\".format(x))\n    print(\"# y      = {}\".format(y))\n    print(\"# myArgs = {}\".format(myArgs))\n    print(\"# myKW   = {}\".format(myKW))\n    print(\"# ----------------------------------------------------------------------\")\n\n# Define a list for demonstration purposes\nmyList    = [\"Left\", \"Right\", \"Up\", \"Down\"]\n# Define a dictionary for demonstration purposes\nmyDict    = {\"Wubba\": \"lubba\", \"Dub\": \"dub\"}\n# Define a dictionary to feed y\nmyArgDict = {'y': \"Why?\", 'y0': \"Why not?\", \"q\": \"Here is a cue!\"}\n\n# The 1st elem of myList feeds y\nf(\"myEx\", *myList, **myDict)\n# x      = myEx\n# y      = Left\n# myArgs = ('Right', 'Up', 'Down')\n# myKW   = {'Wubba': 'lubba', 'Dub': 'dub'}\n# ----------------------------------------------------------------------\n\n# y is matched and fed first\n# The rest of myArgDict becomes additional arguments feeding myKW\nf(\"myEx\", **myArgDict)\n# x      = myEx\n# y      = Why?\n# myArgs = ()\n# myKW   = {'y0': 'Why not?', 'q': 'Here is a cue!'}\n# ----------------------------------------------------------------------\n\n# The rest of myArgDict becomes additional arguments feeding myArgs\nf(\"myEx\", *myArgDict)\n# x      = myEx\n# y      = y\n# myArgs = ('y0', 'q')\n# myKW   = {}\n# ----------------------------------------------------------------------\n\n# Feed extra arguments manually and append even more from my list\nf(\"myEx\", 4, 42, 420, *myList, *myDict, **myDict)\n# x      = myEx\n# y      = 4\n# myArgs = (42, 420, 'Left', 'Right', 'Up', 'Down', 'Wubba', 'Dub')\n# myKW   = {'Wubba': 'lubba', 'Dub': 'dub'}\n# ----------------------------------------------------------------------\n\n# Without the stars, the entire provided list and dict become x, and y:\nf(myList, myDict)\n# x      = ['Left', 'Right', 'Up', 'Down']\n# y      = {'Wubba': 'lubba', 'Dub': 'dub'}\n# myArgs = ()\n# myKW   = {}\n# ----------------------------------------------------------------------\nCaveats\n** is exclusively reserved for dictionaries.\nNon-optional argument assignment happens first.\nYou cannot use a non-optional argument twice.\nIf applicable, ** must come after *, always.\n** is exclusively reserved for dictionaries.**Non-optional argument assignment happens first.You cannot use a non-optional argument twice.If applicable, ** must come after *, always.***",
                "From the Python documentation:\nIf there are more positional arguments than there are formal parameter slots, a TypeError exception is raised, unless a formal parameter using the syntax \"*identifier\" is present; in this case, that formal parameter receives a tuple containing the excess positional arguments (or an empty tuple if there were no excess positional arguments). \nIf any keyword argument does not correspond to a formal parameter name, a TypeError exception is raised, unless a formal parameter using the syntax \"**identifier\" is present; in this case, that formal parameter receives a dictionary containing the excess keyword arguments (using the keywords as keys and the argument values as corresponding values), or a (new) empty dictionary if there were no excess keyword arguments. \nIf there are more positional arguments than there are formal parameter slots, a TypeError exception is raised, unless a formal parameter using the syntax \"*identifier\" is present; in this case, that formal parameter receives a tuple containing the excess positional arguments (or an empty tuple if there were no excess positional arguments). If any keyword argument does not correspond to a formal parameter name, a TypeError exception is raised, unless a formal parameter using the syntax \"**identifier\" is present; in this case, that formal parameter receives a dictionary containing the excess keyword arguments (using the keywords as keys and the argument values as corresponding values), or a (new) empty dictionary if there were no excess keyword arguments. ",
                "* means receive variable arguments as tuple*** means receive variable arguments as dictionary**Used like the following:1) single *1) single *def foo(*args):\n    for arg in args:\n        print(arg)\n\nfoo(\"two\", 3)\ndef foo(*args):\n    for arg in args:\n        print(arg)\n\nfoo(\"two\", 3)\nOutput:Output:two\n3\ntwo\n3\n2) Now **2) Now ****def bar(**kwargs):\n    for key in kwargs:\n        print(key, kwargs[key])\n\nbar(dic1=\"two\", dic2=3)\ndef bar(**kwargs):\n    for key in kwargs:\n        print(key, kwargs[key])\n\nbar(dic1=\"two\", dic2=3)\nOutput:Output:dic1 two\ndic2 3\ndic1 two\ndic2 3\n",
                "In Python 3.5, you can also use this syntax in list, dict, tuple, and set displays (also sometimes called literals). See PEP 488: Additional Unpacking Generalizations.listdicttuplesetPEP 488: Additional Unpacking Generalizations>>> (0, *range(1, 4), 5, *range(6, 8))\n(0, 1, 2, 3, 5, 6, 7)\n>>> [0, *range(1, 4), 5, *range(6, 8)]\n[0, 1, 2, 3, 5, 6, 7]\n>>> {0, *range(1, 4), 5, *range(6, 8)}\n{0, 1, 2, 3, 5, 6, 7}\n>>> d = {'one': 1, 'two': 2, 'three': 3}\n>>> e = {'six': 6, 'seven': 7}\n>>> {'zero': 0, **d, 'five': 5, **e}\n{'five': 5, 'seven': 7, 'two': 2, 'one': 1, 'three': 3, 'six': 6, 'zero': 0}\n>>> (0, *range(1, 4), 5, *range(6, 8))\n(0, 1, 2, 3, 5, 6, 7)\n>>> [0, *range(1, 4), 5, *range(6, 8)]\n[0, 1, 2, 3, 5, 6, 7]\n>>> {0, *range(1, 4), 5, *range(6, 8)}\n{0, 1, 2, 3, 5, 6, 7}\n>>> d = {'one': 1, 'two': 2, 'three': 3}\n>>> e = {'six': 6, 'seven': 7}\n>>> {'zero': 0, **d, 'five': 5, **e}\n{'five': 5, 'seven': 7, 'two': 2, 'one': 1, 'three': 3, 'six': 6, 'zero': 0}\nIt also allows multiple iterables to be unpacked in a single function call.>>> range(*[1, 10], *[2])\nrange(1, 10, 2)\n>>> range(*[1, 10], *[2])\nrange(1, 10, 2)\n(Thanks to mgilson for the PEP link.)",
                "TL;DRIt packs arguments passed to the function into list and dict respectively inside the function body. When you define a function signature like this:listdictdef func(*args, **kwds):\n    # do stuff\ndef func(*args, **kwds):\n    # do stuff\nit can be called with any number of arguments and keyword arguments. The non-keyword arguments get packed into a list called args inside the function body and the keyword arguments get packed into a dict called kwds inside the function body.argskwdsfunc(\"this\", \"is a list of\", \"non-keyowrd\", \"arguments\", keyword=\"ligma\", options=[1,2,3])\nfunc(\"this\", \"is a list of\", \"non-keyowrd\", \"arguments\", keyword=\"ligma\", options=[1,2,3])\nnow inside the function body, when the function is called, there are two local variables, args which is a list having value [\"this\", \"is a list of\", \"non-keyword\", \"arguments\"] and kwds which is a dict having value {\"keyword\" : \"ligma\", \"options\" : [1,2,3]}args[\"this\", \"is a list of\", \"non-keyword\", \"arguments\"]kwdsdict{\"keyword\" : \"ligma\", \"options\" : [1,2,3]}This also works in reverse, i.e. from the caller side. for example if you have a function defined as:def f(a, b, c, d=1, e=10):\n    # do stuff\ndef f(a, b, c, d=1, e=10):\n    # do stuff\nyou can call it with by unpacking iterables or mappings you have in the calling scope:iterable = [1, 20, 500]\nmapping = {\"d\" : 100, \"e\": 3}\nf(*iterable, **mapping)\n# That call is equivalent to\nf(1, 20, 500, d=100, e=3)\niterable = [1, 20, 500]\nmapping = {\"d\" : 100, \"e\": 3}\nf(*iterable, **mapping)\n# That call is equivalent to\nf(1, 20, 500, d=100, e=3)\n",
                "I want to give an example which others haven't  mentioned* can also unpack a generatorgeneratorAn example from Python3 Documentx = [1, 2, 3]\ny = [4, 5, 6]\n\nunzip_x, unzip_y = zip(*zip(x, y))\nx = [1, 2, 3]\ny = [4, 5, 6]\n\nunzip_x, unzip_y = zip(*zip(x, y))\nunzip_x will be (1, 2, 3), unzip_y will be (4, 5, 6)The zip() receives multiple iretable args, and return a generator.zip(*zip(x,y)) -> zip((1, 4), (2, 5), (3, 6))\nzip(*zip(x,y)) -> zip((1, 4), (2, 5), (3, 6))\n",
                "Building on nickd's answer...answerdef foo(param1, *param2):\n    print(param1)\n    print(param2)\n\n\ndef bar(param1, **param2):\n    print(param1)\n    print(param2)\n\n\ndef three_params(param1, *param2, **param3):\n    print(param1)\n    print(param2)\n    print(param3)\n\n\nfoo(1, 2, 3, 4, 5)\nprint(\"\\n\")\nbar(1, a=2, b=3)\nprint(\"\\n\")\nthree_params(1, 2, 3, 4, s=5)\ndef foo(param1, *param2):\n    print(param1)\n    print(param2)\n\n\ndef bar(param1, **param2):\n    print(param1)\n    print(param2)\n\n\ndef three_params(param1, *param2, **param3):\n    print(param1)\n    print(param2)\n    print(param3)\n\n\nfoo(1, 2, 3, 4, 5)\nprint(\"\\n\")\nbar(1, a=2, b=3)\nprint(\"\\n\")\nthree_params(1, 2, 3, 4, s=5)\nOutput:1\n(2, 3, 4, 5)\n\n1\n{'a': 2, 'b': 3}\n\n1\n(2, 3, 4)\n{'s': 5}\n1\n(2, 3, 4, 5)\n\n1\n{'a': 2, 'b': 3}\n\n1\n(2, 3, 4)\n{'s': 5}\nBasically, any number of positional arguments can use *args and any named arguments (or kwargs aka keyword arguments) can use **kwargs.positional argumentsnamed arguments",
                "In addition to function calls, *args and **kwargs are useful in class hierarchies and also avoid having to write __init__ method in Python. Similar usage can seen in frameworks like Django code.__init__For example,def __init__(self, *args, **kwargs):\n    for attribute_name, value in zip(self._expected_attributes, args):\n        setattr(self, attribute_name, value)\n        if kwargs.has_key(attribute_name):\n            kwargs.pop(attribute_name)\n\n    for attribute_name in kwargs.viewkeys():\n        setattr(self, attribute_name, kwargs[attribute_name])\ndef __init__(self, *args, **kwargs):\n    for attribute_name, value in zip(self._expected_attributes, args):\n        setattr(self, attribute_name, value)\n        if kwargs.has_key(attribute_name):\n            kwargs.pop(attribute_name)\n\n    for attribute_name in kwargs.viewkeys():\n        setattr(self, attribute_name, kwargs[attribute_name])\nA subclass can then beclass RetailItem(Item):\n    _expected_attributes = Item._expected_attributes + ['name', 'price', 'category', 'country_of_origin']\n\nclass FoodItem(RetailItem):\n    _expected_attributes = RetailItem._expected_attributes +  ['expiry_date']\nclass RetailItem(Item):\n    _expected_attributes = Item._expected_attributes + ['name', 'price', 'category', 'country_of_origin']\n\nclass FoodItem(RetailItem):\n    _expected_attributes = RetailItem._expected_attributes +  ['expiry_date']\nThe subclass then be instantiated as food_item = FoodItem(name = 'Jam', \n                     price = 12.0, \n                     category = 'Foods', \n                     country_of_origin = 'US', \n                     expiry_date = datetime.datetime.now())\nfood_item = FoodItem(name = 'Jam', \n                     price = 12.0, \n                     category = 'Foods', \n                     country_of_origin = 'US', \n                     expiry_date = datetime.datetime.now())\nAlso, a subclass with a new attribute which makes sense only to that subclass instance can call the Base class __init__ to offload the attributes setting.\nThis is done through *args and **kwargs. kwargs mainly used so that code is readable using named arguments. For example,__init__class ElectronicAccessories(RetailItem):\n    _expected_attributes = RetailItem._expected_attributes +  ['specifications']\n    # Depend on args and kwargs to populate the data as needed.\n    def __init__(self, specifications = None, *args, **kwargs):\n        self.specifications = specifications  # Rest of attributes will make sense to parent class.\n        super(ElectronicAccessories, self).__init__(*args, **kwargs)\nclass ElectronicAccessories(RetailItem):\n    _expected_attributes = RetailItem._expected_attributes +  ['specifications']\n    # Depend on args and kwargs to populate the data as needed.\n    def __init__(self, specifications = None, *args, **kwargs):\n        self.specifications = specifications  # Rest of attributes will make sense to parent class.\n        super(ElectronicAccessories, self).__init__(*args, **kwargs)\nwhich can be instatiated asusb_key = ElectronicAccessories(name = 'Sandisk', \n                                price = '$6.00', \n                                category = 'Electronics',\n                                country_of_origin = 'CN',\n                                specifications = '4GB USB 2.0/USB 3.0')\nusb_key = ElectronicAccessories(name = 'Sandisk', \n                                price = '$6.00', \n                                category = 'Electronics',\n                                country_of_origin = 'CN',\n                                specifications = '4GB USB 2.0/USB 3.0')\nThe complete code is herehere",
                "Given a function that has 3 items as argumentsum = lambda x, y, z: x + y + z\nsum(1,2,3) # sum 3 items\n\nsum([1,2,3]) # error, needs 3 items, not 1 list\n\nx = [1,2,3][0]\ny = [1,2,3][1]\nz = [1,2,3][2]\nsum(x,y,z) # ok\n\nsum(*[1,2,3]) # ok, 1 list becomes 3 items\nsum = lambda x, y, z: x + y + z\nsum(1,2,3) # sum 3 items\n\nsum([1,2,3]) # error, needs 3 items, not 1 list\n\nx = [1,2,3][0]\ny = [1,2,3][1]\nz = [1,2,3][2]\nsum(x,y,z) # ok\n\nsum(*[1,2,3]) # ok, 1 list becomes 3 items\nImagine this toy with a bag of a triangle, a circle and a rectangle item. That bag does not directly fit. You need to unpack the bag to take those 3 items and now they fit. The Python * operator does this unpack process.",
                "*args and **kwargs: allow you to pass a variable number of arguments to a function. *args**kwargs*args: is used to send a non-keyworded variable length argument list to the function:*argsdef args(normal_arg, *argv):\n    print(\"normal argument:\", normal_arg)\n\n    for arg in argv:\n        print(\"Argument in list of arguments from *argv:\", arg)\n\nargs('animals', 'fish', 'duck', 'bird')\ndef args(normal_arg, *argv):\n    print(\"normal argument:\", normal_arg)\n\n    for arg in argv:\n        print(\"Argument in list of arguments from *argv:\", arg)\n\nargs('animals', 'fish', 'duck', 'bird')\nWill produce:normal argument: animals\nArgument in list of arguments from *argv: fish\nArgument in list of arguments from *argv: duck\nArgument in list of arguments from *argv: bird\nnormal argument: animals\nArgument in list of arguments from *argv: fish\nArgument in list of arguments from *argv: duck\nArgument in list of arguments from *argv: bird\n**kwargs***kwargs***kwargs allows you to pass keyworded variable length of arguments to a function. You should use **kwargs if you want to handle named arguments in a function. **kwargs**kwargsdef who(**kwargs):\n    if kwargs is not None:\n        for key, value in kwargs.items():\n            print(\"Your %s is %s.\" % (key, value))\n\nwho(name=\"Nikola\", last_name=\"Tesla\", birthday=\"7.10.1856\", birthplace=\"Croatia\")  \ndef who(**kwargs):\n    if kwargs is not None:\n        for key, value in kwargs.items():\n            print(\"Your %s is %s.\" % (key, value))\n\nwho(name=\"Nikola\", last_name=\"Tesla\", birthday=\"7.10.1856\", birthplace=\"Croatia\")  \nWill produce:Your name is Nikola.\nYour last_name is Tesla.\nYour birthday is 7.10.1856.\nYour birthplace is Croatia.\nYour name is Nikola.\nYour last_name is Tesla.\nYour birthday is 7.10.1856.\nYour birthplace is Croatia.\n",
                "A good example of using both in a function is:>>> def foo(*arg,**kwargs):\n...     print arg\n...     print kwargs\n>>>\n>>> a = (1, 2, 3)\n>>> b = {'aa': 11, 'bb': 22}\n>>>\n>>>\n>>> foo(*a,**b)\n(1, 2, 3)\n{'aa': 11, 'bb': 22}\n>>>\n>>>\n>>> foo(a,**b) \n((1, 2, 3),)\n{'aa': 11, 'bb': 22}\n>>>\n>>>\n>>> foo(a,b) \n((1, 2, 3), {'aa': 11, 'bb': 22})\n{}\n>>>\n>>>\n>>> foo(a,*b)\n((1, 2, 3), 'aa', 'bb')\n{}\n>>> def foo(*arg,**kwargs):\n...     print arg\n...     print kwargs\n>>>\n>>> a = (1, 2, 3)\n>>> b = {'aa': 11, 'bb': 22}\n>>>\n>>>\n>>> foo(*a,**b)\n(1, 2, 3)\n{'aa': 11, 'bb': 22}\n>>>\n>>>\n>>> foo(a,**b) \n((1, 2, 3),)\n{'aa': 11, 'bb': 22}\n>>>\n>>>\n>>> foo(a,b) \n((1, 2, 3), {'aa': 11, 'bb': 22})\n{}\n>>>\n>>>\n>>> foo(a,*b)\n((1, 2, 3), 'aa', 'bb')\n{}\n",
                "This example would help you remember *args, **kwargs and even super and inheritance in Python at once.*args**kwargssuperclass base(object):\n    def __init__(self, base_param):\n        self.base_param = base_param\n\n\nclass child1(base): # inherited from base class\n    def __init__(self, child_param, *args) # *args for non-keyword args\n        self.child_param = child_param\n        super(child1, self).__init__(*args) # call __init__ of the base class and initialize it with a NON-KEYWORD arg\n\nclass child2(base):\n    def __init__(self, child_param, **kwargs):\n        self.child_param = child_param\n        super(child2, self).__init__(**kwargs) # call __init__ of the base class and initialize it with a KEYWORD arg\n\nc1 = child1(1,0)\nc2 = child2(1,base_param=0)\nprint c1.base_param # 0\nprint c1.child_param # 1\nprint c2.base_param # 0\nprint c2.child_param # 1\nclass base(object):\n    def __init__(self, base_param):\n        self.base_param = base_param\n\n\nclass child1(base): # inherited from base class\n    def __init__(self, child_param, *args) # *args for non-keyword args\n        self.child_param = child_param\n        super(child1, self).__init__(*args) # call __init__ of the base class and initialize it with a NON-KEYWORD arg\n\nclass child2(base):\n    def __init__(self, child_param, **kwargs):\n        self.child_param = child_param\n        super(child2, self).__init__(**kwargs) # call __init__ of the base class and initialize it with a KEYWORD arg\n\nc1 = child1(1,0)\nc2 = child2(1,base_param=0)\nprint c1.base_param # 0\nprint c1.child_param # 1\nprint c2.base_param # 0\nprint c2.child_param # 1\n",
                "Context\npython 3.x\nunpacking with **\nuse with string formatting\npython 3.xunpacking with ****use with string formattingUse with string formattingIn addition to the answers in this thread, here is another detail that was not mentioned elsewhere. This expands on the answer by Brad Solomonanswer by Brad SolomonUnpacking with ** is also useful when using python str.format.  **str.formatThis is somewhat similar to what you can do with python f-strings f-string but with the added overhead of declaring a dict to hold the variables (f-string does not require a dict).f-stringsf-stringQuick Example  ## init vars\n  ddvars = dict()\n  ddcalc = dict()\n  pass\n  ddvars['fname']     = 'Huomer'\n  ddvars['lname']     = 'Huimpson'\n  ddvars['motto']     = 'I love donuts!'\n  ddvars['age']       = 33\n  pass\n  ddcalc['ydiff']     = 5\n  ddcalc['ycalc']     = ddvars['age'] + ddcalc['ydiff']\n  pass\n  vdemo = []\n\n  ## ********************\n  ## single unpack supported in py 2.7\n  vdemo.append('''\n  Hello {fname} {lname}!\n\n  Today you are {age} years old!\n\n  We love your motto \"{motto}\" and we agree with you!\n  '''.format(**ddvars)) \n  pass\n\n  ## ********************\n  ## multiple unpack supported in py 3.x\n  vdemo.append('''\n  Hello {fname} {lname}!\n\n  In {ydiff} years you will be {ycalc} years old!\n  '''.format(**ddvars,**ddcalc)) \n  pass\n\n  ## ********************\n  print(vdemo[-1])\n\n  ## init vars\n  ddvars = dict()\n  ddcalc = dict()\n  pass\n  ddvars['fname']     = 'Huomer'\n  ddvars['lname']     = 'Huimpson'\n  ddvars['motto']     = 'I love donuts!'\n  ddvars['age']       = 33\n  pass\n  ddcalc['ydiff']     = 5\n  ddcalc['ycalc']     = ddvars['age'] + ddcalc['ydiff']\n  pass\n  vdemo = []\n\n  ## ********************\n  ## single unpack supported in py 2.7\n  vdemo.append('''\n  Hello {fname} {lname}!\n\n  Today you are {age} years old!\n\n  We love your motto \"{motto}\" and we agree with you!\n  '''.format(**ddvars)) \n  pass\n\n  ## ********************\n  ## multiple unpack supported in py 3.x\n  vdemo.append('''\n  Hello {fname} {lname}!\n\n  In {ydiff} years you will be {ycalc} years old!\n  '''.format(**ddvars,**ddcalc)) \n  pass\n\n  ## ********************\n  print(vdemo[-1])\n\n",
                "*args ( or *any ) means every parametersdef any_param(*param):\n    pass\n\nany_param(1)\nany_param(1,1)\nany_param(1,1,1)\nany_param(1,...)\ndef any_param(*param):\n    pass\n\nany_param(1)\nany_param(1,1)\nany_param(1,1,1)\nany_param(1,...)\nNOTICE : you can don't pass parameters to *argsNOTICEdef any_param(*param):\n    pass\n\nany_param() # will work correct\ndef any_param(*param):\n    pass\n\nany_param() # will work correct\nThe *args is in type tupledef any_param(*param):\n    return type(param)\n\nany_param(1) #tuple\nany_param() # tuple\ndef any_param(*param):\n    return type(param)\n\nany_param(1) #tuple\nany_param() # tuple\nfor access to elements don't use of *def any(*param):\n    param[0] # correct\n\ndef any(*param):\n    *param[0] # incorrect\ndef any(*param):\n    param[0] # correct\n\ndef any(*param):\n    *param[0] # incorrect\nThe **kwd**kwd or **any\nThis is a dict typedef func(**any):\n    return type(any) # dict\n\ndef func(**any):\n    return any\n\nfunc(width=\"10\",height=\"20\") # {width=\"10\",height=\"20\")\n\n\ndef func(**any):\n    return type(any) # dict\n\ndef func(**any):\n    return any\n\nfunc(width=\"10\",height=\"20\") # {width=\"10\",height=\"20\")\n\n\n",
                "\ndef foo(param1, *param2): is a method can accept arbitrary number of values for *param2,\ndef bar(param1, **param2): is a method can accept arbitrary number of values with keys for *param2\nparam1 is a simple parameter.\ndef foo(param1, *param2): is a method can accept arbitrary number of values for *param2,def foo(param1, *param2):*param2def bar(param1, **param2): is a method can accept arbitrary number of values with keys for *param2def bar(param1, **param2):*param2param1 is a simple parameter.param1For example, the syntax for implementing varargs in Java as follows:varargsaccessModifier methodName(datatype\u2026 arg) {\n    // method body\n}\naccessModifier methodName(datatype\u2026 arg) {\n    // method body\n}\n",
                "\"Infinite\" Args with *args and **kwargs*args and **kwargs are just some way to input unlimited characters to functions, like:*args**kwargs\ndef print_all(*args, **kwargs):\n    print(args) # print any number of arguments like: \"print_all(\"foo\", \"bar\")\"\n    print(kwargs.get(\"to_print\")) # print the value of the keyworded argument \"to_print\"\n\n\n# example:\nprint_all(\"Hello\", \"World\", to_print=\"!\")\n# will print:\n\"\"\"\n('Hello', 'World')\n!\n\"\"\"\n\ndef print_all(*args, **kwargs):\n    print(args) # print any number of arguments like: \"print_all(\"foo\", \"bar\")\"\n    print(kwargs.get(\"to_print\")) # print the value of the keyworded argument \"to_print\"\n\n\n# example:\nprint_all(\"Hello\", \"World\", to_print=\"!\")\n# will print:\n\"\"\"\n('Hello', 'World')\n!\n\"\"\"\n",
                "\n*args is the special parameter which can take 0 or more (positional) arguments as a tuple.\n\n**kwargs is the special parameter which can take 0 or more (keyword) arguments as a dictionary.\n\n*args is the special parameter which can take 0 or more (positional) arguments as a tuple.\n*args is the special parameter which can take 0 or more (positional) arguments as a tuple.*args*args**kwargs is the special parameter which can take 0 or more (keyword) arguments as a dictionary.\n**kwargs is the special parameter which can take 0 or more (keyword) arguments as a dictionary.**kwargs**kwargs*In Python, there are 2 kinds of arguments positional argument and keyword argument:positional argument and keyword argumentpositional argument and keyword argument*args:*argsFor example, *args can take 0 or more arguments as a tuple as shown below:*args           \u2193\ndef test(*args):\n    print(args)\n\ntest() # Here\ntest(1, 2, 3, 4) # Here\ntest((1, 2, 3, 4)) # Here\ntest(*(1, 2, 3, 4)) # Here\n           \u2193\ndef test(*args):\n    print(args)\n\ntest() # Here\ntest(1, 2, 3, 4) # Here\ntest((1, 2, 3, 4)) # Here\ntest(*(1, 2, 3, 4)) # Here\nOutput:()\n(1, 2, 3, 4)\n((1, 2, 3, 4),)\n(1, 2, 3, 4)\n()\n(1, 2, 3, 4)\n((1, 2, 3, 4),)\n(1, 2, 3, 4)\nAnd, when printing *args, 4 numbers are printed without parentheses and commas:*argsdef test(*args):\n    print(*args) # Here\n \ntest(1, 2, 3, 4)\ndef test(*args):\n    print(*args) # Here\n \ntest(1, 2, 3, 4)\nOutput:1 2 3 4\n1 2 3 4\nAnd, args has tuple type:argstupledef test(*args):\n    print(type(args)) # Here\n \ntest(1, 2, 3, 4)\ndef test(*args):\n    print(type(args)) # Here\n \ntest(1, 2, 3, 4)\nOutput:<class 'tuple'>\n<class 'tuple'>\nBut, *args has no type:*argsdef test(*args):\n    print(type(*args)) # Here\n \ntest(1, 2, 3, 4)\ndef test(*args):\n    print(type(*args)) # Here\n \ntest(1, 2, 3, 4)\nOutput(Error):\nTypeError: type() takes 1 or 3 arguments\nTypeError: type() takes 1 or 3 argumentsAnd, normal parameters can be put before *args as shown below:*args          \u2193     \u2193\ndef test(num1, num2, *args):\n    print(num1, num2, args)\n    \ntest(1, 2, 3, 4)\n          \u2193     \u2193\ndef test(num1, num2, *args):\n    print(num1, num2, args)\n    \ntest(1, 2, 3, 4)\nOutput:1 2 (3, 4)\n1 2 (3, 4)\nBut, **kwargs cannot be put before *args as shown below:**kwargs*args             \u2193     \ndef test(**kwargs, *args):\n    print(kwargs, args)\n    \ntest(num1=1, num2=2, 3, 4)\n             \u2193     \ndef test(**kwargs, *args):\n    print(kwargs, args)\n    \ntest(num1=1, num2=2, 3, 4)\nOutput(Error):\nSyntaxError: invalid syntax\nSyntaxError: invalid syntaxAnd, normal parameters cannot be put after *args as shown below:*args                 \u2193     \u2193\ndef test(*args, num1, num2):\n    print(args, num1, num2)\n    \ntest(1, 2, 3, 4)\n                 \u2193     \u2193\ndef test(*args, num1, num2):\n    print(args, num1, num2)\n    \ntest(1, 2, 3, 4)\nOutput(Error):\nTypeError: test() missing 2 required keyword-only arguments: 'num1' and 'num2'\nTypeError: test() missing 2 required keyword-only arguments: 'num1' and 'num2'But, if normal parameters have default values, they can be put after *args as shown below:*args                      \u2193         \u2193\ndef test(*args, num1=100, num2=None):\n    print(args, num1, num2)\n    \ntest(1, 2, num1=3, num2=4)\n                      \u2193         \u2193\ndef test(*args, num1=100, num2=None):\n    print(args, num1, num2)\n    \ntest(1, 2, num1=3, num2=4)\nOutput:(1, 2) 3 4\n(1, 2) 3 4\nAnd also, **kwargs can be put after *args as shown below:**kwargs*args                    \u2193\ndef test(*args, **kwargs):\n    print(args, kwargs)\n    \ntest(1, 2, num1=3, num2=4)\n                    \u2193\ndef test(*args, **kwargs):\n    print(args, kwargs)\n    \ntest(1, 2, num1=3, num2=4)\nOutput:(1, 2) {'num1': 3, 'num2': 4}\n(1, 2) {'num1': 3, 'num2': 4}\n**kwargs:**kwargsFor example, **kwargs can take 0 or more arguments as a dictionary as shown below:**kwargs             \u2193\ndef test(**kwargs):\n    print(kwargs)\n\ntest() # Here\ntest(name=\"John\", age=27) # Here\ntest(**{\"name\": \"John\", \"age\": 27}) # Here\n             \u2193\ndef test(**kwargs):\n    print(kwargs)\n\ntest() # Here\ntest(name=\"John\", age=27) # Here\ntest(**{\"name\": \"John\", \"age\": 27}) # Here\nOutput:{}\n{'name': 'John', 'age': 27}\n{'name': 'John', 'age': 27}\n{}\n{'name': 'John', 'age': 27}\n{'name': 'John', 'age': 27}\nAnd, when printing *kwargs, 2 keys are printed:*kwargsdef test(**kwargs):\n    print(*kwargs) # Here\n \ntest(name=\"John\", age=27)\ndef test(**kwargs):\n    print(*kwargs) # Here\n \ntest(name=\"John\", age=27)\nOutput:name age\nname age\nAnd, kwargs has dict type:kwargsdictdef test(**kwargs):\n    print(type(kwargs)) # Here\n \ntest(name=\"John\", age=27)\ndef test(**kwargs):\n    print(type(kwargs)) # Here\n \ntest(name=\"John\", age=27)\nOutput:<class 'dict'>\n<class 'dict'>\nBut, *kwargs and **kwargs have no type:*kwargs**kwargsdef test(**kwargs):\n    print(type(*kwargs)) # Here\n \ntest(name=\"John\", age=27)\ndef test(**kwargs):\n    print(type(*kwargs)) # Here\n \ntest(name=\"John\", age=27)\ndef test(**kwargs):\n    print(type(**kwargs)) # Here\n \ntest(name=\"John\", age=27)\ndef test(**kwargs):\n    print(type(**kwargs)) # Here\n \ntest(name=\"John\", age=27)\nOutput(Error):\nTypeError: type() takes 1 or 3 arguments\nTypeError: type() takes 1 or 3 argumentsAnd, normal parameters can be put before **kwargs as shown below:**kwargs          \u2193     \u2193\ndef test(num1, num2, **kwargs):\n    print(num1, num2, kwargs)\n\ntest(1, 2, name=\"John\", age=27)\n          \u2193     \u2193\ndef test(num1, num2, **kwargs):\n    print(num1, num2, kwargs)\n\ntest(1, 2, name=\"John\", age=27)\nOutput:1 2 {'name': 'John', 'age': 27}\n1 2 {'name': 'John', 'age': 27}\nAnd also, *args can be put before **kwargs as shown below:*args**kwargs           \u2193\ndef test(*args, **kwargs):\n    print(args, kwargs)\n\ntest(1, 2, name=\"John\", age=27)\n           \u2193\ndef test(*args, **kwargs):\n    print(args, kwargs)\n\ntest(1, 2, name=\"John\", age=27)\nOutput:(1, 2) {'name': 'John', 'age': 27}\n(1, 2) {'name': 'John', 'age': 27}\nAnd, normal parameters and *args cannot be put after **kwargs as shown below:*args**kwargs                    \u2193     \u2193\ndef test(**kwargs, num1, num2):\n    print(kwargs, num1, num2)\n\ntest(name=\"John\", age=27, 1, 2)\n                    \u2193     \u2193\ndef test(**kwargs, num1, num2):\n    print(kwargs, num1, num2)\n\ntest(name=\"John\", age=27, 1, 2)\n                     \u2193\ndef test(**kwargs, *args):\n    print(kwargs, args)\n\ntest(name=\"John\", age=27, 1, 2)\n                     \u2193\ndef test(**kwargs, *args):\n    print(kwargs, args)\n\ntest(name=\"John\", age=27, 1, 2)\nOutput(Error):\nSyntaxError: invalid syntax\nSyntaxError: invalid syntaxFor both *args and **kwargs:*args**kwargsActually, you can use other names for *args and **kwargs as shown below. *args and **kwargs are used conventionally:*args**kwargs*args**kwargs            \u2193        \u2193\ndef test(*banana, **orange):\n    print(banana, orange)\n    \ntest(1, 2, num1=3, num2=4)\n            \u2193        \u2193\ndef test(*banana, **orange):\n    print(banana, orange)\n    \ntest(1, 2, num1=3, num2=4)\nOutput:(1, 2) {'num1': 3, 'num2': 4}\n(1, 2) {'num1': 3, 'num2': 4}\n"
            ]
        },
        {
            "tag": "reference",
            "patterns": [
                "How do I pass a variable by reference?",
                "how do you pass variables by reference?",
                "how do i pass a variable with reference?",
                "how do i pass variables by reference?",
                "how do i pass the variable by reference?",
                "what can i do to pass a variable by reference?",
                "how do i pass a variable by reference?"
            ],
            "responses": [
                "Arguments are passed by assignment. The rationale behind this is twofold:passed by assignment\nthe parameter passed in is actually a reference to an object (but the reference is passed by value)\nsome data types are mutable, but others aren't\nthe parameter passed in is actually a reference to an object (but the reference is passed by value)referencesome data types are mutable, but others aren'tSo:\nIf you pass a mutable object into a method, the method gets a reference to that same object and you can mutate it to your heart's delight, but if you rebind the reference in the method, the outer scope will know nothing about it, and after you're done, the outer reference will still point at the original object. \nIf you pass an immutable object to a method, you still can't rebind the outer reference, and you can't even mutate the object.\nIf you pass a mutable object into a method, the method gets a reference to that same object and you can mutate it to your heart's delight, but if you rebind the reference in the method, the outer scope will know nothing about it, and after you're done, the outer reference will still point at the original object. If you pass a mutable object into a method, the method gets a reference to that same object and you can mutate it to your heart's delight, but if you rebind the reference in the method, the outer scope will know nothing about it, and after you're done, the outer reference will still point at the original object. mutableIf you pass an immutable object to a method, you still can't rebind the outer reference, and you can't even mutate the object.If you pass an immutable object to a method, you still can't rebind the outer reference, and you can't even mutate the object.immutableTo make it even more clear, let's have some examples. List - a mutable typeLet's try to modify the list that was passed to a method:Let's try to modify the list that was passed to a method:def try_to_change_list_contents(the_list):\n    print('got', the_list)\n    the_list.append('four')\n    print('changed to', the_list)\n\nouter_list = ['one', 'two', 'three']\n\nprint('before, outer_list =', outer_list)\ntry_to_change_list_contents(outer_list)\nprint('after, outer_list =', outer_list)\ndef try_to_change_list_contents(the_list):\n    print('got', the_list)\n    the_list.append('four')\n    print('changed to', the_list)\n\nouter_list = ['one', 'two', 'three']\n\nprint('before, outer_list =', outer_list)\ntry_to_change_list_contents(outer_list)\nprint('after, outer_list =', outer_list)\nOutput:before, outer_list = ['one', 'two', 'three']\ngot ['one', 'two', 'three']\nchanged to ['one', 'two', 'three', 'four']\nafter, outer_list = ['one', 'two', 'three', 'four']\nbefore, outer_list = ['one', 'two', 'three']\ngot ['one', 'two', 'three']\nchanged to ['one', 'two', 'three', 'four']\nafter, outer_list = ['one', 'two', 'three', 'four']\nSince the parameter passed in is a reference to outer_list, not a copy of it, we can use the mutating list methods to change it and have the changes reflected in the outer scope.outer_listNow let's see what happens when we try to change the reference that was passed in as a parameter:Now let's see what happens when we try to change the reference that was passed in as a parameter:def try_to_change_list_reference(the_list):\n    print('got', the_list)\n    the_list = ['and', 'we', 'can', 'not', 'lie']\n    print('set to', the_list)\n\nouter_list = ['we', 'like', 'proper', 'English']\n\nprint('before, outer_list =', outer_list)\ntry_to_change_list_reference(outer_list)\nprint('after, outer_list =', outer_list)\ndef try_to_change_list_reference(the_list):\n    print('got', the_list)\n    the_list = ['and', 'we', 'can', 'not', 'lie']\n    print('set to', the_list)\n\nouter_list = ['we', 'like', 'proper', 'English']\n\nprint('before, outer_list =', outer_list)\ntry_to_change_list_reference(outer_list)\nprint('after, outer_list =', outer_list)\nOutput:before, outer_list = ['we', 'like', 'proper', 'English']\ngot ['we', 'like', 'proper', 'English']\nset to ['and', 'we', 'can', 'not', 'lie']\nafter, outer_list = ['we', 'like', 'proper', 'English']\nbefore, outer_list = ['we', 'like', 'proper', 'English']\ngot ['we', 'like', 'proper', 'English']\nset to ['and', 'we', 'can', 'not', 'lie']\nafter, outer_list = ['we', 'like', 'proper', 'English']\nSince the the_list parameter was passed by value, assigning a new list to it had no effect that the code outside the method could see. The the_list was a copy of the outer_list reference, and we had the_list point to a new list, but there was no way to change where outer_list pointed.the_listthe_listouter_listthe_listouter_listString - an immutable typeIt's immutable, so there's nothing we can do to change the contents of the stringIt's immutable, so there's nothing we can do to change the contents of the stringNow, let's try to change the referenceNow, let's try to change the referencedef try_to_change_string_reference(the_string):\n    print('got', the_string)\n    the_string = 'In a kingdom by the sea'\n    print('set to', the_string)\n\nouter_string = 'It was many and many a year ago'\n\nprint('before, outer_string =', outer_string)\ntry_to_change_string_reference(outer_string)\nprint('after, outer_string =', outer_string)\ndef try_to_change_string_reference(the_string):\n    print('got', the_string)\n    the_string = 'In a kingdom by the sea'\n    print('set to', the_string)\n\nouter_string = 'It was many and many a year ago'\n\nprint('before, outer_string =', outer_string)\ntry_to_change_string_reference(outer_string)\nprint('after, outer_string =', outer_string)\nOutput:before, outer_string = It was many and many a year ago\ngot It was many and many a year ago\nset to In a kingdom by the sea\nafter, outer_string = It was many and many a year ago\nbefore, outer_string = It was many and many a year ago\ngot It was many and many a year ago\nset to In a kingdom by the sea\nafter, outer_string = It was many and many a year ago\nAgain, since the the_string parameter was passed by value, assigning a new string to it had no effect that the code outside the method could see. The the_string was a copy of the outer_string reference, and we had the_string point to a new string, but there was no way to change where outer_string pointed.the_stringthe_stringouter_stringthe_stringouter_stringI hope this clears things up a little.EDIT: It's been noted that this doesn't answer the question that @David originally asked, \"Is there something I can do to pass the variable by actual reference?\". Let's work on that.EDIT:How do we get around this?As @Andrea's answer shows, you could return the new value. This doesn't change the way things are passed in, but does let you get the information you want back out:def return_a_whole_new_string(the_string):\n    new_string = something_to_do_with_the_old_string(the_string)\n    return new_string\n\n# then you could call it like\nmy_string = return_a_whole_new_string(my_string)\ndef return_a_whole_new_string(the_string):\n    new_string = something_to_do_with_the_old_string(the_string)\n    return new_string\n\n# then you could call it like\nmy_string = return_a_whole_new_string(my_string)\nIf you really wanted to avoid using a return value, you could create a class to hold your value and pass it into the function or use an existing class, like a list:def use_a_wrapper_to_simulate_pass_by_reference(stuff_to_change):\n    new_string = something_to_do_with_the_old_string(stuff_to_change[0])\n    stuff_to_change[0] = new_string\n\n# then you could call it like\nwrapper = [my_string]\nuse_a_wrapper_to_simulate_pass_by_reference(wrapper)\n\ndo_something_with(wrapper[0])\ndef use_a_wrapper_to_simulate_pass_by_reference(stuff_to_change):\n    new_string = something_to_do_with_the_old_string(stuff_to_change[0])\n    stuff_to_change[0] = new_string\n\n# then you could call it like\nwrapper = [my_string]\nuse_a_wrapper_to_simulate_pass_by_reference(wrapper)\n\ndo_something_with(wrapper[0])\nAlthough this seems a little cumbersome.",
                "The problem comes from a misunderstanding of what variables are in Python. If you're used to most traditional languages, you have a mental model of what happens in the following sequence:a = 1\na = 2\na = 1\na = 2\nYou believe that a is a memory location that stores the value 1, then is updated to store the value 2. That's not how things work in Python. Rather, a starts as a reference to an object with the value 1, then gets reassigned as a reference to an object with the value 2. Those two objects may continue to coexist even though a doesn't refer to the first one anymore; in fact they may be shared by any number of other references within the program.a12a12aWhen you call a function with a parameter, a new reference is created that refers to the object passed in. This is separate from the reference that was used in the function call, so there's no way to update that reference and make it refer to a new object. In your example:def __init__(self):\n    self.variable = 'Original'\n    self.Change(self.variable)\n\ndef Change(self, var):\n    var = 'Changed'\ndef __init__(self):\n    self.variable = 'Original'\n    self.Change(self.variable)\n\ndef Change(self, var):\n    var = 'Changed'\nself.variable is a reference to the string object 'Original'. When you call Change you create a second reference var to the object. Inside the function you reassign the reference var to a different string object 'Changed', but the reference self.variable is separate and does not change.self.variable'Original'Changevarvar'Changed'self.variableThe only way around this is to pass a mutable object. Because both references refer to the same object, any changes to the object are reflected in both places.def __init__(self):         \n    self.variable = ['Original']\n    self.Change(self.variable)\n\ndef Change(self, var):\n    var[0] = 'Changed'\ndef __init__(self):         \n    self.variable = ['Original']\n    self.Change(self.variable)\n\ndef Change(self, var):\n    var[0] = 'Changed'\n",
                "I found the other answers rather long and complicated, so I created this simple diagram to explain the way Python treats variables and parameters.\n",
                "It is neither pass-by-value or pass-by-reference - it is call-by-object. See this, by Fredrik Lundh:http://effbot.org/zone/call-by-object.htmhttp://effbot.org/zone/call-by-object.htmHere is a significant quote:\n\"...variables [names] are not objects; they cannot be denoted by other variables or referred to by objects.\"\n\"...variables [names] are not objects; they cannot be denoted by other variables or referred to by objects.\"notIn your example, when the Change method is called--a namespace is created for it; and var becomes a name, within that namespace, for the string object 'Original'. That object then has a name in two namespaces. Next, var = 'Changed' binds var to a new string object, and thus the method's namespace forgets about 'Original'. Finally, that namespace is forgotten, and the string 'Changed' along with it.Changenamespacevar'Original'var = 'Changed'var'Original''Changed'",
                "Think of stuff being passed by assignment instead of by reference/by value. That way, it is always clear, what is happening as long as you understand what happens during the normal assignment.by assignmentSo, when passing a list to a function/method, the list is assigned to the parameter name. Appending to the list will result in the list being modified. Reassigning the list inside the function will not change the original list, since:insidea = [1, 2, 3]\nb = a\nb.append(4)\nb = ['a', 'b']\nprint a, b      # prints [1, 2, 3, 4] ['a', 'b']\na = [1, 2, 3]\nb = a\nb.append(4)\nb = ['a', 'b']\nprint a, b      # prints [1, 2, 3, 4] ['a', 'b']\nSince immutable types cannot be modified, they seem like being passed by value - passing an int into a function means assigning the int to the function's parameter. You can only ever reassign that, but it won't change the original variables value.seem",
                "There are no variables in PythonThe key to understanding parameter passing is to stop thinking about \"variables\". There are names and objects in Python and together they\nappear like variables, but it is useful to always distinguish the three.\nPython has names and objects.\nAssignment binds a name to an object.\nPassing an argument into a function also binds a name (the parameter name of the function) to an object.\nPython has names and objects.Assignment binds a name to an object.Passing an argument into a function also binds a name (the parameter name of the function) to an object.That is all there is to it. Mutability is irrelevant to this question.Example:a = 1\na = 1\nThis binds the name a to an object of type integer that holds the value 1.ab = x\nb = x\nThis binds the name b to the same object that the name x is currently bound to.\nAfterward, the name b has nothing to do with the name x anymore.bxbxSee sections 3.1 and 4.2 in the Python 3 language reference.3.14.2How to read the example in the questionIn the code shown in the question, the statement self.Change(self.variable) binds the name var (in the scope of function Change) to the object that holds the value 'Original' and the assignment var = 'Changed' (in the body of function Change) assigns that same name again: to some other object (that happens to hold a string as well but could have been something else entirely).self.Change(self.variable)varChange'Original'var = 'Changed'ChangeHow to pass by referenceSo if the thing you want to change is a mutable object, there is no problem, as everything is effectively passed by reference.If it is an immutable object (e.g. a bool, number, string), the way to go is to wrap it in a mutable object.\nThe quick-and-dirty solution for this is a one-element list (instead of self.variable, pass [self.variable] and in the function modify var[0]).\nThe more pythonic approach would be to introduce a trivial, one-attribute class. The function receives an instance of the class and manipulates the attribute.immutableself.variable[self.variable]var[0]pythonic",
                "Effbot (aka Fredrik Lundh) has described Python's variable passing style as call-by-object:  http://effbot.org/zone/call-by-object.htmhttp://effbot.org/zone/call-by-object.htmObjects are allocated on the heap and pointers to them can be passed around anywhere.  \nWhen you make an assignment such as x = 1000, a dictionary entry is created that maps the string \"x\" in the current namespace to a pointer to the integer object containing one thousand.   \nWhen you update \"x\" with x = 2000, a new integer object is created and the dictionary is updated to point at the new object.  The old one thousand object is unchanged (and may or may not be alive depending on whether anything else refers to the object).\nWhen you do a new assignment such as y = x, a new dictionary entry \"y\" is created that points to the same object as the entry for \"x\".\nObjects like strings and integers are immutable.  This simply means that there are no methods that can change the object after it has been created.  For example, once the integer object one-thousand is created, it will never change.  Math is done by creating new integer objects.\nObjects like lists are mutable.  This means that the contents of the object can be changed by anything pointing to the object.  For example, x = []; y = x; x.append(10); print y will print [10].  The empty list was created.  Both \"x\" and \"y\" point to the same list.  The append method mutates (updates) the list object (like adding a record to a database) and the result is visible to both \"x\" and \"y\" (just as a database update would be visible to every connection to that database).\nWhen you make an assignment such as x = 1000, a dictionary entry is created that maps the string \"x\" in the current namespace to a pointer to the integer object containing one thousand.   When you make an assignment such as x = 1000, a dictionary entry is created that maps the string \"x\" in the current namespace to a pointer to the integer object containing one thousand.   x = 1000When you update \"x\" with x = 2000, a new integer object is created and the dictionary is updated to point at the new object.  The old one thousand object is unchanged (and may or may not be alive depending on whether anything else refers to the object).When you update \"x\" with x = 2000, a new integer object is created and the dictionary is updated to point at the new object.  The old one thousand object is unchanged (and may or may not be alive depending on whether anything else refers to the object).x = 2000When you do a new assignment such as y = x, a new dictionary entry \"y\" is created that points to the same object as the entry for \"x\".When you do a new assignment such as y = x, a new dictionary entry \"y\" is created that points to the same object as the entry for \"x\".y = xObjects like strings and integers are immutable.  This simply means that there are no methods that can change the object after it has been created.  For example, once the integer object one-thousand is created, it will never change.  Math is done by creating new integer objects.Objects like strings and integers are immutable.  This simply means that there are no methods that can change the object after it has been created.  For example, once the integer object one-thousand is created, it will never change.  Math is done by creating new integer objects.immutableObjects like lists are mutable.  This means that the contents of the object can be changed by anything pointing to the object.  For example, x = []; y = x; x.append(10); print y will print [10].  The empty list was created.  Both \"x\" and \"y\" point to the same list.  The append method mutates (updates) the list object (like adding a record to a database) and the result is visible to both \"x\" and \"y\" (just as a database update would be visible to every connection to that database).Objects like lists are mutable.  This means that the contents of the object can be changed by anything pointing to the object.  For example, x = []; y = x; x.append(10); print y will print [10].  The empty list was created.  Both \"x\" and \"y\" point to the same list.  The append method mutates (updates) the list object (like adding a record to a database) and the result is visible to both \"x\" and \"y\" (just as a database update would be visible to every connection to that database).mutablex = []; y = x; x.append(10); print y[10]appendHope that clarifies the issue for you. ",
                "Technically, Python always uses pass by reference values. I am going to repeat my other answer to support my statement.Python always uses pass by reference valuesmy other answerPython always uses pass-by-reference values. There isn't any exception. Any variable assignment means copying the reference value. No exception. Any variable is the name bound to the reference value. Always.You can think about a reference value as the address of the target object. The address is automatically dereferenced when used. This way, working with the reference value, it seems you work directly with the target object. But there always is a reference in between, one step more to jump to the target.Here is the example that proves that Python uses passing by reference:If the argument was passed by value, the outer lst could not be modified. The green are the target objects (the black is the value stored inside, the red is the object type), the yellow is the memory with the reference value inside -- drawn as the arrow. The blue solid arrow is the reference value that was passed to the function (via the dashed blue arrow path). The ugly dark yellow is the internal dictionary. (It actually could be drawn also as a green ellipse. The colour and the shape only says it is internal.)lstYou can use the id() built-in function to learn what the reference value is (that is, the address of the target object).id()id()In compiled languages, a variable is a memory space that is able to capture the value of the type. In Python, a variable is a name (captured internally as a string) bound to the reference variable that holds the reference value to the target object. The name of the variable is the key in the internal dictionary, the value part of that dictionary item stores the reference value to the target.Reference values are hidden in Python. There isn't any explicit user type for storing the reference value. However, you can use a list element (or element in any other suitable container type) as the reference variable, because all containers do store the elements also as references to the target objects. In other words, elements are actually not contained inside the container -- only the references to elements are.",
                "A simple trick I normally use is to just wrap it in a list:def Change(self, var):\n    var[0] = 'Changed'\n\nvariable = ['Original']\nself.Change(variable)      \nprint variable[0]\ndef Change(self, var):\n    var[0] = 'Changed'\n\nvariable = ['Original']\nself.Change(variable)      \nprint variable[0]\n(Yeah I know this can be inconvenient, but sometimes it is simple enough to do this.)",
                "(edit - Blair has updated his enormously popular answer so that it is now accurate)I think it is important to note that the current post with the most votes (by Blair Conrad), while being correct with respect to its result, is misleading and is borderline incorrect based on its definitions.  While there are many languages (like C) that allow the user to either pass by reference or pass by value, Python is not one of them.David Cournapeau's answer points to the real answer and explains why the behavior in Blair Conrad's post seems to be correct while the definitions are not.To the extent that Python is pass by value, all languages are pass by value since some piece of data (be it a \"value\" or a \"reference\") must be sent. However, that does not mean that Python is pass by value in the sense that a C programmer would think of it.If you want the behavior, Blair Conrad's answer is fine.  But if you want to know the nuts and bolts of why Python is neither pass by value or pass by reference, read David Cournapeau's answer.",
                "You got some really good answers here.x = [ 2, 4, 4, 5, 5 ]\nprint x  # 2, 4, 4, 5, 5\n\ndef go( li ) :\n  li = [ 5, 6, 7, 8 ]  # re-assigning what li POINTS TO, does not\n  # change the value of the ORIGINAL variable x\n\ngo( x ) \nprint x  # 2, 4, 4, 5, 5  [ STILL! ]\n\n\nraw_input( 'press any key to continue' )\nx = [ 2, 4, 4, 5, 5 ]\nprint x  # 2, 4, 4, 5, 5\n\ndef go( li ) :\n  li = [ 5, 6, 7, 8 ]  # re-assigning what li POINTS TO, does not\n  # change the value of the ORIGINAL variable x\n\ngo( x ) \nprint x  # 2, 4, 4, 5, 5  [ STILL! ]\n\n\nraw_input( 'press any key to continue' )\n",
                "Python\u2019s pass-by-assignment scheme isn\u2019t quite the same as C++\u2019s reference parameters option, but it turns out to be very similar to the argument-passing model of the C language (and others) in practice:\nImmutable arguments are effectively passed \u201cby value.\u201d Objects such as integers and strings are passed by object reference instead of by copying, but because you can\u2019t change immutable objects in place anyhow, the effect is much like making a copy.\nMutable arguments are effectively passed \u201cby pointer.\u201d Objects such as lists\nand dictionaries are also passed by object reference, which is similar to the way C\npasses arrays as pointers\u2014mutable objects can be changed in place in the function,\nmuch like C arrays.\nImmutable arguments are effectively passed \u201cby value.\u201d Objects such as integers and strings are passed by object reference instead of by copying, but because you can\u2019t change immutable objects in place anyhow, the effect is much like making a copy.by valueMutable arguments are effectively passed \u201cby pointer.\u201d Objects such as lists\nand dictionaries are also passed by object reference, which is similar to the way C\npasses arrays as pointers\u2014mutable objects can be changed in place in the function,\nmuch like C arrays.by pointer",
                "In this case the variable titled var in the method Change is assigned a reference to self.variable, and you immediately assign a string to var. It's no longer pointing to self.variable. The following code snippet shows what would happen if you modify the data structure pointed to by var and self.variable, in this case a list:varChangeself.variablevarself.variablevarself.variable>>> class PassByReference:\n...     def __init__(self):\n...         self.variable = ['Original']\n...         self.change(self.variable)\n...         print self.variable\n...         \n...     def change(self, var):\n...         var.append('Changed')\n... \n>>> q = PassByReference()\n['Original', 'Changed']\n>>> \n>>> class PassByReference:\n...     def __init__(self):\n...         self.variable = ['Original']\n...         self.change(self.variable)\n...         print self.variable\n...         \n...     def change(self, var):\n...         var.append('Changed')\n... \n>>> q = PassByReference()\n['Original', 'Changed']\n>>> \nI'm sure someone else could clarify this further.",
                "A lot of insights in answers here, but i think an additional point is not clearly mentioned here explicitly.   Quoting from python documentation https://docs.python.org/2/faq/programming.html#what-are-the-rules-for-local-and-global-variables-in-python https://docs.python.org/2/faq/programming.html#what-are-the-rules-for-local-and-global-variables-in-python\"In Python, variables that are only referenced inside a function are implicitly global. If a variable is assigned a new value anywhere within the function\u2019s body, it\u2019s assumed to be a local. If a variable is ever assigned a new value inside the function, the variable is implicitly local, and you need to explicitly declare it as \u2018global\u2019.\nThough a bit surprising at first, a moment\u2019s consideration explains this. On one hand, requiring global for assigned variables provides a bar against unintended side-effects. On the other hand, if global was required for all global references, you\u2019d be using global all the time. You\u2019d have to declare as global every reference to a built-in function or to a component of an imported module. This clutter would defeat the usefulness of the global declaration for identifying side-effects.\"Even when passing a mutable object to a function this still applies. And to me clearly explains the reason for the difference in behavior between assigning to the object and operating on the object in the function.def test(l):\n    print \"Received\", l , id(l)\n    l = [0, 0, 0]\n    print \"Changed to\", l, id(l)  # New local object created, breaking link to global l\n\nl= [1,2,3]\nprint \"Original\", l, id(l)\ntest(l)\nprint \"After\", l, id(l)\ndef test(l):\n    print \"Received\", l , id(l)\n    l = [0, 0, 0]\n    print \"Changed to\", l, id(l)  # New local object created, breaking link to global l\n\nl= [1,2,3]\nprint \"Original\", l, id(l)\ntest(l)\nprint \"After\", l, id(l)\ngives:Original [1, 2, 3] 4454645632\nReceived [1, 2, 3] 4454645632\nChanged to [0, 0, 0] 4474591928\nAfter [1, 2, 3] 4454645632\nOriginal [1, 2, 3] 4454645632\nReceived [1, 2, 3] 4454645632\nChanged to [0, 0, 0] 4474591928\nAfter [1, 2, 3] 4454645632\nThe assignment to an global variable that is not declared global therefore creates a new local object and breaks the link to the original object.",
                "As you can state you need to have a mutable object, but let me suggest you to check over the global variables as they can help you or even solve this kind of issue!http://docs.python.org/3/faq/programming.html#what-are-the-rules-for-local-and-global-variables-in-pythonhttp://docs.python.org/3/faq/programming.html#what-are-the-rules-for-local-and-global-variables-in-pythonexample:>>> def x(y):\n...     global z\n...     z = y\n...\n\n>>> x\n<function x at 0x00000000020E1730>\n>>> y\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'y' is not defined\n>>> z\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'z' is not defined\n\n>>> x(2)\n>>> x\n<function x at 0x00000000020E1730>\n>>> y\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'y' is not defined\n>>> z\n2\n>>> def x(y):\n...     global z\n...     z = y\n...\n\n>>> x\n<function x at 0x00000000020E1730>\n>>> y\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'y' is not defined\n>>> z\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'z' is not defined\n\n>>> x(2)\n>>> x\n<function x at 0x00000000020E1730>\n>>> y\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'y' is not defined\n>>> z\n2\n",
                "Here is the simple (I hope) explanation of the concept pass by object used in Python.\nWhenever you pass an object to the function, the object itself is passed (object in Python is actually what you'd call a value in other programming languages) not the reference to this object. In other words, when you call:pass by objectdef change_me(list):\n   list = [1, 2, 3]\n\nmy_list = [0, 1]\nchange_me(my_list)\ndef change_me(list):\n   list = [1, 2, 3]\n\nmy_list = [0, 1]\nchange_me(my_list)\nThe actual object - [0, 1] (which would be called a value in other programming languages) is being passed. So in fact the function change_me will try to do something like:change_me[0, 1] = [1, 2, 3]\n[0, 1] = [1, 2, 3]\nwhich obviously will not change the object passed to the function. If the function looked like this:def change_me(list):\n   list.append(2)\ndef change_me(list):\n   list.append(2)\nThen the call would result in:[0, 1].append(2)\n[0, 1].append(2)\nwhich obviously will change the object. This answer explains it well.This answer",
                "Aside from all the great explanations on how this stuff works in Python, I don't see a simple suggestion for the problem. As you seem to do create objects and instances, the pythonic way of handling instance variables and changing them is the following:class PassByReference:\n    def __init__(self):\n        self.variable = 'Original'\n        self.Change()\n        print self.variable\n\n    def Change(self):\n        self.variable = 'Changed'\nclass PassByReference:\n    def __init__(self):\n        self.variable = 'Original'\n        self.Change()\n        print self.variable\n\n    def Change(self):\n        self.variable = 'Changed'\nIn instance methods, you normally refer to self to access instance attributes. It is normal to set instance attributes in __init__ and read or change them in instance methods. That is also why you pass self als the first argument to def Change.self__init__selfdef ChangeAnother solution would be to create a static method like this:class PassByReference:\n    def __init__(self):\n        self.variable = 'Original'\n        self.variable = PassByReference.Change(self.variable)\n        print self.variable\n\n    @staticmethod\n    def Change(var):\n        var = 'Changed'\n        return var\nclass PassByReference:\n    def __init__(self):\n        self.variable = 'Original'\n        self.variable = PassByReference.Change(self.variable)\n        print self.variable\n\n    @staticmethod\n    def Change(var):\n        var = 'Changed'\n        return var\n",
                "I used the following method to quickly convert a couple of Fortran codes to Python.  True, it's not pass by reference as the original question was posed, but is a simple work around in some cases.a=0\nb=0\nc=0\ndef myfunc(a,b,c):\n    a=1\n    b=2\n    c=3\n    return a,b,c\n\na,b,c = myfunc(a,b,c)\nprint a,b,c\na=0\nb=0\nc=0\ndef myfunc(a,b,c):\n    a=1\n    b=2\n    c=3\n    return a,b,c\n\na,b,c = myfunc(a,b,c)\nprint a,b,c\n",
                "There is a little trick to pass an object by reference, even though the language doesn't make it possible. It works in Java too, it's the list with one item. ;-)class PassByReference:\n    def __init__(self, name):\n        self.name = name\n\ndef changeRef(ref):\n    ref[0] = PassByReference('Michael')\n\nobj = PassByReference('Peter')\nprint obj.name\n\np = [obj] # A pointer to obj! ;-)\nchangeRef(p)\n\nprint p[0].name # p->name\nclass PassByReference:\n    def __init__(self, name):\n        self.name = name\n\ndef changeRef(ref):\n    ref[0] = PassByReference('Michael')\n\nobj = PassByReference('Peter')\nprint obj.name\n\np = [obj] # A pointer to obj! ;-)\nchangeRef(p)\n\nprint p[0].name # p->name\nIt's an ugly hack, but it works. ;-P",
                "given the way python handles values and references to them, the only way you can reference an arbitrary instance attribute is by name:class PassByReferenceIsh:\n    def __init__(self):\n        self.variable = 'Original'\n        self.change('variable')\n        print self.variable\n\n    def change(self, var):\n        self.__dict__[var] = 'Changed'\nclass PassByReferenceIsh:\n    def __init__(self):\n        self.variable = 'Original'\n        self.change('variable')\n        print self.variable\n\n    def change(self, var):\n        self.__dict__[var] = 'Changed'\nin real code you would, of course, add error checking on the dict lookup.",
                "Since it seems to be nowhere mentioned an approach to simulate references as known from e.g. C++ is to use an \"update\" function and pass that instead of the actual variable (or rather, \"name\"):def need_to_modify(update):\n    update(42) # set new value 42\n    # other code\n\ndef call_it():\n    value = 21\n    def update_value(new_value):\n        nonlocal value\n        value = new_value\n    need_to_modify(update_value)\n    print(value) # prints 42\ndef need_to_modify(update):\n    update(42) # set new value 42\n    # other code\n\ndef call_it():\n    value = 21\n    def update_value(new_value):\n        nonlocal value\n        value = new_value\n    need_to_modify(update_value)\n    print(value) # prints 42\nThis is mostly useful for \"out-only references\" or in a situation with multiple threads / processes (by making the update function thread / multiprocessing safe).Obviously the above does not allow reading the value, only updating it.reading",
                "Since your example happens to be object-oriented, you could make the following change to achieve a similar result:class PassByReference:\n    def __init__(self):\n        self.variable = 'Original'\n        self.change('variable')\n        print(self.variable)\n\n    def change(self, var):\n        setattr(self, var, 'Changed')\n\n# o.variable will equal 'Changed'\no = PassByReference()\nassert o.variable == 'Changed'\nclass PassByReference:\n    def __init__(self):\n        self.variable = 'Original'\n        self.change('variable')\n        print(self.variable)\n\n    def change(self, var):\n        setattr(self, var, 'Changed')\n\n# o.variable will equal 'Changed'\no = PassByReference()\nassert o.variable == 'Changed'\n",
                "Since dictionaries are passed by reference, you can use a dict variable to store any referenced values inside it.# returns the result of adding numbers `a` and `b`\ndef AddNumbers(a, b, ref): # using a dict for reference\n    result = a + b\n    ref['multi'] = a * b # reference the multi. ref['multi'] is number\n    ref['msg'] = \"The result: \" + str(result) + \" was nice!\"\n    return result\n\nnumber1 = 5\nnumber2 = 10\nref = {} # init a dict like that so it can save all the referenced values. this is because all dictionaries are passed by reference, while strings and numbers do not.\n\nsum = AddNumbers(number1, number2, ref)\nprint(\"sum: \", sum)             # the returned value\nprint(\"multi: \", ref['multi'])  # a referenced value\nprint(\"msg: \", ref['msg'])      # a referenced value\n# returns the result of adding numbers `a` and `b`\ndef AddNumbers(a, b, ref): # using a dict for reference\n    result = a + b\n    ref['multi'] = a * b # reference the multi. ref['multi'] is number\n    ref['msg'] = \"The result: \" + str(result) + \" was nice!\"\n    return result\n\nnumber1 = 5\nnumber2 = 10\nref = {} # init a dict like that so it can save all the referenced values. this is because all dictionaries are passed by reference, while strings and numbers do not.\n\nsum = AddNumbers(number1, number2, ref)\nprint(\"sum: \", sum)             # the returned value\nprint(\"multi: \", ref['multi'])  # a referenced value\nprint(\"msg: \", ref['msg'])      # a referenced value\n",
                "While pass by reference is nothing that fits well into python and should be rarely used there are some workarounds that actually can work to get the object currently assigned to a local variable or even reassign a local variable from inside of a called function.The basic idea is to have a function that can do that access and can be passed as object into other functions or stored in a class.One way is to use global (for global variables) or nonlocal (for local variables in a function) in a wrapper function.globalnonlocaldef change(wrapper):\n    wrapper(7)\n\nx = 5\ndef setter(val):\n    global x\n    x = val\nprint(x)\ndef change(wrapper):\n    wrapper(7)\n\nx = 5\ndef setter(val):\n    global x\n    x = val\nprint(x)\nThe same idea works for reading and deleting a variable.delFor just reading there is even a shorter way of just using lambda: x which returns a callable that when called returns the current value of x. This is somewhat like \"call by name\" used in languages in the distant past.lambda: xPassing 3 wrappers to access a variable is a bit unwieldy so those can be wrapped into a class that has a proxy attribute:class ByRef:\n    def __init__(self, r, w, d):\n        self._read = r\n        self._write = w\n        self._delete = d\n    def set(self, val):\n        self._write(val)\n    def get(self):\n        return self._read()\n    def remove(self):\n        self._delete()\n    wrapped = property(get, set, remove)\n\n# left as an exercise for the reader: define set, get, remove as local functions using global / nonlocal\nr = ByRef(get, set, remove)\nr.wrapped = 15\nclass ByRef:\n    def __init__(self, r, w, d):\n        self._read = r\n        self._write = w\n        self._delete = d\n    def set(self, val):\n        self._write(val)\n    def get(self):\n        return self._read()\n    def remove(self):\n        self._delete()\n    wrapped = property(get, set, remove)\n\n# left as an exercise for the reader: define set, get, remove as local functions using global / nonlocal\nr = ByRef(get, set, remove)\nr.wrapped = 15\nPythons \"reflection\" support makes it possible to get a object that is capable of reassigning a name/variable in a given scope without defining functions explicitly in that scope:class ByRef:\n    def __init__(self, locs, name):\n        self._locs = locs\n        self._name = name\n    def set(self, val):\n        self._locs[self._name] = val\n    def get(self):\n        return self._locs[self._name]\n    def remove(self):\n        del self._locs[self._name]\n    wrapped = property(get, set, remove)\n\ndef change(x):\n    x.wrapped = 7\n\ndef test_me():\n    x = 6\n    print(x)\n    change(ByRef(locals(), \"x\"))\n    print(x)\nclass ByRef:\n    def __init__(self, locs, name):\n        self._locs = locs\n        self._name = name\n    def set(self, val):\n        self._locs[self._name] = val\n    def get(self):\n        return self._locs[self._name]\n    def remove(self):\n        del self._locs[self._name]\n    wrapped = property(get, set, remove)\n\ndef change(x):\n    x.wrapped = 7\n\ndef test_me():\n    x = 6\n    print(x)\n    change(ByRef(locals(), \"x\"))\n    print(x)\nHere the ByRef class wraps a dictionary access. So attribute access to wrapped is translated to a item access in the passed dictionary. By passing the result of the builtin locals and the name of a local variable this ends up accessing a local variable. The python documentation as of 3.5 advises that changing the dictionary might not work but it seems to work for me.ByRefwrappedlocals",
                "You can merely use an empty class as an instance to store reference objects because internally object attributes are stored in an instance dictionary. See the example.an empty classclass RefsObj(object):\n    \"A class which helps to create references to variables.\"\n    pass\n\n...\n\n# an example of usage\ndef change_ref_var(ref_obj):\n    ref_obj.val = 24\n\nref_obj = RefsObj()\nref_obj.val = 1\nprint(ref_obj.val) # or print ref_obj.val for python2\nchange_ref_var(ref_obj)\nprint(ref_obj.val)\nclass RefsObj(object):\n    \"A class which helps to create references to variables.\"\n    pass\n\n...\n\n# an example of usage\ndef change_ref_var(ref_obj):\n    ref_obj.val = 24\n\nref_obj = RefsObj()\nref_obj.val = 1\nprint(ref_obj.val) # or print ref_obj.val for python2\nchange_ref_var(ref_obj)\nprint(ref_obj.val)\n",
                "Pass-By-Reference in Python is quite different from the concept of pass by reference in C++/Java. \nJava&C#: primitive types(include string)pass by value(copy), Reference type is passed by reference(address copy) so all changes made in the parameter in the called function are visible to the caller.\nC++: Both pass-by-reference or pass-by-value are allowed. If a parameter is passed by reference, you can either modify it or not depending upon whether the parameter was passed as const or not. However, const or not, the parameter maintains the reference to the object and reference cannot be assigned to point to a different object within the called function.\nPython: \nPython is \u201cpass-by-object-reference\u201d, of which it is often said: \u201cObject references are passed by value.\u201d[Read here]1. Both the caller and the function refer to the same object but the parameter in the function is a new variable which is just holding a copy of the object in the caller. Like C++, a parameter can be either modified or not in function - This depends upon the type of object passed. eg; An immutable object type cannot be modified in the called function whereas a mutable object can be either updated or re-initialized. A crucial difference between updating or re-assigning/re-initializing the mutable variable is that updated value gets reflected back in the called function whereas the reinitialized value does not. Scope of any assignment of new object to a mutable variable is local to the function in the python. Examples provided by @blair-conrad are great to understand this.\nJava&C#: primitive types(include string)pass by value(copy), Reference type is passed by reference(address copy) so all changes made in the parameter in the called function are visible to the caller.Java&C#:C++: Both pass-by-reference or pass-by-value are allowed. If a parameter is passed by reference, you can either modify it or not depending upon whether the parameter was passed as const or not. However, const or not, the parameter maintains the reference to the object and reference cannot be assigned to point to a different object within the called function.C++:Python: \nPython is \u201cpass-by-object-reference\u201d, of which it is often said: \u201cObject references are passed by value.\u201d[Read here]1. Both the caller and the function refer to the same object but the parameter in the function is a new variable which is just holding a copy of the object in the caller. Like C++, a parameter can be either modified or not in function - This depends upon the type of object passed. eg; An immutable object type cannot be modified in the called function whereas a mutable object can be either updated or re-initialized. A crucial difference between updating or re-assigning/re-initializing the mutable variable is that updated value gets reflected back in the called function whereas the reinitialized value does not. Scope of any assignment of new object to a mutable variable is local to the function in the python. Examples provided by @blair-conrad are great to understand this.Python:1",
                "I am new to Python, started yesterday (though I have been programming for 45 years).I came here because I was writing a function where I wanted to have two so called out-parameters. If it would have been only one out-parameter, I wouldn't get hung up right now on checking how reference/value works in Python. I would just have used the return value of the function instead. But since I needed two such out-parameters I felt I needed to sort it out.twoIn this post I am going to show how I solved my situation. Perhaps others coming here can find it valuable, even though it is not exactly an answer to the topic question. Experienced Python programmers of course already know about the solution I used, but it was new to me.From the answers here I could quickly see that Python works a bit like Javascript in this regard, and that you need to use workarounds if you want the reference functionality.But then I found something neat in Python that I don't think I have seen in other languages before, namely that you can return more than one value from a function, in a simple comma separated way, like this:def somefunction(p):\n    a=p+1\n    b=p+2\n    c=-p\n    return a, b, c\ndef somefunction(p):\n    a=p+1\n    b=p+2\n    c=-p\n    return a, b, c\nand that you can handle that on the calling side similarly, like thisx, y, z = somefunction(w)\nx, y, z = somefunction(w)\nThat was good enough for me and I was satisfied. No need to use some workaround.In other languages you can of course also return many values, but then usually in the from of an object, and you need to adjust the calling side accordingly.The Python way of doing it was nice and simple.If you want to mimic by reference even more, you could do as follows:by referencedef somefunction(a, b, c):\n    a = a * 2\n    b = b + a\n    c = a * b * c\n    return a, b, c\n\nx = 3\ny = 5\nz = 10\nprint(F\"Before : {x}, {y}, {z}\")\n\nx, y, z = somefunction(x, y, z)\n\nprint(F\"After  : {x}, {y}, {z}\")\ndef somefunction(a, b, c):\n    a = a * 2\n    b = b + a\n    c = a * b * c\n    return a, b, c\n\nx = 3\ny = 5\nz = 10\nprint(F\"Before : {x}, {y}, {z}\")\n\nx, y, z = somefunction(x, y, z)\n\nprint(F\"After  : {x}, {y}, {z}\")\nwhich gives this result\nBefore : 3, 5, 10  \nAfter  : 6, 11, 660  \n",
                "alternatively you could use ctypes witch would look something like thisimport ctypes\n\ndef f(a):\n    a.value=2398 ## resign the value in a function\n\na = ctypes.c_int(0)\nprint(\"pre f\", a)\nf(a)\nprint(\"post f\", a)\nimport ctypes\n\ndef f(a):\n    a.value=2398 ## resign the value in a function\n\na = ctypes.c_int(0)\nprint(\"pre f\", a)\nf(a)\nprint(\"post f\", a)\nas a is a c int and not a python integer and apperently passed by reference. however you have to be carefull as strange things could happen and is therefor not advised",
                "Most likely not the most reliable method but this works, keep in mind that you are overloading the built-in str function which is typically something you don't want to do:import builtins\n\nclass sstr(str):\n    def __str__(self):\n        if hasattr(self, 'changed'):\n            return self.changed\n\n        return self\n\n    def change(self, value):\n        self.changed = value\n\nbuiltins.str = sstr\n\ndef change_the_value(val):\n    val.change('After')\n\nval = str('Before')\nprint (val)\nchange_the_value(val)\nprint (val)\nimport builtins\n\nclass sstr(str):\n    def __str__(self):\n        if hasattr(self, 'changed'):\n            return self.changed\n\n        return self\n\n    def change(self, value):\n        self.changed = value\n\nbuiltins.str = sstr\n\ndef change_the_value(val):\n    val.change('After')\n\nval = str('Before')\nprint (val)\nchange_the_value(val)\nprint (val)\n",
                "What about dataclasses? Also, it allows you to apply type restriction (aka \"type hint\").dataclassesfrom dataclasses import dataclass\n\n@dataclass\nclass Holder:\n    obj: your_type # Need any type? Use \"obj: object\" then.\n\ndef foo(ref: Holder):\n    ref.obj = do_something()\nfrom dataclasses import dataclass\n\n@dataclass\nclass Holder:\n    obj: your_type # Need any type? Use \"obj: object\" then.\n\ndef foo(ref: Holder):\n    ref.obj = do_something()\nI agree with folks that in most cases you'd better consider not to use it.And yet, when we're talking about contexts it's worth to know that way.contextscontextsYou can design explicit context class though. When prototyping I prefer dataclasses, just because it's easy to serialize them back and forth.Cheers!"
            ]
        },
        {
            "tag": "clone_list",
            "patterns": [
                "How do I clone a list so that it doesn't change unexpectedly after assignment?",
                "how do i clone my list for later use?",
                "how can i clone a list to make sure it does not change unexpectedly?",
                "how do i clone a list so that it doesn't change unexpectedly?",
                "how do i clone a list so it doesn't change unexpectedly after assignment?"
            ],
            "responses": [
                "new_list = my_list doesn't actually create a second list. The assignment just copies the reference to the list, not the actual list, so both new_list and my_list refer to the same list after the assignment.new_list = my_listnew_listmy_listTo actually copy the list, you have several options:\nYou can use the builtin list.copy() method (available since Python 3.3):\nnew_list = old_list.copy()\n\n\nYou can slice it:\nnew_list = old_list[:]\n\nAlex Martelli's opinion (at least back in 2007) about this is, that it is a weird syntax and it does not make sense to use it ever. ;) (In his opinion, the next one is more readable).\n\nYou can use the built in list() constructor:\nnew_list = list(old_list)\n\n\nYou can use generic copy.copy():\nimport copy\nnew_list = copy.copy(old_list)\n\nThis is a little slower than list() because it has to find out the datatype of old_list first.\n\nIf you need to copy the elements of the list as well, use generic copy.deepcopy():\nimport copy\nnew_list = copy.deepcopy(old_list)\n\nObviously the slowest and most memory-needing method, but sometimes unavoidable. This operates recursively; it will handle any number of levels of nested lists (or other containers).\n\nYou can use the builtin list.copy() method (available since Python 3.3):\nnew_list = old_list.copy()\n\nYou can use the builtin list.copy() method (available since Python 3.3):list.copy()list.copy()new_list = old_list.copy()\nnew_list = old_list.copy()\nYou can slice it:\nnew_list = old_list[:]\n\nAlex Martelli's opinion (at least back in 2007) about this is, that it is a weird syntax and it does not make sense to use it ever. ;) (In his opinion, the next one is more readable).\nYou can slice it:new_list = old_list[:]\nnew_list = old_list[:]\nAlex Martelli's opinion (at least back in 2007) about this is, that it is a weird syntax and it does not make sense to use it ever. ;) (In his opinion, the next one is more readable).Alex Martelliback in 2007it is a weird syntax and it does not make sense to use it everYou can use the built in list() constructor:\nnew_list = list(old_list)\n\nYou can use the built in list() constructor:list()list()new_list = list(old_list)\nnew_list = list(old_list)\nYou can use generic copy.copy():\nimport copy\nnew_list = copy.copy(old_list)\n\nThis is a little slower than list() because it has to find out the datatype of old_list first.\nYou can use generic copy.copy():copy.copy()copy.copy()import copy\nnew_list = copy.copy(old_list)\nimport copy\nnew_list = copy.copy(old_list)\nThis is a little slower than list() because it has to find out the datatype of old_list first.list()old_listIf you need to copy the elements of the list as well, use generic copy.deepcopy():\nimport copy\nnew_list = copy.deepcopy(old_list)\n\nObviously the slowest and most memory-needing method, but sometimes unavoidable. This operates recursively; it will handle any number of levels of nested lists (or other containers).\nIf you need to copy the elements of the list as well, use generic copy.deepcopy():copy.deepcopy()copy.deepcopy()import copy\nnew_list = copy.deepcopy(old_list)\nimport copy\nnew_list = copy.deepcopy(old_list)\nObviously the slowest and most memory-needing method, but sometimes unavoidable. This operates recursively; it will handle any number of levels of nested lists (or other containers).Example:Example:import copy\n\nclass Foo(object):\n    def __init__(self, val):\n         self.val = val\n\n    def __repr__(self):\n        return f'Foo({self.val!r})'\n\nfoo = Foo(1)\n\na = ['foo', foo]\nb = a.copy()\nc = a[:]\nd = list(a)\ne = copy.copy(a)\nf = copy.deepcopy(a)\n\n# edit orignal list and instance \na.append('baz')\nfoo.val = 5\n\nprint(f'original: {a}\\nlist.copy(): {b}\\nslice: {c}\\nlist(): {d}\\ncopy: {e}\\ndeepcopy: {f}')\nimport copy\n\nclass Foo(object):\n    def __init__(self, val):\n         self.val = val\n\n    def __repr__(self):\n        return f'Foo({self.val!r})'\n\nfoo = Foo(1)\n\na = ['foo', foo]\nb = a.copy()\nc = a[:]\nd = list(a)\ne = copy.copy(a)\nf = copy.deepcopy(a)\n\n# edit orignal list and instance \na.append('baz')\nfoo.val = 5\n\nprint(f'original: {a}\\nlist.copy(): {b}\\nslice: {c}\\nlist(): {d}\\ncopy: {e}\\ndeepcopy: {f}')\nResult:original: ['foo', Foo(5), 'baz']\nlist.copy(): ['foo', Foo(5)]\nslice: ['foo', Foo(5)]\nlist(): ['foo', Foo(5)]\ncopy: ['foo', Foo(5)]\ndeepcopy: ['foo', Foo(1)]\noriginal: ['foo', Foo(5), 'baz']\nlist.copy(): ['foo', Foo(5)]\nslice: ['foo', Foo(5)]\nlist(): ['foo', Foo(5)]\ncopy: ['foo', Foo(5)]\ndeepcopy: ['foo', Foo(1)]\n",
                "Felix already provided an excellent answer, but I thought I'd do a speed comparison of the various methods:\n10.59 sec (105.9 \u00b5s/itn) -  copy.deepcopy(old_list)\n10.16 sec (101.6 \u00b5s/itn) - pure Python Copy() method copying classes with deepcopy\n1.488 sec (14.88 \u00b5s/itn) - pure Python Copy() method not copying classes (only dicts/lists/tuples)\n0.325 sec (3.25 \u00b5s/itn) - for item in old_list: new_list.append(item)\n0.217 sec (2.17 \u00b5s/itn) - [i for i in old_list] (a list comprehension)\n0.186 sec (1.86 \u00b5s/itn) - copy.copy(old_list)\n0.075 sec (0.75 \u00b5s/itn) - list(old_list)\n0.053 sec (0.53 \u00b5s/itn) - new_list = []; new_list.extend(old_list)\n0.039 sec (0.39 \u00b5s/itn) - old_list[:] (list slicing)\n10.59 sec (105.9 \u00b5s/itn) -  copy.deepcopy(old_list)copy.deepcopy(old_list)copy.deepcopy(old_list)10.16 sec (101.6 \u00b5s/itn) - pure Python Copy() method copying classes with deepcopyCopy()1.488 sec (14.88 \u00b5s/itn) - pure Python Copy() method not copying classes (only dicts/lists/tuples)Copy()0.325 sec (3.25 \u00b5s/itn) - for item in old_list: new_list.append(item)for item in old_list: new_list.append(item)0.217 sec (2.17 \u00b5s/itn) - [i for i in old_list] (a list comprehension)[i for i in old_list]list comprehension0.186 sec (1.86 \u00b5s/itn) - copy.copy(old_list)copy.copy(old_list)copy.copy(old_list)0.075 sec (0.75 \u00b5s/itn) - list(old_list)list(old_list)0.053 sec (0.53 \u00b5s/itn) - new_list = []; new_list.extend(old_list)new_list = []; new_list.extend(old_list)0.039 sec (0.39 \u00b5s/itn) - old_list[:] (list slicing)old_list[:]list slicingSo the fastest is list slicing. But be aware that copy.copy(), list[:] and list(list), unlike copy.deepcopy() and the python version don't copy any lists, dictionaries and class instances in the list, so if the originals change, they will change in the copied list too and vice versa.copy.copy()list[:]list(list)copy.deepcopy()(Here's the script if anyone's interested or wants to raise any issues:)from copy import deepcopy\n\nclass old_class:\n    def __init__(self):\n        self.blah = 'blah'\n\nclass new_class(object):\n    def __init__(self):\n        self.blah = 'blah'\n\ndignore = {str: None, unicode: None, int: None, type(None): None}\n\ndef Copy(obj, use_deepcopy=True):\n    t = type(obj)\n\n    if t in (list, tuple):\n        if t == tuple:\n            # Convert to a list if a tuple to\n            # allow assigning to when copying\n            is_tuple = True\n            obj = list(obj)\n        else:\n            # Otherwise just do a quick slice copy\n            obj = obj[:]\n            is_tuple = False\n\n        # Copy each item recursively\n        for x in xrange(len(obj)):\n            if type(obj[x]) in dignore:\n                continue\n            obj[x] = Copy(obj[x], use_deepcopy)\n\n        if is_tuple:\n            # Convert back into a tuple again\n            obj = tuple(obj)\n\n    elif t == dict:\n        # Use the fast shallow dict copy() method and copy any\n        # values which aren't immutable (like lists, dicts etc)\n        obj = obj.copy()\n        for k in obj:\n            if type(obj[k]) in dignore:\n                continue\n            obj[k] = Copy(obj[k], use_deepcopy)\n\n    elif t in dignore:\n        # Numeric or string/unicode?\n        # It's immutable, so ignore it!\n        pass\n\n    elif use_deepcopy:\n        obj = deepcopy(obj)\n    return obj\n\nif __name__ == '__main__':\n    import copy\n    from time import time\n\n    num_times = 100000\n    L = [None, 'blah', 1, 543.4532,\n         ['foo'], ('bar',), {'blah': 'blah'},\n         old_class(), new_class()]\n\n    t = time()\n    for i in xrange(num_times):\n        Copy(L)\n    print 'Custom Copy:', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        Copy(L, use_deepcopy=False)\n    print 'Custom Copy Only Copying Lists/Tuples/Dicts (no classes):', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        copy.copy(L)\n    print 'copy.copy:', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        copy.deepcopy(L)\n    print 'copy.deepcopy:', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        L[:]\n    print 'list slicing [:]:', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        list(L)\n    print 'list(L):', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        [i for i in L]\n    print 'list expression(L):', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        a = []\n        a.extend(L)\n    print 'list extend:', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        a = []\n        for y in L:\n            a.append(y)\n    print 'list append:', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        a = []\n        a.extend(i for i in L)\n    print 'generator expression extend:', time()-t\nfrom copy import deepcopy\n\nclass old_class:\n    def __init__(self):\n        self.blah = 'blah'\n\nclass new_class(object):\n    def __init__(self):\n        self.blah = 'blah'\n\ndignore = {str: None, unicode: None, int: None, type(None): None}\n\ndef Copy(obj, use_deepcopy=True):\n    t = type(obj)\n\n    if t in (list, tuple):\n        if t == tuple:\n            # Convert to a list if a tuple to\n            # allow assigning to when copying\n            is_tuple = True\n            obj = list(obj)\n        else:\n            # Otherwise just do a quick slice copy\n            obj = obj[:]\n            is_tuple = False\n\n        # Copy each item recursively\n        for x in xrange(len(obj)):\n            if type(obj[x]) in dignore:\n                continue\n            obj[x] = Copy(obj[x], use_deepcopy)\n\n        if is_tuple:\n            # Convert back into a tuple again\n            obj = tuple(obj)\n\n    elif t == dict:\n        # Use the fast shallow dict copy() method and copy any\n        # values which aren't immutable (like lists, dicts etc)\n        obj = obj.copy()\n        for k in obj:\n            if type(obj[k]) in dignore:\n                continue\n            obj[k] = Copy(obj[k], use_deepcopy)\n\n    elif t in dignore:\n        # Numeric or string/unicode?\n        # It's immutable, so ignore it!\n        pass\n\n    elif use_deepcopy:\n        obj = deepcopy(obj)\n    return obj\n\nif __name__ == '__main__':\n    import copy\n    from time import time\n\n    num_times = 100000\n    L = [None, 'blah', 1, 543.4532,\n         ['foo'], ('bar',), {'blah': 'blah'},\n         old_class(), new_class()]\n\n    t = time()\n    for i in xrange(num_times):\n        Copy(L)\n    print 'Custom Copy:', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        Copy(L, use_deepcopy=False)\n    print 'Custom Copy Only Copying Lists/Tuples/Dicts (no classes):', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        copy.copy(L)\n    print 'copy.copy:', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        copy.deepcopy(L)\n    print 'copy.deepcopy:', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        L[:]\n    print 'list slicing [:]:', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        list(L)\n    print 'list(L):', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        [i for i in L]\n    print 'list expression(L):', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        a = []\n        a.extend(L)\n    print 'list extend:', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        a = []\n        for y in L:\n            a.append(y)\n    print 'list append:', time()-t\n\n    t = time()\n    for i in xrange(num_times):\n        a = []\n        a.extend(i for i in L)\n    print 'generator expression extend:', time()-t\n",
                "I've been told that Python 3.3+ adds the list.copy() method, which should be as fast as slicing:been toldadds the list.copy()list.copy()newlist = old_list.copy()\nnewlist = old_list.copy()\n",
                "\nWhat are the options to clone or copy a list in Python?\nWhat are the options to clone or copy a list in Python?In Python 3, a shallow copy can be made with:a_copy = a_list.copy()\na_copy = a_list.copy()\nIn Python 2 and 3, you can get a shallow copy with a full slice of the original:a_copy = a_list[:]\na_copy = a_list[:]\nExplanationThere are two semantic ways to copy a list. A shallow copy creates a new list of the same objects, a deep copy creates a new list containing new equivalent objects.Shallow list copyA shallow copy only copies the list itself, which is a container of references to the objects in the list. If the objects contained themselves are mutable and one is changed, the change will be reflected in both lists. There are different ways to do this in Python 2 and 3. The Python 2 ways will also work in Python 3.Python 2In Python 2, the idiomatic way of making a shallow copy of a list is with a complete slice of the original:a_copy = a_list[:]\na_copy = a_list[:]\nYou can also accomplish the same thing by passing the list through the list constructor, a_copy = list(a_list)\na_copy = list(a_list)\nbut using the constructor is less efficient:>>> timeit\n>>> l = range(20)\n>>> min(timeit.repeat(lambda: l[:]))\n0.30504298210144043\n>>> min(timeit.repeat(lambda: list(l)))\n0.40698814392089844\n>>> timeit\n>>> l = range(20)\n>>> min(timeit.repeat(lambda: l[:]))\n0.30504298210144043\n>>> min(timeit.repeat(lambda: list(l)))\n0.40698814392089844\nPython 3In Python 3, lists get the list.copy method:list.copya_copy = a_list.copy()\na_copy = a_list.copy()\nIn Python 3.5:>>> import timeit\n>>> l = list(range(20))\n>>> min(timeit.repeat(lambda: l[:]))\n0.38448613602668047\n>>> min(timeit.repeat(lambda: list(l)))\n0.6309100328944623\n>>> min(timeit.repeat(lambda: l.copy()))\n0.38122922903858125\n>>> import timeit\n>>> l = list(range(20))\n>>> min(timeit.repeat(lambda: l[:]))\n0.38448613602668047\n>>> min(timeit.repeat(lambda: list(l)))\n0.6309100328944623\n>>> min(timeit.repeat(lambda: l.copy()))\n0.38122922903858125\nMaking another pointer does not make a copynot\nUsing new_list = my_list then modifies new_list every time my_list changes. Why is this?\nUsing new_list = my_list then modifies new_list every time my_list changes. Why is this?Using new_list = my_list then modifies new_list every time my_list changes. Why is this?my_list is just a name that points to the actual list in memory. When you say new_list = my_list you're not making a copy, you're just adding another name that points at that original list in memory. We can have similar issues when we make copies of lists. my_listnew_list = my_list>>> l = [[], [], []]\n>>> l_copy = l[:]\n>>> l_copy\n[[], [], []]\n>>> l_copy[0].append('foo')\n>>> l_copy\n[['foo'], [], []]\n>>> l\n[['foo'], [], []]\n>>> l = [[], [], []]\n>>> l_copy = l[:]\n>>> l_copy\n[[], [], []]\n>>> l_copy[0].append('foo')\n>>> l_copy\n[['foo'], [], []]\n>>> l\n[['foo'], [], []]\nThe list is just an array of pointers to the contents, so a shallow copy just copies the pointers, and so you have two different lists, but they have the same contents. To make copies of the contents, you need a deep copy.Deep copiesTo make a deep copy of a list, in Python 2 or 3, use deepcopy in the copy module:deep copy of a list, in Python 2 or 3, use deepcopy in the copy moduledeepcopycopyimport copy\na_deep_copy = copy.deepcopy(a_list)\nimport copy\na_deep_copy = copy.deepcopy(a_list)\nTo demonstrate how this allows us to make new sub-lists:>>> import copy\n>>> l\n[['foo'], [], []]\n>>> l_deep_copy = copy.deepcopy(l)\n>>> l_deep_copy[0].pop()\n'foo'\n>>> l_deep_copy\n[[], [], []]\n>>> l\n[['foo'], [], []]\n>>> import copy\n>>> l\n[['foo'], [], []]\n>>> l_deep_copy = copy.deepcopy(l)\n>>> l_deep_copy[0].pop()\n'foo'\n>>> l_deep_copy\n[[], [], []]\n>>> l\n[['foo'], [], []]\nAnd so we see that the deep copied list is an entirely different list from the original. You could roll your own function - but don't. You're likely to create bugs you otherwise wouldn't have by using the standard library's deepcopy function.Don't use evalevalYou may see this used as a way to deepcopy, but don't do it:problematic_deep_copy = eval(repr(a_list))\nproblematic_deep_copy = eval(repr(a_list))\n\nIt's dangerous, particularly if you're evaluating something from a source you don't trust.\nIt's not reliable, if a subelement you're copying doesn't have a representation that can be eval'd to reproduce an equivalent element.\nIt's also less performant. \nIt's dangerous, particularly if you're evaluating something from a source you don't trust.It's not reliable, if a subelement you're copying doesn't have a representation that can be eval'd to reproduce an equivalent element.It's also less performant. In 64 bit Python 2.7:>>> import timeit\n>>> import copy\n>>> l = range(10)\n>>> min(timeit.repeat(lambda: copy.deepcopy(l)))\n27.55826997756958\n>>> min(timeit.repeat(lambda: eval(repr(l))))\n29.04534101486206\n>>> import timeit\n>>> import copy\n>>> l = range(10)\n>>> min(timeit.repeat(lambda: copy.deepcopy(l)))\n27.55826997756958\n>>> min(timeit.repeat(lambda: eval(repr(l))))\n29.04534101486206\non 64 bit Python 3.5:>>> import timeit\n>>> import copy\n>>> l = list(range(10))\n>>> min(timeit.repeat(lambda: copy.deepcopy(l)))\n16.84255409205798\n>>> min(timeit.repeat(lambda: eval(repr(l))))\n34.813894678023644\n>>> import timeit\n>>> import copy\n>>> l = list(range(10))\n>>> min(timeit.repeat(lambda: copy.deepcopy(l)))\n16.84255409205798\n>>> min(timeit.repeat(lambda: eval(repr(l))))\n34.813894678023644\n",
                "Let's start from the beginning and explore this question.So let's suppose you have two lists:list_1 = ['01', '98']\nlist_2 = [['01', '98']]\nlist_1 = ['01', '98']\nlist_2 = [['01', '98']]\nAnd we have to copy both lists, now starting from the first list:So first let's try by setting the variable copy to our original list, list_1:copylist_1copy = list_1\ncopy = list_1\nNow if you are thinking copy copied the list_1, then you are wrong. The id function can show us if two variables can point to the same object. Let's try this:list_1idprint(id(copy))\nprint(id(list_1))\nprint(id(copy))\nprint(id(list_1))\nThe output is:4329485320\n4329485320\n4329485320\n4329485320\nBoth variables are the exact same argument. Are you surprised?So as we know, Python doesn't store anything in a variable, Variables are just referencing to the object and object store the value. Here object is a list but we created two references to that same object by two different variable names. This means that both variables are pointing to the same object, just with different names.listWhen you do copy = list_1, it is actually doing:copy = list_1Here in the image list_1 and copy are two variable names, but the object is same for both variable which is list.list_1copylistSo if you try to modify copied list then it will modify the original list too because the list is only one there, you will modify that list no matter you do from the copied list or from the original list:copy[0] = \"modify\"\n\nprint(copy)\nprint(list_1)\ncopy[0] = \"modify\"\n\nprint(copy)\nprint(list_1)\nOutput:['modify', '98']\n['modify', '98']\n['modify', '98']\n['modify', '98']\nSo it modified the original list:Now let's move onto a Pythonic method for copying lists.copy_1 = list_1[:]\ncopy_1 = list_1[:]\nThis method fixes the first issue we had:print(id(copy_1))\nprint(id(list_1))\n\n4338792136\n4338791432\nprint(id(copy_1))\nprint(id(list_1))\n\n4338792136\n4338791432\nSo as we can see our both list having different id and it means that both variables are pointing to different objects. So what actually going on here is:Now let's try to modify the list and let's see if we still face the previous problem:copy_1[0] = \"modify\"\n\nprint(list_1)\nprint(copy_1)\ncopy_1[0] = \"modify\"\n\nprint(list_1)\nprint(copy_1)\nThe output is:['01', '98']\n['modify', '98']\n['01', '98']\n['modify', '98']\nAs you can see, it only modified the copied list. That means it worked.Do you think we're done? No. Let's try to copy our nested list.copy_2 = list_2[:]\ncopy_2 = list_2[:]\nlist_2 should reference to another object which is copy of list_2. Let's check:list_2list_2print(id((list_2)), id(copy_2))\nprint(id((list_2)), id(copy_2))\nWe get the output:4330403592 4330403528\n4330403592 4330403528\nNow we can assume both lists are pointing different object, so now let's try to modify it and let's see it is giving what we want:copy_2[0][1] = \"modify\"\n\nprint(list_2, copy_2)\ncopy_2[0][1] = \"modify\"\n\nprint(list_2, copy_2)\nThis gives us the output:[['01', 'modify']] [['01', 'modify']]\n[['01', 'modify']] [['01', 'modify']]\nThis may seem a little bit confusing, because the same method we previously used worked. Let's try to understand this.When you do:copy_2 = list_2[:]\ncopy_2 = list_2[:]\nYou're only copying the outer list, not the inside list. We can use the id function once again to check this.idprint(id(copy_2[0]))\nprint(id(list_2[0]))\nprint(id(copy_2[0]))\nprint(id(list_2[0]))\nThe output is:4329485832\n4329485832\n4329485832\n4329485832\nWhen we do copy_2 = list_2[:], this happens:copy_2 = list_2[:]It creates the copy of list, but only outer list copy, not the nested list copy. The nested list is same for both variable, so if you try to modify the nested list then it will modify the original list too as the nested list object is same for both lists.What is the solution? The solution is the deepcopy function.deepcopyfrom copy import deepcopy\ndeep = deepcopy(list_2)\nfrom copy import deepcopy\ndeep = deepcopy(list_2)\nLet's check this:print(id((list_2)), id(deep))\n\n4322146056 4322148040\nprint(id((list_2)), id(deep))\n\n4322146056 4322148040\nBoth outer lists have different IDs. Let's try this on the inner nested lists.print(id(deep[0]))\nprint(id(list_2[0]))\nprint(id(deep[0]))\nprint(id(list_2[0]))\nThe output is:4322145992\n4322145800\n4322145992\n4322145800\nAs you can see both IDs are different, meaning we can assume that both nested lists are pointing different object now.This means when you do deep = deepcopy(list_2) what actually happens:deep = deepcopy(list_2)Both nested lists are pointing different object and they have separate copy of nested list now.Now let's try to modify the nested list and see if it solved the previous issue or not:deep[0][1] = \"modify\"\nprint(list_2, deep)\ndeep[0][1] = \"modify\"\nprint(list_2, deep)\nIt outputs:[['01', '98']] [['01', 'modify']]\n[['01', '98']] [['01', 'modify']]\nAs you can see, it didn't modify the original nested list, it only modified the copied list.",
                "There are many answers already that tell you how to make a proper copy, but none of them say why your original 'copy' failed. Python doesn't store values in variables; it binds names to objects. Your original assignment took the object referred to by my_list and bound it to new_list as well. No matter which name you use there is still only one list, so changes made when referring to it as my_list will persist when referring to it as new_list. Each of the other answers to this question give you different ways of creating a new object to bind to new_list. my_listnew_listmy_listnew_listnew_listEach element of a list acts like a name, in that each element binds non-exclusively to an object. A shallow copy creates a new list whose elements bind to the same objects as before.new_list = list(my_list)  # or my_list[:], but I prefer this syntax\n# is simply a shorter way of:\nnew_list = [element for element in my_list]\nnew_list = list(my_list)  # or my_list[:], but I prefer this syntax\n# is simply a shorter way of:\nnew_list = [element for element in my_list]\nTo take your list copy one step further, copy each object that your list refers to, and bind those element copies to a new list. import copy  \n# each element must have __copy__ defined for this...\nnew_list = [copy.copy(element) for element in my_list]\nimport copy  \n# each element must have __copy__ defined for this...\nnew_list = [copy.copy(element) for element in my_list]\nThis is not yet a deep copy, because each element of a list may refer to other objects, just like the list is bound to its elements. To recursively copy every element in the list, and then each other object referred to by each element, and so on: perform a deep copy. import copy\n# each element must have __deepcopy__ defined for this...\nnew_list = copy.deepcopy(my_list)\nimport copy\n# each element must have __deepcopy__ defined for this...\nnew_list = copy.deepcopy(my_list)\nSee the documentation for more information about corner cases in copying.the documentation",
                "Use thing[:]thing[:]>>> a = [1,2]\n>>> b = a[:]\n>>> a += [3]\n>>> a\n[1, 2, 3]\n>>> b\n[1, 2]\n>>> \n>>> a = [1,2]\n>>> b = a[:]\n>>> a += [3]\n>>> a\n[1, 2, 3]\n>>> b\n[1, 2]\n>>> \n",
                "Python 3.6 TimingsHere are the timing results using Python 3.6.8. Keep in mind these times are relative to one another, not absolute.I stuck to only doing shallow copies, and also added some new methods that weren't possible in Python\u00a02, such as list.copy() (the Python\u00a03 slice equivalent) and two forms of list unpacking (*new_list, = list and new_list = [*list]):list.copy()slice equivalentlist unpacking*new_list, = listnew_list = [*list]METHOD                TIME TAKEN\nb = [*a]               2.75180600000021\nb = a * 1              3.50215399999990\nb = a[:]               3.78278899999986  # Python 2 winner (see above)\nb = a.copy()           4.20556500000020  # Python 3 \"slice equivalent\" (see above)\nb = []; b.extend(a)    4.68069800000012\nb = a[0:len(a)]        6.84498999999959\n*b, = a                7.54031799999984\nb = list(a)            7.75815899999997\nb = [i for i in a]    18.4886440000000\nb = copy.copy(a)      18.8254879999999\nb = []\nfor item in a:\n  b.append(item)      35.4729199999997\nMETHOD                TIME TAKEN\nb = [*a]               2.75180600000021\nb = a * 1              3.50215399999990\nb = a[:]               3.78278899999986  # Python 2 winner (see above)\nb = a.copy()           4.20556500000020  # Python 3 \"slice equivalent\" (see above)\nb = []; b.extend(a)    4.68069800000012\nb = a[0:len(a)]        6.84498999999959\n*b, = a                7.54031799999984\nb = list(a)            7.75815899999997\nb = [i for i in a]    18.4886440000000\nb = copy.copy(a)      18.8254879999999\nb = []\nfor item in a:\n  b.append(item)      35.4729199999997\nWe can see the Python 2 winner still does well, but doesn't edge out Python 3 list.copy() by much, especially considering the superior readability of the latter.list.copy()The dark horse is the unpacking and repacking method (b = [*a]), which is ~25% faster than raw slicing, and more than twice as fast as the other unpacking method (*b, = a).b = [*a]*b, = ab = a * 1 also does surprisingly well.b = a * 1Note that these methods do not output equivalent results for any input other than lists. They all work for sliceable objects, a few work for any iterable, but only copy.copy() works for more general Python objects.Note that these methods do not output equivalent results for any input other than lists.notcopy.copy()Here is the testing code for interested parties (Template from here):Template from hereimport timeit\n\nCOUNT = 50000000\nprint(\"Array duplicating. Tests run\", COUNT, \"times\")\nsetup = 'a = [0,1,2,3,4,5,6,7,8,9]; import copy'\n\nprint(\"b = list(a)\\t\\t\", timeit.timeit(stmt='b = list(a)', setup=setup, number=COUNT))\nprint(\"b = copy.copy(a)\\t\", timeit.timeit(stmt='b = copy.copy(a)', setup=setup, number=COUNT))\nprint(\"b = a.copy()\\t\\t\", timeit.timeit(stmt='b = a.copy()', setup=setup, number=COUNT))\nprint(\"b = a[:]\\t\\t\", timeit.timeit(stmt='b = a[:]', setup=setup, number=COUNT))\nprint(\"b = a[0:len(a)]\\t\\t\", timeit.timeit(stmt='b = a[0:len(a)]', setup=setup, number=COUNT))\nprint(\"*b, = a\\t\\t\\t\", timeit.timeit(stmt='*b, = a', setup=setup, number=COUNT))\nprint(\"b = []; b.extend(a)\\t\", timeit.timeit(stmt='b = []; b.extend(a)', setup=setup, number=COUNT))\nprint(\"b = []; for item in a: b.append(item)\\t\", timeit.timeit(stmt='b = []\\nfor item in a:  b.append(item)', setup=setup, number=COUNT))\nprint(\"b = [i for i in a]\\t\", timeit.timeit(stmt='b = [i for i in a]', setup=setup, number=COUNT))\nprint(\"b = [*a]\\t\\t\", timeit.timeit(stmt='b = [*a]', setup=setup, number=COUNT))\nprint(\"b = a * 1\\t\\t\", timeit.timeit(stmt='b = a * 1', setup=setup, number=COUNT))\nimport timeit\n\nCOUNT = 50000000\nprint(\"Array duplicating. Tests run\", COUNT, \"times\")\nsetup = 'a = [0,1,2,3,4,5,6,7,8,9]; import copy'\n\nprint(\"b = list(a)\\t\\t\", timeit.timeit(stmt='b = list(a)', setup=setup, number=COUNT))\nprint(\"b = copy.copy(a)\\t\", timeit.timeit(stmt='b = copy.copy(a)', setup=setup, number=COUNT))\nprint(\"b = a.copy()\\t\\t\", timeit.timeit(stmt='b = a.copy()', setup=setup, number=COUNT))\nprint(\"b = a[:]\\t\\t\", timeit.timeit(stmt='b = a[:]', setup=setup, number=COUNT))\nprint(\"b = a[0:len(a)]\\t\\t\", timeit.timeit(stmt='b = a[0:len(a)]', setup=setup, number=COUNT))\nprint(\"*b, = a\\t\\t\\t\", timeit.timeit(stmt='*b, = a', setup=setup, number=COUNT))\nprint(\"b = []; b.extend(a)\\t\", timeit.timeit(stmt='b = []; b.extend(a)', setup=setup, number=COUNT))\nprint(\"b = []; for item in a: b.append(item)\\t\", timeit.timeit(stmt='b = []\\nfor item in a:  b.append(item)', setup=setup, number=COUNT))\nprint(\"b = [i for i in a]\\t\", timeit.timeit(stmt='b = [i for i in a]', setup=setup, number=COUNT))\nprint(\"b = [*a]\\t\\t\", timeit.timeit(stmt='b = [*a]', setup=setup, number=COUNT))\nprint(\"b = a * 1\\t\\t\", timeit.timeit(stmt='b = a * 1', setup=setup, number=COUNT))\n",
                "Python's idiom for doing this is newList = oldList[:]newList = oldList[:]",
                "All of the other contributors gave great answers, which work when you have a single dimension (leveled) list, however of the methods mentioned so far, only copy.deepcopy() works to clone/copy a list and not have it point to the nested list objects when you are working with multidimensional, nested lists (list of lists). While Felix Kling refers to it in his answer, there is a little bit more to the issue and possibly a workaround using built-ins that might prove a faster alternative to deepcopy.greatcopy.deepcopy()listFelix KlingdeepcopyWhile new_list = old_list[:], copy.copy(old_list)' and for Py3k old_list.copy() work for single-leveled lists, they revert to pointing at the list objects nested within the old_list and the new_list, and changes to one of the list objects are perpetuated in the other.new_list = old_list[:]copy.copy(old_list)'old_list.copy()listold_listnew_listlistEdit: New information brought to light\nAs was pointed out by both Aaron Hall and PM 2Ring using eval() is not only a bad idea, it is also much slower than copy.deepcopy().\nThis means that for multidimensional lists, the only option is copy.deepcopy(). With that being said, it really isn't an option as the performance goes way south when you try to use it on a moderately sized multidimensional array.  I tried to timeit using a 42x42 array, not unheard of or even that large for bioinformatics applications, and I gave up on waiting for a response and just started typing my edit to this post.\nIt would seem that the only real option then is to initialize multiple lists and work on them independently. If anyone has any other suggestions, for how to handle multidimensional list copying, it would be appreciated.\nAs was pointed out by both Aaron Hall and PM 2Ring using eval() is not only a bad idea, it is also much slower than copy.deepcopy().Aaron HallPM 2Ringusing eval() is not only a bad idea, it is also much slower than copy.deepcopy().eval()copy.deepcopy()This means that for multidimensional lists, the only option is copy.deepcopy(). With that being said, it really isn't an option as the performance goes way south when you try to use it on a moderately sized multidimensional array.  I tried to timeit using a 42x42 array, not unheard of or even that large for bioinformatics applications, and I gave up on waiting for a response and just started typing my edit to this post.copy.deepcopy()timeitIt would seem that the only real option then is to initialize multiple lists and work on them independently. If anyone has any other suggestions, for how to handle multidimensional list copying, it would be appreciated.As others have stated, there  are significant performance issues using the copy module and copy.deepcopy for multidimensional lists.are significantare significantcopycopy.deepcopyfor multidimensional listsfor multidimensional lists",
                "It surprises me that this hasn't been mentioned yet, so for the sake of completeness...You can perform list unpacking with the \"splat operator\": *, which will also copy elements of your list.*old_list = [1, 2, 3]\n\nnew_list = [*old_list]\n\nnew_list.append(4)\nold_list == [1, 2, 3]\nnew_list == [1, 2, 3, 4]\nold_list = [1, 2, 3]\n\nnew_list = [*old_list]\n\nnew_list.append(4)\nold_list == [1, 2, 3]\nnew_list == [1, 2, 3, 4]\nThe obvious downside to this method is that it is only available in Python 3.5+.Timing wise though, this appears to perform better than other common methods.x = [random.random() for _ in range(1000)]\n\n%timeit a = list(x)\n%timeit a = x.copy()\n%timeit a = x[:]\n\n%timeit a = [*x]\n\n#: 2.47 \u00b5s \u00b1 38.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n#: 2.47 \u00b5s \u00b1 54.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n#: 2.39 \u00b5s \u00b1 58.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n#: 2.22 \u00b5s \u00b1 43.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\nx = [random.random() for _ in range(1000)]\n\n%timeit a = list(x)\n%timeit a = x.copy()\n%timeit a = x[:]\n\n%timeit a = [*x]\n\n#: 2.47 \u00b5s \u00b1 38.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n#: 2.47 \u00b5s \u00b1 54.6 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n#: 2.39 \u00b5s \u00b1 58.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n\n#: 2.22 \u00b5s \u00b1 43.2 ns per loop (mean \u00b1 std. dev. of 7 runs, 100000 loops each)\n",
                "new_list = my_list[:]\nnew_list = my_list[:]\nnew_list = my_listnew_list = my_listTry to understand this. Let's say that my_list is in the heap memory at location X, i.e., my_list is pointing to the X. Now by assigning new_list = my_list you're letting new_list point to the X. This is known as a shallow copy.my_listmy_listnew_list = my_listnew_listshallow copyNow if you assign new_list = my_list[:], you're simply copying each object of my_list to new_list. This is known as a deep copy.new_list = my_list[:]my_listnew_listdeep copyThe other ways you can do this are:other\n\nnew_list = list(old_list)\n\n\n\nimport copy\nnew_list = copy.deepcopy(old_list)\n\n\n\nnew_list = list(old_list)\n\nnew_list = list(old_list)\nnew_list = list(old_list)\n\nimport copy\nnew_list = copy.deepcopy(old_list)\n\nimport copy\nnew_list = copy.deepcopy(old_list)\nimport copy\nnew_list = copy.deepcopy(old_list)\n",
                "A very simple approach independent of python version was missing in already-given answers which you can use most of the time (at least I do):new_list = my_list * 1       # Solution 1 when you are not using nested lists\nnew_list = my_list * 1       # Solution 1 when you are not using nested lists\nHowever, if my_list contains other containers (for example, nested lists) you must use deepcopy as others suggested in the answers above from the copy library. For example:ifmy_listdeepcopyimport copy\nnew_list = copy.deepcopy(my_list)   # Solution 2 when you are using nested lists\nimport copy\nnew_list = copy.deepcopy(my_list)   # Solution 2 when you are using nested lists\n.Bonus: If you don't want to copy elements use (AKA shallow copy):Bonusnew_list = my_list[:]\nnew_list = my_list[:]\nLet's understand difference between solution #1 and solution #2>>> a = range(5)\n>>> b = a*1\n>>> a,b\n([0, 1, 2, 3, 4], [0, 1, 2, 3, 4])\n>>> a[2] = 55\n>>> a,b\n([0, 1, 55, 3, 4], [0, 1, 2, 3, 4])\n>>> a = range(5)\n>>> b = a*1\n>>> a,b\n([0, 1, 2, 3, 4], [0, 1, 2, 3, 4])\n>>> a[2] = 55\n>>> a,b\n([0, 1, 55, 3, 4], [0, 1, 2, 3, 4])\nAs you can see, solution #1 worked perfectly when we were not using the nested lists. Let's check what will happen when we apply solution #1 to nested lists.>>> from copy import deepcopy\n>>> a = [range(i,i+4) for i in range(3)]\n>>> a\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5]]\n>>> b = a*1\n>>> c = deepcopy(a)\n>>> for i in (a, b, c): print i\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5]]\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5]]\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5]]\n>>> a[2].append('99')\n>>> for i in (a, b, c): print i\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5, 99]]\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5, 99]]   # Solution #1 didn't work in nested list\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5]]       # Solution #2 - DeepCopy worked in nested list\n>>> from copy import deepcopy\n>>> a = [range(i,i+4) for i in range(3)]\n>>> a\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5]]\n>>> b = a*1\n>>> c = deepcopy(a)\n>>> for i in (a, b, c): print i\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5]]\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5]]\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5]]\n>>> a[2].append('99')\n>>> for i in (a, b, c): print i\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5, 99]]\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5, 99]]   # Solution #1 didn't work in nested list\n[[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5]]       # Solution #2 - DeepCopy worked in nested list\n",
                "I wanted to post something a bit different than some of the other answers. Even though this is most likely not the most understandable, or fastest option, it provides a bit of an inside view of how deep copy works, as well as being another alternative option for deep copying. It doesn't really matter if my function has bugs, since the point of this is to show a way to copy objects like the question answers, but also to use this as a point to explain how deepcopy works at its core.At the core of any deep copy function is way to make a shallow copy. How? Simple. Any deep copy function only duplicates the containers of immutable objects. When you deepcopy a nested list, you are only duplicating the outer lists, not the mutable objects inside of the lists. You are only duplicating the containers. The same works for classes, too. When you deepcopy a class, you deepcopy all of its mutable attributes. So, how? How come you only have to copy the containers, like lists, dicts, tuples, iters, classes, and class instances?It's simple. A mutable object can't really be duplicated. It can never be changed, so it is only a single value. That means you never have to duplicate strings, numbers, bools, or any of those. But how would you duplicate the containers? Simple. You make just initialize a new container with all of the values. Deepcopy relies on recursion. It duplicates all the containers, even ones with containers inside of them, until no containers are left. A container is an immutable object.Once you know that, completely duplicating an object without any references is pretty easy. Here's a function for deepcopying basic data-types (wouldn't work for custom classes but you could always add that)def deepcopy(x):\n  immutables = (str, int, bool, float)\n  mutables = (list, dict, tuple)\n  if isinstance(x, immutables):\n    return x\n  elif isinstance(x, mutables):\n    if isinstance(x, tuple):\n      return tuple(deepcopy(list(x)))\n    elif isinstance(x, list):\n      return [deepcopy(y) for y in x]\n    elif isinstance(x, dict):\n      values = [deepcopy(y) for y in list(x.values())]\n      keys = list(x.keys())\n      return dict(zip(keys, values))\ndef deepcopy(x):\n  immutables = (str, int, bool, float)\n  mutables = (list, dict, tuple)\n  if isinstance(x, immutables):\n    return x\n  elif isinstance(x, mutables):\n    if isinstance(x, tuple):\n      return tuple(deepcopy(list(x)))\n    elif isinstance(x, list):\n      return [deepcopy(y) for y in x]\n    elif isinstance(x, dict):\n      values = [deepcopy(y) for y in list(x.values())]\n      keys = list(x.keys())\n      return dict(zip(keys, values))\nPython's own built-in deepcopy is based around that example. The only difference is it supports other types, and also supports user-classes by duplicating the attributes into a new duplicate class, and also blocks infinite-recursion with a reference to an object it's already seen using a memo list or dictionary. And that's really it for making deep copies. At its core, making a deep copy is just making shallow copies. I hope this answer adds something to the question.EXAMPLESEXAMPLESSay you have this list: [1, 2, 3]. The immutable numbers cannot be duplicated, but the other layer can. You can duplicate it using a list comprehension: [x for x in [1, 2, 3]][1, 2, 3][x for x in [1, 2, 3]]Now, imagine you have this list: [[1, 2], [3, 4], [5, 6]]. This time, you want to make a function, which uses recursion to deep copy all layers of the list. Instead of the previous list comprehension:[[1, 2], [3, 4], [5, 6]][x for x in _list]\n[x for x in _list]\nIt uses a new one for lists:[deepcopy_list(x) for x in _list]\n[deepcopy_list(x) for x in _list]\nAnd deepcopy_list looks like this:deepcopy_listdef deepcopy_list(x):\n  if isinstance(x, (str, bool, float, int)):\n    return x\n  else:\n    return [deepcopy_list(y) for y in x]\ndef deepcopy_list(x):\n  if isinstance(x, (str, bool, float, int)):\n    return x\n  else:\n    return [deepcopy_list(y) for y in x]\nThen now you have a function which can deepcopy any list of strs, bools, floast, ints and even lists to infinitely many layers using recursion. And there you have it, deepcopying.strs, bools, floast, intslistsTLDR: Deepcopy uses recursion to duplicate objects, and merely returns the same immutable objects as before, as immutable objects cannot be duplicated. However, it deepcopies the most inner layers of mutable objects until it reaches the outermost mutable layer of an object.TLDR",
                "Note that there are some cases where if you have defined your own custom class and you want to keep the attributes then you should use copy.copy() or copy.deepcopy() rather than the alternatives, for example in Python 3:copy.copy()copy.deepcopy()import copy\n\nclass MyList(list):\n    pass\n\nlst = MyList([1,2,3])\n\nlst.name = 'custom list'\n\nd = {\n'original': lst,\n'slicecopy' : lst[:],\n'lstcopy' : lst.copy(),\n'copycopy': copy.copy(lst),\n'deepcopy': copy.deepcopy(lst)\n}\n\n\nfor k,v in d.items():\n    print('lst: {}'.format(k), end=', ')\n    try:\n        name = v.name\n    except AttributeError:\n        name = 'NA'\n    print('name: {}'.format(name))\nimport copy\n\nclass MyList(list):\n    pass\n\nlst = MyList([1,2,3])\n\nlst.name = 'custom list'\n\nd = {\n'original': lst,\n'slicecopy' : lst[:],\n'lstcopy' : lst.copy(),\n'copycopy': copy.copy(lst),\n'deepcopy': copy.deepcopy(lst)\n}\n\n\nfor k,v in d.items():\n    print('lst: {}'.format(k), end=', ')\n    try:\n        name = v.name\n    except AttributeError:\n        name = 'NA'\n    print('name: {}'.format(name))\nOutputs:lst: original, name: custom list\nlst: slicecopy, name: NA\nlst: lstcopy, name: NA\nlst: copycopy, name: custom list\nlst: deepcopy, name: custom list\nlst: original, name: custom list\nlst: slicecopy, name: NA\nlst: lstcopy, name: NA\nlst: copycopy, name: custom list\nlst: deepcopy, name: custom list\n",
                "Remember that in Python when you do:    list1 = ['apples','bananas','pineapples']\n    list2 = list1\n    list1 = ['apples','bananas','pineapples']\n    list2 = list1\nList2 isn't storing the actual list, but a reference to list1. So when you do anything to list1, list2 changes as well. use the copy module (not default, download on pip) to make an original copy of the list(copy.copy() for simple lists, copy.deepcopy() for nested ones). This makes a copy that doesn't change with the first list.copy.copy()copy.deepcopy()",
                "A slight practical perspective to look into memory through id and gc. >>> b = a = ['hell', 'word']\n>>> c = ['hell', 'word']\n\n>>> id(a), id(b), id(c)\n(4424020872, 4424020872, 4423979272) \n     |           |\n      -----------\n\n>>> id(a[0]), id(b[0]), id(c[0])\n(4424018328, 4424018328, 4424018328) # all referring to same 'hell'\n     |           |           |\n      -----------------------\n\n>>> id(a[0][0]), id(b[0][0]), id(c[0][0])\n(4422785208, 4422785208, 4422785208) # all referring to same 'h'\n     |           |           |\n      -----------------------\n\n>>> a[0] += 'o'\n>>> a,b,c\n(['hello', 'word'], ['hello', 'word'], ['hell', 'word'])  # b changed too\n>>> id(a[0]), id(b[0]), id(c[0])\n(4424018384, 4424018384, 4424018328) # augmented assignment changed a[0],b[0]\n     |           |\n      -----------\n\n>>> b = a = ['hell', 'word']\n>>> id(a[0]), id(b[0]), id(c[0])\n(4424018328, 4424018328, 4424018328) # the same hell\n     |           |           |\n      -----------------------\n\n>>> import gc\n>>> gc.get_referrers(a[0]) \n[['hell', 'word'], ['hell', 'word']]  # one copy belong to a,b, the another for c\n>>> gc.get_referrers(('hell'))\n[['hell', 'word'], ['hell', 'word'], ('hell', None)] # ('hello', None) \n>>> b = a = ['hell', 'word']\n>>> c = ['hell', 'word']\n\n>>> id(a), id(b), id(c)\n(4424020872, 4424020872, 4423979272) \n     |           |\n      -----------\n\n>>> id(a[0]), id(b[0]), id(c[0])\n(4424018328, 4424018328, 4424018328) # all referring to same 'hell'\n     |           |           |\n      -----------------------\n\n>>> id(a[0][0]), id(b[0][0]), id(c[0][0])\n(4422785208, 4422785208, 4422785208) # all referring to same 'h'\n     |           |           |\n      -----------------------\n\n>>> a[0] += 'o'\n>>> a,b,c\n(['hello', 'word'], ['hello', 'word'], ['hell', 'word'])  # b changed too\n>>> id(a[0]), id(b[0]), id(c[0])\n(4424018384, 4424018384, 4424018328) # augmented assignment changed a[0],b[0]\n     |           |\n      -----------\n\n>>> b = a = ['hell', 'word']\n>>> id(a[0]), id(b[0]), id(c[0])\n(4424018328, 4424018328, 4424018328) # the same hell\n     |           |           |\n      -----------------------\n\n>>> import gc\n>>> gc.get_referrers(a[0]) \n[['hell', 'word'], ['hell', 'word']]  # one copy belong to a,b, the another for c\n>>> gc.get_referrers(('hell'))\n[['hell', 'word'], ['hell', 'word'], ('hell', None)] # ('hello', None) \n",
                "There is another way of copying a list that was not listed until now: adding an empty list: l2 = l + [].l2 = l + []I tested it with Python 3.8:l = [1,2,3]\nl2 = l + []\nprint(l,l2)\nl[0] = 'a'\nprint(l,l2)\nl = [1,2,3]\nl2 = l + []\nprint(l,l2)\nl[0] = 'a'\nprint(l,l2)\nIt is not the best answer, but it works.",
                "The deepcopy option is the only method that works for me:from copy import deepcopy\n\na = [   [ list(range(1, 3)) for i in range(3) ]   ]\nb = deepcopy(a)\nb[0][1]=[3]\nprint('Deep:')\nprint(a)\nprint(b)\nprint('-----------------------------')\na = [   [ list(range(1, 3)) for i in range(3) ]   ]\nb = a*1\nb[0][1]=[3]\nprint('*1:')\nprint(a)\nprint(b)\nprint('-----------------------------')\na = [   [ list(range(1, 3)) for i in range(3) ] ]\nb = a[:]\nb[0][1]=[3]\nprint('Vector copy:')\nprint(a)\nprint(b)\nprint('-----------------------------')\na = [   [ list(range(1, 3)) for i in range(3) ]  ]\nb = list(a)\nb[0][1]=[3]\nprint('List copy:')\nprint(a)\nprint(b)\nprint('-----------------------------')\na = [   [ list(range(1, 3)) for i in range(3) ]  ]\nb = a.copy()\nb[0][1]=[3]\nprint('.copy():')\nprint(a)\nprint(b)\nprint('-----------------------------')\na = [   [ list(range(1, 3)) for i in range(3) ]  ]\nb = a\nb[0][1]=[3]\nprint('Shallow:')\nprint(a)\nprint(b)\nprint('-----------------------------')\nfrom copy import deepcopy\n\na = [   [ list(range(1, 3)) for i in range(3) ]   ]\nb = deepcopy(a)\nb[0][1]=[3]\nprint('Deep:')\nprint(a)\nprint(b)\nprint('-----------------------------')\na = [   [ list(range(1, 3)) for i in range(3) ]   ]\nb = a*1\nb[0][1]=[3]\nprint('*1:')\nprint(a)\nprint(b)\nprint('-----------------------------')\na = [   [ list(range(1, 3)) for i in range(3) ] ]\nb = a[:]\nb[0][1]=[3]\nprint('Vector copy:')\nprint(a)\nprint(b)\nprint('-----------------------------')\na = [   [ list(range(1, 3)) for i in range(3) ]  ]\nb = list(a)\nb[0][1]=[3]\nprint('List copy:')\nprint(a)\nprint(b)\nprint('-----------------------------')\na = [   [ list(range(1, 3)) for i in range(3) ]  ]\nb = a.copy()\nb[0][1]=[3]\nprint('.copy():')\nprint(a)\nprint(b)\nprint('-----------------------------')\na = [   [ list(range(1, 3)) for i in range(3) ]  ]\nb = a\nb[0][1]=[3]\nprint('Shallow:')\nprint(a)\nprint(b)\nprint('-----------------------------')\nleads to output of:Deep:\n[[[1, 2], [1, 2], [1, 2]]]\n[[[1, 2], [3], [1, 2]]]\n-----------------------------\n*1:\n[[[1, 2], [3], [1, 2]]]\n[[[1, 2], [3], [1, 2]]]\n-----------------------------\nVector copy:\n[[[1, 2], [3], [1, 2]]]\n[[[1, 2], [3], [1, 2]]]\n-----------------------------\nList copy:\n[[[1, 2], [3], [1, 2]]]\n[[[1, 2], [3], [1, 2]]]\n-----------------------------\n.copy():\n[[[1, 2], [3], [1, 2]]]\n[[[1, 2], [3], [1, 2]]]\n-----------------------------\nShallow:\n[[[1, 2], [3], [1, 2]]]\n[[[1, 2], [3], [1, 2]]]\n-----------------------------\nDeep:\n[[[1, 2], [1, 2], [1, 2]]]\n[[[1, 2], [3], [1, 2]]]\n-----------------------------\n*1:\n[[[1, 2], [3], [1, 2]]]\n[[[1, 2], [3], [1, 2]]]\n-----------------------------\nVector copy:\n[[[1, 2], [3], [1, 2]]]\n[[[1, 2], [3], [1, 2]]]\n-----------------------------\nList copy:\n[[[1, 2], [3], [1, 2]]]\n[[[1, 2], [3], [1, 2]]]\n-----------------------------\n.copy():\n[[[1, 2], [3], [1, 2]]]\n[[[1, 2], [3], [1, 2]]]\n-----------------------------\nShallow:\n[[[1, 2], [3], [1, 2]]]\n[[[1, 2], [3], [1, 2]]]\n-----------------------------\n",
                "This is because, the line new_list = my_list assigns a new reference to the variable my_list which is new_list\nThis is similar to the C code given below,new_list = my_listmy_listnew_listCint my_list[] = [1,2,3,4];\nint *new_list;\nnew_list = my_list;\nint my_list[] = [1,2,3,4];\nint *new_list;\nnew_list = my_list;\nYou should use the copy module to create a new list byimport copy\nnew_list = copy.deepcopy(my_list)\nimport copy\nnew_list = copy.deepcopy(my_list)\n",
                "The method to use depends on the contents of the list being copied. If the list contains nested dicts than deepcopy is the only method that works, otherwise most of the methods listed in the answers (slice, loop [for], copy, extend, combine, or unpack) will work and execute in similar time (except for loop and deepcopy, which preformed the worst).dictsScriptfrom random import randint\nfrom time import time\nimport copy\n\nitem_count = 100000\n\ndef copy_type(l1: list, l2: list):\n  if l1 == l2:\n    return 'shallow'\n  return 'deep'\n\ndef run_time(start, end):\n  run = end - start\n  return int(run * 1000000)\n\ndef list_combine(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = [] + l1\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'combine', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_extend(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = []\n  l2.extend(l1)\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'extend', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_unpack(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = [*l1]\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'unpack', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_deepcopy(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = copy.deepcopy(l1)\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'deepcopy', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_copy(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = list.copy(l1)\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'copy', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_slice(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = l1[:]\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'slice', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_loop(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = []\n  for i in range(len(l1)):\n    l2.append(l1[i])\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'loop', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_list(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = list(l1)\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'list()', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\nif __name__ == '__main__':\n  list_type = [{'list[dict]': {'test': [1, 1]}}, \n          {'list[list]': [1, 1]}]\n  store = []\n  for data in list_type:\n    key = list(data.keys())[0]\n    store.append({key: [list_unpack(data[key]), list_extend(data[key]), \n                list_combine(data[key]), list_deepcopy(data[key]), \n                list_copy(data[key]), list_slice(data[key]),           \n                list_loop(data[key])]})\n  print(store)\nfrom random import randint\nfrom time import time\nimport copy\n\nitem_count = 100000\n\ndef copy_type(l1: list, l2: list):\n  if l1 == l2:\n    return 'shallow'\n  return 'deep'\n\ndef run_time(start, end):\n  run = end - start\n  return int(run * 1000000)\n\ndef list_combine(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = [] + l1\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'combine', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_extend(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = []\n  l2.extend(l1)\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'extend', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_unpack(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = [*l1]\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'unpack', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_deepcopy(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = copy.deepcopy(l1)\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'deepcopy', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_copy(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = list.copy(l1)\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'copy', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_slice(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = l1[:]\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'slice', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_loop(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = []\n  for i in range(len(l1)):\n    l2.append(l1[i])\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'loop', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\ndef list_list(data):\n  l1 = [data for i in range(item_count)]\n  start = time()\n  l2 = list(l1)\n  end = time()\n  if type(data) == dict:\n    l2[0]['test'].append(1)\n  elif type(data) == list:\n    l2.append(1)\n  return {'method': 'list()', 'copy_type': copy_type(l1, l2), \n          'time_\u00b5s': run_time(start, end)}\n\nif __name__ == '__main__':\n  list_type = [{'list[dict]': {'test': [1, 1]}}, \n          {'list[list]': [1, 1]}]\n  store = []\n  for data in list_type:\n    key = list(data.keys())[0]\n    store.append({key: [list_unpack(data[key]), list_extend(data[key]), \n                list_combine(data[key]), list_deepcopy(data[key]), \n                list_copy(data[key]), list_slice(data[key]),           \n                list_loop(data[key])]})\n  print(store)\nResults[{\"list[dict]\": [\n  {\"method\": \"unpack\", \"copy_type\": \"shallow\", \"time_\u00b5s\": 56149},\n  {\"method\": \"extend\", \"copy_type\": \"shallow\", \"time_\u00b5s\": 52991},\n  {\"method\": \"combine\", \"copy_type\": \"shallow\", \"time_\u00b5s\": 53726},\n  {\"method\": \"deepcopy\", \"copy_type\": \"deep\", \"time_\u00b5s\": 2702616},\n  {\"method\": \"copy\", \"copy_type\": \"shallow\", \"time_\u00b5s\": 52204},\n  {\"method\": \"slice\", \"copy_type\": \"shallow\", \"time_\u00b5s\": 52223},\n  {\"method\": \"loop\", \"copy_type\": \"shallow\", \"time_\u00b5s\": 836928}]},\n{\"list[list]\": [\n  {\"method\": \"unpack\", \"copy_type\": \"deep\", \"time_\u00b5s\": 52313},\n  {\"method\": \"extend\", \"copy_type\": \"deep\", \"time_\u00b5s\": 52550},\n  {\"method\": \"combine\", \"copy_type\": \"deep\", \"time_\u00b5s\": 53203},\n  {\"method\": \"deepcopy\", \"copy_type\": \"deep\", \"time_\u00b5s\": 2608560},\n  {\"method\": \"copy\", \"copy_type\": \"deep\", \"time_\u00b5s\": 53210},\n  {\"method\": \"slice\", \"copy_type\": \"deep\", \"time_\u00b5s\": 52937},\n  {\"method\": \"loop\", \"copy_type\": \"deep\", \"time_\u00b5s\": 834774}\n]}]\n[{\"list[dict]\": [\n  {\"method\": \"unpack\", \"copy_type\": \"shallow\", \"time_\u00b5s\": 56149},\n  {\"method\": \"extend\", \"copy_type\": \"shallow\", \"time_\u00b5s\": 52991},\n  {\"method\": \"combine\", \"copy_type\": \"shallow\", \"time_\u00b5s\": 53726},\n  {\"method\": \"deepcopy\", \"copy_type\": \"deep\", \"time_\u00b5s\": 2702616},\n  {\"method\": \"copy\", \"copy_type\": \"shallow\", \"time_\u00b5s\": 52204},\n  {\"method\": \"slice\", \"copy_type\": \"shallow\", \"time_\u00b5s\": 52223},\n  {\"method\": \"loop\", \"copy_type\": \"shallow\", \"time_\u00b5s\": 836928}]},\n{\"list[list]\": [\n  {\"method\": \"unpack\", \"copy_type\": \"deep\", \"time_\u00b5s\": 52313},\n  {\"method\": \"extend\", \"copy_type\": \"deep\", \"time_\u00b5s\": 52550},\n  {\"method\": \"combine\", \"copy_type\": \"deep\", \"time_\u00b5s\": 53203},\n  {\"method\": \"deepcopy\", \"copy_type\": \"deep\", \"time_\u00b5s\": 2608560},\n  {\"method\": \"copy\", \"copy_type\": \"deep\", \"time_\u00b5s\": 53210},\n  {\"method\": \"slice\", \"copy_type\": \"deep\", \"time_\u00b5s\": 52937},\n  {\"method\": \"loop\", \"copy_type\": \"deep\", \"time_\u00b5s\": 834774}\n]}]\n",
                "Frame challenge: do you actually need to copy, for your application?I often see code that tries to modify a copy of the list in some iterative fashion. To construct a trivial example, suppose we had non-working (because x should not be modified) code like:xx = [8, 6, 7, 5, 3, 0, 9]\ny = x\nfor index, element in enumerate(y):\n    y[index] = element * 2\n# Expected result:\n# x = [8, 6, 7, 5, 3, 0, 9] <-- this is where the code is wrong.\n# y = [16, 12, 14, 10, 6, 0, 18]\nx = [8, 6, 7, 5, 3, 0, 9]\ny = x\nfor index, element in enumerate(y):\n    y[index] = element * 2\n# Expected result:\n# x = [8, 6, 7, 5, 3, 0, 9] <-- this is where the code is wrong.\n# y = [16, 12, 14, 10, 6, 0, 18]\nNaturally people will ask how to make y be a copy of x, rather than a name for the same list, so that the for loop will do the right thing.yxforBut this is the wrong approach. Functionally, what we really want to do is make a new list that is based on the original.reallynew listbased onWe don't need to make a copy first to do that, and we typically shouldn't.We don't need to make a copy first to do that, and we typically shouldn't.When we need to apply logic to each elementThe natural tool for this is a list comprehension. This way, we write the logic that tells us how the elements in the desired result, relate to the original elements. It's simple, elegant and expressive; and we avoid the need for workarounds to modify the y copy in a for loop (since assigning to the iteration variable doesn't affect the list - for the same reason that we wanted the copy in the first place!).yforassigning to the iteration variable doesn't affect the listfor the same reason that we wanted the copy in the first place!For the above example, it looks like:x = [8, 6, 7, 5, 3, 0, 9]\ny = [element * 2 for element in x]\nx = [8, 6, 7, 5, 3, 0, 9]\ny = [element * 2 for element in x]\nList comprehensions are quite powerful; we can also use them to filter out elements by a rule with an if clause, and we can chain for and if clauses (it works like the corresponding imperative code, with the same clauses in the same order; only the value that will ultimately end up in the result list, is moved to the front instead of being in the \"innermost\" part). If the plan was to iterate over the original while modifying the copy to avoid problems, there is generally a much more pleasant way to do that with a filtering list comprehension.ifforifin the same orderto avoid problemsWhen we need to reject or insert specific elements by positionSuppose instead that we had something likex = [8, 6, 7, 5, 3, 0, 9]\ny = x\ndel y[2:-2] # oops, x was changed inappropriately\nx = [8, 6, 7, 5, 3, 0, 9]\ny = x\ndel y[2:-2] # oops, x was changed inappropriately\nRather than making y a separate copy first in order to delete the part we don't want, we can build a list by putting together the parts that we do want. Thus:yputting togetherdox = [8, 6, 7, 5, 3, 0, 9]\ny = x[:2] + x[-2:]\nx = [8, 6, 7, 5, 3, 0, 9]\ny = x[:2] + x[-2:]\nHandling insertion, replacement etc. by slicing is left as an exercise. Just reason out which subsequences you want the result to contain. A special case of this is making a reversed copy - assuming we need a new list at all (rather than just to iterate in reverse), we can directly create it by slicing, rather than cloning and then using .reverse.making a reversed copyiterate in reverse.reverse\nThese approaches - like the list comprehension - also have the advantage that they create the desired result as an expression, rather than by procedurally modifying an existing object in-place (and returning None). This is more convenient for writing code in a \"fluent\" style.\nThese approaches - like the list comprehension - also have the advantage that they create the desired result as an expression, rather than by procedurally modifying an existing object in-place (and returning None). This is more convenient for writing code in a \"fluent\" style.as an expressionreturning NoneNone",
                "Short and simple explanations of each copy mode:A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the objects found in the original - creating a shallow copy:shallow copynew_list = my_list\nnew_list = my_list\nA deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the original - creating a deep copy:deep copynew_list = list(my_list)\nnew_list = list(my_list)\nlist() works fine for deep copy of simple lists, like:list()my_list = [\"A\",\"B\",\"C\"]\nmy_list = [\"A\",\"B\",\"C\"]\nBut, for complex lists like...my_complex_list = [{'A' : 500, 'B' : 501},{'C' : 502}]\nmy_complex_list = [{'A' : 500, 'B' : 501},{'C' : 502}]\n...use deepcopy():deepcopy()import copy\nnew_complex_list = copy.deepcopy(my_complex_list)\nimport copy\nnew_complex_list = copy.deepcopy(my_complex_list)\n"
            ]
        },
        {
            "tag": "delay",
            "patterns": [
                "How do I make a time delay? [duplicate]",
                "how can you make a delay?",
                "what can i do to make time delays?",
                "how do i get a time delay?",
                "how do i make a time delay?"
            ],
            "responses": [
                "This delays for 2.5 seconds:import time\ntime.sleep(2.5)\nimport time\ntime.sleep(2.5)\nHere is another example where something is run approximately once a minute:import time\nwhile True:\n    print(\"This prints once a minute.\")\n    time.sleep(60) # Delay for 1 minute (60 seconds).\nimport time\nwhile True:\n    print(\"This prints once a minute.\")\n    time.sleep(60) # Delay for 1 minute (60 seconds).\n",
                "Use sleep() from the time module. It can take a float argument for sub-second resolution.sleep()sleep()timefrom time import sleep\nsleep(0.1)  # Time in seconds\nfrom time import sleep\nsleep(0.1)  # Time in seconds\n",
                "\nHow can I make a time delay in Python?\nHow can I make a time delay in Python?In a single thread I suggest the sleep function:sleep function>>> from time import sleep\n\n>>> sleep(4)\n>>> from time import sleep\n\n>>> sleep(4)\nThis function actually suspends the processing of the thread in which it is called by the operating system, allowing other threads and processes to execute while it sleeps.Use it for that purpose, or simply to delay a function from executing. For example:>>> def party_time():\n...     print('hooray!')\n...\n>>> sleep(3); party_time()\nhooray!\n>>> def party_time():\n...     print('hooray!')\n...\n>>> sleep(3); party_time()\nhooray!\n\"hooray!\" is printed 3 seconds after I hit Enter.EnterExample using sleep with multiple threads and processessleepAgain, sleep suspends your thread - it uses next to zero processing power.sleepTo demonstrate, create a script like this (I first attempted this in an interactive Python 3.5 shell, but sub-processes can't find the party_later function for some reason):party_laterfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\nfrom time import sleep, time\n\ndef party_later(kind='', n=''):\n    sleep(3)\n    return kind + n + ' party time!: ' + __name__\n\ndef main():\n    with ProcessPoolExecutor() as proc_executor:\n        with ThreadPoolExecutor() as thread_executor:\n            start_time = time()\n            proc_future1 = proc_executor.submit(party_later, kind='proc', n='1')\n            proc_future2 = proc_executor.submit(party_later, kind='proc', n='2')\n            thread_future1 = thread_executor.submit(party_later, kind='thread', n='1')\n            thread_future2 = thread_executor.submit(party_later, kind='thread', n='2')\n            for f in as_completed([\n              proc_future1, proc_future2, thread_future1, thread_future2,]):\n                print(f.result())\n            end_time = time()\n    print('total time to execute four 3-sec functions:', end_time - start_time)\n\nif __name__ == '__main__':\n    main()\nfrom concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\nfrom time import sleep, time\n\ndef party_later(kind='', n=''):\n    sleep(3)\n    return kind + n + ' party time!: ' + __name__\n\ndef main():\n    with ProcessPoolExecutor() as proc_executor:\n        with ThreadPoolExecutor() as thread_executor:\n            start_time = time()\n            proc_future1 = proc_executor.submit(party_later, kind='proc', n='1')\n            proc_future2 = proc_executor.submit(party_later, kind='proc', n='2')\n            thread_future1 = thread_executor.submit(party_later, kind='thread', n='1')\n            thread_future2 = thread_executor.submit(party_later, kind='thread', n='2')\n            for f in as_completed([\n              proc_future1, proc_future2, thread_future1, thread_future2,]):\n                print(f.result())\n            end_time = time()\n    print('total time to execute four 3-sec functions:', end_time - start_time)\n\nif __name__ == '__main__':\n    main()\nExample output from this script:thread1 party time!: __main__\nthread2 party time!: __main__\nproc1 party time!: __mp_main__\nproc2 party time!: __mp_main__\ntotal time to execute four 3-sec functions: 3.4519670009613037\nthread1 party time!: __main__\nthread2 party time!: __main__\nproc1 party time!: __mp_main__\nproc2 party time!: __mp_main__\ntotal time to execute four 3-sec functions: 3.4519670009613037\nMultithreadingYou can trigger a function to be called at a later time in a separate thread with the Timer threading object:Timerthreading>>> from threading import Timer\n>>> t = Timer(3, party_time, args=None, kwargs=None)\n>>> t.start()\n>>>\n>>> hooray!\n\n>>>\n>>> from threading import Timer\n>>> t = Timer(3, party_time, args=None, kwargs=None)\n>>> t.start()\n>>>\n>>> hooray!\n\n>>>\nThe blank line illustrates that the function printed to my standard output, and I had to hit Enter to ensure I was on a prompt.EnterThe upside of this method is that while the Timer thread was waiting, I was able to do other things, in this case, hitting Enter one time - before the function executed (see the first empty prompt).TimerEnterThere isn't a respective object in the multiprocessing library. You can create one, but it probably doesn't exist for a reason. A sub-thread makes a lot more sense for a simple timer than a whole new subprocess.multiprocessing library",
                "Delays can be also implemented by using the following methods.The first method:import time\ntime.sleep(5) # Delay for 5 seconds.\nimport time\ntime.sleep(5) # Delay for 5 seconds.\nThe second method to delay would be using the implicit wait method: driver.implicitly_wait(5)\n driver.implicitly_wait(5)\nThe third method is more useful when you have to wait until a particular action is completed or until an element is found:self.wait.until(EC.presence_of_element_located((By.ID, 'UserName'))\nself.wait.until(EC.presence_of_element_located((By.ID, 'UserName'))\n",
                "There are five methods which I know: time.sleep(), pygame.time.wait(), matplotlib's pyplot.pause(), .after(), and asyncio.sleep().time.sleep()pygame.time.wait()pyplot.pause().after()asyncio.sleep()time.sleep() example (do not use if using tkinter):time.sleep()import time\nprint('Hello')\ntime.sleep(5) # Number of seconds\nprint('Bye')\nimport time\nprint('Hello')\ntime.sleep(5) # Number of seconds\nprint('Bye')\npygame.time.wait() example (not recommended if you are not using the pygame window, but you could exit the window instantly):pygame.time.wait()import pygame\n# If you are going to use the time module\n# don't do \"from pygame import *\"\npygame.init()\nprint('Hello')\npygame.time.wait(5000) # Milliseconds\nprint('Bye')\nimport pygame\n# If you are going to use the time module\n# don't do \"from pygame import *\"\npygame.init()\nprint('Hello')\npygame.time.wait(5000) # Milliseconds\nprint('Bye')\nmatplotlib's function pyplot.pause() example (not recommended if you are not using the graph, but you could exit the graph instantly):pyplot.pause()import matplotlib\nprint('Hello')\nmatplotlib.pyplot.pause(5) # Seconds\nprint('Bye')\nimport matplotlib\nprint('Hello')\nmatplotlib.pyplot.pause(5) # Seconds\nprint('Bye')\nThe .after() method (best with Tkinter):.after()import tkinter as tk # Tkinter for Python&nbsp;2\nroot = tk.Tk()\nprint('Hello')\ndef ohhi():\n    print('Oh, hi!')\nroot.after(5000, ohhi) # Milliseconds and then a function\nprint('Bye')\nimport tkinter as tk # Tkinter for Python&nbsp;2\nroot = tk.Tk()\nprint('Hello')\ndef ohhi():\n    print('Oh, hi!')\nroot.after(5000, ohhi) # Milliseconds and then a function\nprint('Bye')\nFinally, the asyncio.sleep() method (has to be in an async loop):asyncio.sleep()await asyncio.sleep(5)\nawait asyncio.sleep(5)\n",
                "A bit of fun with a sleepy generator.generatorThe question is about time delay. It can be fixed time, but in some cases we might need a delay measured since last time. Here is one possible solution:Delay measured since last time (waking up regularly)The situation can be, we want to do something as regularly as possible and we do not want to bother with all the last_time, next_time stuff all around our code.last_timenext_timeBuzzer generatorThe following code (sleepy.py) defines a buzzergen generator:sleepy.pybuzzergenimport time\nfrom itertools import count\n\ndef buzzergen(period):\n    nexttime = time.time() + period\n    for i in count():\n        now = time.time()\n        tosleep = nexttime - now\n        if tosleep > 0:\n            time.sleep(tosleep)\n            nexttime += period\n        else:\n            nexttime = now + period\n        yield i, nexttime\nimport time\nfrom itertools import count\n\ndef buzzergen(period):\n    nexttime = time.time() + period\n    for i in count():\n        now = time.time()\n        tosleep = nexttime - now\n        if tosleep > 0:\n            time.sleep(tosleep)\n            nexttime += period\n        else:\n            nexttime = now + period\n        yield i, nexttime\nInvoking regular buzzergenfrom sleepy import buzzergen\nimport time\nbuzzer = buzzergen(3) # Planning to wake up each 3 seconds\nprint time.time()\nbuzzer.next()\nprint time.time()\ntime.sleep(2)\nbuzzer.next()\nprint time.time()\ntime.sleep(5) # Sleeping a bit longer than usually\nbuzzer.next()\nprint time.time()\nbuzzer.next()\nprint time.time()\nfrom sleepy import buzzergen\nimport time\nbuzzer = buzzergen(3) # Planning to wake up each 3 seconds\nprint time.time()\nbuzzer.next()\nprint time.time()\ntime.sleep(2)\nbuzzer.next()\nprint time.time()\ntime.sleep(5) # Sleeping a bit longer than usually\nbuzzer.next()\nprint time.time()\nbuzzer.next()\nprint time.time()\nAnd running it we see:1400102636.46\n1400102639.46\n1400102642.46\n1400102647.47\n1400102650.47\n1400102636.46\n1400102639.46\n1400102642.46\n1400102647.47\n1400102650.47\nWe can also use it directly in a loop:import random\nfor ring in buzzergen(3):\n    print \"now\", time.time()\n    print \"ring\", ring\n    time.sleep(random.choice([0, 2, 4, 6]))\nimport random\nfor ring in buzzergen(3):\n    print \"now\", time.time()\n    print \"ring\", ring\n    time.sleep(random.choice([0, 2, 4, 6]))\nAnd running it we might see:now 1400102751.46\nring (0, 1400102754.461676)\nnow 1400102754.46\nring (1, 1400102757.461676)\nnow 1400102757.46\nring (2, 1400102760.461676)\nnow 1400102760.46\nring (3, 1400102763.461676)\nnow 1400102766.47\nring (4, 1400102769.47115)\nnow 1400102769.47\nring (5, 1400102772.47115)\nnow 1400102772.47\nring (6, 1400102775.47115)\nnow 1400102775.47\nring (7, 1400102778.47115)\nnow 1400102751.46\nring (0, 1400102754.461676)\nnow 1400102754.46\nring (1, 1400102757.461676)\nnow 1400102757.46\nring (2, 1400102760.461676)\nnow 1400102760.46\nring (3, 1400102763.461676)\nnow 1400102766.47\nring (4, 1400102769.47115)\nnow 1400102769.47\nring (5, 1400102772.47115)\nnow 1400102772.47\nring (6, 1400102775.47115)\nnow 1400102775.47\nring (7, 1400102778.47115)\nAs we see, this buzzer is not too rigid and allow us to catch up with regular sleepy intervals even if we oversleep and get out of regular schedule.",
                "The Tkinter library in the Python standard library is an interactive tool which you can import. Basically, you can create buttons and boxes and popups and stuff that appear as windows which you manipulate with code.TkinterIf you use Tkinter, do not use time.sleep(), because it will muck up your program. This happened to me. Instead, use root.after() and replace the values for however many seconds, with a milliseconds. For example, time.sleep(1) is equivalent to root.after(1000) in Tkinter.do not use time.sleep()do not use time.sleep()time.sleep()root.after()time.sleep(1)root.after(1000)Otherwise, time.sleep(), which many answers have pointed out, which is the way to go.time.sleep()",
                "Delays are done with the time library, specifically the time.sleep() function.time librarytime.sleep()time.sleep()To just make it wait for a second:from time import sleep\nsleep(1)\nfrom time import sleep\nsleep(1)\nThis works because by doing:from time import sleep\nfrom time import sleep\nYou extract the sleep function only from the time library, which means you can just call it with:sleep functiononlytime librarysleep(seconds)\nsleep(seconds)\nRather than having to type outtime.sleep()\ntime.sleep()\nWhich is awkwardly long to type.With this method, you wouldn't get access to the other features of the time library and you can't have a variable called sleep. But you could create a variable called time.time librarysleeptimeDoing from [library] import [function] (, [function2]) is great if you just want certain parts of a module.from [library] import [function] (, [function2])from [library] import [function] (, [function2])You could equally do it as:import time\ntime.sleep(1)\nimport time\ntime.sleep(1)\nand you would have access to the other features of the time library like time.clock() as long as you type time.[function](), but you couldn't create the variable time because it would overwrite the import. A solution to this to dotime librarytime.clock()time.clock()time.[function]()import time as t\nimport time as t\nwhich would allow you to reference the time library as t, allowing you to do:time librarytt.sleep()\nt.sleep()\nThis works on any library.",
                "If you would like to put a time delay in a Python script:Use time.sleep or Event().wait like this:time.sleeptime.sleepEvent().waitEvent().waitfrom threading import Event\nfrom time import sleep\n\ndelay_in_sec = 2\n\n# Use time.sleep like this\nsleep(delay_in_sec)         # Returns None\nprint(f'slept for {delay_in_sec} seconds')\n\n# Or use Event().wait like this\nEvent().wait(delay_in_sec)  # Returns False\nprint(f'waited for {delay_in_sec} seconds')\nfrom threading import Event\nfrom time import sleep\n\ndelay_in_sec = 2\n\n# Use time.sleep like this\nsleep(delay_in_sec)         # Returns None\nprint(f'slept for {delay_in_sec} seconds')\n\n# Or use Event().wait like this\nEvent().wait(delay_in_sec)  # Returns False\nprint(f'waited for {delay_in_sec} seconds')\nHowever, if you want to delay the execution of a function do this:Use threading.Timer like this:threading.Timerthreading.Timerthreading.Timerfrom threading import Timer\n\ndelay_in_sec = 2\n\ndef hello(delay_in_sec):\n    print(f'function called after {delay_in_sec} seconds')\n\nt = Timer(delay_in_sec, hello, [delay_in_sec])  # Hello function will be called 2 seconds later with [delay_in_sec] as the *args parameter\nt.start()  # Returns None\nprint(\"Started\")\nfrom threading import Timer\n\ndelay_in_sec = 2\n\ndef hello(delay_in_sec):\n    print(f'function called after {delay_in_sec} seconds')\n\nt = Timer(delay_in_sec, hello, [delay_in_sec])  # Hello function will be called 2 seconds later with [delay_in_sec] as the *args parameter\nt.start()  # Returns None\nprint(\"Started\")\nOutputs:Started\nfunction called after 2 seconds\nStarted\nfunction called after 2 seconds\nWhy use the later approach?\nIt does not stop execution of the whole script (except for the function you pass it).\nAfter starting the timer you can also stop it by doing timer_obj.cancel().\nIt does not stop execution of the whole script (except for the function you pass it).notAfter starting the timer you can also stop it by doing timer_obj.cancel().timer_obj.cancel()",
                "asyncio.sleepNotice in recent Python versions (Python\u00a03.4 or higher) you can use asyncio.sleep. It's related to asynchronous programming and asyncio. Check out next example:asyncio.sleepimport asyncio\nfrom datetime import datetime\n\n@asyncio.coroutine\ndef countdown(iteration_name, countdown_sec):\n    \"\"\"\n    Just count for some countdown_sec seconds and do nothing else\n    \"\"\"\n    while countdown_sec > 0:\n       print(f'{iteration_name} iterates: {countdown_sec} seconds')\n       yield from asyncio.sleep(1)\n       countdown_sec -= 1\n\nloop = asyncio.get_event_loop()\ntasks = [asyncio.ensure_future(countdown('First Count', 2)),\n         asyncio.ensure_future(countdown('Second Count', 3))]\n\nstart_time = datetime.utcnow()\n\n# Run both methods. How much time will both run...?\nloop.run_until_complete(asyncio.wait(tasks))\n\nloop.close()\n\nprint(f'total running time: {datetime.utcnow() - start_time}')\nimport asyncio\nfrom datetime import datetime\n\n@asyncio.coroutine\ndef countdown(iteration_name, countdown_sec):\n    \"\"\"\n    Just count for some countdown_sec seconds and do nothing else\n    \"\"\"\n    while countdown_sec > 0:\n       print(f'{iteration_name} iterates: {countdown_sec} seconds')\n       yield from asyncio.sleep(1)\n       countdown_sec -= 1\n\nloop = asyncio.get_event_loop()\ntasks = [asyncio.ensure_future(countdown('First Count', 2)),\n         asyncio.ensure_future(countdown('Second Count', 3))]\n\nstart_time = datetime.utcnow()\n\n# Run both methods. How much time will both run...?\nloop.run_until_complete(asyncio.wait(tasks))\n\nloop.close()\n\nprint(f'total running time: {datetime.utcnow() - start_time}')\nWe may think it will \"sleep\" for 2 seconds for first method and then 3 seconds in the second method, a total of 5 seconds running time of this code. But it will print:total_running_time: 0:00:03.01286\ntotal_running_time: 0:00:03.01286\nIt is recommended to read asyncio official documentation for more details.asyncio official documentation",
                "While everyone else has suggested the de facto time module, I thought I'd share a different method using matplotlib's pyplot function, pause.timematplotlibpyplotpausepauseAn examplefrom matplotlib import pyplot as plt\nplt.pause(5)    # Pauses the program for 5 seconds\nfrom matplotlib import pyplot as plt\nplt.pause(5)    # Pauses the program for 5 seconds\nTypically this is used to prevent the plot from disappearing as soon as it is plotted or to make crude animations.This would save you an import if you already have matplotlib imported.importmatplotlib",
                "This is an easy example of a time delay:import time\n\ndef delay(period='5'):\n    # If the user enters nothing, it'll wait 5 seconds\n    try:\n        # If the user not enters a int, I'll just return ''\n        time.sleep(period)\n    except:\n        return ''\nimport time\n\ndef delay(period='5'):\n    # If the user enters nothing, it'll wait 5 seconds\n    try:\n        # If the user not enters a int, I'll just return ''\n        time.sleep(period)\n    except:\n        return ''\nAnother, in Tkinter:Tkinterimport tkinter\n\ndef tick():\n    pass\n\nroot = Tk()\ndelay = 100 # Time in milliseconds\nroot.after(delay, tick)\nroot.mainloop()\nimport tkinter\n\ndef tick():\n    pass\n\nroot = Tk()\ndelay = 100 # Time in milliseconds\nroot.after(delay, tick)\nroot.mainloop()\n",
                "You also can try this:import time\n# The time now\nstart = time.time() \nwhile time.time() - start < 10: # Run 1- seconds\n    pass\n# Do the job\nimport time\n# The time now\nstart = time.time() \nwhile time.time() - start < 10: # Run 1- seconds\n    pass\n# Do the job\nNow the shell will not crash or not react."
            ]
        },
        {
            "tag": "delete_file",
            "patterns": [
                "How can I delete a file or folder in Python?",
                "how do i delete my python file?",
                "how do you delete a document in python?",
                "how do you delete a directory in python?",
                "how do i delete files in python?",
                "how can you delete folders in python?",
                "how do i delete a file in python?",
                "how can i delete a folder from python?",
                "how do i delete a file or folder in python?"
            ],
            "responses": [
                "\nos.remove() removes a file.\nos.rmdir() removes an empty directory.\nshutil.rmtree() deletes a directory and all its contents.\nos.remove() removes a file.os.remove() removes a file.os.remove()os.remove()os.rmdir() removes an empty directory.os.rmdir() removes an empty directory.os.rmdir()os.rmdir()shutil.rmtree() deletes a directory and all its contents.shutil.rmtree() deletes a directory and all its contents.shutil.rmtree()shutil.rmtree()Path objects from the Python 3.4+ pathlib module also expose these instance methods:PathPathpathlibpathlib\npathlib.Path.unlink() removes a file or symbolic link.\npathlib.Path.rmdir() removes an empty directory.\npathlib.Path.unlink() removes a file or symbolic link.pathlib.Path.unlink() removes a file or symbolic link.pathlib.Path.unlink()pathlib.Path.unlink()pathlib.Path.rmdir() removes an empty directory.pathlib.Path.rmdir() removes an empty directory.pathlib.Path.rmdir()pathlib.Path.rmdir()",
                "Python syntax to delete a fileimport os\nos.remove(\"/tmp/<file_name>.txt\")\nimport os\nos.remove(\"/tmp/<file_name>.txt\")\norimport os\nos.unlink(\"/tmp/<file_name>.txt\")\nimport os\nos.unlink(\"/tmp/<file_name>.txt\")\norpathlib Library for Python version >= 3.4pathlib Library for Python version >= 3.4pathlibfile_to_rem = pathlib.Path(\"/tmp/<file_name>.txt\")\nfile_to_rem.unlink()\nfile_to_rem = pathlib.Path(\"/tmp/<file_name>.txt\")\nfile_to_rem.unlink()\nPath.unlink(missing_ok=False)Unlink method used to remove the file or the symbolik link.\n\nIf missing_ok is false (the default), FileNotFoundError is raised if    the path does not exist.\nIf missing_ok is true, FileNotFoundError exceptions will be ignored    (same behavior as the POSIX rm -f command).\nChanged in version 3.8: The missing_ok parameter was added.\n\n\nIf missing_ok is false (the default), FileNotFoundError is raised if    the path does not exist.\nIf missing_ok is true, FileNotFoundError exceptions will be ignored    (same behavior as the POSIX rm -f command).\nChanged in version 3.8: The missing_ok parameter was added.\nIf missing_ok is false (the default), FileNotFoundError is raised if    the path does not exist.If missing_ok is true, FileNotFoundError exceptions will be ignored    (same behavior as the POSIX rm -f command).Changed in version 3.8: The missing_ok parameter was added.Best practiceFirst, check if the file or folder exists and then delete it. You can achieve this in two ways:\nos.path.isfile(\"/path/to/file\")\nUse exception handling.\nos.path.isfile(\"/path/to/file\")os.path.isfile(\"/path/to/file\")Use exception handling.exception handling.EXAMPLE for os.path.isfileEXAMPLEos.path.isfile#!/usr/bin/python\nimport os\n\nmyfile = \"/tmp/foo.txt\"\n# If file exists, delete it.\nif os.path.isfile(myfile):\n    os.remove(myfile)\nelse:\n    # If it fails, inform the user.\n    print(\"Error: %s file not found\" % myfile)\n#!/usr/bin/python\nimport os\n\nmyfile = \"/tmp/foo.txt\"\n# If file exists, delete it.\nif os.path.isfile(myfile):\n    os.remove(myfile)\nelse:\n    # If it fails, inform the user.\n    print(\"Error: %s file not found\" % myfile)\nException Handling#!/usr/bin/python\nimport os\n\n# Get input.\nmyfile = raw_input(\"Enter file name to delete: \")\n\n# Try to delete the file.\ntry:\n    os.remove(myfile)\nexcept OSError as e:\n    # If it fails, inform the user.\n    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n#!/usr/bin/python\nimport os\n\n# Get input.\nmyfile = raw_input(\"Enter file name to delete: \")\n\n# Try to delete the file.\ntry:\n    os.remove(myfile)\nexcept OSError as e:\n    # If it fails, inform the user.\n    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\nRespective output\nEnter file name to delete : demo.txt\nError: demo.txt - No such file or directory.\n\nEnter file name to delete : rrr.txt\nError: rrr.txt - Operation not permitted.\n\nEnter file name to delete : foo.txt\nPython syntax to delete a foldershutil.rmtree()\nshutil.rmtree()\nExample for shutil.rmtree()shutil.rmtree()#!/usr/bin/python\nimport os\nimport sys\nimport shutil\n\n# Get directory name\nmydir = raw_input(\"Enter directory name: \")\n\n# Try to remove the tree; if it fails, throw an error using try...except.\ntry:\n    shutil.rmtree(mydir)\nexcept OSError as e:\n    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n#!/usr/bin/python\nimport os\nimport sys\nimport shutil\n\n# Get directory name\nmydir = raw_input(\"Enter directory name: \")\n\n# Try to remove the tree; if it fails, throw an error using try...except.\ntry:\n    shutil.rmtree(mydir)\nexcept OSError as e:\n    print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
                "Use shutil.rmtree(path[, ignore_errors[, onerror]])\nshutil.rmtree(path[, ignore_errors[, onerror]])\n(See complete documentation on shutil) and/orshutilos.remove\nos.remove\nandos.rmdir\nos.rmdir\n(Complete documentation on os.)os",
                "Here is a robust function that uses both os.remove and shutil.rmtree:os.removeshutil.rmtreedef remove(path):\n    \"\"\" param <path> could either be relative or absolute. \"\"\"\n    if os.path.isfile(path) or os.path.islink(path):\n        os.remove(path)  # remove the file\n    elif os.path.isdir(path):\n        shutil.rmtree(path)  # remove dir and all contains\n    else:\n        raise ValueError(\"file {} is not a file or dir.\".format(path))\ndef remove(path):\n    \"\"\" param <path> could either be relative or absolute. \"\"\"\n    if os.path.isfile(path) or os.path.islink(path):\n        os.remove(path)  # remove the file\n    elif os.path.isdir(path):\n        shutil.rmtree(path)  # remove dir and all contains\n    else:\n        raise ValueError(\"file {} is not a file or dir.\".format(path))\n",
                "You can use the built-in pathlib module (requires Python 3.4+, but there are backports for older versions on PyPI: pathlib, pathlib2). pathlibpathlibpathlibpathlibpathlib2pathlib2To remove a file there is the unlink method:unlinkunlinkimport pathlib\npath = pathlib.Path(name_of_file)\npath.unlink()\nimport pathlib\npath = pathlib.Path(name_of_file)\npath.unlink()\nOr the rmdir method to remove an empty folder:rmdirrmdiremptyimport pathlib\npath = pathlib.Path(name_of_folder)\npath.rmdir()\nimport pathlib\npath = pathlib.Path(name_of_folder)\npath.rmdir()\n",
                "Deleting a file or folder in PythonThere are multiple ways to Delete a File in Python but the best ways are the following:There are multiple ways to Delete a File in Python but the best ways are the following:\nos.remove() removes a file.\nos.unlink() removes a file. it is a Unix name of remove() method.\nshutil.rmtree() deletes a directory and all its contents.\npathlib.Path.unlink() deletes a single file The pathlib module is available in Python 3.4 and above.\nos.remove() removes a file.os.remove()os.unlink() removes a file. it is a Unix name of remove() method.os.unlink()shutil.rmtree() deletes a directory and all its contents.shutil.rmtree()pathlib.Path.unlink() deletes a single file The pathlib module is available in Python 3.4 and above.pathlib.Path.unlink()os.remove()Example 1: Basic Example to Remove a File Using os.remove() Method.import os\nos.remove(\"test_file.txt\")\nprint(\"File removed successfully\")\nimport os\nos.remove(\"test_file.txt\")\nprint(\"File removed successfully\")\nExample 2: Checking if File Exists using os.path.isfile and Deleting it With os.removeimport os\n#checking if file exist or not\nif(os.path.isfile(\"test.txt\")):\n    #os.remove() function to remove the file\n    os.remove(\"test.txt\")\n    #Printing the confirmation message of deletion\n    print(\"File Deleted successfully\")\nelse:\nprint(\"File does not exist\")\n#Showing the message instead of throwig an error\nimport os\n#checking if file exist or not\nif(os.path.isfile(\"test.txt\")):\n    #os.remove() function to remove the file\n    os.remove(\"test.txt\")\n    #Printing the confirmation message of deletion\n    print(\"File Deleted successfully\")\nelse:\nprint(\"File does not exist\")\n#Showing the message instead of throwig an error\nExample 3: Python Program to Delete all files with a specific extensionimport os \nfrom os import listdir\nmy_path = 'C:\\Python Pool\\Test\\'\nfor file_name in listdir(my_path):\n    if file_name.endswith('.txt'):\n        os.remove(my_path + file_name)\nimport os \nfrom os import listdir\nmy_path = 'C:\\Python Pool\\Test\\'\nfor file_name in listdir(my_path):\n    if file_name.endswith('.txt'):\n        os.remove(my_path + file_name)\nExample 4: Python Program to Delete All Files Inside a FolderTo delete all files inside a particular directory, you simply have to use the * symbol as the pattern string.\n#Importing os and glob modules\nimport os, glob\n#Loop Through the folder projects all files and deleting them one by one\nfor file in glob.glob(\"pythonpool/*\"):\nos.remove(file)\nprint(\"Deleted \" + str(file))os.unlink()os.unlink() is an alias or another name of os.remove() . As in the Unix OS remove is also known as unlink.\nNote: All the functionalities and syntax is the same of os.unlink() and os.remove(). Both of them are used to delete the Python file path.\nBoth are methods in the os module in Python\u2019s standard libraries which performs the deletion function.shutil.rmtree()Example 1: Python Program to Delete a File Using shutil.rmtree()import shutil \nimport os \n# location \nlocation = \"E:/Projects/PythonPool/\"\n# directory \ndir = \"Test\"\n# path \npath = os.path.join(location, dir) \n# removing directory \nshutil.rmtree(path) \nimport shutil \nimport os \n# location \nlocation = \"E:/Projects/PythonPool/\"\n# directory \ndir = \"Test\"\n# path \npath = os.path.join(location, dir) \n# removing directory \nshutil.rmtree(path) \nExample 2: Python Program to Delete a File Using shutil.rmtree()import shutil \nimport os \nlocation = \"E:/Projects/PythonPool/\"\ndir = \"Test\"    \npath = os.path.join(location, dir) \nshutil.rmtree(path) \nimport shutil \nimport os \nlocation = \"E:/Projects/PythonPool/\"\ndir = \"Test\"    \npath = os.path.join(location, dir) \nshutil.rmtree(path) \npathlib.Path.rmdir() to remove Empty DirectoryPathlib module provides different ways to interact with your files. Rmdir is one of the path functions which allows you to delete an empty folder. Firstly, you need to select the Path() for the directory, and then calling rmdir() method will check the folder size. If it\u2019s empty, it\u2019ll delete it.This is a good way to deleting empty folders without any fear of losing actual data.from pathlib import Path\nq = Path('foldername')\nq.rmdir()\nfrom pathlib import Path\nq = Path('foldername')\nq.rmdir()\n",
                "\nHow do I delete a file or folder in Python?\nHow do I delete a file or folder in Python?For Python 3, to remove the file and directory individually, use the unlink and rmdir Path object methods respectively:unlinkunlinkrmdirrmdirPathfrom pathlib import Path\ndir_path = Path.home() / 'directory' \nfile_path = dir_path / 'file'\n\nfile_path.unlink() # remove file\n\ndir_path.rmdir()   # remove directory\nfrom pathlib import Path\ndir_path = Path.home() / 'directory' \nfile_path = dir_path / 'file'\n\nfile_path.unlink() # remove file\n\ndir_path.rmdir()   # remove directory\nNote that you can also use relative paths with Path objects, and you can check your current working directory with Path.cwd.PathPath.cwdFor removing individual files and directories in Python 2, see the section so labeled below.To remove a directory with contents, use shutil.rmtree, and note that this is available in Python 2 and 3:shutil.rmtreeshutil.rmtreefrom shutil import rmtree\n\nrmtree(dir_path)\nfrom shutil import rmtree\n\nrmtree(dir_path)\nDemonstrationNew in Python 3.4 is the Path object. PathLet's use one to create a directory and file to demonstrate usage. Note that we use the / to join the parts of the path, this works around issues between operating systems and issues from using backslashes on Windows (where you'd need to either double up your backslashes like \\\\ or use raw strings, like r\"foo\\bar\"):/\\\\r\"foo\\bar\"from pathlib import Path\n\n# .home() is new in 3.5, otherwise use os.path.expanduser('~')\ndirectory_path = Path.home() / 'directory'\ndirectory_path.mkdir()\n\nfile_path = directory_path / 'file'\nfile_path.touch()\nfrom pathlib import Path\n\n# .home() is new in 3.5, otherwise use os.path.expanduser('~')\ndirectory_path = Path.home() / 'directory'\ndirectory_path.mkdir()\n\nfile_path = directory_path / 'file'\nfile_path.touch()\nand now:>>> file_path.is_file()\nTrue\n>>> file_path.is_file()\nTrue\nNow let's delete them. First the file:>>> file_path.unlink()     # remove file\n>>> file_path.is_file()\nFalse\n>>> file_path.exists()\nFalse\n>>> file_path.unlink()     # remove file\n>>> file_path.is_file()\nFalse\n>>> file_path.exists()\nFalse\nWe can use globbing to remove multiple files - first let's create a few files for this:>>> (directory_path / 'foo.my').touch()\n>>> (directory_path / 'bar.my').touch()\n>>> (directory_path / 'foo.my').touch()\n>>> (directory_path / 'bar.my').touch()\nThen just iterate over the glob pattern:>>> for each_file_path in directory_path.glob('*.my'):\n...     print(f'removing {each_file_path}')\n...     each_file_path.unlink()\n... \nremoving ~/directory/foo.my\nremoving ~/directory/bar.my\n>>> for each_file_path in directory_path.glob('*.my'):\n...     print(f'removing {each_file_path}')\n...     each_file_path.unlink()\n... \nremoving ~/directory/foo.my\nremoving ~/directory/bar.my\nNow, demonstrating removing the directory:>>> directory_path.rmdir() # remove directory\n>>> directory_path.is_dir()\nFalse\n>>> directory_path.exists()\nFalse\n>>> directory_path.rmdir() # remove directory\n>>> directory_path.is_dir()\nFalse\n>>> directory_path.exists()\nFalse\nWhat if we want to remove a directory  and everything in it? \nFor this use-case, use shutil.rmtreeshutil.rmtreeLet's recreate our directory and file:file_path.parent.mkdir()\nfile_path.touch()\nfile_path.parent.mkdir()\nfile_path.touch()\nand note that rmdir fails unless it's empty, which is why rmtree is so convenient:rmdir>>> directory_path.rmdir()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"~/anaconda3/lib/python3.6/pathlib.py\", line 1270, in rmdir\n    self._accessor.rmdir(self)\n  File \"~/anaconda3/lib/python3.6/pathlib.py\", line 387, in wrapped\n    return strfunc(str(pathobj), *args)\nOSError: [Errno 39] Directory not empty: '/home/username/directory'\n>>> directory_path.rmdir()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"~/anaconda3/lib/python3.6/pathlib.py\", line 1270, in rmdir\n    self._accessor.rmdir(self)\n  File \"~/anaconda3/lib/python3.6/pathlib.py\", line 387, in wrapped\n    return strfunc(str(pathobj), *args)\nOSError: [Errno 39] Directory not empty: '/home/username/directory'\nNow, import rmtree and pass the directory to the funtion:from shutil import rmtree\nrmtree(directory_path)      # remove everything \nfrom shutil import rmtree\nrmtree(directory_path)      # remove everything \nand we can see the whole thing has been removed:>>> directory_path.exists()\nFalse\n>>> directory_path.exists()\nFalse\nPython 2If you're on Python 2, there's a backport of the pathlib module called pathlib2, which can be installed with pip:backport of the pathlib module called pathlib2$ pip install pathlib2\n$ pip install pathlib2\nAnd then you can alias the library to pathlibpathlibimport pathlib2 as pathlib\nimport pathlib2 as pathlib\nOr just directly import the Path object (as demonstrated here):Pathfrom pathlib2 import Path\nfrom pathlib2 import Path\nIf that's too much, you can remove files with os.remove or os.unlinkos.remove or os.unlinkos.removeos.unlinkfrom os import unlink, remove\nfrom os.path import join, expanduser\n\nremove(join(expanduser('~'), 'directory/file'))\nfrom os import unlink, remove\nfrom os.path import join, expanduser\n\nremove(join(expanduser('~'), 'directory/file'))\norunlink(join(expanduser('~'), 'directory/file'))\nunlink(join(expanduser('~'), 'directory/file'))\nand you can remove directories with os.rmdir:os.rmdiros.rmdirfrom os import rmdir\n\nrmdir(join(expanduser('~'), 'directory'))\nfrom os import rmdir\n\nrmdir(join(expanduser('~'), 'directory'))\nNote that there is also a os.removedirs - it only removes empty directories recursively, but it may suit your use-case.os.removedirsos.removedirs",
                "This is my function for deleting dirs. The \"path\" requires the full pathname.import os\n\ndef rm_dir(path):\n    cwd = os.getcwd()\n    if not os.path.exists(os.path.join(cwd, path)):\n        return False\n    os.chdir(os.path.join(cwd, path))\n\n    for file in os.listdir():\n        print(\"file = \" + file)\n        os.remove(file)\n    print(cwd)\n    os.chdir(cwd)\n    os.rmdir(os.path.join(cwd, path))\nimport os\n\ndef rm_dir(path):\n    cwd = os.getcwd()\n    if not os.path.exists(os.path.join(cwd, path)):\n        return False\n    os.chdir(os.path.join(cwd, path))\n\n    for file in os.listdir():\n        print(\"file = \" + file)\n        os.remove(file)\n    print(cwd)\n    os.chdir(cwd)\n    os.rmdir(os.path.join(cwd, path))\n",
                "shutil.rmtree is the asynchronous function, \nso if you want to check when it complete, you can use while...loopimport os\nimport shutil\n\nshutil.rmtree(path)\n\nwhile os.path.exists(path):\n  pass\n\nprint('done')\nimport os\nimport shutil\n\nshutil.rmtree(path)\n\nwhile os.path.exists(path):\n  pass\n\nprint('done')\n",
                "import os\n\nfolder = '/Path/to/yourDir/'\nfileList = os.listdir(folder)\n\nfor f in fileList:\n    filePath = folder + '/'+f\n\n    if os.path.isfile(filePath):\n        os.remove(filePath)\n\n    elif os.path.isdir(filePath):\n        newFileList = os.listdir(filePath)\n        for f1 in newFileList:\n            insideFilePath = filePath + '/' + f1\n\n            if os.path.isfile(insideFilePath):\n                os.remove(insideFilePath)\nimport os\n\nfolder = '/Path/to/yourDir/'\nfileList = os.listdir(folder)\n\nfor f in fileList:\n    filePath = folder + '/'+f\n\n    if os.path.isfile(filePath):\n        os.remove(filePath)\n\n    elif os.path.isdir(filePath):\n        newFileList = os.listdir(filePath)\n        for f1 in newFileList:\n            insideFilePath = filePath + '/' + f1\n\n            if os.path.isfile(insideFilePath):\n                os.remove(insideFilePath)\n",
                "For deleting files:os.unlink(path, *, dir_fd=None)\nos.unlink(path, *, dir_fd=None)\nor os.remove(path, *, dir_fd=None)\nos.remove(path, *, dir_fd=None)\nBoth functions are semantically same. This functions removes (deletes) the file path. If path is not a file and it is directory, then exception is raised.For deleting folders:shutil.rmtree(path, ignore_errors=False, onerror=None)\nshutil.rmtree(path, ignore_errors=False, onerror=None)\noros.rmdir(path, *, dir_fd=None)\nos.rmdir(path, *, dir_fd=None)\nIn order to remove whole directory trees, shutil.rmtree() can be used. os.rmdir only works when the directory is empty and exists.shutil.rmtree()os.rmdirFor deleting folders recursively towards parent:os.removedirs(name)\nos.removedirs(name)\nIt remove every empty parent directory with self until parent which has some content\nex. os.removedirs('abc/xyz/pqr') will remove the directories by order 'abc/xyz/pqr', 'abc/xyz' and 'abc' if they are empty. \nex. os.removedirs('abc/xyz/pqr') will remove the directories by order 'abc/xyz/pqr', 'abc/xyz' and 'abc' if they are empty. For more info check official doc: os.unlink , os.remove, os.rmdir , shutil.rmtree, os.removedirsos.unlinkos.unlinkos.removeos.removeos.rmdiros.rmdirshutil.rmtreeshutil.rmtreeos.removedirsos.removedirs",
                "To remove all files in folderimport os\nimport glob\n\nfiles = glob.glob(os.path.join('path/to/folder/*'))\nfiles = glob.glob(os.path.join('path/to/folder/*.csv')) // It will give all csv files in folder\nfor file in files:\n    os.remove(file)\nimport os\nimport glob\n\nfiles = glob.glob(os.path.join('path/to/folder/*'))\nfiles = glob.glob(os.path.join('path/to/folder/*.csv')) // It will give all csv files in folder\nfor file in files:\n    os.remove(file)\nTo remove all folders in a directoryfrom shutil import rmtree\nimport os\n\n// os.path.join()  # current working directory.\n\nfor dirct in os.listdir(os.path.join('path/to/folder')):\n    rmtree(os.path.join('path/to/folder',dirct))\nfrom shutil import rmtree\nimport os\n\n// os.path.join()  # current working directory.\n\nfor dirct in os.listdir(os.path.join('path/to/folder')):\n    rmtree(os.path.join('path/to/folder',dirct))\n",
                "To avoid the TOCTOU issue highlighted by \u00c9ric Araujo's comment, you can catch an exception to call the correct method:TOCTOU\u00c9ric Araujo's commentdef remove_file_or_dir(path: str) -> None:\n    \"\"\" Remove a file or directory \"\"\"\n    try:\n        shutil.rmtree(path)\n    except NotADirectoryError:\n        os.remove(path)\ndef remove_file_or_dir(path: str) -> None:\n    \"\"\" Remove a file or directory \"\"\"\n    try:\n        shutil.rmtree(path)\n    except NotADirectoryError:\n        os.remove(path)\nSince shutil.rmtree() will only remove directories and os.remove() or os.unlink() will only remove files.shutil.rmtree()os.remove()os.unlink()",
                "My personal preference is to work with pathlib objects - it offers a more pythonic and less error-prone way to interact with the filesystem, especially if You develop cross-platform code.In that case, You might use pathlib3x - it offers a backport of the latest (at the date of writing this answer Python 3.10.a0) Python pathlib for Python 3.6 or newer, and a few additional functions like \"copy\", \"copy2\", \"copytree\", \"rmtree\" etc ...It also wraps shutil.rmtree:shutil.rmtree$> python -m pip install pathlib3x\n$> python\n>>> import pathlib3x as pathlib\n\n# delete a directory tree\n>>> my_dir_to_delete=pathlib.Path('c:/temp/some_dir')\n>>> my_dir_to_delete.rmtree(ignore_errors=True)\n\n# delete a file\n>>> my_file_to_delete=pathlib.Path('c:/temp/some_file.txt')\n>>> my_file_to_delete.unlink(missing_ok=True)\n\n$> python -m pip install pathlib3x\n$> python\n>>> import pathlib3x as pathlib\n\n# delete a directory tree\n>>> my_dir_to_delete=pathlib.Path('c:/temp/some_dir')\n>>> my_dir_to_delete.rmtree(ignore_errors=True)\n\n# delete a file\n>>> my_file_to_delete=pathlib.Path('c:/temp/some_file.txt')\n>>> my_file_to_delete.unlink(missing_ok=True)\n\nyou can find it on github or PyPigithubPyPi\nDisclaimer: I'm the author of the pathlib3x library.\nDisclaimer: I'm the author of the pathlib3x library.Disclaimer: I'm the author of the pathlib3x library.",
                "I recommend using subprocess if writing a beautiful and readable code is your cup of tea:subprocessimport subprocess\nsubprocess.Popen(\"rm -r my_dir\", shell=True)\nimport subprocess\nsubprocess.Popen(\"rm -r my_dir\", shell=True)\nAnd if you are not a software engineer, then maybe consider using Jupyter; you can simply type bash commands:!rm -r my_dir\n!rm -r my_dir\nTraditionally, you use shutil:shutilimport shutil\nshutil.rmtree(my_dir) \nimport shutil\nshutil.rmtree(my_dir) \n"
            ]
        },
        {
            "tag": "append_extend",
            "patterns": [
                "What is the difference between Python's list methods append and extend?",
                "tell me the difference between append and extended list methods?",
                "tell me the difference between append and extend methods?",
                "tell me the difference between list and extended in python?",
                "show the differences between list extend and append methods?",
                "tell me the difference between python extend and append?",
                "tell me the difference between list apis append and extend?",
                "tell me the difference between list method append and extend?",
                "tell me the difference between list methods append and extend?"
            ],
            "responses": [
                "append appends a specified object at the end of the list:appendappend>>> x = [1, 2, 3]\n>>> x.append([4, 5])\n>>> print(x)\n[1, 2, 3, [4, 5]]\n>>> x = [1, 2, 3]\n>>> x.append([4, 5])\n>>> print(x)\n[1, 2, 3, [4, 5]]\nextend extends the list by appending elements from the specified iterable:extendextend>>> x = [1, 2, 3]\n>>> x.extend([4, 5])\n>>> print(x)\n[1, 2, 3, 4, 5]\n>>> x = [1, 2, 3]\n>>> x.extend([4, 5])\n>>> print(x)\n[1, 2, 3, 4, 5]\n",
                "append adds an element to a list. extend concatenates the first list with another list/iterable.appendextend>>> xs = ['A', 'B']\n>>> xs\n['A', 'B']\n\n>>> xs.append(\"D\")\n>>> xs\n['A', 'B', 'D']\n\n>>> xs.append([\"E\", \"F\"])\n>>> xs\n['A', 'B', 'D', ['E', 'F']]\n\n>>> xs.insert(2, \"C\")\n>>> xs\n['A', 'B', 'C', 'D', ['E', 'F']]\n\n>>> xs.extend([\"G\", \"H\"])\n>>> xs\n['A', 'B', 'C', 'D', ['E', 'F'], 'G', 'H']\n>>> xs = ['A', 'B']\n>>> xs\n['A', 'B']\n\n>>> xs.append(\"D\")\n>>> xs\n['A', 'B', 'D']\n\n>>> xs.append([\"E\", \"F\"])\n>>> xs\n['A', 'B', 'D', ['E', 'F']]\n\n>>> xs.insert(2, \"C\")\n>>> xs\n['A', 'B', 'C', 'D', ['E', 'F']]\n\n>>> xs.extend([\"G\", \"H\"])\n>>> xs\n['A', 'B', 'C', 'D', ['E', 'F'], 'G', 'H']\n",
                "\nWhat is the difference between the list methods append and extend?\nWhat is the difference between the list methods append and extend?\nappend adds its argument as a single element to the end of a list. The length of the list itself will increase by one.\nextend iterates over its argument adding each element to the list, extending the list. The length of the list will increase by however many elements were in the iterable argument.\nappend adds its argument as a single element to the end of a list. The length of the list itself will increase by one.appendextend iterates over its argument adding each element to the list, extending the list. The length of the list will increase by however many elements were in the iterable argument.extendappendappendThe list.append method appends an object to the end of the list.list.appendmy_list.append(object) \nmy_list.append(object) \nWhatever the object is, whether a number, a string, another list, or something else, it gets added onto the end of my_list as a single entry on the list.my_list>>> my_list\n['foo', 'bar']\n>>> my_list.append('baz')\n>>> my_list\n['foo', 'bar', 'baz']\n>>> my_list\n['foo', 'bar']\n>>> my_list.append('baz')\n>>> my_list\n['foo', 'bar', 'baz']\nSo keep in mind that a list is an object. If you append another list onto a list, the first list will be a single object at the end of the list (which may not be what you want):>>> another_list = [1, 2, 3]\n>>> my_list.append(another_list)\n>>> my_list\n['foo', 'bar', 'baz', [1, 2, 3]]\n                     #^^^^^^^^^--- single item at the end of the list.\n>>> another_list = [1, 2, 3]\n>>> my_list.append(another_list)\n>>> my_list\n['foo', 'bar', 'baz', [1, 2, 3]]\n                     #^^^^^^^^^--- single item at the end of the list.\nextendextendThe list.extend method extends a list by appending elements from an iterable:list.extendmy_list.extend(iterable)\nmy_list.extend(iterable)\nSo with extend, each element of the iterable gets appended onto the list. For example:>>> my_list\n['foo', 'bar']\n>>> another_list = [1, 2, 3]\n>>> my_list.extend(another_list)\n>>> my_list\n['foo', 'bar', 1, 2, 3]\n>>> my_list\n['foo', 'bar']\n>>> another_list = [1, 2, 3]\n>>> my_list.extend(another_list)\n>>> my_list\n['foo', 'bar', 1, 2, 3]\nKeep in mind that a string is an iterable, so if you extend a list with a string, you'll append each character as you iterate over the string (which may not be what you want):>>> my_list.extend('baz')\n>>> my_list\n['foo', 'bar', 1, 2, 3, 'b', 'a', 'z']\n>>> my_list.extend('baz')\n>>> my_list\n['foo', 'bar', 1, 2, 3, 'b', 'a', 'z']\nOperator Overload, __add__ (+) and __iadd__ (+=)__add__+__iadd__+=Both + and += operators are defined for list. They are semantically similar to extend.++=listmy_list + another_list creates a third list in memory, so you can return the result of it, but it requires that the second iterable be a list.my_list + another_listmy_list += another_list modifies the list in-place (it is the in-place operator, and lists are mutable objects, as we've seen) so it does not create a new list. It also works like extend, in that the second iterable can be any kind of iterable.my_list += another_listisDon't get confused - my_list = my_list + another_list is not equivalent to += - it gives you a brand new list assigned to my_list.my_list = my_list + another_list+=Time ComplexityAppend has (amortized) constant time complexity, O(1).amortizedconstant time complexityExtend has time complexity, O(k).Iterating through the multiple calls to append adds to the complexity, making it equivalent to that of extend, and since extend's iteration is implemented in C, it will always be faster if you intend to append successive items from an iterable onto a list.appendRegarding \"amortized\" - from the list object implementation source:list object implementation source    /* This over-allocates proportional to the list size, making room\n     * for additional growth.  The over-allocation is mild, but is\n     * enough to give linear-time amortized behavior over a long\n     * sequence of appends() in the presence of a poorly-performing\n     * system realloc().\n    /* This over-allocates proportional to the list size, making room\n     * for additional growth.  The over-allocation is mild, but is\n     * enough to give linear-time amortized behavior over a long\n     * sequence of appends() in the presence of a poorly-performing\n     * system realloc().\nThis means that we get the benefits of a larger than needed memory reallocation up front, but we may pay for it on the next marginal reallocation with an even larger one. Total time for all appends is linear at O(n), and that time allocated per append, becomes O(1).PerformanceYou may wonder what is more performant, since append can be used to achieve the same outcome as extend. The following functions do the same thing:def append(alist, iterable):\n    for item in iterable:\n        alist.append(item)\n        \ndef extend(alist, iterable):\n    alist.extend(iterable)\ndef append(alist, iterable):\n    for item in iterable:\n        alist.append(item)\n        \ndef extend(alist, iterable):\n    alist.extend(iterable)\nSo let's time them:import timeit\n\n>>> min(timeit.repeat(lambda: append([], \"abcdefghijklmnopqrstuvwxyz\")))\n2.867846965789795\n>>> min(timeit.repeat(lambda: extend([], \"abcdefghijklmnopqrstuvwxyz\")))\n0.8060121536254883\nimport timeit\n\n>>> min(timeit.repeat(lambda: append([], \"abcdefghijklmnopqrstuvwxyz\")))\n2.867846965789795\n>>> min(timeit.repeat(lambda: extend([], \"abcdefghijklmnopqrstuvwxyz\")))\n0.8060121536254883\nAddressing a comment on timingsA commenter said:\nPerfect answer, I just miss the timing of comparing adding only one element\nPerfect answer, I just miss the timing of comparing adding only one elementDo the semantically correct thing. If you want to append all elements in an iterable, use extend. If you're just adding one element, use append.extendappendOk, so let's create an experiment to see how this works out in time:def append_one(a_list, element):\n    a_list.append(element)\n\ndef extend_one(a_list, element):\n    \"\"\"creating a new list is semantically the most direct\n    way to create an iterable to give to extend\"\"\"\n    a_list.extend([element])\n\nimport timeit\ndef append_one(a_list, element):\n    a_list.append(element)\n\ndef extend_one(a_list, element):\n    \"\"\"creating a new list is semantically the most direct\n    way to create an iterable to give to extend\"\"\"\n    a_list.extend([element])\n\nimport timeit\nAnd we see that going out of our way to create an iterable just to use extend is a (minor) waste of time:>>> min(timeit.repeat(lambda: append_one([], 0)))\n0.2082819009956438\n>>> min(timeit.repeat(lambda: extend_one([], 0)))\n0.2397019260097295\n>>> min(timeit.repeat(lambda: append_one([], 0)))\n0.2082819009956438\n>>> min(timeit.repeat(lambda: extend_one([], 0)))\n0.2397019260097295\nWe learn from this that there's nothing gained from using extend when we have only one element to append.extendoneAlso, these timings are not that important. I am just showing them to make the point that, in Python, doing the semantically correct thing is doing things the Right Way\u2122.RightIt's conceivable that you might test timings on two comparable operations and get an ambiguous or inverse result. Just focus on doing the semantically correct thing.ConclusionWe see that extend is semantically clearer, and that it can run much faster than append, when you intend to append each element in an iterable to a list.extendappendwhen you intend to append each element in an iterable to a list.If you only have a single element (not in an iterable) to add to the list, use append.append",
                "append appends a single element. extend appends a list of elements.appendextendNote that if you pass a list to append, it still adds one element:>>> a = [1, 2, 3]\n>>> a.append([4, 5, 6])\n>>> a\n[1, 2, 3, [4, 5, 6]]\n>>> a = [1, 2, 3]\n>>> a.append([4, 5, 6])\n>>> a\n[1, 2, 3, [4, 5, 6]]\n",
                " Append vs ExtendWith append you can append a single element that will extend the list:>>> a = [1,2]\n>>> a.append(3)\n>>> a\n[1,2,3]\n>>> a = [1,2]\n>>> a.append(3)\n>>> a\n[1,2,3]\nIf you want to extend more than one element you should use extend, because you can only append one elment or one list of element:>>> a.append([4,5])\n>>> a\n>>> [1,2,3,[4,5]]\n>>> a.append([4,5])\n>>> a\n>>> [1,2,3,[4,5]]\nSo that you get a nested listInstead with extend, you can extend a single element like this>>> a = [1,2]\n>>> a.extend([3])\n>>> a\n[1,2,3]\n>>> a = [1,2]\n>>> a.extend([3])\n>>> a\n[1,2,3]\nOr, differently, from append, extend more elements in one time without nesting the list into the original one (that's the reason of the name extend)>>> a.extend([4,5,6])\n>>> a\n[1,2,3,4,5,6]\n>>> a.extend([4,5,6])\n>>> a\n[1,2,3,4,5,6]\n Adding one element with both methodsBoth append and extend can add one element to the end of the list, though append is simpler. append 1 element >>> x = [1,2]\n>>> x.append(3)\n>>> x\n[1,2,3]\n>>> x = [1,2]\n>>> x.append(3)\n>>> x\n[1,2,3]\n extend one element >>> x = [1,2]\n>>> x.extend([3])\n>>> x\n[1,2,3]\n>>> x = [1,2]\n>>> x.extend([3])\n>>> x\n[1,2,3]\n Adding more elements... with different results If you use append for more than one element, you have to pass a list of elements as arguments and you will obtain a NESTED list!>>> x = [1,2]\n>>> x.append([3,4])\n>>> x\n[1,2,[3,4]]\n>>> x = [1,2]\n>>> x.append([3,4])\n>>> x\n[1,2,[3,4]]\nWith extend, instead, you pass a list as an argument, but you will obtain a list with the new element that is not nested in the old one.>>> z = [1,2] \n>>> z.extend([3,4])\n>>> z\n[1,2,3,4]\n>>> z = [1,2] \n>>> z.extend([3,4])\n>>> z\n[1,2,3,4]\nSo, with more elements, you will use extend to get a list with more items.\nHowever, appending a list will not add more elements to the list, but one element that is a nested list as you can clearly see in the output of the code.",
                "The following two snippets are semantically equivalent:for item in iterator:\n    a_list.append(item)\nfor item in iterator:\n    a_list.append(item)\nanda_list.extend(iterator)\na_list.extend(iterator)\nThe latter may be faster as the loop is implemented in C.",
                "The append() method adds a single item to the end of the list.append()x = [1, 2, 3]\nx.append([4, 5])\nx.append('abc')\nprint(x)\n# gives you\n[1, 2, 3, [4, 5], 'abc']\nx = [1, 2, 3]\nx.append([4, 5])\nx.append('abc')\nprint(x)\n# gives you\n[1, 2, 3, [4, 5], 'abc']\nThe extend() method takes one argument, a list, and appends each of the items of the argument to the original list. (Lists are implemented as classes. \u201cCreating\u201d a list is really instantiating a class. As such, a list has methods that operate on it.)extend()x = [1, 2, 3]\nx.extend([4, 5])\nx.extend('abc')\nprint(x)\n# gives you\n[1, 2, 3, 4, 5, 'a', 'b', 'c']\nx = [1, 2, 3]\nx.extend([4, 5])\nx.extend('abc')\nprint(x)\n# gives you\n[1, 2, 3, 4, 5, 'a', 'b', 'c']\nFrom Dive Into Python.Dive Into PythonDive Into Python",
                "You can use \"+\" for returning extend, instead of extending in place.l1=range(10)\n\nl1+[11]\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n\nl2=range(10,1,-1)\n\nl1+l2\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 8, 7, 6, 5, 4, 3, 2]\nl1=range(10)\n\nl1+[11]\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]\n\nl2=range(10,1,-1)\n\nl1+l2\n\n[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 8, 7, 6, 5, 4, 3, 2]\nSimilarly += for in place behavior, but with slight differences from append & extend. One of the biggest differences of += from append and extend is when it is used in function scopes, see this blog post.+=appendextend+=appendextendthis blog post",
                "append(object) updates the list by adding the object to the list.append(object)x = [20]\n# List passed to the append(object) method is treated as a single object.\nx.append([21, 22, 23])\n# Hence the resultant list length will be 2\nprint(x)\n--> [20, [21, 22, 23]]\nx = [20]\n# List passed to the append(object) method is treated as a single object.\nx.append([21, 22, 23])\n# Hence the resultant list length will be 2\nprint(x)\n--> [20, [21, 22, 23]]\nextend(list) concatenates the two lists essentially.extend(list)x = [20]\n# The parameter passed to extend(list) method is treated as a list.\n# Eventually it is two lists being concatenated.\nx.extend([21, 22, 23])\n# Here the resultant list's length is 4\nprint(x)\n--> [20, 21, 22, 23]\nx = [20]\n# The parameter passed to extend(list) method is treated as a list.\n# Eventually it is two lists being concatenated.\nx.extend([21, 22, 23])\n# Here the resultant list's length is 4\nprint(x)\n--> [20, 21, 22, 23]\n",
                "This is the equivalent of append and extend using the + operator:appendextend+>>> x = [1,2,3]\n>>> x\n[1, 2, 3]\n>>> x = x + [4,5,6] # Extend\n>>> x\n[1, 2, 3, 4, 5, 6]\n>>> x = x + [[7,8]] # Append\n>>> x\n[1, 2, 3, 4, 5, 6, [7, 8]]\n>>> x = [1,2,3]\n>>> x\n[1, 2, 3]\n>>> x = x + [4,5,6] # Extend\n>>> x\n[1, 2, 3, 4, 5, 6]\n>>> x = x + [[7,8]] # Append\n>>> x\n[1, 2, 3, 4, 5, 6, [7, 8]]\n",
                "extend() can be used with an iterator argument. Here is an example. You wish to make a list out of a list of lists this way:extend()Fromlist2d = [[1,2,3],[4,5,6], [7], [8,9]]\nlist2d = [[1,2,3],[4,5,6], [7], [8,9]]\nyou want>>>\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n>>>\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\nYou may use itertools.chain.from_iterable() to do so. This method's output is an iterator. Its implementation is equivalent toitertools.chain.from_iterable()def from_iterable(iterables):\n    # chain.from_iterable(['ABC', 'DEF']) --> A B C D E F\n    for it in iterables:\n        for element in it:\n            yield element\ndef from_iterable(iterables):\n    # chain.from_iterable(['ABC', 'DEF']) --> A B C D E F\n    for it in iterables:\n        for element in it:\n            yield element\nBack to our example, we can doimport itertools\nlist2d = [[1,2,3],[4,5,6], [7], [8,9]]\nmerged = list(itertools.chain.from_iterable(list2d))\nimport itertools\nlist2d = [[1,2,3],[4,5,6], [7], [8,9]]\nmerged = list(itertools.chain.from_iterable(list2d))\nand get the wanted list.Here is how equivalently extend() can be used with an iterator argument:extend()merged = []\nmerged.extend(itertools.chain.from_iterable(list2d))\nprint(merged)\n>>>\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\nmerged = []\nmerged.extend(itertools.chain.from_iterable(list2d))\nprint(merged)\n>>>\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
                "append(): It is basically used in Python to add one element.append()append()\nExample 1:\nExample 1:>> a = [1, 2, 3, 4]\n>> a.append(5)\n>> print(a)\n>> a = [1, 2, 3, 4, 5]\n>> a = [1, 2, 3, 4]\n>> a.append(5)\n>> print(a)\n>> a = [1, 2, 3, 4, 5]\n\nExample 2:\nExample 2:>> a = [1, 2, 3, 4]\n>> a.append([5, 6])\n>> print(a)\n>> a = [1, 2, 3, 4, [5, 6]]\n>> a = [1, 2, 3, 4]\n>> a.append([5, 6])\n>> print(a)\n>> a = [1, 2, 3, 4, [5, 6]]\nextend(): Where extend(), is used to merge two lists or insert multiple elements in one list.extend()extend()\nExample 1:\nExample 1:>> a = [1, 2, 3, 4]\n>> b = [5, 6, 7, 8]\n>> a.extend(b)\n>> print(a)\n>> a = [1, 2, 3, 4, 5, 6, 7, 8]\n>> a = [1, 2, 3, 4]\n>> b = [5, 6, 7, 8]\n>> a.extend(b)\n>> print(a)\n>> a = [1, 2, 3, 4, 5, 6, 7, 8]\n\nExample 2:\nExample 2:>> a = [1, 2, 3, 4]\n>> a.extend([5, 6])\n>> print(a)\n>> a = [1, 2, 3, 4, 5, 6]\n>> a = [1, 2, 3, 4]\n>> a.extend([5, 6])\n>> print(a)\n>> a = [1, 2, 3, 4, 5, 6]\n",
                "An interesting point that has been hinted, but not explained, is that extend is faster than append. For any loop that has append inside should be considered to be replaced by list.extend(processed_elements).Bear in mind that apprending new elements might result in the realloaction of the whole list to a better location in memory. If this is done several times because we are appending 1 element at a time, overall performance suffers. In this sense, list.extend is analogous to \"\".join(stringlist).",
                "Append adds the entire data at once. The whole data will be added to the newly created index. On the other hand, extend, as it name suggests, extends the current array. extendFor examplelist1 = [123, 456, 678]\nlist2 = [111, 222]\nlist1 = [123, 456, 678]\nlist2 = [111, 222]\nWith append we get:appendresult = [123, 456, 678, [111, 222]]\nresult = [123, 456, 678, [111, 222]]\nWhile on extend we get:extendresult = [123, 456, 678, 111, 222]\nresult = [123, 456, 678, 111, 222]\n",
                "An English dictionary defines the words append and extend as:appendextendappend: add (something) to the end of a written document. \nextend: make larger. Enlarge or expandappendextendWith that knowledge, now let's understand1) The difference between append and extendThe difference between append and extendappendextendappend:appendappend\nAppends any Python object as-is to the end of the list (i.e. as a\nthe last element in the list).\nThe resulting list may be nested and contain heterogeneous elements (i.e. list, string, tuple, dictionary, set, etc.)\nAppends any Python object as-is to the end of the list (i.e. as a\nthe last element in the list).any Python object as-isThe resulting list may be nested and contain heterogeneous elements (i.e. list, string, tuple, dictionary, set, etc.)extend:extendextend\nAccepts any iterable as its argument and makes the list larger.\nThe resulting list is always one-dimensional list (i.e. no nesting) and it may contain heterogeneous elements in it (e.g. characters, integers, float) as a result of applying list(iterable).\nAccepts any iterable as its argument and makes the list larger.iterablelargerThe resulting list is always one-dimensional list (i.e. no nesting) and it may contain heterogeneous elements in it (e.g. characters, integers, float) as a result of applying list(iterable).list(iterable)2) Similarity between append and extendSimilarity between append and extendappendextend\nBoth take exactly one argument.\nBoth modify the list in-place.\nAs a result, both returns None.\nBoth take exactly one argument.Both modify the list in-place.in-placeAs a result, both returns None.NoneExampleExamplelis = [1, 2, 3]\n\n# 'extend' is equivalent to this\nlis = lis + list(iterable)\n\n# 'append' simply appends its argument as the last element to the list\n# as long as the argument is a valid Python object\nlist.append(object)\nlis = [1, 2, 3]\n\n# 'extend' is equivalent to this\nlis = lis + list(iterable)\n\n# 'append' simply appends its argument as the last element to the list\n# as long as the argument is a valid Python object\nlist.append(object)\n",
                "I hope I can make a useful supplement to this question. If your list stores a specific type object, for example Info, here is a situation that extend method is not suitable: In a for loop and and generating an Info object every time and using extend to store it into your list, it will fail. The exception is like below:InfoextendforInfoextend\nTypeError: 'Info' object is not iterable\nTypeError: 'Info' object is not iterableBut if you use the append method, the result is OK. Because every time using the extend method, it will always treat it as a list or any other collection type, iterate it, and place it after the previous list. A specific object can not be iterated, obviously.appendextend",
                "To distinguish them intuitivelyl1 = ['a', 'b', 'c']\nl2 = ['d', 'e', 'f']\nl1.append(l2)\nl1\n['a', 'b', 'c', ['d', 'e', 'f']]\nl1 = ['a', 'b', 'c']\nl2 = ['d', 'e', 'f']\nl1.append(l2)\nl1\n['a', 'b', 'c', ['d', 'e', 'f']]\nIt's like l1 reproduce a body inside her body(nested).l1# Reset l1 = ['a', 'b', 'c']\nl1.extend(l2)\nl1\n['a', 'b', 'c', 'd', 'e', 'f']\n# Reset l1 = ['a', 'b', 'c']\nl1.extend(l2)\nl1\n['a', 'b', 'c', 'd', 'e', 'f']\nIt's like that two separated individuals get married and construct an united family.Besides I make an exhaustive cheatsheet of all list's methods for your reference.list_methods = {'Add': {'extend', 'append', 'insert'},\n                'Remove': {'pop', 'remove', 'clear'}\n                'Sort': {'reverse', 'sort'},\n                'Search': {'count', 'index'},\n                'Copy': {'copy'},\n                }\nlist_methods = {'Add': {'extend', 'append', 'insert'},\n                'Remove': {'pop', 'remove', 'clear'}\n                'Sort': {'reverse', 'sort'},\n                'Search': {'count', 'index'},\n                'Copy': {'copy'},\n                }\n",
                "extend(L) extends the list by appending all the items in the given list L.extend(L)L>>> a\n[1, 2, 3]\na.extend([4])  #is eqivalent of a[len(a):] = [4]\n>>> a\n[1, 2, 3, 4]\na = [1, 2, 3]\n>>> a\n[1, 2, 3]\n>>> a[len(a):] = [4]\n>>> a\n[1, 2, 3, 4]\n>>> a\n[1, 2, 3]\na.extend([4])  #is eqivalent of a[len(a):] = [4]\n>>> a\n[1, 2, 3, 4]\na = [1, 2, 3]\n>>> a\n[1, 2, 3]\n>>> a[len(a):] = [4]\n>>> a\n[1, 2, 3, 4]\n",
                "append \"extends\" the list (in place) by only one item, the single object passed (as argument).appendonly one itemextend \"extends\" the list (in place) by as many items as the object passed (as argument) contains.extendas many items asThis may be slightly confusing for str objects.str\nIf you pass a string as argument:\nappend will add a single string item at the end but\nextend will add as many \"single\" 'str' items as the length of that string.\nIf you pass a list of strings as argument:\nappend will still add a single 'list' item at the end and\nextend will add as many 'list' items as the length of the passed list.\nIf you pass a string as argument:\nappend will add a single string item at the end but\nextend will add as many \"single\" 'str' items as the length of that string.appendextendIf you pass a list of strings as argument:\nappend will still add a single 'list' item at the end and\nextend will add as many 'list' items as the length of the passed list.appendextend\ndef append_o(a_list, element):\n    a_list.append(element)\n    print('append:', end = ' ')\n    for item in a_list:\n        print(item, end = ',')\n    print()\n\ndef extend_o(a_list, element):\n    a_list.extend(element)\n    print('extend:', end = ' ')\n    for item in a_list:\n        print(item, end = ',')\n    print()\nappend_o(['ab'],'cd')\n\nextend_o(['ab'],'cd')\nappend_o(['ab'],['cd', 'ef'])\nextend_o(['ab'],['cd', 'ef'])\nappend_o(['ab'],['cd'])\nextend_o(['ab'],['cd'])\n\ndef append_o(a_list, element):\n    a_list.append(element)\n    print('append:', end = ' ')\n    for item in a_list:\n        print(item, end = ',')\n    print()\n\ndef extend_o(a_list, element):\n    a_list.extend(element)\n    print('extend:', end = ' ')\n    for item in a_list:\n        print(item, end = ',')\n    print()\nappend_o(['ab'],'cd')\n\nextend_o(['ab'],'cd')\nappend_o(['ab'],['cd', 'ef'])\nextend_o(['ab'],['cd', 'ef'])\nappend_o(['ab'],['cd'])\nextend_o(['ab'],['cd'])\ndef append_o(a_list, element):\n    a_list.append(element)\n    print('append:', end = ' ')\n    for item in a_list:\n        print(item, end = ',')\n    print()\n\ndef extend_o(a_list, element):\n    a_list.extend(element)\n    print('extend:', end = ' ')\n    for item in a_list:\n        print(item, end = ',')\n    print()\nappend_o(['ab'],'cd')\n\nextend_o(['ab'],'cd')\nappend_o(['ab'],['cd', 'ef'])\nextend_o(['ab'],['cd', 'ef'])\nappend_o(['ab'],['cd'])\nextend_o(['ab'],['cd'])\nproduces:append: ab,cd,\nextend: ab,c,d,\nappend: ab,['cd', 'ef'],\nextend: ab,cd,ef,\nappend: ab,['cd'],\nextend: ab,cd,\nappend: ab,cd,\nextend: ab,c,d,\nappend: ab,['cd', 'ef'],\nextend: ab,cd,ef,\nappend: ab,['cd'],\nextend: ab,cd,\n",
                "Append and extend are one of the extensibility mechanisms in python. Append: Adds an element to the end of the list. my_list = [1,2,3,4]\nmy_list = [1,2,3,4]\nTo add a new element to the list, we can use append method in the following way.my_list.append(5)\nmy_list.append(5)\nThe default location that the new element will be added is always in the (length+1) position. Insert: The insert method was used to overcome the limitations of append. With insert, we can explicitly define the exact position we want our new element to be inserted at. Method descriptor of insert(index, object). It takes two arguments, first being the index we want to insert our element and second the element itself. Example: my_list = [1,2,3,4]\nmy_list[4, 'a']\nmy_list\n[1,2,3,4,'a']\nExample: my_list = [1,2,3,4]\nmy_list[4, 'a']\nmy_list\n[1,2,3,4,'a']\nExtend: This is very useful when we want to join two or more lists into a single list. Without extend, if we want to join two lists, the resulting object will contain a list of lists. a = [1,2]\nb = [3]\na.append(b)\nprint (a)\n[1,2,[3]]\na = [1,2]\nb = [3]\na.append(b)\nprint (a)\n[1,2,[3]]\nIf we try to access the element at pos 2, we get a list ([3]), instead of the element. To join two lists, we'll have to use append. a = [1,2]\nb = [3]\na.extend(b)\nprint (a)\n[1,2,3]\na = [1,2]\nb = [3]\na.extend(b)\nprint (a)\n[1,2,3]\nTo join multiple listsa = [1]\nb = [2]\nc = [3]\na.extend(b+c)\nprint (a)\n[1,2,3]\na = [1]\nb = [2]\nc = [3]\na.extend(b+c)\nprint (a)\n[1,2,3]\n"
            ]
        },
        {
            "tag": "super",
            "patterns": [
                "Understanding Python super() with __init__() methods [duplicate]",
                "Understanding Python super() with __init__() methods [duplicate]"
            ],
            "responses": [
                "super() lets you avoid referring to the base class explicitly, which can be nice. But the main advantage comes with multiple inheritance, where all sorts of fun stuff can happen. See the standard docs on super if you haven't already.super()fun stuffstandard docs on superNote that the syntax changed in Python 3.0: you can just say super().__init__() instead of super(ChildB, self).__init__() which IMO is quite a bit nicer. The standard docs also refer to a guide to using super() which is quite explanatory.the syntax changed in Python 3.0super().__init__()super(ChildB, self).__init__()guide to using super()super()",
                "\nI'm trying to understand super()\nI'm trying to understand super()super()The reason we use super is so that child classes that may be using cooperative multiple inheritance will call the correct next parent class function in the Method Resolution Order (MRO).superIn Python 3, we can call it like this:class ChildB(Base):\n    def __init__(self):\n        super().__init__()\nclass ChildB(Base):\n    def __init__(self):\n        super().__init__()\nIn Python 2, we were required to call super like this with the defining class's name and self, but we'll avoid this from now on because it's redundant, slower (due to the name lookups), and more verbose (so update your Python if you haven't already!):superself        super(ChildB, self).__init__()\n        super(ChildB, self).__init__()\nWithout super, you are limited in your ability to use multiple inheritance because you hard-wire the next parent's call:        Base.__init__(self) # Avoid this.\n        Base.__init__(self) # Avoid this.\nI further explain below.\n\"What difference is there actually in this code?:\"\n\"What difference is there actually in this code?:\"class ChildA(Base):\n    def __init__(self):\n        Base.__init__(self)\n\nclass ChildB(Base):\n    def __init__(self):\n        super().__init__()\nclass ChildA(Base):\n    def __init__(self):\n        Base.__init__(self)\n\nclass ChildB(Base):\n    def __init__(self):\n        super().__init__()\nThe primary difference in this code is that in ChildB you get a layer of indirection in the __init__ with super, which uses the class in which it is defined to determine the next class's __init__ to look up in the MRO.ChildB__init__super__init__I illustrate this difference in an answer at the canonical question, How to use 'super' in Python?, which demonstrates dependency injection and cooperative multiple inheritance.canonical question, How to use 'super' in Python?dependency injectioncooperative multiple inheritanceIf Python didn't have supersuperHere's code that's actually closely equivalent to super (how it's implemented in C, minus some checking and fallback behavior, and translated to Python):superclass ChildB(Base):\n    def __init__(self):\n        mro = type(self).mro()\n        check_next = mro.index(ChildB) + 1 # next after *this* class.\n        while check_next < len(mro):\n            next_class = mro[check_next]\n            if '__init__' in next_class.__dict__:\n                next_class.__init__(self)\n                break\n            check_next += 1\nclass ChildB(Base):\n    def __init__(self):\n        mro = type(self).mro()\n        check_next = mro.index(ChildB) + 1 # next after *this* class.\n        while check_next < len(mro):\n            next_class = mro[check_next]\n            if '__init__' in next_class.__dict__:\n                next_class.__init__(self)\n                break\n            check_next += 1\nWritten a little more like native Python:class ChildB(Base):\n    def __init__(self):\n        mro = type(self).mro()\n        for next_class in mro[mro.index(ChildB) + 1:]: # slice to end\n            if hasattr(next_class, '__init__'):\n                next_class.__init__(self)\n                break\nclass ChildB(Base):\n    def __init__(self):\n        mro = type(self).mro()\n        for next_class in mro[mro.index(ChildB) + 1:]: # slice to end\n            if hasattr(next_class, '__init__'):\n                next_class.__init__(self)\n                break\nIf we didn't have the super object, we'd have to write this manual code everywhere (or recreate it!) to ensure that we call the proper next method in the Method Resolution Order!superHow does super do this in Python 3 without being told explicitly which class and instance from the method it was called from?It gets the calling stack frame, and finds the class (implicitly stored as a local free variable, __class__, making the calling function a closure over the class) and the first argument to that function, which should be the instance or class that informs it which Method Resolution Order (MRO) to use.__class__Since it requires that first argument for the MRO, using super with static methods is impossible as they do not have access to the MRO of the class from which they are called.using super with static methods is impossible as they do not have access to the MRO of the class from which they are calledsuperCriticisms of other answers:\nsuper() lets you avoid referring to the base class explicitly, which can be nice. . But the main advantage comes with multiple inheritance, where all sorts of fun stuff can happen. See the standard docs on super if you haven't already.\nsuper() lets you avoid referring to the base class explicitly, which can be nice. . But the main advantage comes with multiple inheritance, where all sorts of fun stuff can happen. See the standard docs on super if you haven't already.It's rather hand-wavey and doesn't tell us much, but the point of super is not to avoid writing the parent class. The point is to ensure that the next method in line in the method resolution order (MRO) is called. This becomes important in multiple inheritance.superI'll explain here.class Base(object):\n    def __init__(self):\n        print(\"Base init'ed\")\n\nclass ChildA(Base):\n    def __init__(self):\n        print(\"ChildA init'ed\")\n        Base.__init__(self)\n\nclass ChildB(Base):\n    def __init__(self):\n        print(\"ChildB init'ed\")\n        super().__init__()\nclass Base(object):\n    def __init__(self):\n        print(\"Base init'ed\")\n\nclass ChildA(Base):\n    def __init__(self):\n        print(\"ChildA init'ed\")\n        Base.__init__(self)\n\nclass ChildB(Base):\n    def __init__(self):\n        print(\"ChildB init'ed\")\n        super().__init__()\nAnd let's create a dependency that we want to be called after the Child:class UserDependency(Base):\n    def __init__(self):\n        print(\"UserDependency init'ed\")\n        super().__init__()\nclass UserDependency(Base):\n    def __init__(self):\n        print(\"UserDependency init'ed\")\n        super().__init__()\nNow remember, ChildB uses super, ChildA does not:ChildBChildAclass UserA(ChildA, UserDependency):\n    def __init__(self):\n        print(\"UserA init'ed\")\n        super().__init__()\n\nclass UserB(ChildB, UserDependency):\n    def __init__(self):\n        print(\"UserB init'ed\")\n        super().__init__()\nclass UserA(ChildA, UserDependency):\n    def __init__(self):\n        print(\"UserA init'ed\")\n        super().__init__()\n\nclass UserB(ChildB, UserDependency):\n    def __init__(self):\n        print(\"UserB init'ed\")\n        super().__init__()\nAnd UserA does not call the UserDependency method:UserA>>> UserA()\nUserA init'ed\nChildA init'ed\nBase init'ed\n<__main__.UserA object at 0x0000000003403BA8>\n>>> UserA()\nUserA init'ed\nChildA init'ed\nBase init'ed\n<__main__.UserA object at 0x0000000003403BA8>\nBut UserB does in-fact call UserDependency because ChildB invokes super:UserBChildBsuper>>> UserB()\nUserB init'ed\nChildB init'ed\nUserDependency init'ed\nBase init'ed\n<__main__.UserB object at 0x0000000003403438>\n>>> UserB()\nUserB init'ed\nChildB init'ed\nUserDependency init'ed\nBase init'ed\n<__main__.UserB object at 0x0000000003403438>\nCriticism for another answerIn no circumstance should you do the following, which another answer suggests, as you'll definitely get errors when you subclass ChildB:super(self.__class__, self).__init__()  # DON'T DO THIS! EVER.\nsuper(self.__class__, self).__init__()  # DON'T DO THIS! EVER.\n(That answer is not clever or particularly interesting, but in spite of direct criticism in the comments and over 17 downvotes, the answerer persisted in suggesting it until a kind editor fixed his problem.)(That answer is not clever or particularly interesting, but in spite of direct criticism in the comments and over 17 downvotes, the answerer persisted in suggesting it until a kind editor fixed his problem.)Explanation: Using self.__class__ as a substitute for the class name in super() will lead to recursion. super lets us look up the next parent in the MRO (see the first section of this answer) for child classes. If you tell super we're in the child instance's method, it will then lookup the next method in line (probably this one) resulting in recursion, probably causing a logical failure (in the answerer's example, it does) or a RuntimeError when the recursion depth is exceeded.self.__class__super()supersuperRuntimeError>>> class Polygon(object):\n...     def __init__(self, id):\n...         self.id = id\n...\n>>> class Rectangle(Polygon):\n...     def __init__(self, id, width, height):\n...         super(self.__class__, self).__init__(id)\n...         self.shape = (width, height)\n...\n>>> class Square(Rectangle):\n...     pass\n...\n>>> Square('a', 10, 10)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in __init__\nTypeError: __init__() missing 2 required positional arguments: 'width' and 'height'\n>>> class Polygon(object):\n...     def __init__(self, id):\n...         self.id = id\n...\n>>> class Rectangle(Polygon):\n...     def __init__(self, id, width, height):\n...         super(self.__class__, self).__init__(id)\n...         self.shape = (width, height)\n...\n>>> class Square(Rectangle):\n...     pass\n...\n>>> Square('a', 10, 10)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in __init__\nTypeError: __init__() missing 2 required positional arguments: 'width' and 'height'\nPython 3's new super() calling method with no arguments fortunately allows us to sidestep this issue.super()",
                "It's been noted that in Python 3.0+ you can usesuper().__init__()\nsuper().__init__()\nto make your call, which is concise and does not require you to reference the parent OR class names explicitly, which can be handy. I just want to add that for Python 2.7 or under, some people implement a name-insensitive behaviour by writing self.__class__ instead of the class name, i.e.self.__class__super(self.__class__, self).__init__()  # DON'T DO THIS!\nsuper(self.__class__, self).__init__()  # DON'T DO THIS!\nHOWEVER, this breaks calls to super for any classes that inherit from your class, where self.__class__ could return a child class. For example:superself.__class__class Polygon(object):\n    def __init__(self, id):\n        self.id = id\n\nclass Rectangle(Polygon):\n    def __init__(self, id, width, height):\n        super(self.__class__, self).__init__(id)\n        self.shape = (width, height)\n\nclass Square(Rectangle):\n    pass\nclass Polygon(object):\n    def __init__(self, id):\n        self.id = id\n\nclass Rectangle(Polygon):\n    def __init__(self, id, width, height):\n        super(self.__class__, self).__init__(id)\n        self.shape = (width, height)\n\nclass Square(Rectangle):\n    pass\nHere I have a class Square, which is a sub-class of Rectangle. Say I don't want to write a separate constructor for Square because the constructor for Rectangle is good enough, but for whatever reason I want to implement a Square so I can reimplement some other method.SquareRectangleSquareRectangleWhen I create a Square using mSquare = Square('a', 10,10), Python calls the constructor for Rectangle because I haven't given Square its own constructor. However, in the constructor for Rectangle, the call super(self.__class__,self) is going to return the superclass of mSquare, so it calls the constructor for Rectangle again. This is how the infinite loop happens, as was mentioned by @S_C. In this case, when I run super(...).__init__() I am calling the constructor for Rectangle but since I give it no arguments, I will get an error.SquaremSquare = Square('a', 10,10)RectangleSquareRectanglesuper(self.__class__,self)mSquareRectanglesuper(...).__init__()Rectangle",
                "Super has no side effectsBase = ChildB\n\nBase()\nBase = ChildB\n\nBase()\nworks as expectedBase = ChildA\n\nBase()\nBase = ChildA\n\nBase()\ngets into infinite recursion.",
                "Just a heads up... with Python 2.7, and I believe ever since super() was introduced in version 2.2, you can only call super() if one of the parents inherit from a class that eventually inherits object (new-style classes).super()super()super()objectnew-style classesPersonally, as for python 2.7 code, I'm going to continue using BaseClassName.__init__(self, args) until I actually get the advantage of using super().BaseClassName.__init__(self, args)super()",
                "There isn't, really. super() looks at the next class in the MRO (method resolution order, accessed with cls.__mro__) to call the methods. Just calling the base __init__ calls the base __init__. As it happens, the MRO has exactly one item-- the base. So you're really doing the exact same thing, but in a nicer way with super() (particularly if you get into multiple inheritance later).super()cls.__mro____init____init__super()",
                "The main difference is that ChildA.__init__ will unconditionally call Base.__init__ whereas ChildB.__init__ will call __init__ in whatever class happens to be ChildB ancestor in self's line of ancestors\n(which may differ from what you expect).ChildA.__init__Base.__init__ChildB.__init____init__whatever class happens to be ChildB ancestor in self's line of ancestorsChildBselfIf you add a ClassC that uses multiple inheritance:ClassCclass Mixin(Base):\n  def __init__(self):\n    print \"Mixin stuff\"\n    super(Mixin, self).__init__()\n\nclass ChildC(ChildB, Mixin):  # Mixin is now between ChildB and Base\n  pass\n\nChildC()\nhelp(ChildC) # shows that the Method Resolution Order is ChildC->ChildB->Mixin->Base\nclass Mixin(Base):\n  def __init__(self):\n    print \"Mixin stuff\"\n    super(Mixin, self).__init__()\n\nclass ChildC(ChildB, Mixin):  # Mixin is now between ChildB and Base\n  pass\n\nChildC()\nhelp(ChildC) # shows that the Method Resolution Order is ChildC->ChildB->Mixin->Base\nthen Base is no longer the parent of ChildB for ChildC instances. Now super(ChildB, self) will point to Mixin if self is a ChildC instance.Base is no longer the parent of ChildBBaseChildBChildCsuper(ChildB, self)MixinselfChildCYou have inserted Mixin in between ChildB and Base. And you can take advantage of it with super()MixinChildBBasesuper()So if you are designed your classes so that they can be used in a Cooperative Multiple Inheritance scenario, you use super because you don't really know who is going to be the ancestor at runtime.superThe super considered super post and pycon 2015 accompanying video explain this pretty well.super considered super postpycon 2015 accompanying video"
            ]
        },
        {
            "tag": "decorators",
            "patterns": [
                "How to make function decorators and chain them together?",
                "how do you tie the function decorators together?",
                "how can i make function decorators and chain them together?"
            ],
            "responses": [
                "If you are not into long explanations, see Paolo Bergantino\u2019s answer.Paolo Bergantino\u2019s answerDecorator BasicsPython\u2019s functions are objectsTo understand decorators, you must first understand that functions are objects in Python. This has important consequences. Let\u2019s see why with a simple example :def shout(word=\"yes\"):\n    return word.capitalize()+\"!\"\n\nprint(shout())\n# outputs : 'Yes!'\n\n# As an object, you can assign the function to a variable like any other object \nscream = shout\n\n# Notice we don't use parentheses: we are not calling the function,\n# we are putting the function \"shout\" into the variable \"scream\".\n# It means you can then call \"shout\" from \"scream\":\n\nprint(scream())\n# outputs : 'Yes!'\n\n# More than that, it means you can remove the old name 'shout',\n#\u00a0and the function will still be accessible from 'scream'\n\ndel shout\ntry:\n    print(shout())\nexcept NameError as e:\n    print(e)\n    #outputs: \"name 'shout' is not defined\"\n\nprint(scream())\n# outputs: 'Yes!'\ndef shout(word=\"yes\"):\n    return word.capitalize()+\"!\"\n\nprint(shout())\n# outputs : 'Yes!'\n\n# As an object, you can assign the function to a variable like any other object \nscream = shout\n\n# Notice we don't use parentheses: we are not calling the function,\n# we are putting the function \"shout\" into the variable \"scream\".\n# It means you can then call \"shout\" from \"scream\":\n\nprint(scream())\n# outputs : 'Yes!'\n\n# More than that, it means you can remove the old name 'shout',\n#\u00a0and the function will still be accessible from 'scream'\n\ndel shout\ntry:\n    print(shout())\nexcept NameError as e:\n    print(e)\n    #outputs: \"name 'shout' is not defined\"\n\nprint(scream())\n# outputs: 'Yes!'\nKeep this in mind. We\u2019ll circle back to it shortly.Another interesting property of Python functions is they can be defined inside another function!def talk():\n\n    # You can define a function on the fly in \"talk\" ...\n    def whisper(word=\"yes\"):\n        return word.lower()+\"...\"\n\n    # ... and use it right away!\n    print(whisper())\n\n# You call \"talk\", that defines \"whisper\" EVERY TIME you call it, then\n# \"whisper\" is called in \"talk\". \ntalk()\n# outputs: \n# \"yes...\"\n\n# But \"whisper\" DOES NOT EXIST outside \"talk\":\n\ntry:\n    print(whisper())\nexcept NameError as e:\n    print(e)\n    #outputs : \"name 'whisper' is not defined\"*\n    #Python's functions are objects\ndef talk():\n\n    # You can define a function on the fly in \"talk\" ...\n    def whisper(word=\"yes\"):\n        return word.lower()+\"...\"\n\n    # ... and use it right away!\n    print(whisper())\n\n# You call \"talk\", that defines \"whisper\" EVERY TIME you call it, then\n# \"whisper\" is called in \"talk\". \ntalk()\n# outputs: \n# \"yes...\"\n\n# But \"whisper\" DOES NOT EXIST outside \"talk\":\n\ntry:\n    print(whisper())\nexcept NameError as e:\n    print(e)\n    #outputs : \"name 'whisper' is not defined\"*\n    #Python's functions are objects\nFunctions referencesOkay, still here? Now the fun part...You\u2019ve seen that functions are objects. Therefore, functions:\ncan be assigned to a variable\ncan be defined in another function\ncan be assigned to a variablecan be defined in another functionThat means that a function can return another function.a function can return another functionreturndef getTalk(kind=\"shout\"):\n\n    # We define functions on the fly\n    def shout(word=\"yes\"):\n        return word.capitalize()+\"!\"\n\n    def whisper(word=\"yes\") :\n        return word.lower()+\"...\"\n\n    # Then we return one of them\n    if kind == \"shout\":\n        # We don't use \"()\", we are not calling the function,\n        # we are returning the function object\n        return shout  \n    else:\n        return whisper\n\n# How do you use this strange beast?\n\n# Get the function and assign it to a variable\ntalk = getTalk()      \n\n# You can see that \"talk\" is here a function object:\nprint(talk)\n#outputs : <function shout at 0xb7ea817c>\n\n# The object is the one returned by the function:\nprint(talk())\n#outputs : Yes!\n\n# And you can even use it directly if you feel wild:\nprint(getTalk(\"whisper\")())\n#outputs : yes...\ndef getTalk(kind=\"shout\"):\n\n    # We define functions on the fly\n    def shout(word=\"yes\"):\n        return word.capitalize()+\"!\"\n\n    def whisper(word=\"yes\") :\n        return word.lower()+\"...\"\n\n    # Then we return one of them\n    if kind == \"shout\":\n        # We don't use \"()\", we are not calling the function,\n        # we are returning the function object\n        return shout  \n    else:\n        return whisper\n\n# How do you use this strange beast?\n\n# Get the function and assign it to a variable\ntalk = getTalk()      \n\n# You can see that \"talk\" is here a function object:\nprint(talk)\n#outputs : <function shout at 0xb7ea817c>\n\n# The object is the one returned by the function:\nprint(talk())\n#outputs : Yes!\n\n# And you can even use it directly if you feel wild:\nprint(getTalk(\"whisper\")())\n#outputs : yes...\nThere\u2019s more!If you can return a function, you can pass one as a parameter:returndef doSomethingBefore(func): \n    print(\"I do something before then I call the function you gave me\")\n    print(func())\n\ndoSomethingBefore(scream)\n#outputs: \n#I do something before then I call the function you gave me\n#Yes!\ndef doSomethingBefore(func): \n    print(\"I do something before then I call the function you gave me\")\n    print(func())\n\ndoSomethingBefore(scream)\n#outputs: \n#I do something before then I call the function you gave me\n#Yes!\nWell, you just have everything needed to understand decorators. You see, decorators are \u201cwrappers\u201d, which means that they let you execute code before and after the function they decorate without modifying the function itself.they let you execute code before and after the function they decorateHandcrafted decoratorsHow you\u2019d do it manually:# A decorator is a function that expects ANOTHER function as parameter\ndef my_shiny_new_decorator(a_function_to_decorate):\n\n    # Inside, the decorator defines a function on the fly: the wrapper.\n    # This function is going to be wrapped around the original function\n    # so it can execute code before and after it.\n    def the_wrapper_around_the_original_function():\n\n        # Put here the code you want to be executed BEFORE the original function is called\n        print(\"Before the function runs\")\n\n        # Call the function here (using parentheses)\n        a_function_to_decorate()\n\n        # Put here the code you want to be executed AFTER the original function is called\n        print(\"After the function runs\")\n\n    # At this point, \"a_function_to_decorate\" HAS NEVER BEEN EXECUTED.\n    # We return the wrapper function we have just created.\n    # The wrapper contains the function and the code to execute before and after. It\u2019s ready to use!\n    return the_wrapper_around_the_original_function\n\n# Now imagine you create a function you don't want to ever touch again.\ndef a_stand_alone_function():\n    print(\"I am a stand alone function, don't you dare modify me\")\n\na_stand_alone_function() \n#outputs: I am a stand alone function, don't you dare modify me\n\n# Well, you can decorate it to extend its behavior.\n# Just pass it to the decorator, it will wrap it dynamically in \n# any code you want and return you a new function ready to be used:\n\na_stand_alone_function_decorated = my_shiny_new_decorator(a_stand_alone_function)\na_stand_alone_function_decorated()\n#outputs:\n#Before the function runs\n#I am a stand alone function, don't you dare modify me\n#After the function runs\n# A decorator is a function that expects ANOTHER function as parameter\ndef my_shiny_new_decorator(a_function_to_decorate):\n\n    # Inside, the decorator defines a function on the fly: the wrapper.\n    # This function is going to be wrapped around the original function\n    # so it can execute code before and after it.\n    def the_wrapper_around_the_original_function():\n\n        # Put here the code you want to be executed BEFORE the original function is called\n        print(\"Before the function runs\")\n\n        # Call the function here (using parentheses)\n        a_function_to_decorate()\n\n        # Put here the code you want to be executed AFTER the original function is called\n        print(\"After the function runs\")\n\n    # At this point, \"a_function_to_decorate\" HAS NEVER BEEN EXECUTED.\n    # We return the wrapper function we have just created.\n    # The wrapper contains the function and the code to execute before and after. It\u2019s ready to use!\n    return the_wrapper_around_the_original_function\n\n# Now imagine you create a function you don't want to ever touch again.\ndef a_stand_alone_function():\n    print(\"I am a stand alone function, don't you dare modify me\")\n\na_stand_alone_function() \n#outputs: I am a stand alone function, don't you dare modify me\n\n# Well, you can decorate it to extend its behavior.\n# Just pass it to the decorator, it will wrap it dynamically in \n# any code you want and return you a new function ready to be used:\n\na_stand_alone_function_decorated = my_shiny_new_decorator(a_stand_alone_function)\na_stand_alone_function_decorated()\n#outputs:\n#Before the function runs\n#I am a stand alone function, don't you dare modify me\n#After the function runs\nNow, you probably want that every time you call a_stand_alone_function, a_stand_alone_function_decorated is called instead. That\u2019s easy, just overwrite a_stand_alone_function with the function returned by my_shiny_new_decorator:a_stand_alone_functiona_stand_alone_function_decorateda_stand_alone_functionmy_shiny_new_decoratora_stand_alone_function = my_shiny_new_decorator(a_stand_alone_function)\na_stand_alone_function()\n#outputs:\n#Before the function runs\n#I am a stand alone function, don't you dare modify me\n#After the function runs\n\n# That\u2019s EXACTLY what decorators do!\na_stand_alone_function = my_shiny_new_decorator(a_stand_alone_function)\na_stand_alone_function()\n#outputs:\n#Before the function runs\n#I am a stand alone function, don't you dare modify me\n#After the function runs\n\n# That\u2019s EXACTLY what decorators do!\nDecorators demystifiedThe previous example, using the decorator syntax:@my_shiny_new_decorator\ndef another_stand_alone_function():\n    print(\"Leave me alone\")\n\nanother_stand_alone_function()  \n#outputs:  \n#Before the function runs\n#Leave me alone\n#After the function runs\n@my_shiny_new_decorator\ndef another_stand_alone_function():\n    print(\"Leave me alone\")\n\nanother_stand_alone_function()  \n#outputs:  \n#Before the function runs\n#Leave me alone\n#After the function runs\nYes, that\u2019s all, it\u2019s that simple. @decorator is just a shortcut to:@decoratoranother_stand_alone_function = my_shiny_new_decorator(another_stand_alone_function)\nanother_stand_alone_function = my_shiny_new_decorator(another_stand_alone_function)\nDecorators are just a pythonic variant of the decorator design pattern. There are several classic design patterns embedded in Python to ease development (like iterators).decorator design patternOf course, you can accumulate decorators:def bread(func):\n    def wrapper():\n        print(\"</''''''\\>\")\n        func()\n        print(\"<\\______/>\")\n    return wrapper\n\ndef ingredients(func):\n    def wrapper():\n        print(\"#tomatoes#\")\n        func()\n        print(\"~salad~\")\n    return wrapper\n\ndef sandwich(food=\"--ham--\"):\n    print(food)\n\nsandwich()\n#outputs: --ham--\nsandwich = bread(ingredients(sandwich))\nsandwich()\n#outputs:\n#</''''''\\>\n# #tomatoes#\n# --ham--\n# ~salad~\n#<\\______/>\ndef bread(func):\n    def wrapper():\n        print(\"</''''''\\>\")\n        func()\n        print(\"<\\______/>\")\n    return wrapper\n\ndef ingredients(func):\n    def wrapper():\n        print(\"#tomatoes#\")\n        func()\n        print(\"~salad~\")\n    return wrapper\n\ndef sandwich(food=\"--ham--\"):\n    print(food)\n\nsandwich()\n#outputs: --ham--\nsandwich = bread(ingredients(sandwich))\nsandwich()\n#outputs:\n#</''''''\\>\n# #tomatoes#\n# --ham--\n# ~salad~\n#<\\______/>\nUsing the Python decorator syntax:@bread\n@ingredients\ndef sandwich(food=\"--ham--\"):\n    print(food)\n\nsandwich()\n#outputs:\n#</''''''\\>\n# #tomatoes#\n# --ham--\n# ~salad~\n#<\\______/>\n@bread\n@ingredients\ndef sandwich(food=\"--ham--\"):\n    print(food)\n\nsandwich()\n#outputs:\n#</''''''\\>\n# #tomatoes#\n# --ham--\n# ~salad~\n#<\\______/>\nThe order you set the decorators MATTERS:@ingredients\n@bread\ndef strange_sandwich(food=\"--ham--\"):\n    print(food)\n\nstrange_sandwich()\n#outputs:\n##tomatoes#\n#</''''''\\>\n# --ham--\n#<\\______/>\n# ~salad~\n@ingredients\n@bread\ndef strange_sandwich(food=\"--ham--\"):\n    print(food)\n\nstrange_sandwich()\n#outputs:\n##tomatoes#\n#</''''''\\>\n# --ham--\n#<\\______/>\n# ~salad~\nNow: to answer the question...As a conclusion, you can easily see how to answer the question:# The decorator to make it bold\ndef makebold(fn):\n    # The new function the decorator returns\n    def wrapper():\n        # Insertion of some code before and after\n        return \"<b>\" + fn() + \"</b>\"\n    return wrapper\n\n# The decorator to make it italic\ndef makeitalic(fn):\n    # The new function the decorator returns\n    def wrapper():\n        # Insertion of some code before and after\n        return \"<i>\" + fn() + \"</i>\"\n    return wrapper\n\n@makebold\n@makeitalic\ndef say():\n    return \"hello\"\n\nprint(say())\n#outputs: <b><i>hello</i></b>\n\n# This is the exact equivalent to \ndef say():\n    return \"hello\"\nsay = makebold(makeitalic(say))\n\nprint(say())\n#outputs: <b><i>hello</i></b>\n# The decorator to make it bold\ndef makebold(fn):\n    # The new function the decorator returns\n    def wrapper():\n        # Insertion of some code before and after\n        return \"<b>\" + fn() + \"</b>\"\n    return wrapper\n\n# The decorator to make it italic\ndef makeitalic(fn):\n    # The new function the decorator returns\n    def wrapper():\n        # Insertion of some code before and after\n        return \"<i>\" + fn() + \"</i>\"\n    return wrapper\n\n@makebold\n@makeitalic\ndef say():\n    return \"hello\"\n\nprint(say())\n#outputs: <b><i>hello</i></b>\n\n# This is the exact equivalent to \ndef say():\n    return \"hello\"\nsay = makebold(makeitalic(say))\n\nprint(say())\n#outputs: <b><i>hello</i></b>\nYou can now just leave happy, or burn your brain a little bit more and see advanced uses of decorators.Taking decorators to the next levelPassing arguments to the decorated function# It\u2019s not black magic, you just have to let the wrapper \n# pass the argument:\n\ndef a_decorator_passing_arguments(function_to_decorate):\n    def a_wrapper_accepting_arguments(arg1, arg2):\n        print(\"I got args! Look: {0}, {1}\".format(arg1, arg2))\n        function_to_decorate(arg1, arg2)\n    return a_wrapper_accepting_arguments\n\n# Since when you are calling the function returned by the decorator, you are\n# calling the wrapper, passing arguments to the wrapper will let it pass them to \n# the decorated function\n\n@a_decorator_passing_arguments\ndef print_full_name(first_name, last_name):\n    print(\"My name is {0} {1}\".format(first_name, last_name))\n    \nprint_full_name(\"Peter\", \"Venkman\")\n# outputs:\n#I got args! Look: Peter Venkman\n#My name is Peter Venkman\n# It\u2019s not black magic, you just have to let the wrapper \n# pass the argument:\n\ndef a_decorator_passing_arguments(function_to_decorate):\n    def a_wrapper_accepting_arguments(arg1, arg2):\n        print(\"I got args! Look: {0}, {1}\".format(arg1, arg2))\n        function_to_decorate(arg1, arg2)\n    return a_wrapper_accepting_arguments\n\n# Since when you are calling the function returned by the decorator, you are\n# calling the wrapper, passing arguments to the wrapper will let it pass them to \n# the decorated function\n\n@a_decorator_passing_arguments\ndef print_full_name(first_name, last_name):\n    print(\"My name is {0} {1}\".format(first_name, last_name))\n    \nprint_full_name(\"Peter\", \"Venkman\")\n# outputs:\n#I got args! Look: Peter Venkman\n#My name is Peter Venkman\nDecorating methodsOne nifty thing about Python is that methods and functions are really the same.  The only difference is that methods expect that their first argument is a reference to the current object (self).selfThat means you can build a decorator for methods the same way! Just remember to take self into consideration:selfdef method_friendly_decorator(method_to_decorate):\n    def wrapper(self, lie):\n        lie = lie - 3 # very friendly, decrease age even more :-)\n        return method_to_decorate(self, lie)\n    return wrapper\n    \n    \nclass Lucy(object):\n    \n    def __init__(self):\n        self.age = 32\n    \n    @method_friendly_decorator\n    def sayYourAge(self, lie):\n        print(\"I am {0}, what did you think?\".format(self.age + lie))\n        \nl = Lucy()\nl.sayYourAge(-3)\n#outputs: I am 26, what did you think?\ndef method_friendly_decorator(method_to_decorate):\n    def wrapper(self, lie):\n        lie = lie - 3 # very friendly, decrease age even more :-)\n        return method_to_decorate(self, lie)\n    return wrapper\n    \n    \nclass Lucy(object):\n    \n    def __init__(self):\n        self.age = 32\n    \n    @method_friendly_decorator\n    def sayYourAge(self, lie):\n        print(\"I am {0}, what did you think?\".format(self.age + lie))\n        \nl = Lucy()\nl.sayYourAge(-3)\n#outputs: I am 26, what did you think?\nIf you\u2019re making general-purpose decorator--one you\u2019ll apply to any function or method, no matter its arguments--then just use *args, **kwargs:*args, **kwargsdef a_decorator_passing_arbitrary_arguments(function_to_decorate):\n    # The wrapper accepts any arguments\n    def a_wrapper_accepting_arbitrary_arguments(*args, **kwargs):\n        print(\"Do I have args?:\")\n        print(args)\n        print(kwargs)\n        # Then you unpack the arguments, here *args, **kwargs\n        # If you are not familiar with unpacking, check:\n        # http://www.saltycrane.com/blog/2008/01/how-to-use-args-and-kwargs-in-python/\n        function_to_decorate(*args, **kwargs)\n    return a_wrapper_accepting_arbitrary_arguments\n\n@a_decorator_passing_arbitrary_arguments\ndef function_with_no_argument():\n    print(\"Python is cool, no argument here.\")\n\nfunction_with_no_argument()\n#outputs\n#Do I have args?:\n#()\n#{}\n#Python is cool, no argument here.\n\n@a_decorator_passing_arbitrary_arguments\ndef function_with_arguments(a, b, c):\n    print(a, b, c)\n    \nfunction_with_arguments(1,2,3)\n#outputs\n#Do I have args?:\n#(1, 2, 3)\n#{}\n#1 2 3 \n \n@a_decorator_passing_arbitrary_arguments\ndef function_with_named_arguments(a, b, c, platypus=\"Why not ?\"):\n    print(\"Do {0}, {1} and {2} like platypus? {3}\".format(a, b, c, platypus))\n\nfunction_with_named_arguments(\"Bill\", \"Linus\", \"Steve\", platypus=\"Indeed!\")\n#outputs\n#Do I have args ? :\n#('Bill', 'Linus', 'Steve')\n#{'platypus': 'Indeed!'}\n#Do Bill, Linus and Steve like platypus? Indeed!\n\nclass Mary(object):\n    \n    def __init__(self):\n        self.age = 31\n    \n    @a_decorator_passing_arbitrary_arguments\n    def sayYourAge(self, lie=-3): # You can now add a default value\n        print(\"I am {0}, what did you think?\".format(self.age + lie))\n\nm = Mary()\nm.sayYourAge()\n#outputs\n# Do I have args?:\n#(<__main__.Mary object at 0xb7d303ac>,)\n#{}\n#I am 28, what did you think?\ndef a_decorator_passing_arbitrary_arguments(function_to_decorate):\n    # The wrapper accepts any arguments\n    def a_wrapper_accepting_arbitrary_arguments(*args, **kwargs):\n        print(\"Do I have args?:\")\n        print(args)\n        print(kwargs)\n        # Then you unpack the arguments, here *args, **kwargs\n        # If you are not familiar with unpacking, check:\n        # http://www.saltycrane.com/blog/2008/01/how-to-use-args-and-kwargs-in-python/\n        function_to_decorate(*args, **kwargs)\n    return a_wrapper_accepting_arbitrary_arguments\n\n@a_decorator_passing_arbitrary_arguments\ndef function_with_no_argument():\n    print(\"Python is cool, no argument here.\")\n\nfunction_with_no_argument()\n#outputs\n#Do I have args?:\n#()\n#{}\n#Python is cool, no argument here.\n\n@a_decorator_passing_arbitrary_arguments\ndef function_with_arguments(a, b, c):\n    print(a, b, c)\n    \nfunction_with_arguments(1,2,3)\n#outputs\n#Do I have args?:\n#(1, 2, 3)\n#{}\n#1 2 3 \n \n@a_decorator_passing_arbitrary_arguments\ndef function_with_named_arguments(a, b, c, platypus=\"Why not ?\"):\n    print(\"Do {0}, {1} and {2} like platypus? {3}\".format(a, b, c, platypus))\n\nfunction_with_named_arguments(\"Bill\", \"Linus\", \"Steve\", platypus=\"Indeed!\")\n#outputs\n#Do I have args ? :\n#('Bill', 'Linus', 'Steve')\n#{'platypus': 'Indeed!'}\n#Do Bill, Linus and Steve like platypus? Indeed!\n\nclass Mary(object):\n    \n    def __init__(self):\n        self.age = 31\n    \n    @a_decorator_passing_arbitrary_arguments\n    def sayYourAge(self, lie=-3): # You can now add a default value\n        print(\"I am {0}, what did you think?\".format(self.age + lie))\n\nm = Mary()\nm.sayYourAge()\n#outputs\n# Do I have args?:\n#(<__main__.Mary object at 0xb7d303ac>,)\n#{}\n#I am 28, what did you think?\nPassing arguments to the decoratorGreat, now what would you say about passing arguments to the decorator itself?This can get somewhat twisted, since a decorator must accept a function as an argument. Therefore, you cannot pass the decorated function\u2019s arguments directly to the decorator.Before rushing to the solution, let\u2019s write a little reminder:# Decorators are ORDINARY functions\ndef my_decorator(func):\n    print(\"I am an ordinary function\")\n    def wrapper():\n        print(\"I am function returned by the decorator\")\n        func()\n    return wrapper\n\n# Therefore, you can call it without any \"@\"\n\ndef lazy_function():\n    print(\"zzzzzzzz\")\n\ndecorated_function = my_decorator(lazy_function)\n#outputs: I am an ordinary function\n            \n# It outputs \"I am an ordinary function\", because that\u2019s just what you do:\n# calling a function. Nothing magic.\n\n@my_decorator\ndef lazy_function():\n    print(\"zzzzzzzz\")\n    \n#outputs: I am an ordinary function\n# Decorators are ORDINARY functions\ndef my_decorator(func):\n    print(\"I am an ordinary function\")\n    def wrapper():\n        print(\"I am function returned by the decorator\")\n        func()\n    return wrapper\n\n# Therefore, you can call it without any \"@\"\n\ndef lazy_function():\n    print(\"zzzzzzzz\")\n\ndecorated_function = my_decorator(lazy_function)\n#outputs: I am an ordinary function\n            \n# It outputs \"I am an ordinary function\", because that\u2019s just what you do:\n# calling a function. Nothing magic.\n\n@my_decorator\ndef lazy_function():\n    print(\"zzzzzzzz\")\n    \n#outputs: I am an ordinary function\nIt\u2019s exactly the same. \"my_decorator\" is called. So when you @my_decorator, you are telling Python to call the function 'labelled by the variable \"my_decorator\"'.my_decorator@my_decoratormy_decoratorThis is important! The label you give can point directly to the decorator\u2014or not.or notLet\u2019s get evil. \u263adef decorator_maker():\n    \n    print(\"I make decorators! I am executed only once: \"\n          \"when you make me create a decorator.\")\n            \n    def my_decorator(func):\n        \n        print(\"I am a decorator! I am executed only when you decorate a function.\")\n               \n        def wrapped():\n            print(\"I am the wrapper around the decorated function. \"\n                  \"I am called when you call the decorated function. \"\n                  \"As the wrapper, I return the RESULT of the decorated function.\")\n            return func()\n        \n        print(\"As the decorator, I return the wrapped function.\")\n        \n        return wrapped\n    \n    print(\"As a decorator maker, I return a decorator\")\n    return my_decorator\n            \n# Let\u2019s create a decorator. It\u2019s just a new function after all.\nnew_decorator = decorator_maker()       \n#outputs:\n#I make decorators! I am executed only once: when you make me create a decorator.\n#As a decorator maker, I return a decorator\n\n# Then we decorate the function\n            \ndef decorated_function():\n    print(\"I am the decorated function.\")\n   \ndecorated_function = new_decorator(decorated_function)\n#outputs:\n#I am a decorator! I am executed only when you decorate a function.\n#As the decorator, I return the wrapped function\n     \n# Let\u2019s call the function:\ndecorated_function()\n#outputs:\n#I am the wrapper around the decorated function. I am called when you call the decorated function.\n#As the wrapper, I return the RESULT of the decorated function.\n#I am the decorated function.\ndef decorator_maker():\n    \n    print(\"I make decorators! I am executed only once: \"\n          \"when you make me create a decorator.\")\n            \n    def my_decorator(func):\n        \n        print(\"I am a decorator! I am executed only when you decorate a function.\")\n               \n        def wrapped():\n            print(\"I am the wrapper around the decorated function. \"\n                  \"I am called when you call the decorated function. \"\n                  \"As the wrapper, I return the RESULT of the decorated function.\")\n            return func()\n        \n        print(\"As the decorator, I return the wrapped function.\")\n        \n        return wrapped\n    \n    print(\"As a decorator maker, I return a decorator\")\n    return my_decorator\n            \n# Let\u2019s create a decorator. It\u2019s just a new function after all.\nnew_decorator = decorator_maker()       \n#outputs:\n#I make decorators! I am executed only once: when you make me create a decorator.\n#As a decorator maker, I return a decorator\n\n# Then we decorate the function\n            \ndef decorated_function():\n    print(\"I am the decorated function.\")\n   \ndecorated_function = new_decorator(decorated_function)\n#outputs:\n#I am a decorator! I am executed only when you decorate a function.\n#As the decorator, I return the wrapped function\n     \n# Let\u2019s call the function:\ndecorated_function()\n#outputs:\n#I am the wrapper around the decorated function. I am called when you call the decorated function.\n#As the wrapper, I return the RESULT of the decorated function.\n#I am the decorated function.\nNo surprise here.Let\u2019s do EXACTLY the same thing, but skip all the pesky intermediate variables:def decorated_function():\n    print(\"I am the decorated function.\")\ndecorated_function = decorator_maker()(decorated_function)\n#outputs:\n#I make decorators! I am executed only once: when you make me create a decorator.\n#As a decorator maker, I return a decorator\n#I am a decorator! I am executed only when you decorate a function.\n#As the decorator, I return the wrapped function.\n\n# Finally:\ndecorated_function()    \n#outputs:\n#I am the wrapper around the decorated function. I am called when you call the decorated function.\n#As the wrapper, I return the RESULT of the decorated function.\n#I am the decorated function.\ndef decorated_function():\n    print(\"I am the decorated function.\")\ndecorated_function = decorator_maker()(decorated_function)\n#outputs:\n#I make decorators! I am executed only once: when you make me create a decorator.\n#As a decorator maker, I return a decorator\n#I am a decorator! I am executed only when you decorate a function.\n#As the decorator, I return the wrapped function.\n\n# Finally:\ndecorated_function()    \n#outputs:\n#I am the wrapper around the decorated function. I am called when you call the decorated function.\n#As the wrapper, I return the RESULT of the decorated function.\n#I am the decorated function.\nLet\u2019s make it even shorter:even shorter@decorator_maker()\ndef decorated_function():\n    print(\"I am the decorated function.\")\n#outputs:\n#I make decorators! I am executed only once: when you make me create a decorator.\n#As a decorator maker, I return a decorator\n#I am a decorator! I am executed only when you decorate a function.\n#As the decorator, I return the wrapped function.\n\n#Eventually: \ndecorated_function()    \n#outputs:\n#I am the wrapper around the decorated function. I am called when you call the decorated function.\n#As the wrapper, I return the RESULT of the decorated function.\n#I am the decorated function.\n@decorator_maker()\ndef decorated_function():\n    print(\"I am the decorated function.\")\n#outputs:\n#I make decorators! I am executed only once: when you make me create a decorator.\n#As a decorator maker, I return a decorator\n#I am a decorator! I am executed only when you decorate a function.\n#As the decorator, I return the wrapped function.\n\n#Eventually: \ndecorated_function()    \n#outputs:\n#I am the wrapper around the decorated function. I am called when you call the decorated function.\n#As the wrapper, I return the RESULT of the decorated function.\n#I am the decorated function.\nHey, did you see that? We used a function call with the \"@\" syntax! :-)@So, back to decorators with arguments. If we can use functions to generate the decorator on the fly, we can pass arguments to that function, right?def decorator_maker_with_arguments(decorator_arg1, decorator_arg2):\n    \n    print(\"I make decorators! And I accept arguments: {0}, {1}\".format(decorator_arg1, decorator_arg2))\n            \n    def my_decorator(func):\n        # The ability to pass arguments here is a gift from closures.\n        # If you are not comfortable with closures, you can assume it\u2019s ok,\n        # or read: https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python\n        print(\"I am the decorator. Somehow you passed me arguments: {0}, {1}\".format(decorator_arg1, decorator_arg2))\n               \n        # Don't confuse decorator arguments and function arguments!\n        def wrapped(function_arg1, function_arg2) :\n            print(\"I am the wrapper around the decorated function.\\n\"\n                  \"I can access all the variables\\n\"\n                  \"\\t- from the decorator: {0} {1}\\n\"\n                  \"\\t- from the function call: {2} {3}\\n\"\n                  \"Then I can pass them to the decorated function\"\n                  .format(decorator_arg1, decorator_arg2,\n                          function_arg1, function_arg2))\n            return func(function_arg1, function_arg2)\n        \n        return wrapped\n    \n    return my_decorator\n\n@decorator_maker_with_arguments(\"Leonard\", \"Sheldon\")\ndef decorated_function_with_arguments(function_arg1, function_arg2):\n    print(\"I am the decorated function and only knows about my arguments: {0}\"\n           \" {1}\".format(function_arg1, function_arg2))\n          \ndecorated_function_with_arguments(\"Rajesh\", \"Howard\")\n#outputs:\n#I make decorators! And I accept arguments: Leonard Sheldon\n#I am the decorator. Somehow you passed me arguments: Leonard Sheldon\n#I am the wrapper around the decorated function. \n#I can access all the variables \n#   - from the decorator: Leonard Sheldon \n#   - from the function call: Rajesh Howard \n#Then I can pass them to the decorated function\n#I am the decorated function and only knows about my arguments: Rajesh Howard\ndef decorator_maker_with_arguments(decorator_arg1, decorator_arg2):\n    \n    print(\"I make decorators! And I accept arguments: {0}, {1}\".format(decorator_arg1, decorator_arg2))\n            \n    def my_decorator(func):\n        # The ability to pass arguments here is a gift from closures.\n        # If you are not comfortable with closures, you can assume it\u2019s ok,\n        # or read: https://stackoverflow.com/questions/13857/can-you-explain-closures-as-they-relate-to-python\n        print(\"I am the decorator. Somehow you passed me arguments: {0}, {1}\".format(decorator_arg1, decorator_arg2))\n               \n        # Don't confuse decorator arguments and function arguments!\n        def wrapped(function_arg1, function_arg2) :\n            print(\"I am the wrapper around the decorated function.\\n\"\n                  \"I can access all the variables\\n\"\n                  \"\\t- from the decorator: {0} {1}\\n\"\n                  \"\\t- from the function call: {2} {3}\\n\"\n                  \"Then I can pass them to the decorated function\"\n                  .format(decorator_arg1, decorator_arg2,\n                          function_arg1, function_arg2))\n            return func(function_arg1, function_arg2)\n        \n        return wrapped\n    \n    return my_decorator\n\n@decorator_maker_with_arguments(\"Leonard\", \"Sheldon\")\ndef decorated_function_with_arguments(function_arg1, function_arg2):\n    print(\"I am the decorated function and only knows about my arguments: {0}\"\n           \" {1}\".format(function_arg1, function_arg2))\n          \ndecorated_function_with_arguments(\"Rajesh\", \"Howard\")\n#outputs:\n#I make decorators! And I accept arguments: Leonard Sheldon\n#I am the decorator. Somehow you passed me arguments: Leonard Sheldon\n#I am the wrapper around the decorated function. \n#I can access all the variables \n#   - from the decorator: Leonard Sheldon \n#   - from the function call: Rajesh Howard \n#Then I can pass them to the decorated function\n#I am the decorated function and only knows about my arguments: Rajesh Howard\nHere it is: a decorator with arguments. Arguments can be set as variable:c1 = \"Penny\"\nc2 = \"Leslie\"\n\n@decorator_maker_with_arguments(\"Leonard\", c1)\ndef decorated_function_with_arguments(function_arg1, function_arg2):\n    print(\"I am the decorated function and only knows about my arguments:\"\n           \" {0} {1}\".format(function_arg1, function_arg2))\n\ndecorated_function_with_arguments(c2, \"Howard\")\n#outputs:\n#I make decorators! And I accept arguments: Leonard Penny\n#I am the decorator. Somehow you passed me arguments: Leonard Penny\n#I am the wrapper around the decorated function. \n#I can access all the variables \n#   - from the decorator: Leonard Penny \n#   - from the function call: Leslie Howard \n#Then I can pass them to the decorated function\n#I am the decorated function and only know about my arguments: Leslie Howard\nc1 = \"Penny\"\nc2 = \"Leslie\"\n\n@decorator_maker_with_arguments(\"Leonard\", c1)\ndef decorated_function_with_arguments(function_arg1, function_arg2):\n    print(\"I am the decorated function and only knows about my arguments:\"\n           \" {0} {1}\".format(function_arg1, function_arg2))\n\ndecorated_function_with_arguments(c2, \"Howard\")\n#outputs:\n#I make decorators! And I accept arguments: Leonard Penny\n#I am the decorator. Somehow you passed me arguments: Leonard Penny\n#I am the wrapper around the decorated function. \n#I can access all the variables \n#   - from the decorator: Leonard Penny \n#   - from the function call: Leslie Howard \n#Then I can pass them to the decorated function\n#I am the decorated function and only know about my arguments: Leslie Howard\nAs you can see, you can pass arguments to the decorator like any function using this trick. You can even use *args, **kwargs if you wish. But remember decorators are called only once. Just when Python imports the script. You can't dynamically set the arguments afterwards. When you do \"import x\", the function is already decorated, so you can't\nchange anything.*args, **kwargsonly oncethe function is already decoratedLet\u2019s practice: decorating a decoratorOkay, as a bonus, I'll give you a snippet to make any decorator accept generically any argument. After all, in order to accept arguments, we created our decorator using another function.We wrapped the decorator.Anything else we saw recently that wrapped function?Oh yes, decorators!Let\u2019s have some fun and write a decorator for the decorators:def decorator_with_args(decorator_to_enhance):\n    \"\"\" \n    This function is supposed to be used as a decorator.\n    It must decorate an other function, that is intended to be used as a decorator.\n    Take a cup of coffee.\n    It will allow any decorator to accept an arbitrary number of arguments,\n    saving you the headache to remember how to do that every time.\n    \"\"\"\n    \n    # We use the same trick we did to pass arguments\n    def decorator_maker(*args, **kwargs):\n       \n        # We create on the fly a decorator that accepts only a function\n        # but keeps the passed arguments from the maker.\n        def decorator_wrapper(func):\n       \n            # We return the result of the original decorator, which, after all, \n            # IS JUST AN ORDINARY FUNCTION (which returns a function).\n            # Only pitfall: the decorator must have this specific signature or it won't work:\n            return decorator_to_enhance(func, *args, **kwargs)\n        \n        return decorator_wrapper\n    \n    return decorator_maker\n       \ndef decorator_with_args(decorator_to_enhance):\n    \"\"\" \n    This function is supposed to be used as a decorator.\n    It must decorate an other function, that is intended to be used as a decorator.\n    Take a cup of coffee.\n    It will allow any decorator to accept an arbitrary number of arguments,\n    saving you the headache to remember how to do that every time.\n    \"\"\"\n    \n    # We use the same trick we did to pass arguments\n    def decorator_maker(*args, **kwargs):\n       \n        # We create on the fly a decorator that accepts only a function\n        # but keeps the passed arguments from the maker.\n        def decorator_wrapper(func):\n       \n            # We return the result of the original decorator, which, after all, \n            # IS JUST AN ORDINARY FUNCTION (which returns a function).\n            # Only pitfall: the decorator must have this specific signature or it won't work:\n            return decorator_to_enhance(func, *args, **kwargs)\n        \n        return decorator_wrapper\n    \n    return decorator_maker\n       \nIt can be used as follows:# You create the function you will use as a decorator. And stick a decorator on it :-)\n# Don't forget, the signature is \"decorator(func, *args, **kwargs)\"\n@decorator_with_args \ndef decorated_decorator(func, *args, **kwargs): \n    def wrapper(function_arg1, function_arg2):\n        print(\"Decorated with {0} {1}\".format(args, kwargs))\n        return func(function_arg1, function_arg2)\n    return wrapper\n    \n# Then you decorate the functions you wish with your brand new decorated decorator.\n\n@decorated_decorator(42, 404, 1024)\ndef decorated_function(function_arg1, function_arg2):\n    print(\"Hello {0} {1}\".format(function_arg1, function_arg2))\n\ndecorated_function(\"Universe and\", \"everything\")\n#outputs:\n#Decorated with (42, 404, 1024) {}\n#Hello Universe and everything\n\n# Whoooot!\n# You create the function you will use as a decorator. And stick a decorator on it :-)\n# Don't forget, the signature is \"decorator(func, *args, **kwargs)\"\n@decorator_with_args \ndef decorated_decorator(func, *args, **kwargs): \n    def wrapper(function_arg1, function_arg2):\n        print(\"Decorated with {0} {1}\".format(args, kwargs))\n        return func(function_arg1, function_arg2)\n    return wrapper\n    \n# Then you decorate the functions you wish with your brand new decorated decorator.\n\n@decorated_decorator(42, 404, 1024)\ndef decorated_function(function_arg1, function_arg2):\n    print(\"Hello {0} {1}\".format(function_arg1, function_arg2))\n\ndecorated_function(\"Universe and\", \"everything\")\n#outputs:\n#Decorated with (42, 404, 1024) {}\n#Hello Universe and everything\n\n# Whoooot!\nI know, the last time you had this feeling, it was after listening a guy saying: \"before understanding recursion, you must first understand recursion\". But now, don't you feel good about mastering this?Best practices: decorators\nDecorators were introduced in Python 2.4, so be sure your code will be run on >= 2.4.\nDecorators slow down the function call. Keep that in mind.\nYou cannot un-decorate a function. (There are hacks to create decorators that can be removed, but nobody uses them.) So once a function is decorated, it\u2019s decorated for all the code.\nDecorators wrap functions, which can make them hard to debug.  (This gets better from Python >= 2.5; see below.)\nDecorators were introduced in Python 2.4, so be sure your code will be run on >= 2.4.Decorators slow down the function call. Keep that in mind.You cannot un-decorate a function. (There are hacks to create decorators that can be removed, but nobody uses them.) So once a function is decorated, it\u2019s decorated for all the code.You cannot un-decorate a function.arefor all the codeDecorators wrap functions, which can make them hard to debug.  (This gets better from Python >= 2.5; see below.)The functools module was introduced in Python 2.5. It includes the function functools.wraps(), which copies the name, module, and docstring of the decorated function to its wrapper.functoolsfunctools.wraps()(Fun fact: functools.wraps() is a decorator! \u263a)functools.wraps()# For debugging, the stacktrace prints you the function __name__\ndef foo():\n    print(\"foo\")\n    \nprint(foo.__name__)\n#outputs: foo\n    \n# With a decorator, it gets messy    \ndef bar(func):\n    def wrapper():\n        print(\"bar\")\n        return func()\n    return wrapper\n\n@bar\ndef foo():\n    print(\"foo\")\n\nprint(foo.__name__)\n#outputs: wrapper\n\n# \"functools\" can help for that\n\nimport functools\n\ndef bar(func):\n    # We say that \"wrapper\", is wrapping \"func\"\n    # and the magic begins\n    @functools.wraps(func)\n    def wrapper():\n        print(\"bar\")\n        return func()\n    return wrapper\n\n@bar\ndef foo():\n    print(\"foo\")\n\nprint(foo.__name__)\n#outputs: foo\n# For debugging, the stacktrace prints you the function __name__\ndef foo():\n    print(\"foo\")\n    \nprint(foo.__name__)\n#outputs: foo\n    \n# With a decorator, it gets messy    \ndef bar(func):\n    def wrapper():\n        print(\"bar\")\n        return func()\n    return wrapper\n\n@bar\ndef foo():\n    print(\"foo\")\n\nprint(foo.__name__)\n#outputs: wrapper\n\n# \"functools\" can help for that\n\nimport functools\n\ndef bar(func):\n    # We say that \"wrapper\", is wrapping \"func\"\n    # and the magic begins\n    @functools.wraps(func)\n    def wrapper():\n        print(\"bar\")\n        return func()\n    return wrapper\n\n@bar\ndef foo():\n    print(\"foo\")\n\nprint(foo.__name__)\n#outputs: foo\nHow can the decorators be useful?Now the big question: What can I use decorators for?Now the big question:Seem cool and powerful, but a practical example would be great. Well, there are 1000 possibilities. Classic uses are extending a function behavior from an external lib (you can't modify it), or for debugging (you don't want to modify it because it\u2019s temporary).You can use them to extend several functions in a DRY\u2019s way, like so:def benchmark(func):\n    \"\"\"\n    A decorator that prints the time a function takes\n    to execute.\n    \"\"\"\n    import time\n    def wrapper(*args, **kwargs):\n        t = time.clock()\n        res = func(*args, **kwargs)\n        print(\"{0} {1}\".format(func.__name__, time.clock()-t))\n        return res\n    return wrapper\n\n\ndef logging(func):\n    \"\"\"\n    A decorator that logs the activity of the script.\n    (it actually just prints it, but it could be logging!)\n    \"\"\"\n    def wrapper(*args, **kwargs):\n        res = func(*args, **kwargs)\n        print(\"{0} {1} {2}\".format(func.__name__, args, kwargs))\n        return res\n    return wrapper\n\n\ndef counter(func):\n    \"\"\"\n    A decorator that counts and prints the number of times a function has been executed\n    \"\"\"\n    def wrapper(*args, **kwargs):\n        wrapper.count = wrapper.count + 1\n        res = func(*args, **kwargs)\n        print(\"{0} has been used: {1}x\".format(func.__name__, wrapper.count))\n        return res\n    wrapper.count = 0\n    return wrapper\n\n@counter\n@benchmark\n@logging\ndef reverse_string(string):\n    return str(reversed(string))\n\nprint(reverse_string(\"Able was I ere I saw Elba\"))\nprint(reverse_string(\"A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!\"))\n\n#outputs:\n#reverse_string ('Able was I ere I saw Elba',) {}\n#wrapper 0.0\n#wrapper has been used: 1x \n#ablE was I ere I saw elbA\n#reverse_string ('A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!',) {}\n#wrapper 0.0\n#wrapper has been used: 2x\n#!amanaP :lanac a ,noep a ,stah eros ,raj a ,hsac ,oloR a ,tur a ,mapS ,snip ,eperc a ,)lemac a ro( niaga gab ananab a ,gat a ,nat a ,gab ananab a ,gag a ,inoracam ,elacrep ,epins ,spam ,arutaroloc a ,shajar ,soreh ,atsap ,eonac a ,nalp a ,nam A\ndef benchmark(func):\n    \"\"\"\n    A decorator that prints the time a function takes\n    to execute.\n    \"\"\"\n    import time\n    def wrapper(*args, **kwargs):\n        t = time.clock()\n        res = func(*args, **kwargs)\n        print(\"{0} {1}\".format(func.__name__, time.clock()-t))\n        return res\n    return wrapper\n\n\ndef logging(func):\n    \"\"\"\n    A decorator that logs the activity of the script.\n    (it actually just prints it, but it could be logging!)\n    \"\"\"\n    def wrapper(*args, **kwargs):\n        res = func(*args, **kwargs)\n        print(\"{0} {1} {2}\".format(func.__name__, args, kwargs))\n        return res\n    return wrapper\n\n\ndef counter(func):\n    \"\"\"\n    A decorator that counts and prints the number of times a function has been executed\n    \"\"\"\n    def wrapper(*args, **kwargs):\n        wrapper.count = wrapper.count + 1\n        res = func(*args, **kwargs)\n        print(\"{0} has been used: {1}x\".format(func.__name__, wrapper.count))\n        return res\n    wrapper.count = 0\n    return wrapper\n\n@counter\n@benchmark\n@logging\ndef reverse_string(string):\n    return str(reversed(string))\n\nprint(reverse_string(\"Able was I ere I saw Elba\"))\nprint(reverse_string(\"A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!\"))\n\n#outputs:\n#reverse_string ('Able was I ere I saw Elba',) {}\n#wrapper 0.0\n#wrapper has been used: 1x \n#ablE was I ere I saw elbA\n#reverse_string ('A man, a plan, a canoe, pasta, heros, rajahs, a coloratura, maps, snipe, percale, macaroni, a gag, a banana bag, a tan, a tag, a banana bag again (or a camel), a crepe, pins, Spam, a rut, a Rolo, cash, a jar, sore hats, a peon, a canal: Panama!',) {}\n#wrapper 0.0\n#wrapper has been used: 2x\n#!amanaP :lanac a ,noep a ,stah eros ,raj a ,hsac ,oloR a ,tur a ,mapS ,snip ,eperc a ,)lemac a ro( niaga gab ananab a ,gat a ,nat a ,gab ananab a ,gag a ,inoracam ,elacrep ,epins ,spam ,arutaroloc a ,shajar ,soreh ,atsap ,eonac a ,nalp a ,nam A\nOf course the good thing with decorators is that you can use them right away on almost anything without rewriting. DRY, I said:@counter\n@benchmark\n@logging\ndef get_random_futurama_quote():\n    from urllib import urlopen\n    result = urlopen(\"http://subfusion.net/cgi-bin/quote.pl?quote=futurama\").read()\n    try:\n        value = result.split(\"<br><b><hr><br>\")[1].split(\"<br><br><hr>\")[0]\n        return value.strip()\n    except:\n        return \"No, I'm ... doesn't!\"\n\n    \nprint(get_random_futurama_quote())\nprint(get_random_futurama_quote())\n\n#outputs:\n#get_random_futurama_quote () {}\n#wrapper 0.02\n#wrapper has been used: 1x\n#The laws of science be a harsh mistress.\n#get_random_futurama_quote () {}\n#wrapper 0.01\n#wrapper has been used: 2x\n#Curse you, merciful Poseidon!\n@counter\n@benchmark\n@logging\ndef get_random_futurama_quote():\n    from urllib import urlopen\n    result = urlopen(\"http://subfusion.net/cgi-bin/quote.pl?quote=futurama\").read()\n    try:\n        value = result.split(\"<br><b><hr><br>\")[1].split(\"<br><br><hr>\")[0]\n        return value.strip()\n    except:\n        return \"No, I'm ... doesn't!\"\n\n    \nprint(get_random_futurama_quote())\nprint(get_random_futurama_quote())\n\n#outputs:\n#get_random_futurama_quote () {}\n#wrapper 0.02\n#wrapper has been used: 1x\n#The laws of science be a harsh mistress.\n#get_random_futurama_quote () {}\n#wrapper 0.01\n#wrapper has been used: 2x\n#Curse you, merciful Poseidon!\nPython itself provides several decorators: property, staticmethod, etc.propertystaticmethod\nDjango uses decorators to manage caching and view permissions.\nTwisted to fake inlining asynchronous functions calls.\nDjango uses decorators to manage caching and view permissions.Twisted to fake inlining asynchronous functions calls.This really is a large playground.",
                "Check out the documentation to see how decorators work. Here is what you asked for:the documentationfrom functools import wraps\n\ndef makebold(fn):\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        return \"<b>\" + fn(*args, **kwargs) + \"</b>\"\n    return wrapper\n\ndef makeitalic(fn):\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        return \"<i>\" + fn(*args, **kwargs) + \"</i>\"\n    return wrapper\n\n@makebold\n@makeitalic\ndef hello():\n    return \"hello world\"\n\n@makebold\n@makeitalic\ndef log(s):\n    return s\n\nprint hello()        # returns \"<b><i>hello world</i></b>\"\nprint hello.__name__ # with functools.wraps() this returns \"hello\"\nprint log('hello')   # returns \"<b><i>hello</i></b>\"\nfrom functools import wraps\n\ndef makebold(fn):\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        return \"<b>\" + fn(*args, **kwargs) + \"</b>\"\n    return wrapper\n\ndef makeitalic(fn):\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        return \"<i>\" + fn(*args, **kwargs) + \"</i>\"\n    return wrapper\n\n@makebold\n@makeitalic\ndef hello():\n    return \"hello world\"\n\n@makebold\n@makeitalic\ndef log(s):\n    return s\n\nprint hello()        # returns \"<b><i>hello world</i></b>\"\nprint hello.__name__ # with functools.wraps() this returns \"hello\"\nprint log('hello')   # returns \"<b><i>hello</i></b>\"\n",
                "Alternatively, you could write a factory function which return a decorator which wraps the return value of the decorated function in a tag passed to the factory function. For example:from functools import wraps\n\ndef wrap_in_tag(tag):\n    def factory(func):\n        @wraps(func)\n        def decorator():\n            return '<%(tag)s>%(rv)s</%(tag)s>' % (\n                {'tag': tag, 'rv': func()})\n        return decorator\n    return factory\nfrom functools import wraps\n\ndef wrap_in_tag(tag):\n    def factory(func):\n        @wraps(func)\n        def decorator():\n            return '<%(tag)s>%(rv)s</%(tag)s>' % (\n                {'tag': tag, 'rv': func()})\n        return decorator\n    return factory\nThis enables you to write:@wrap_in_tag('b')\n@wrap_in_tag('i')\ndef say():\n    return 'hello'\n@wrap_in_tag('b')\n@wrap_in_tag('i')\ndef say():\n    return 'hello'\normakebold = wrap_in_tag('b')\nmakeitalic = wrap_in_tag('i')\n\n@makebold\n@makeitalic\ndef say():\n    return 'hello'\nmakebold = wrap_in_tag('b')\nmakeitalic = wrap_in_tag('i')\n\n@makebold\n@makeitalic\ndef say():\n    return 'hello'\nPersonally I would have written the decorator somewhat differently:from functools import wraps\n\ndef wrap_in_tag(tag):\n    def factory(func):\n        @wraps(func)\n        def decorator(val):\n            return func('<%(tag)s>%(val)s</%(tag)s>' %\n                        {'tag': tag, 'val': val})\n        return decorator\n    return factory\nfrom functools import wraps\n\ndef wrap_in_tag(tag):\n    def factory(func):\n        @wraps(func)\n        def decorator(val):\n            return func('<%(tag)s>%(val)s</%(tag)s>' %\n                        {'tag': tag, 'val': val})\n        return decorator\n    return factory\nwhich would yield:@wrap_in_tag('b')\n@wrap_in_tag('i')\ndef say(val):\n    return val\nsay('hello')\n@wrap_in_tag('b')\n@wrap_in_tag('i')\ndef say(val):\n    return val\nsay('hello')\nDon't forget the construction for which decorator syntax is a shorthand:say = wrap_in_tag('b')(wrap_in_tag('i')(say)))\nsay = wrap_in_tag('b')(wrap_in_tag('i')(say)))\n",
                "Decorators are just syntactical sugar.This@decorator\ndef func():\n    ...\n@decorator\ndef func():\n    ...\nexpands todef func():\n    ...\nfunc = decorator(func)\ndef func():\n    ...\nfunc = decorator(func)\n",
                "And of course you can return lambdas as well from a decorator function:def makebold(f): \n    return lambda: \"<b>\" + f() + \"</b>\"\ndef makeitalic(f): \n    return lambda: \"<i>\" + f() + \"</i>\"\n\n@makebold\n@makeitalic\ndef say():\n    return \"Hello\"\n\nprint say()\ndef makebold(f): \n    return lambda: \"<b>\" + f() + \"</b>\"\ndef makeitalic(f): \n    return lambda: \"<i>\" + f() + \"</i>\"\n\n@makebold\n@makeitalic\ndef say():\n    return \"Hello\"\n\nprint say()\n",
                "Python decorators add extra functionality to another functionAn italics decorator could be likedef makeitalic(fn):\n    def newFunc():\n        return \"<i>\" + fn() + \"</i>\"\n    return newFunc\ndef makeitalic(fn):\n    def newFunc():\n        return \"<i>\" + fn() + \"</i>\"\n    return newFunc\nNote that a function is defined inside a function.\nWhat it basically does is replace a function with the newly defined one. For example, I have this classclass foo:\n    def bar(self):\n        print \"hi\"\n    def foobar(self):\n        print \"hi again\"\nclass foo:\n    def bar(self):\n        print \"hi\"\n    def foobar(self):\n        print \"hi again\"\nNow say, I want both functions to print \"---\" after and before they are done.\nI could add a print \"---\" before and after each print statement.\nBut because I don't like repeating myself, I will make a decoratordef addDashes(fn): # notice it takes a function as an argument\n    def newFunction(self): # define a new function\n        print \"---\"\n        fn(self) # call the original function\n        print \"---\"\n    return newFunction\n    # Return the newly defined function - it will \"replace\" the original\ndef addDashes(fn): # notice it takes a function as an argument\n    def newFunction(self): # define a new function\n        print \"---\"\n        fn(self) # call the original function\n        print \"---\"\n    return newFunction\n    # Return the newly defined function - it will \"replace\" the original\nSo now I can change my class to class foo:\n    @addDashes\n    def bar(self):\n        print \"hi\"\n\n    @addDashes\n    def foobar(self):\n        print \"hi again\"\nclass foo:\n    @addDashes\n    def bar(self):\n        print \"hi\"\n\n    @addDashes\n    def foobar(self):\n        print \"hi again\"\nFor more on decorators, check\nhttp://www.ibm.com/developerworks/linux/library/l-cpdecor.htmlhttp://www.ibm.com/developerworks/linux/library/l-cpdecor.html",
                "You could make two separate decorators that do what you want as illustrated directly below. Note the use of *args, **kwargs in the declaration of the wrapped() function which supports the decorated function having multiple arguments (which isn't really necessary for the example say() function, but is included for generality).could*args, **kwargswrapped()say()For similar reasons, the functools.wraps decorator is used to change the meta attributes of the wrapped function to be those of the one being decorated. This makes error messages and embedded function documentation (func.__doc__) be those of the decorated function instead of wrapped()'s.functools.wrapsfunc.__doc__wrapped()from functools import wraps\n\ndef makebold(fn):\n    @wraps(fn)\n    def wrapped(*args, **kwargs):\n        return \"<b>\" + fn(*args, **kwargs) + \"</b>\"\n    return wrapped\n\ndef makeitalic(fn):\n    @wraps(fn)\n    def wrapped(*args, **kwargs):\n        return \"<i>\" + fn(*args, **kwargs) + \"</i>\"\n    return wrapped\n\n@makebold\n@makeitalic\ndef say():\n    return 'Hello'\n\nprint(say())  # -> <b><i>Hello</i></b>\nfrom functools import wraps\n\ndef makebold(fn):\n    @wraps(fn)\n    def wrapped(*args, **kwargs):\n        return \"<b>\" + fn(*args, **kwargs) + \"</b>\"\n    return wrapped\n\ndef makeitalic(fn):\n    @wraps(fn)\n    def wrapped(*args, **kwargs):\n        return \"<i>\" + fn(*args, **kwargs) + \"</i>\"\n    return wrapped\n\n@makebold\n@makeitalic\ndef say():\n    return 'Hello'\n\nprint(say())  # -> <b><i>Hello</i></b>\nRefinementsAs you can see there's a lot of duplicate code in these two decorators. Given this similarity it would be better for you to instead make a generic one that was actually a decorator factory\u2014in other words, a decorator function that makes other decorators. That way there would be less code repetition\u2014and allow the DRY principle to be followed.decorator factoryDRYdef html_deco(tag):\n    def decorator(fn):\n        @wraps(fn)\n        def wrapped(*args, **kwargs):\n            return '<%s>' % tag + fn(*args, **kwargs) + '</%s>' % tag\n        return wrapped\n    return decorator\n\n@html_deco('b')\n@html_deco('i')\ndef greet(whom=''):\n    return 'Hello' + (' ' + whom) if whom else ''\n\nprint(greet('world'))  # -> <b><i>Hello world</i></b>\ndef html_deco(tag):\n    def decorator(fn):\n        @wraps(fn)\n        def wrapped(*args, **kwargs):\n            return '<%s>' % tag + fn(*args, **kwargs) + '</%s>' % tag\n        return wrapped\n    return decorator\n\n@html_deco('b')\n@html_deco('i')\ndef greet(whom=''):\n    return 'Hello' + (' ' + whom) if whom else ''\n\nprint(greet('world'))  # -> <b><i>Hello world</i></b>\nTo make the code more readable, you can assign a more descriptive name to the factory-generated decorators:makebold = html_deco('b')\nmakeitalic = html_deco('i')\n\n@makebold\n@makeitalic\ndef greet(whom=''):\n    return 'Hello' + (' ' + whom) if whom else ''\n\nprint(greet('world'))  # -> <b><i>Hello world</i></b>\nmakebold = html_deco('b')\nmakeitalic = html_deco('i')\n\n@makebold\n@makeitalic\ndef greet(whom=''):\n    return 'Hello' + (' ' + whom) if whom else ''\n\nprint(greet('world'))  # -> <b><i>Hello world</i></b>\nor even combine them like this:makebolditalic = lambda fn: makebold(makeitalic(fn))\n\n@makebolditalic\ndef greet(whom=''):\n    return 'Hello' + (' ' + whom) if whom else ''\n\nprint(greet('world'))  # -> <b><i>Hello world</i></b>\nmakebolditalic = lambda fn: makebold(makeitalic(fn))\n\n@makebolditalic\ndef greet(whom=''):\n    return 'Hello' + (' ' + whom) if whom else ''\n\nprint(greet('world'))  # -> <b><i>Hello world</i></b>\nEfficiencyWhile the above examples do all work, the code generated involves a fair amount of overhead in the form of extraneous function calls when multiple decorators are applied at once. This may not matter, depending the exact usage (which might be I/O-bound, for instance).If speed of the decorated function is important, the overhead can be kept to a single extra function call by writing a slightly different decorator factory-function which implements adding all the tags at once, so it can generate code that avoids the addtional function calls incurred by using separate decorators for each tag.This requires more code in the decorator itself, but this only runs when it's being applied to function definitions, not later when they themselves are called. This also applies when creating more readable names by using lambda functions as previously illustrated. Sample:lambdadef multi_html_deco(*tags):\n    start_tags, end_tags = [], []\n    for tag in tags:\n        start_tags.append('<%s>' % tag)\n        end_tags.append('</%s>' % tag)\n    start_tags = ''.join(start_tags)\n    end_tags = ''.join(reversed(end_tags))\n\n    def decorator(fn):\n        @wraps(fn)\n        def wrapped(*args, **kwargs):\n            return start_tags + fn(*args, **kwargs) + end_tags\n        return wrapped\n    return decorator\n\nmakebolditalic = multi_html_deco('b', 'i')\n\n@makebolditalic\ndef greet(whom=''):\n    return 'Hello' + (' ' + whom) if whom else ''\n\nprint(greet('world'))  # -> <b><i>Hello world</i></b>\ndef multi_html_deco(*tags):\n    start_tags, end_tags = [], []\n    for tag in tags:\n        start_tags.append('<%s>' % tag)\n        end_tags.append('</%s>' % tag)\n    start_tags = ''.join(start_tags)\n    end_tags = ''.join(reversed(end_tags))\n\n    def decorator(fn):\n        @wraps(fn)\n        def wrapped(*args, **kwargs):\n            return start_tags + fn(*args, **kwargs) + end_tags\n        return wrapped\n    return decorator\n\nmakebolditalic = multi_html_deco('b', 'i')\n\n@makebolditalic\ndef greet(whom=''):\n    return 'Hello' + (' ' + whom) if whom else ''\n\nprint(greet('world'))  # -> <b><i>Hello world</i></b>\n",
                "Another way of doing the same thing:class bol(object):\n  def __init__(self, f):\n    self.f = f\n  def __call__(self):\n    return \"<b>{}</b>\".format(self.f())\n\nclass ita(object):\n  def __init__(self, f):\n    self.f = f\n  def __call__(self):\n    return \"<i>{}</i>\".format(self.f())\n\n@bol\n@ita\ndef sayhi():\n  return 'hi'\nclass bol(object):\n  def __init__(self, f):\n    self.f = f\n  def __call__(self):\n    return \"<b>{}</b>\".format(self.f())\n\nclass ita(object):\n  def __init__(self, f):\n    self.f = f\n  def __call__(self):\n    return \"<i>{}</i>\".format(self.f())\n\n@bol\n@ita\ndef sayhi():\n  return 'hi'\nOr, more flexibly:class sty(object):\n  def __init__(self, tag):\n    self.tag = tag\n  def __call__(self, f):\n    def newf():\n      return \"<{tag}>{res}</{tag}>\".format(res=f(), tag=self.tag)\n    return newf\n\n@sty('b')\n@sty('i')\ndef sayhi():\n  return 'hi'\nclass sty(object):\n  def __init__(self, tag):\n    self.tag = tag\n  def __call__(self, f):\n    def newf():\n      return \"<{tag}>{res}</{tag}>\".format(res=f(), tag=self.tag)\n    return newf\n\n@sty('b')\n@sty('i')\ndef sayhi():\n  return 'hi'\n",
                "\nHow can I make two decorators in Python that would do the following?\nHow can I make two decorators in Python that would do the following?You want the following function, when called:\n@makebold\n@makeitalic\ndef say():\n    return \"Hello\"\n\n@makebold\n@makeitalic\ndef say():\n    return \"Hello\"\n@makebold\n@makeitalic\ndef say():\n    return \"Hello\"\nTo return:\n<b><i>Hello</i></b>\n\n<b><i>Hello</i></b>\n<b><i>Hello</i></b>\nSimple solutionTo most simply do this, make decorators that return lambdas (anonymous functions) that close over the function (closures) and call it:def makeitalic(fn):\n    return lambda: '<i>' + fn() + '</i>'\n\ndef makebold(fn):\n    return lambda: '<b>' + fn() + '</b>'\ndef makeitalic(fn):\n    return lambda: '<i>' + fn() + '</i>'\n\ndef makebold(fn):\n    return lambda: '<b>' + fn() + '</b>'\nNow use them as desired:@makebold\n@makeitalic\ndef say():\n    return 'Hello'\n@makebold\n@makeitalic\ndef say():\n    return 'Hello'\nand now:>>> say()\n'<b><i>Hello</i></b>'\n>>> say()\n'<b><i>Hello</i></b>'\nProblems with the simple solutionBut we seem to have nearly lost the original function. >>> say\n<function <lambda> at 0x4ACFA070>\n>>> say\n<function <lambda> at 0x4ACFA070>\nTo find it, we'd need to dig into the closure of each lambda, one of which is buried in the other:>>> say.__closure__[0].cell_contents\n<function <lambda> at 0x4ACFA030>\n>>> say.__closure__[0].cell_contents.__closure__[0].cell_contents\n<function say at 0x4ACFA730>\n>>> say.__closure__[0].cell_contents\n<function <lambda> at 0x4ACFA030>\n>>> say.__closure__[0].cell_contents.__closure__[0].cell_contents\n<function say at 0x4ACFA730>\nSo if we put documentation on this function, or wanted to be able to decorate functions that take more than one argument, or we just wanted to know what function we were looking at in a debugging session, we need to do a bit more with our wrapper.Full featured solution - overcoming most of these problemsWe have the decorator wraps from the functools module in the standard library! wrapsfunctoolsfrom functools import wraps\n\ndef makeitalic(fn):\n    # must assign/update attributes from wrapped function to wrapper\n    # __module__, __name__, __doc__, and __dict__ by default\n    @wraps(fn) # explicitly give function whose attributes it is applying\n    def wrapped(*args, **kwargs):\n        return '<i>' + fn(*args, **kwargs) + '</i>'\n    return wrapped\n\ndef makebold(fn):\n    @wraps(fn)\n    def wrapped(*args, **kwargs):\n        return '<b>' + fn(*args, **kwargs) + '</b>'\n    return wrapped\nfrom functools import wraps\n\ndef makeitalic(fn):\n    # must assign/update attributes from wrapped function to wrapper\n    # __module__, __name__, __doc__, and __dict__ by default\n    @wraps(fn) # explicitly give function whose attributes it is applying\n    def wrapped(*args, **kwargs):\n        return '<i>' + fn(*args, **kwargs) + '</i>'\n    return wrapped\n\ndef makebold(fn):\n    @wraps(fn)\n    def wrapped(*args, **kwargs):\n        return '<b>' + fn(*args, **kwargs) + '</b>'\n    return wrapped\nIt is unfortunate that there's still some boilerplate, but this is about as simple as we can make it. In Python 3, you also get __qualname__ and __annotations__ assigned by default.__qualname____annotations__So now:@makebold\n@makeitalic\ndef say():\n    \"\"\"This function returns a bolded, italicized 'hello'\"\"\"\n    return 'Hello'\n@makebold\n@makeitalic\ndef say():\n    \"\"\"This function returns a bolded, italicized 'hello'\"\"\"\n    return 'Hello'\nAnd now:>>> say\n<function say at 0x14BB8F70>\n>>> help(say)\nHelp on function say in module __main__:\n\nsay(*args, **kwargs)\n    This function returns a bolded, italicized 'hello'\n>>> say\n<function say at 0x14BB8F70>\n>>> help(say)\nHelp on function say in module __main__:\n\nsay(*args, **kwargs)\n    This function returns a bolded, italicized 'hello'\nConclusionSo we see that wraps makes the wrapping function do almost everything except tell us exactly what the function takes as arguments. wrapsThere are other modules that may attempt to tackle the problem, but the solution is not yet in the standard library.",
                "A decorator takes the function definition and creates a new function that executes this function and transforms the result.@deco\ndef do():\n    ...\n@deco\ndef do():\n    ...\nis equivalent to:do = deco(do)\ndo = deco(do)\nExample:def deco(func):\n    def inner(letter):\n        return func(letter).upper()  #upper\n    return inner\ndef deco(func):\n    def inner(letter):\n        return func(letter).upper()  #upper\n    return inner\nThis@deco\ndef do(number):\n    return chr(number)  # number to letter\n@deco\ndef do(number):\n    return chr(number)  # number to letter\nis equivalent to thisdef do2(number):\n    return chr(number)\n\ndo2 = deco(do2)\ndef do2(number):\n    return chr(number)\n\ndo2 = deco(do2)\n65 <=> 'a'print(do(65))\nprint(do2(65))\n>>> B\n>>> B\nprint(do(65))\nprint(do2(65))\n>>> B\n>>> B\nTo understand the decorator, it is important to notice, that decorator created a new function do which is inner that executes function and transforms the result.",
                "This answer has long been answered, but I thought I would share my Decorator class which makes writing new decorators easy and compact.from abc import ABCMeta, abstractclassmethod\n\nclass Decorator(metaclass=ABCMeta):\n    \"\"\" Acts as a base class for all decorators \"\"\"\n\n    def __init__(self):\n        self.method = None\n\n    def __call__(self, method):\n        self.method = method\n        return self.call\n\n    @abstractclassmethod\n    def call(self, *args, **kwargs):\n        return self.method(*args, **kwargs)\nfrom abc import ABCMeta, abstractclassmethod\n\nclass Decorator(metaclass=ABCMeta):\n    \"\"\" Acts as a base class for all decorators \"\"\"\n\n    def __init__(self):\n        self.method = None\n\n    def __call__(self, method):\n        self.method = method\n        return self.call\n\n    @abstractclassmethod\n    def call(self, *args, **kwargs):\n        return self.method(*args, **kwargs)\nFor one I think this makes the behavior of decorators very clear, but it also makes it easy to define new decorators very concisely. For the example listed above, you could then solve it as:class MakeBold(Decorator):\n    def call():\n        return \"<b>\" + self.method() + \"</b>\"\n\nclass MakeItalic(Decorator):\n    def call():\n        return \"<i>\" + self.method() + \"</i>\"\n\n@MakeBold()\n@MakeItalic()\ndef say():\n   return \"Hello\"\nclass MakeBold(Decorator):\n    def call():\n        return \"<b>\" + self.method() + \"</b>\"\n\nclass MakeItalic(Decorator):\n    def call():\n        return \"<i>\" + self.method() + \"</i>\"\n\n@MakeBold()\n@MakeItalic()\ndef say():\n   return \"Hello\"\nYou could also use it to do more complex tasks, like for instance a decorator which automatically makes the function get applied recursively to all arguments in an iterator:class ApplyRecursive(Decorator):\n    def __init__(self, *types):\n        super().__init__()\n        if not len(types):\n            types = (dict, list, tuple, set)\n        self._types = types\n\n    def call(self, arg):\n        if dict in self._types and isinstance(arg, dict):\n            return {key: self.call(value) for key, value in arg.items()}\n\n        if set in self._types and isinstance(arg, set):\n            return set(self.call(value) for value in arg)\n\n        if tuple in self._types and isinstance(arg, tuple):\n            return tuple(self.call(value) for value in arg)\n\n        if list in self._types and isinstance(arg, list):\n            return list(self.call(value) for value in arg)\n\n        return self.method(arg)\n\n\n@ApplyRecursive(tuple, set, dict)\ndef double(arg):\n    return 2*arg\n\nprint(double(1))\nprint(double({'a': 1, 'b': 2}))\nprint(double({1, 2, 3}))\nprint(double((1, 2, 3, 4)))\nprint(double([1, 2, 3, 4, 5]))\nclass ApplyRecursive(Decorator):\n    def __init__(self, *types):\n        super().__init__()\n        if not len(types):\n            types = (dict, list, tuple, set)\n        self._types = types\n\n    def call(self, arg):\n        if dict in self._types and isinstance(arg, dict):\n            return {key: self.call(value) for key, value in arg.items()}\n\n        if set in self._types and isinstance(arg, set):\n            return set(self.call(value) for value in arg)\n\n        if tuple in self._types and isinstance(arg, tuple):\n            return tuple(self.call(value) for value in arg)\n\n        if list in self._types and isinstance(arg, list):\n            return list(self.call(value) for value in arg)\n\n        return self.method(arg)\n\n\n@ApplyRecursive(tuple, set, dict)\ndef double(arg):\n    return 2*arg\n\nprint(double(1))\nprint(double({'a': 1, 'b': 2}))\nprint(double({1, 2, 3}))\nprint(double((1, 2, 3, 4)))\nprint(double([1, 2, 3, 4, 5]))\nWhich prints:2\n{'a': 2, 'b': 4}\n{2, 4, 6}\n(2, 4, 6, 8)\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n2\n{'a': 2, 'b': 4}\n{2, 4, 6}\n(2, 4, 6, 8)\n[1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\nNotice that this example didn't include the list type in the instantiation of the decorator, so in the final print statement the method gets applied to the list itself, not the elements of the list.list",
                "#decorator.py\ndef makeHtmlTag(tag, *args, **kwds):\n    def real_decorator(fn):\n        css_class = \" class='{0}'\".format(kwds[\"css_class\"]) \\\n                                 if \"css_class\" in kwds else \"\"\n        def wrapped(*args, **kwds):\n            return \"<\"+tag+css_class+\">\" + fn(*args, **kwds) + \"</\"+tag+\">\"\n        return wrapped\n    # return decorator dont call it\n    return real_decorator\n\n@makeHtmlTag(tag=\"b\", css_class=\"bold_css\")\n@makeHtmlTag(tag=\"i\", css_class=\"italic_css\")\ndef hello():\n    return \"hello world\"\n\nprint hello()\n#decorator.py\ndef makeHtmlTag(tag, *args, **kwds):\n    def real_decorator(fn):\n        css_class = \" class='{0}'\".format(kwds[\"css_class\"]) \\\n                                 if \"css_class\" in kwds else \"\"\n        def wrapped(*args, **kwds):\n            return \"<\"+tag+css_class+\">\" + fn(*args, **kwds) + \"</\"+tag+\">\"\n        return wrapped\n    # return decorator dont call it\n    return real_decorator\n\n@makeHtmlTag(tag=\"b\", css_class=\"bold_css\")\n@makeHtmlTag(tag=\"i\", css_class=\"italic_css\")\ndef hello():\n    return \"hello world\"\n\nprint hello()\nYou can also write decorator in Class#class.py\nclass makeHtmlTagClass(object):\n    def __init__(self, tag, css_class=\"\"):\n        self._tag = tag\n        self._css_class = \" class='{0}'\".format(css_class) \\\n                                       if css_class != \"\" else \"\"\n\n    def __call__(self, fn):\n        def wrapped(*args, **kwargs):\n            return \"<\" + self._tag + self._css_class+\">\"  \\\n                       + fn(*args, **kwargs) + \"</\" + self._tag + \">\"\n        return wrapped\n\n@makeHtmlTagClass(tag=\"b\", css_class=\"bold_css\")\n@makeHtmlTagClass(tag=\"i\", css_class=\"italic_css\")\ndef hello(name):\n    return \"Hello, {}\".format(name)\n\nprint hello(\"Your name\")\n#class.py\nclass makeHtmlTagClass(object):\n    def __init__(self, tag, css_class=\"\"):\n        self._tag = tag\n        self._css_class = \" class='{0}'\".format(css_class) \\\n                                       if css_class != \"\" else \"\"\n\n    def __call__(self, fn):\n        def wrapped(*args, **kwargs):\n            return \"<\" + self._tag + self._css_class+\">\"  \\\n                       + fn(*args, **kwargs) + \"</\" + self._tag + \">\"\n        return wrapped\n\n@makeHtmlTagClass(tag=\"b\", css_class=\"bold_css\")\n@makeHtmlTagClass(tag=\"i\", css_class=\"italic_css\")\ndef hello(name):\n    return \"Hello, {}\".format(name)\n\nprint hello(\"Your name\")\n",
                "Here is a simple example of chaining decorators.  Note the last line - it shows what is going on under the covers.############################################################\n#\n#    decorators\n#\n############################################################\n\ndef bold(fn):\n    def decorate():\n        # surround with bold tags before calling original function\n        return \"<b>\" + fn() + \"</b>\"\n    return decorate\n\n\ndef uk(fn):\n    def decorate():\n        # swap month and day\n        fields = fn().split('/')\n        date = fields[1] + \"/\" + fields[0] + \"/\" + fields[2]\n        return date\n    return decorate\n\nimport datetime\ndef getDate():\n    now = datetime.datetime.now()\n    return \"%d/%d/%d\" % (now.day, now.month, now.year)\n\n@bold\ndef getBoldDate(): \n    return getDate()\n\n@uk\ndef getUkDate():\n    return getDate()\n\n@bold\n@uk\ndef getBoldUkDate():\n    return getDate()\n\n\nprint getDate()\nprint getBoldDate()\nprint getUkDate()\nprint getBoldUkDate()\n# what is happening under the covers\nprint bold(uk(getDate))()\n############################################################\n#\n#    decorators\n#\n############################################################\n\ndef bold(fn):\n    def decorate():\n        # surround with bold tags before calling original function\n        return \"<b>\" + fn() + \"</b>\"\n    return decorate\n\n\ndef uk(fn):\n    def decorate():\n        # swap month and day\n        fields = fn().split('/')\n        date = fields[1] + \"/\" + fields[0] + \"/\" + fields[2]\n        return date\n    return decorate\n\nimport datetime\ndef getDate():\n    now = datetime.datetime.now()\n    return \"%d/%d/%d\" % (now.day, now.month, now.year)\n\n@bold\ndef getBoldDate(): \n    return getDate()\n\n@uk\ndef getUkDate():\n    return getDate()\n\n@bold\n@uk\ndef getBoldUkDate():\n    return getDate()\n\n\nprint getDate()\nprint getBoldDate()\nprint getUkDate()\nprint getBoldUkDate()\n# what is happening under the covers\nprint bold(uk(getDate))()\nThe output looks like:17/6/2013\n<b>17/6/2013</b>\n6/17/2013\n<b>6/17/2013</b>\n<b>6/17/2013</b>\n17/6/2013\n<b>17/6/2013</b>\n6/17/2013\n<b>6/17/2013</b>\n<b>6/17/2013</b>\n",
                "Speaking of the counter example - as given above, the counter will be shared between all functions that use the decorator:def counter(func):\n    def wrapped(*args, **kws):\n        print 'Called #%i' % wrapped.count\n        wrapped.count += 1\n        return func(*args, **kws)\n    wrapped.count = 0\n    return wrapped\ndef counter(func):\n    def wrapped(*args, **kws):\n        print 'Called #%i' % wrapped.count\n        wrapped.count += 1\n        return func(*args, **kws)\n    wrapped.count = 0\n    return wrapped\nThat way, your decorator can be reused for different functions (or used to decorate the same function multiple times: func_counter1 = counter(func); func_counter2 = counter(func)), and the counter variable will remain private to each. func_counter1 = counter(func); func_counter2 = counter(func)",
                "Decorate functions with different number of arguments:def frame_tests(fn):\n    def wrapper(*args):\n        print \"\\nStart: %s\" %(fn.__name__)\n        fn(*args)\n        print \"End: %s\\n\" %(fn.__name__)\n    return wrapper\n\n@frame_tests\ndef test_fn1():\n    print \"This is only a test!\"\n\n@frame_tests\ndef test_fn2(s1):\n    print \"This is only a test! %s\" %(s1)\n\n@frame_tests\ndef test_fn3(s1, s2):\n    print \"This is only a test! %s %s\" %(s1, s2)\n\nif __name__ == \"__main__\":\n    test_fn1()\n    test_fn2('OK!')\n    test_fn3('OK!', 'Just a test!')\ndef frame_tests(fn):\n    def wrapper(*args):\n        print \"\\nStart: %s\" %(fn.__name__)\n        fn(*args)\n        print \"End: %s\\n\" %(fn.__name__)\n    return wrapper\n\n@frame_tests\ndef test_fn1():\n    print \"This is only a test!\"\n\n@frame_tests\ndef test_fn2(s1):\n    print \"This is only a test! %s\" %(s1)\n\n@frame_tests\ndef test_fn3(s1, s2):\n    print \"This is only a test! %s %s\" %(s1, s2)\n\nif __name__ == \"__main__\":\n    test_fn1()\n    test_fn2('OK!')\n    test_fn3('OK!', 'Just a test!')\nResult:Start: test_fn1  \nThis is only a test!  \nEnd: test_fn1  \n  \n  \nStart: test_fn2  \nThis is only a test! OK!  \nEnd: test_fn2  \n  \n  \nStart: test_fn3  \nThis is only a test! OK! Just a test!  \nEnd: test_fn3  \nStart: test_fn1  \nThis is only a test!  \nEnd: test_fn1  \n  \n  \nStart: test_fn2  \nThis is only a test! OK!  \nEnd: test_fn2  \n  \n  \nStart: test_fn3  \nThis is only a test! OK! Just a test!  \nEnd: test_fn3  \n",
                "Paolo Bergantino's answer has the great advantage of only using the stdlib, and works for this simple example where there are no decorator arguments nor decorated function arguments. Paolo Bergantino's answerdecoratordecorated functionHowever it has 3 major limitations if you want to tackle more general cases:\nas already noted in several answers, you can not easily modify the code to add optional decorator arguments. For example creating a makestyle(style='bold') decorator is non-trivial.\nbesides, wrappers created with @functools.wraps do not preserve the signature, so if bad arguments are provided they will start executing, and might raise a different kind of error than the usual TypeError.\nfinally, it is quite difficult in wrappers created with @functools.wraps to access an argument based on its name. Indeed the argument can appear in *args, in **kwargs, or may not appear at all (if it is optional).\nas already noted in several answers, you can not easily modify the code to add optional decorator arguments. For example creating a makestyle(style='bold') decorator is non-trivial.add optional decorator argumentsmakestyle(style='bold')besides, wrappers created with @functools.wraps do not preserve the signature, so if bad arguments are provided they will start executing, and might raise a different kind of error than the usual TypeError.@functools.wrapsdo not preserve the signatureTypeErrorfinally, it is quite difficult in wrappers created with @functools.wraps to access an argument based on its name. Indeed the argument can appear in *args, in **kwargs, or may not appear at all (if it is optional).@functools.wrapsaccess an argument based on its name*args**kwargsI wrote decopatch to solve the first issue, and wrote makefun.wraps to solve the other two. Note that makefun leverages the same trick than the famous decorator lib.decopatchdecopatchmakefun.wrapsmakefun.wrapsmakefundecoratordecoratorThis is how you would create a decorator with arguments, returning truly signature-preserving wrappers:from decopatch import function_decorator, DECORATED\nfrom makefun import wraps\n\n@function_decorator\ndef makestyle(st='b', fn=DECORATED):\n    open_tag = \"<%s>\" % st\n    close_tag = \"</%s>\" % st\n\n    @wraps(fn)\n    def wrapped(*args, **kwargs):\n        return open_tag + fn(*args, **kwargs) + close_tag\n\n    return wrapped\nfrom decopatch import function_decorator, DECORATED\nfrom makefun import wraps\n\n@function_decorator\ndef makestyle(st='b', fn=DECORATED):\n    open_tag = \"<%s>\" % st\n    close_tag = \"</%s>\" % st\n\n    @wraps(fn)\n    def wrapped(*args, **kwargs):\n        return open_tag + fn(*args, **kwargs) + close_tag\n\n    return wrapped\ndecopatch provides you with two other development styles that hide or show the various python concepts, depending on your preferences. The most compact style is the following:decopatchfrom decopatch import function_decorator, WRAPPED, F_ARGS, F_KWARGS\n\n@function_decorator\ndef makestyle(st='b', fn=WRAPPED, f_args=F_ARGS, f_kwargs=F_KWARGS):\n    open_tag = \"<%s>\" % st\n    close_tag = \"</%s>\" % st\n    return open_tag + fn(*f_args, **f_kwargs) + close_tag\nfrom decopatch import function_decorator, WRAPPED, F_ARGS, F_KWARGS\n\n@function_decorator\ndef makestyle(st='b', fn=WRAPPED, f_args=F_ARGS, f_kwargs=F_KWARGS):\n    open_tag = \"<%s>\" % st\n    close_tag = \"</%s>\" % st\n    return open_tag + fn(*f_args, **f_kwargs) + close_tag\nIn both cases you can check that the decorator works as expected:@makestyle\n@makestyle('i')\ndef hello(who):\n    return \"hello %s\" % who\n\nassert hello('world') == '<b><i>hello world</i></b>'    \n@makestyle\n@makestyle('i')\ndef hello(who):\n    return \"hello %s\" % who\n\nassert hello('world') == '<b><i>hello world</i></b>'    \nPlease refer to the documentation for details.documentation",
                "I add a case when you need to add custom parameters in decorator, pass it to final function and then work it with.the very decorators:def jwt_or_redirect(fn):\n  @wraps(fn)\n  def decorator(*args, **kwargs):\n    ...\n    return fn(*args, **kwargs)\n  return decorator\n\ndef jwt_refresh(fn):\n  @wraps(fn)\n  def decorator(*args, **kwargs):\n    ...\n    new_kwargs = {'refreshed_jwt': 'xxxxx-xxxxxx'}\n    new_kwargs.update(kwargs)\n    return fn(*args, **new_kwargs)\n  return decorator\ndef jwt_or_redirect(fn):\n  @wraps(fn)\n  def decorator(*args, **kwargs):\n    ...\n    return fn(*args, **kwargs)\n  return decorator\n\ndef jwt_refresh(fn):\n  @wraps(fn)\n  def decorator(*args, **kwargs):\n    ...\n    new_kwargs = {'refreshed_jwt': 'xxxxx-xxxxxx'}\n    new_kwargs.update(kwargs)\n    return fn(*args, **new_kwargs)\n  return decorator\nand the final function:@app.route('/')\n@jwt_or_redirect\n@jwt_refresh\ndef home_page(*args, **kwargs):\n  return kwargs['refreched_jwt']\n@app.route('/')\n@jwt_or_redirect\n@jwt_refresh\ndef home_page(*args, **kwargs):\n  return kwargs['refreched_jwt']\n",
                "Yet another example of nested decorators for plotting an image:import matplotlib.pylab as plt\n\ndef remove_axis(func):\n    def inner(img, alpha):\n        plt.axis('off')\n        func(img, alpha)\n    return inner\n\ndef plot_gray(func):\n    def inner(img, alpha):\n        plt.gray()\n        func(img, alpha)\n    return inner\n\n@remove_axis\n@plot_gray\ndef plot_image(img, alpha):\n    plt.imshow(img, alpha=alpha)\n    plt.show()\nimport matplotlib.pylab as plt\n\ndef remove_axis(func):\n    def inner(img, alpha):\n        plt.axis('off')\n        func(img, alpha)\n    return inner\n\ndef plot_gray(func):\n    def inner(img, alpha):\n        plt.gray()\n        func(img, alpha)\n    return inner\n\n@remove_axis\n@plot_gray\ndef plot_image(img, alpha):\n    plt.imshow(img, alpha=alpha)\n    plt.show()\nNow, let's show a color image first without axis labels using the nested decorators:plot_image(plt.imread('lena_color.jpg'), 0.4)\nplot_image(plt.imread('lena_color.jpg'), 0.4)\nNext, let's show a gray scale image without axis labels using the nested decorators remove_axis and plot_gray (we need to cmap='gray', otherwise the default colormap is viridis, so a grayscale image is by default not displayed in black and white shades, unless explicitly specified)remove_axisplot_graycmap='gray'viridisplot_image(plt.imread('lena_bw.jpg'), 0.8)\nplot_image(plt.imread('lena_bw.jpg'), 0.8)\nThe above function call reduces down to the following nested callremove_axis(plot_gray(plot_image))(img, alpha)\nremove_axis(plot_gray(plot_image))(img, alpha)\n",
                "With make_bold() and make_italic() below:make_bold()make_italic()def make_bold(func):\n    def core(*args, **kwargs):\n        result = func(*args, **kwargs)\n        return \"<b>\" + result + \"</b>\"\n    return core\n\ndef make_italic(func):\n    def core(*args, **kwargs):\n        result = func(*args, **kwargs)\n        return \"<i>\" + result + \"</i>\"\n    return core\ndef make_bold(func):\n    def core(*args, **kwargs):\n        result = func(*args, **kwargs)\n        return \"<b>\" + result + \"</b>\"\n    return core\n\ndef make_italic(func):\n    def core(*args, **kwargs):\n        result = func(*args, **kwargs)\n        return \"<i>\" + result + \"</i>\"\n    return core\nYou can use them as decorators with say() as shown below:say()@make_bold\n@make_italic\ndef say():\n   return \"Hello\"\n\nprint(say())\n@make_bold\n@make_italic\ndef say():\n   return \"Hello\"\n\nprint(say())\nOutput:<b><i>Hello</i></b>\n<b><i>Hello</i></b>\nAnd of course, you can directly use make_bold() and make_italic() without decorators as shown below:make_bold()make_italic()def say():\n    return \"Hello\"\n    \nf1 = make_italic(say)\nf2 = make_bold(f1)\nresult = f2()\nprint(result)\ndef say():\n    return \"Hello\"\n    \nf1 = make_italic(say)\nf2 = make_bold(f1)\nresult = f2()\nprint(result)\nIn short:def say():\n    return \"Hello\"\n    \nresult = make_bold(make_italic(say))()\nprint(result)\ndef say():\n    return \"Hello\"\n    \nresult = make_bold(make_italic(say))()\nprint(result)\nOutput:<b><i>Hello</i></b>\n<b><i>Hello</i></b>\n",
                "Consider the following decorator, note that we are returning the wrapper() function as an objectdef make_bold(func):\n    def wrapper():\n        return '<b>'+func()+'</b>'\n    return wrapper\ndef make_bold(func):\n    def wrapper():\n        return '<b>'+func()+'</b>'\n    return wrapper\nSo This@make_bold\ndef say():\n    return \"Hello\"\n@make_bold\ndef say():\n    return \"Hello\"\nevaluates to thisx = make_bold(say)\nx = make_bold(say)\nNote that x is not the say() but the wrapper object that calls say() internally. That is how decorator works. It always returns the wrapper object which calls the actual function.\nIn case of chaining this@make_italic\n@make_bold\ndef say():\n    return \"Hello\"\n@make_italic\n@make_bold\ndef say():\n    return \"Hello\"\ngets converted to thisx = make_bold(say)\ny = make_italic(x)\nx = make_bold(say)\ny = make_italic(x)\nBelow is the complete codedef make_italic(func):\n    def wrapper():\n        return '<i>'+func()+'</i>'\n    return wrapper\n\n\ndef make_bold(func):\n    def wrapper():\n        return '<b>'+func()+'</b>'\n    return wrapper\n\n\n@make_italic\n@make_bold\ndef say():\n    return \"Hello\"\n\n\nif __name__ == '__main__':\n    # x = make_bold(say) When you wrap say with make_bold decorator\n    # y = make_italic(x) When you also add make_italic as part of chaining\n    # print(y())\n    print(say())\n\n\ndef make_italic(func):\n    def wrapper():\n        return '<i>'+func()+'</i>'\n    return wrapper\n\n\ndef make_bold(func):\n    def wrapper():\n        return '<b>'+func()+'</b>'\n    return wrapper\n\n\n@make_italic\n@make_bold\ndef say():\n    return \"Hello\"\n\n\nif __name__ == '__main__':\n    # x = make_bold(say) When you wrap say with make_bold decorator\n    # y = make_italic(x) When you also add make_italic as part of chaining\n    # print(y())\n    print(say())\n\n\nThe above code will return<i><b>Hello</b></i>\n\n<i><b>Hello</b></i>\n\nHope this helps"
            ]
        },
        {
            "tag": "Matplotlib",
            "patterns": [
                "How do I change the size of figures drawn with Matplotlib?",
                "how can you change the size of the figures?",
                "how do i alter the size of figures drawn using matplotlib?",
                "how do i alter the size of the figures drawn with matplotlib?",
                "how do i change the size of figures drawn using matplotlib?",
                "how do i change the size of the figures drawn with matplotlib?",
                "how do i change the size of figures drawn in matplotlib?"
            ],
            "responses": [
                "figure tells you the call signature:figurefigurefrom matplotlib.pyplot import figure\n\nfigure(figsize=(8, 6), dpi=80)\nfrom matplotlib.pyplot import figure\n\nfigure(figsize=(8, 6), dpi=80)\nfigure(figsize=(1,1)) would create an inch-by-inch image, which would be 80-by-80 pixels unless you also give a different dpi argument.figure(figsize=(1,1))",
                "If you've already got the figure created, you can use figure.set_size_inches to adjust the figure size:figure.set_size_inchesfigure.set_size_inchesfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(18.5, 10.5)\nfig.savefig('test2png.png', dpi=100)\nfig = matplotlib.pyplot.gcf()\nfig.set_size_inches(18.5, 10.5)\nfig.savefig('test2png.png', dpi=100)\nTo propagate the size change to an existing GUI window, add forward=True:forward=Truefig.set_size_inches(18.5, 10.5, forward=True)\nfig.set_size_inches(18.5, 10.5, forward=True)\nAdditionally as Erik Shilts mentioned in the comments you can also use figure.set_dpi to \"[s]et the resolution of the figure in dots-per-inch\"Erik Shiltsfigure.set_dpifigure.set_dpifig.set_dpi(100)\nfig.set_dpi(100)\n",
                "Using plt.rcParamsThere is also this workaround in case you want to change the size without using the figure environment. So in case you are using plt.plot() for example, you can set a tuple with width and height.plt.plot()plt.plot()import matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (20,3)\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (20,3)\nThis is very useful when you plot inline (e.g., with IPython Notebook). As asmaier noticed, it is preferable to not put this statement in the same cell of the imports statements.IPython Notebookasmaier noticedTo reset the global figure size back to default for subsequent plots:plt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\nplt.rcParams[\"figure.figsize\"] = plt.rcParamsDefault[\"figure.figsize\"]\nConversion to cmThe figsize tuple accepts inches, so if you want to set it in centimetres you have to divide them by 2.54. Have a look at this question.figsizethis question",
                "\nDeprecation note:\nAs per the official Matplotlib guide, usage of the pylab module is no longer recommended. Please consider using the matplotlib.pyplot module instead, as described by this other answer.\nDeprecation note:\nAs per the official Matplotlib guide, usage of the pylab module is no longer recommended. Please consider using the matplotlib.pyplot module instead, as described by this other answer.Deprecation note:official Matplotlib guidepylabmatplotlib.pyplotthis other answerThe following seems to work:from pylab import rcParams\nrcParams['figure.figsize'] = 5, 10\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 5, 10\nThis makes the figure's width 5 inches, and its height 10 inches.inchesThe Figure class then uses this as the default value for one of its arguments.",
                "In case you're looking for a way to change the figure size in Pandas, you could do:Pandasdf['some_column'].plot(figsize=(10, 5))\ndf['some_column'].plot(figsize=(10, 5))\nwhere df is a Pandas dataframe. Or, to use an existing figure or axes:dffig, ax = plt.subplots(figsize=(10, 5))\ndf['some_column'].plot(ax=ax)\nfig, ax = plt.subplots(figsize=(10, 5))\ndf['some_column'].plot(ax=ax)\nIf you want to change the default settings, you could do the following:import matplotlib\n\nmatplotlib.rc('figure', figsize=(10, 5))\nimport matplotlib\n\nmatplotlib.rc('figure', figsize=(10, 5))\nFor more details, check out the docs: pd.DataFrame.plot.pd.DataFrame.plotpd.DataFrame.plot",
                "The first link in Google for 'matplotlib figure size' is AdjustingImageSize (Google cache of the page).'matplotlib figure size'AdjustingImageSizeGoogle cache of the pageHere's a test script from the above page. It creates test[1-3].png files of different sizes of the same image:test[1-3].png#!/usr/bin/env python\n\"\"\"\nThis is a small demo file that helps teach how to adjust figure sizes\nfor matplotlib\n\n\"\"\"\n\nimport matplotlib\nprint \"using MPL version:\", matplotlib.__version__\nmatplotlib.use(\"WXAgg\") # do this before pylab so you don'tget the default back end.\n\nimport pylab\nimport numpy as np\n\n# Generate and plot some simple data:\nx = np.arange(0, 2*np.pi, 0.1)\ny = np.sin(x)\n\npylab.plot(x,y)\nF = pylab.gcf()\n\n# Now check everything with the defaults:\nDPI = F.get_dpi()\nprint \"DPI:\", DPI\nDefaultSize = F.get_size_inches()\nprint \"Default size in Inches\", DefaultSize\nprint \"Which should result in a %i x %i Image\"%(DPI*DefaultSize[0], DPI*DefaultSize[1])\n# the default is 100dpi for savefig:\nF.savefig(\"test1.png\")\n# this gives me a 797 x 566 pixel image, which is about 100 DPI\n\n# Now make the image twice as big, while keeping the fonts and all the\n# same size\nF.set_size_inches( (DefaultSize[0]*2, DefaultSize[1]*2) )\nSize = F.get_size_inches()\nprint \"Size in Inches\", Size\nF.savefig(\"test2.png\")\n# this results in a 1595x1132 image\n\n# Now make the image twice as big, making all the fonts and lines\n# bigger too.\n\nF.set_size_inches( DefaultSize )# resetthe size\nSize = F.get_size_inches()\nprint \"Size in Inches\", Size\nF.savefig(\"test3.png\", dpi = (200)) # change the dpi\n# this also results in a 1595x1132 image, but the fonts are larger.\n#!/usr/bin/env python\n\"\"\"\nThis is a small demo file that helps teach how to adjust figure sizes\nfor matplotlib\n\n\"\"\"\n\nimport matplotlib\nprint \"using MPL version:\", matplotlib.__version__\nmatplotlib.use(\"WXAgg\") # do this before pylab so you don'tget the default back end.\n\nimport pylab\nimport numpy as np\n\n# Generate and plot some simple data:\nx = np.arange(0, 2*np.pi, 0.1)\ny = np.sin(x)\n\npylab.plot(x,y)\nF = pylab.gcf()\n\n# Now check everything with the defaults:\nDPI = F.get_dpi()\nprint \"DPI:\", DPI\nDefaultSize = F.get_size_inches()\nprint \"Default size in Inches\", DefaultSize\nprint \"Which should result in a %i x %i Image\"%(DPI*DefaultSize[0], DPI*DefaultSize[1])\n# the default is 100dpi for savefig:\nF.savefig(\"test1.png\")\n# this gives me a 797 x 566 pixel image, which is about 100 DPI\n\n# Now make the image twice as big, while keeping the fonts and all the\n# same size\nF.set_size_inches( (DefaultSize[0]*2, DefaultSize[1]*2) )\nSize = F.get_size_inches()\nprint \"Size in Inches\", Size\nF.savefig(\"test2.png\")\n# this results in a 1595x1132 image\n\n# Now make the image twice as big, making all the fonts and lines\n# bigger too.\n\nF.set_size_inches( DefaultSize )# resetthe size\nSize = F.get_size_inches()\nprint \"Size in Inches\", Size\nF.savefig(\"test3.png\", dpi = (200)) # change the dpi\n# this also results in a 1595x1132 image, but the fonts are larger.\nOutput:using MPL version: 0.98.1\nDPI: 80\nDefault size in Inches [ 8.  6.]\nWhich should result in a 640 x 480 Image\nSize in Inches [ 16.  12.]\nSize in Inches [ 16.  12.]\nusing MPL version: 0.98.1\nDPI: 80\nDefault size in Inches [ 8.  6.]\nWhich should result in a 640 x 480 Image\nSize in Inches [ 16.  12.]\nSize in Inches [ 16.  12.]\nTwo notes:\nThe module comments and the actual output differ.\n\nThis answer allows easily to combine all three images in one image file to see the difference in sizes.\n\nThe module comments and the actual output differ.\nThe module comments and the actual output differ.This answer allows easily to combine all three images in one image file to see the difference in sizes.\nThis answer allows easily to combine all three images in one image file to see the difference in sizes.This answer",
                "You can simply use (from matplotlib.figure.Figure):matplotlib.figure.Figurefig.set_size_inches(width,height)\nfig.set_size_inches(width,height)\nAs of Matplotlib 2.0.0, changes to your canvas will be visible immediately, as the forward keyword defaults to True.forwarddefaults to TrueTrueIf you want to just change the width or height instead of both, you can use change the width or heightorfig.set_figwidth(val) or fig.set_figheight(val)fig.set_figwidth(val)fig.set_figheight(val)These will also immediately update your canvas, but only in Matplotlib 2.2.0 and newer.For Older VersionsYou need to specify forward=True explicitly in order to live-update your canvas in versions older than what is specified above. Note that the set_figwidth and set_figheight functions don\u2019t support the forward parameter in versions older than Matplotlib 1.5.0.forward=Trueset_figwidthset_figheightforward",
                "Try commenting out the fig = ... linefig = ...import numpy as np\nimport matplotlib.pyplot as plt\n\nN = 50\nx = np.random.rand(N)\ny = np.random.rand(N)\narea = np.pi * (15 * np.random.rand(N))**2\n\nfig = plt.figure(figsize=(18, 18))\nplt.scatter(x, y, s=area, alpha=0.5)\nplt.show()\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nN = 50\nx = np.random.rand(N)\ny = np.random.rand(N)\narea = np.pi * (15 * np.random.rand(N))**2\n\nfig = plt.figure(figsize=(18, 18))\nplt.scatter(x, y, s=area, alpha=0.5)\nplt.show()\n",
                "This works well for me:from matplotlib import pyplot as plt\n\nF = plt.gcf()\nSize = F.get_size_inches()\nF.set_size_inches(Size[0]*2, Size[1]*2, forward=True) # Set forward to True to resize window along with plot in figure.\nplt.show() # Or plt.imshow(z_array) if using an animation, where z_array is a matrix or NumPy array\nfrom matplotlib import pyplot as plt\n\nF = plt.gcf()\nSize = F.get_size_inches()\nF.set_size_inches(Size[0]*2, Size[1]*2, forward=True) # Set forward to True to resize window along with plot in figure.\nplt.show() # Or plt.imshow(z_array) if using an animation, where z_array is a matrix or NumPy array\nThis forum post might also help: Resizing figure windowsResizing figure windowsResizing figure windows",
                "Comparison of different approaches to set exact image sizes in pixelsComparison of different approaches to set exact image sizes in pixelsThis answer will focus on:\nsavefig: how to save to a file, not just show on screen\nsetting the size in pixels\nsavefig: how to save to a file, not just show on screensavefigsetting the size in pixelsHere is a quick comparison of some of the approaches I've tried with images showing what the give.Summary of current status: things are messy, and I am not sure if it is a fundamental limitation, or if the use case just didn't get enough attention from developers. I couldn't easily find an upstream discussion about this.Baseline example without trying to set the image dimensionsBaseline example without trying to set the image dimensionsJust to have a comparison point:base.py#!/usr/bin/env python3\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfig, ax = plt.subplots()\nprint('fig.dpi = {}'.format(fig.dpi))\nprint('fig.get_size_inches() = ' + str(fig.get_size_inches())\nt = np.arange(-10., 10., 1.)\nplt.plot(t, t, '.')\nplt.plot(t, t**2, '.')\nax.text(0., 60., 'Hello', fontdict=dict(size=25))\nplt.savefig('base.png', format='png')\n#!/usr/bin/env python3\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfig, ax = plt.subplots()\nprint('fig.dpi = {}'.format(fig.dpi))\nprint('fig.get_size_inches() = ' + str(fig.get_size_inches())\nt = np.arange(-10., 10., 1.)\nplt.plot(t, t, '.')\nplt.plot(t, t**2, '.')\nax.text(0., 60., 'Hello', fontdict=dict(size=25))\nplt.savefig('base.png', format='png')\nRun:./base.py\nidentify base.png\n./base.py\nidentify base.png\nOutputs:fig.dpi = 100.0\nfig.get_size_inches() = [6.4 4.8]\nbase.png PNG 640x480 640x480+0+0 8-bit sRGB 13064B 0.000u 0:00.000\nfig.dpi = 100.0\nfig.get_size_inches() = [6.4 4.8]\nbase.png PNG 640x480 640x480+0+0 8-bit sRGB 13064B 0.000u 0:00.000\nMy best approach so far: plt.savefig(dpi=h/fig.get_size_inches()[1] height-only controlMy best approach so far: plt.savefig(dpi=h/fig.get_size_inches()[1] height-only controlplt.savefig(dpi=h/fig.get_size_inches()[1]I think this is what I'll go with most of the time, as it is simple and scales:get_size.py#!/usr/bin/env python3\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nheight = int(sys.argv[1])\nfig, ax = plt.subplots()\nt = np.arange(-10., 10., 1.)\nplt.plot(t, t, '.')\nplt.plot(t, t**2, '.')\nax.text(0., 60., 'Hello', fontdict=dict(size=25))\nplt.savefig(\n    'get_size.png',\n    format='png',\n    dpi=height/fig.get_size_inches()[1]\n)\n#!/usr/bin/env python3\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nheight = int(sys.argv[1])\nfig, ax = plt.subplots()\nt = np.arange(-10., 10., 1.)\nplt.plot(t, t, '.')\nplt.plot(t, t**2, '.')\nax.text(0., 60., 'Hello', fontdict=dict(size=25))\nplt.savefig(\n    'get_size.png',\n    format='png',\n    dpi=height/fig.get_size_inches()[1]\n)\nRun:./get_size.py 431\n./get_size.py 431\nOutputs:get_size.png PNG 574x431 574x431+0+0 8-bit sRGB 10058B 0.000u 0:00.000\nget_size.png PNG 574x431 574x431+0+0 8-bit sRGB 10058B 0.000u 0:00.000\nand./get_size.py 1293\n./get_size.py 1293\nOutputs:main.png PNG 1724x1293 1724x1293+0+0 8-bit sRGB 46709B 0.000u 0:00.000\nmain.png PNG 1724x1293 1724x1293+0+0 8-bit sRGB 46709B 0.000u 0:00.000\nI tend to set just the height because I'm usually most concerned about how much vertical space the image is going to take up in the middle of my text.plt.savefig(bbox_inches='tight' changes image sizeplt.savefig(bbox_inches='tight' changes image sizeplt.savefig(bbox_inches='tight'I always feel that there is too much white space around images, and tended to add bbox_inches='tight' from:\nRemoving white space around a saved imagebbox_inches='tight'Removing white space around a saved imageHowever, that works by cropping the image, and you won't get the desired sizes with it.Instead, this other approach proposed in the same question seems to work well:plt.tight_layout(pad=1)\nplt.savefig(...\nplt.tight_layout(pad=1)\nplt.savefig(...\nwhich gives the exact desired height for height equals 431:Fixed height, set_aspect, automatically sized width and small marginsFixed height, set_aspect, automatically sized width and small marginsset_aspectErmmm, set_aspect messes things up again and prevents plt.tight_layout from actually removing the margins... this is an important use case that I don't have a great solution for yet.set_aspectplt.tight_layoutAsked at: How to obtain a fixed height in pixels, fixed data x/y aspect ratio and automatically remove remove horizontal whitespace margin in Matplotlib?How to obtain a fixed height in pixels, fixed data x/y aspect ratio and automatically remove remove horizontal whitespace margin in Matplotlib?plt.savefig(dpi=h/fig.get_size_inches()[1] + width controlplt.savefig(dpi=h/fig.get_size_inches()[1] + width controlplt.savefig(dpi=h/fig.get_size_inches()[1]If you really need a specific width in addition to height, this seems to work OK:width.py#!/usr/bin/env python3\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nh = int(sys.argv[1])\nw = int(sys.argv[2])\nfig, ax = plt.subplots()\nwi, hi = fig.get_size_inches()\nfig.set_size_inches(hi*(w/h), hi)\nt = np.arange(-10., 10., 1.)\nplt.plot(t, t, '.')\nplt.plot(t, t**2, '.')\nax.text(0., 60., 'Hello', fontdict=dict(size=25))\nplt.savefig(\n    'width.png',\n    format='png',\n    dpi=h/hi\n)\n#!/usr/bin/env python3\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nh = int(sys.argv[1])\nw = int(sys.argv[2])\nfig, ax = plt.subplots()\nwi, hi = fig.get_size_inches()\nfig.set_size_inches(hi*(w/h), hi)\nt = np.arange(-10., 10., 1.)\nplt.plot(t, t, '.')\nplt.plot(t, t**2, '.')\nax.text(0., 60., 'Hello', fontdict=dict(size=25))\nplt.savefig(\n    'width.png',\n    format='png',\n    dpi=h/hi\n)\nRun:./width.py 431 869\n./width.py 431 869\nOutput:width.png PNG 869x431 869x431+0+0 8-bit sRGB 10965B 0.000u 0:00.000\nwidth.png PNG 869x431 869x431+0+0 8-bit sRGB 10965B 0.000u 0:00.000\nand for a small width:./width.py 431 869\n./width.py 431 869\nOutput:width.png PNG 211x431 211x431+0+0 8-bit sRGB 6949B 0.000u 0:00.000\nwidth.png PNG 211x431 211x431+0+0 8-bit sRGB 6949B 0.000u 0:00.000\nSo it does seem that fonts are scaling correctly, we just get some trouble for very small widths with labels getting cut off, e.g. the 100 on the top left.100I managed to work around those with Removing white space around a saved imageRemoving white space around a saved imageplt.tight_layout(pad=1)\nplt.tight_layout(pad=1)\nwhich gives:width.png PNG 211x431 211x431+0+0 8-bit sRGB 7134B 0.000u 0:00.000\nwidth.png PNG 211x431 211x431+0+0 8-bit sRGB 7134B 0.000u 0:00.000\nFrom this, we also see that tight_layout removes a lot of the empty space at the top of the image, so I just generally always use it.tight_layoutFixed magic base height, dpi on fig.set_size_inches and plt.savefig(dpi= scalingFixed magic base height, dpi on fig.set_size_inches and plt.savefig(dpi= scalingdpifig.set_size_inchesplt.savefig(dpi=I believe that this is equivalent to the approach mentioned at: https://stackoverflow.com/a/13714720/895245https://stackoverflow.com/a/13714720/895245magic.py#!/usr/bin/env python3\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nmagic_height = 300\nw = int(sys.argv[1])\nh = int(sys.argv[2])\ndpi = 80\nfig, ax = plt.subplots(dpi=dpi)\nfig.set_size_inches(magic_height*w/(h*dpi), magic_height/dpi)\nt = np.arange(-10., 10., 1.)\nplt.plot(t, t, '.')\nplt.plot(t, t**2, '.')\nax.text(0., 60., 'Hello', fontdict=dict(size=25))\nplt.savefig(\n    'magic.png',\n    format='png',\n    dpi=h/magic_height*dpi,\n)\n#!/usr/bin/env python3\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nmagic_height = 300\nw = int(sys.argv[1])\nh = int(sys.argv[2])\ndpi = 80\nfig, ax = plt.subplots(dpi=dpi)\nfig.set_size_inches(magic_height*w/(h*dpi), magic_height/dpi)\nt = np.arange(-10., 10., 1.)\nplt.plot(t, t, '.')\nplt.plot(t, t**2, '.')\nax.text(0., 60., 'Hello', fontdict=dict(size=25))\nplt.savefig(\n    'magic.png',\n    format='png',\n    dpi=h/magic_height*dpi,\n)\nRun:./magic.py 431 231\n./magic.py 431 231\nOutputs:magic.png PNG 431x231 431x231+0+0 8-bit sRGB 7923B 0.000u 0:00.000\nmagic.png PNG 431x231 431x231+0+0 8-bit sRGB 7923B 0.000u 0:00.000\nAnd to see if it scales nicely:./magic.py 1291 693\n./magic.py 1291 693\nOutputs:magic.png PNG 1291x693 1291x693+0+0 8-bit sRGB 25013B 0.000u 0:00.000\nmagic.png PNG 1291x693 1291x693+0+0 8-bit sRGB 25013B 0.000u 0:00.000\nSo we see that this approach also does work well. The only problem I have with it is that you have to set that magic_height parameter or equivalent.magic_heightFixed DPI + set_size_inchesFixed DPI + set_size_inchesset_size_inchesThis approach gave a slightly wrong pixel size, and it makes it is hard to scale everything seamlessly.set_size_inches.py#!/usr/bin/env python3\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nw = int(sys.argv[1])\nh = int(sys.argv[2])\nfig, ax = plt.subplots()\nfig.set_size_inches(w/fig.dpi, h/fig.dpi)\nt = np.arange(-10., 10., 1.)\nplt.plot(t, t, '.')\nplt.plot(t, t**2, '.')\nax.text(\n    0,\n    60.,\n    'Hello',\n    # Keep font size fixed independently of DPI.\n    # https://stackoverflow.com/questions/39395616/matplotlib-change-figsize-but-keep-fontsize-constant\n    fontdict=dict(size=10*h/fig.dpi),\n)\nplt.savefig(\n    'set_size_inches.png',\n    format='png',\n)\n#!/usr/bin/env python3\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nw = int(sys.argv[1])\nh = int(sys.argv[2])\nfig, ax = plt.subplots()\nfig.set_size_inches(w/fig.dpi, h/fig.dpi)\nt = np.arange(-10., 10., 1.)\nplt.plot(t, t, '.')\nplt.plot(t, t**2, '.')\nax.text(\n    0,\n    60.,\n    'Hello',\n    # Keep font size fixed independently of DPI.\n    # https://stackoverflow.com/questions/39395616/matplotlib-change-figsize-but-keep-fontsize-constant\n    fontdict=dict(size=10*h/fig.dpi),\n)\nplt.savefig(\n    'set_size_inches.png',\n    format='png',\n)\nRun:./set_size_inches.py 431 231\n./set_size_inches.py 431 231\nOutputs:set_size_inches.png PNG 430x231 430x231+0+0 8-bit sRGB 8078B 0.000u 0:00.000\nset_size_inches.png PNG 430x231 430x231+0+0 8-bit sRGB 8078B 0.000u 0:00.000\nSo the height is slightly off, and the image:The pixel sizes are also correct if I make it 3 times larger:./set_size_inches.py 1291 693\n./set_size_inches.py 1291 693\nOutputs:set_size_inches.png PNG 1291x693 1291x693+0+0 8-bit sRGB 19798B 0.000u 0:00.000\nset_size_inches.png PNG 1291x693 1291x693+0+0 8-bit sRGB 19798B 0.000u 0:00.000\nWe understand from this however that for this approach to scale nicely, you need to make every DPI-dependant setting proportional to the size in inches.In the previous example, we only made the \"Hello\" text proportional, and it did retain its height between 60 and 80 as we'd expect. But everything for which we didn't do that, looks tiny, including:\nline width of axes\ntick labels\npoint markers\nline width of axestick labelspoint markersSVGSVGI could not find how to set it for SVG images, my approaches only worked for PNG, e.g.:get_size_svg.py#!/usr/bin/env python3\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nheight = int(sys.argv[1])\nfig, ax = plt.subplots()\nt = np.arange(-10., 10., 1.)\nplt.plot(t, t, '.')\nplt.plot(t, t**2, '.')\nax.text(0., 60., 'Hello', fontdict=dict(size=25))\nplt.savefig(\n    'get_size_svg.svg',\n    format='svg',\n    dpi=height/fig.get_size_inches()[1]\n)\n#!/usr/bin/env python3\n\nimport sys\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nheight = int(sys.argv[1])\nfig, ax = plt.subplots()\nt = np.arange(-10., 10., 1.)\nplt.plot(t, t, '.')\nplt.plot(t, t**2, '.')\nax.text(0., 60., 'Hello', fontdict=dict(size=25))\nplt.savefig(\n    'get_size_svg.svg',\n    format='svg',\n    dpi=height/fig.get_size_inches()[1]\n)\nRun:./get_size_svg.py 431\n./get_size_svg.py 431\nAnd the generated output contains:<svg height=\"345.6pt\" version=\"1.1\" viewBox=\"0 0 460.8 345.6\" width=\"460.8pt\"\n<svg height=\"345.6pt\" version=\"1.1\" viewBox=\"0 0 460.8 345.6\" width=\"460.8pt\"\nAnd identify says:get_size_svg.svg SVG 614x461 614x461+0+0 8-bit sRGB 17094B 0.000u 0:00.000\nget_size_svg.svg SVG 614x461 614x461+0+0 8-bit sRGB 17094B 0.000u 0:00.000\nAnd if I open it in Chromium 86 the browser debug tools mouse image hover confirm that height as 460.79.But of course, since SVG is a vector format, everything should in theory scale, so you can just convert to any fixed sized format without loss of resolution, e.g.:inkscape -h 431 get_size_svg.svg -b FFF -e get_size_svg.png\ninkscape -h 431 get_size_svg.svg -b FFF -e get_size_svg.png\ngives the exact height:I use Inkscape instead of ImageMagick's convert here because you need to mess with -density as well to get sharp SVG resizes with ImageMagick:InkscapeImageMagickconvert-density\nhttps://superuser.com/questions/598849/imagemagick-convert-how-to-produce-sharp-resized-png-files-from-svg-files/1602059#1602059\nHow to convert a SVG to a PNG with ImageMagick?\nhttps://superuser.com/questions/598849/imagemagick-convert-how-to-produce-sharp-resized-png-files-from-svg-files/1602059#1602059https://superuser.com/questions/598849/imagemagick-convert-how-to-produce-sharp-resized-png-files-from-svg-files/1602059#1602059How to convert a SVG to a PNG with ImageMagick?How to convert a SVG to a PNG with ImageMagick?And setting <img height=\"\" on the HTML should also just work for the browser.<img height=\"\"It was tested on matplotlib 3.2.2.",
                "Generalizing and simplifying psihodelia's answer:psihodelia's answerIf you want to change the current size of the figure by a factor sizefactor:sizefactorimport matplotlib.pyplot as plt\n\n# Here goes your code\n\nfig_size = plt.gcf().get_size_inches() # Get current size\nsizefactor = 0.8 # Set a zoom factor\n# Modify the current size by the factor\nplt.gcf().set_size_inches(sizefactor * fig_size) \nimport matplotlib.pyplot as plt\n\n# Here goes your code\n\nfig_size = plt.gcf().get_size_inches() # Get current size\nsizefactor = 0.8 # Set a zoom factor\n# Modify the current size by the factor\nplt.gcf().set_size_inches(sizefactor * fig_size) \nAfter changing the current size, it might occur that you have to fine tune the subplot layout. You can do that in the figure window GUI, or by means of the command subplots_adjustsubplot layoutsubplots_adjustFor example,plt.subplots_adjust(left=0.16, bottom=0.19, top=0.82)\nplt.subplots_adjust(left=0.16, bottom=0.19, top=0.82)\n",
                "This resizes the figure immediately even after the figure has been drawn (at least using Qt4Agg/TkAgg - but not Mac\u00a0OS\u00a0X - with Matplotlib 1.4.0):matplotlib.pyplot.get_current_fig_manager().resize(width_px, height_px)\nmatplotlib.pyplot.get_current_fig_manager().resize(width_px, height_px)\n",
                "I always use the following pattern:x_inches = 150*(1/25.4)     # [mm]*constant\ny_inches = x_inches*(0.8)\ndpi = 96\n\nfig = plt.figure(1, figsize = (x_inches,y_inches), dpi = dpi, constrained_layout = True)\nx_inches = 150*(1/25.4)     # [mm]*constant\ny_inches = x_inches*(0.8)\ndpi = 96\n\nfig = plt.figure(1, figsize = (x_inches,y_inches), dpi = dpi, constrained_layout = True)\nWith this example you are able to set figure dimensions in inches or in millimetres. When setting constrained_layout to True, plots fill your figure without borders.constrained_layoutTrue"
            ]
        },
        {
            "tag": "exception",
            "patterns": [
                "Manually raising (throwing) an exception in Python",
                "manually raising an exception in python"
            ],
            "responses": [
                "\nHow do I manually throw/raise an exception in Python?\nHow do I manually throw/raise an exception in Python?Use the most specific Exception constructor that semantically fits your issue.Use the most specific Exception constructor that semantically fits your issueBe specific in your message, e.g.:raise ValueError('A very specific bad thing happened.')\nraise ValueError('A very specific bad thing happened.')\nDon't raise generic exceptionsAvoid raising a generic Exception. To catch it, you'll have to catch all other more specific exceptions that subclass it.ExceptionProblem 1: Hiding bugsraise Exception('I know Python!') # Don't! If you catch, likely to hide bugs.\nraise Exception('I know Python!') # Don't! If you catch, likely to hide bugs.\nFor example:def demo_bad_catch():\n    try:\n        raise ValueError('Represents a hidden bug, do not catch this')\n        raise Exception('This is the exception you expect to handle')\n    except Exception as error:\n        print('Caught this error: ' + repr(error))\n\n>>> demo_bad_catch()\nCaught this error: ValueError('Represents a hidden bug, do not catch this',)\ndef demo_bad_catch():\n    try:\n        raise ValueError('Represents a hidden bug, do not catch this')\n        raise Exception('This is the exception you expect to handle')\n    except Exception as error:\n        print('Caught this error: ' + repr(error))\n\n>>> demo_bad_catch()\nCaught this error: ValueError('Represents a hidden bug, do not catch this',)\nProblem 2: Won't catchAnd more specific catches won't catch the general exception:def demo_no_catch():\n    try:\n        raise Exception('general exceptions not caught by specific handling')\n    except ValueError as e:\n        print('we will not catch exception: Exception')\n \n\n>>> demo_no_catch()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in demo_no_catch\nException: general exceptions not caught by specific handling\ndef demo_no_catch():\n    try:\n        raise Exception('general exceptions not caught by specific handling')\n    except ValueError as e:\n        print('we will not catch exception: Exception')\n \n\n>>> demo_no_catch()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in demo_no_catch\nException: general exceptions not caught by specific handling\nBest Practices: raise statementraiseInstead, use the most specific Exception constructor that semantically fits your issue.Instead, use the most specific Exception constructor that semantically fits your issueraise ValueError('A very specific bad thing happened')\nraise ValueError('A very specific bad thing happened')\nwhich also handily allows an arbitrary number of arguments to be passed to the constructor:raise ValueError('A very specific bad thing happened', 'foo', 'bar', 'baz') \nraise ValueError('A very specific bad thing happened', 'foo', 'bar', 'baz') \nThese arguments are accessed by the args attribute on the Exception object. For example:argsExceptiontry:\n    some_code_that_may_raise_our_value_error()\nexcept ValueError as err:\n    print(err.args)\ntry:\n    some_code_that_may_raise_our_value_error()\nexcept ValueError as err:\n    print(err.args)\nprints('message', 'foo', 'bar', 'baz')    \n('message', 'foo', 'bar', 'baz')    \nIn Python 2.5, an actual message attribute was added to BaseException in favor of encouraging users to subclass Exceptions and stop using args, but the introduction of message and the original deprecation of args has been retracted.messageBaseExceptionargsthe introduction of message and the original deprecation of args has been retractedmessageBest Practices: except clauseexceptWhen inside an except clause, you might want to, for example, log that a specific type of error happened, and then re-raise. The best way to do this while preserving the stack trace is to use a bare raise statement. For example:logger = logging.getLogger(__name__)\n\ntry:\n    do_something_in_app_that_breaks_easily()\nexcept AppError as error:\n    logger.error(error)\n    raise                 # just this!\n    # raise AppError      # Don't do this, you'll lose the stack trace!\nlogger = logging.getLogger(__name__)\n\ntry:\n    do_something_in_app_that_breaks_easily()\nexcept AppError as error:\n    logger.error(error)\n    raise                 # just this!\n    # raise AppError      # Don't do this, you'll lose the stack trace!\nDon't modify your errors... but if you insist.You can preserve the stacktrace (and error value) with sys.exc_info(), but this is way more error prone and has compatibility problems between Python 2 and 3, prefer to use a bare raise to re-raise.sys.exc_info()this is way more error pronehas compatibility problems between Python 2 and 3raiseTo explain - the sys.exc_info() returns the type, value, and traceback.sys.exc_info()type, value, traceback = sys.exc_info()\ntype, value, traceback = sys.exc_info()\nThis is the syntax in Python 2 - note this is not compatible with Python 3:raise AppError, error, sys.exc_info()[2] # avoid this.\n# Equivalently, as error *is* the second object:\nraise sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2]\nraise AppError, error, sys.exc_info()[2] # avoid this.\n# Equivalently, as error *is* the second object:\nraise sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2]\nIf you want to, you can modify what happens with your new raise - e.g. setting new args for the instance:argsdef error():\n    raise ValueError('oops!')\n\ndef catch_error_modify_message():\n    try:\n        error()\n    except ValueError:\n        error_type, error_instance, traceback = sys.exc_info()\n        error_instance.args = (error_instance.args[0] + ' <modification>',)\n        raise error_type, error_instance, traceback\ndef error():\n    raise ValueError('oops!')\n\ndef catch_error_modify_message():\n    try:\n        error()\n    except ValueError:\n        error_type, error_instance, traceback = sys.exc_info()\n        error_instance.args = (error_instance.args[0] + ' <modification>',)\n        raise error_type, error_instance, traceback\nAnd we have preserved the whole traceback while modifying the args. Note that this is not a best practice and it is invalid syntax in Python 3 (making keeping compatibility much harder to work around).not a best practiceinvalid syntax>>> catch_error_modify_message()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in catch_error_modify_message\n  File \"<stdin>\", line 2, in error\nValueError: oops! <modification>\n>>> catch_error_modify_message()\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"<stdin>\", line 3, in catch_error_modify_message\n  File \"<stdin>\", line 2, in error\nValueError: oops! <modification>\nIn Python 3:Python 3raise error.with_traceback(sys.exc_info()[2])\nraise error.with_traceback(sys.exc_info()[2])\nAgain: avoid manually manipulating tracebacks. It's less efficient and more error prone. And if you're using threading and sys.exc_info you may even get the wrong traceback (especially if you're using exception handling for control flow - which I'd personally tend to avoid.)less efficientsys.exc_infoPython 3, Exception chainingIn Python 3, you can chain Exceptions, which preserve tracebacks:raise RuntimeError('specific message') from error\nraise RuntimeError('specific message') from error\nBe aware:\nthis does allow changing the error type raised, and\nthis is not compatible with Python 2.\nthis does allow changing the error type raised, anddoesthis is not compatible with Python 2.notDeprecated Methods:These can easily hide and even get into production code. You want to raise an exception, and doing them will raise an exception, but not the one intended!but not the one intended!Valid in Python 2, but not in Python 3 is the following:Valid in Python 2, but not in Python 3raise ValueError, 'message' # Don't do this, it's deprecated!\nraise ValueError, 'message' # Don't do this, it's deprecated!\nOnly valid in much older versions of Python (2.4 and lower), you may still see people raising strings:valid in much older versions of Pythonraise 'message' # really really wrong. don't do this.\nraise 'message' # really really wrong. don't do this.\nIn all modern versions, this will actually raise a TypeError, because you're not raising a BaseException type. If you're not checking for the right exception and don't have a reviewer that's aware of the issue, it could get into production.TypeErrorBaseExceptionExample UsageI raise Exceptions to warn consumers of my API if they're using it incorrectly:def api_func(foo):\n    '''foo should be either 'baz' or 'bar'. returns something very useful.'''\n    if foo not in _ALLOWED_ARGS:\n        raise ValueError('{foo} wrong, use \"baz\" or \"bar\"'.format(foo=repr(foo)))\ndef api_func(foo):\n    '''foo should be either 'baz' or 'bar'. returns something very useful.'''\n    if foo not in _ALLOWED_ARGS:\n        raise ValueError('{foo} wrong, use \"baz\" or \"bar\"'.format(foo=repr(foo)))\nCreate your own error types when apropos\n\"I want to make an error on purpose, so that it would go into the except\"\n\"I want to make an error on purpose, so that it would go into the except\"\"I want to make an error on purpose, so that it would go into the except\"You can create your own error types, if you want to indicate something specific is wrong with your application, just subclass the appropriate point in the exception hierarchy:class MyAppLookupError(LookupError):\n    '''raise this when there's a lookup error for my app'''\nclass MyAppLookupError(LookupError):\n    '''raise this when there's a lookup error for my app'''\nand usage:if important_key not in resource_dict and not ok_to_be_missing:\n    raise MyAppLookupError('resource is missing, and that is not ok.')\nif important_key not in resource_dict and not ok_to_be_missing:\n    raise MyAppLookupError('resource is missing, and that is not ok.')\n",
                "\nDon't do this. Raising a bare Exception is absolutely not the right thing to do; see Aaron Hall's excellent answer instead.\nDon't do this. Raising a bare Exception is absolutely not the right thing to do; see Aaron Hall's excellent answer instead.Don't do thisDon't do thisExceptionnotAaron Hall's excellent answerIt can't get much more Pythonic than this:raise Exception(\"I know Python!\")\nraise Exception(\"I know Python!\")\nReplace Exception with the specific type of exception you want to throw.ExceptionSee the raise statement documentation for Python if you'd like more information.the raise statement documentation",
                "In Python 3 there are four different syntaxes for raising exceptions:\nraise exception\nraise exception (args)\nraise\nraise exception (args) from original_exception\nraise exceptionraise exception (args)raiseraise exception (args) from original_exception1. Raise exception vs. 2. raise exception (args)If you use raise exception (args) to raise an exception then the args will be printed when you print the exception object - as shown in the example below.raise exception (args)args  # Raise exception (args)\n    try:\n        raise ValueError(\"I have raised an Exception\")\n    except ValueError as exp:\n          print (\"Error\", exp)     # Output -> Error I have raised an Exception\n\n\n  # Raise exception\n    try:\n        raise ValueError\n    except ValueError as exp:\n          print (\"Error\", exp)     # Output -> Error\n  # Raise exception (args)\n    try:\n        raise ValueError(\"I have raised an Exception\")\n    except ValueError as exp:\n          print (\"Error\", exp)     # Output -> Error I have raised an Exception\n\n\n  # Raise exception\n    try:\n        raise ValueError\n    except ValueError as exp:\n          print (\"Error\", exp)     # Output -> Error\n3. Statement raiseraiseThe raise statement without any arguments re-raises the last exception.raiseThis is useful if you need to perform some actions after catching the exception and then want to re-raise it. But if there wasn't any exception before, the raise statement raises  a TypeError Exception.raiseTypeErrordef somefunction():\n    print(\"some cleaning\")\n\na=10\nb=0\nresult=None\n\ntry:\n    result=a/b\n    print(result)\n\nexcept Exception:            # Output ->\n    somefunction()           # Some cleaning\n    raise                    # Traceback (most recent call last):\n                             # File \"python\", line 8, in <module>\n                             # ZeroDivisionError: division by zero\ndef somefunction():\n    print(\"some cleaning\")\n\na=10\nb=0\nresult=None\n\ntry:\n    result=a/b\n    print(result)\n\nexcept Exception:            # Output ->\n    somefunction()           # Some cleaning\n    raise                    # Traceback (most recent call last):\n                             # File \"python\", line 8, in <module>\n                             # ZeroDivisionError: division by zero\n4. Raise exception (args) from original_exceptionThis statement is used to create exception chaining in which an exception that is raised in response to another exception can contain the details of the original exception - as shown in the example below.class MyCustomException(Exception):\npass\n\na=10\nb=0\nreuslt=None\ntry:\n    try:\n        result=a/b\n\n    except ZeroDivisionError as exp:\n        print(\"ZeroDivisionError -- \",exp)\n        raise MyCustomException(\"Zero Division \") from exp\n\nexcept MyCustomException as exp:\n        print(\"MyException\",exp)\n        print(exp.__cause__)\nclass MyCustomException(Exception):\npass\n\na=10\nb=0\nreuslt=None\ntry:\n    try:\n        result=a/b\n\n    except ZeroDivisionError as exp:\n        print(\"ZeroDivisionError -- \",exp)\n        raise MyCustomException(\"Zero Division \") from exp\n\nexcept MyCustomException as exp:\n        print(\"MyException\",exp)\n        print(exp.__cause__)\nOutput:Output:ZeroDivisionError --  division by zero\nMyException Zero Division\ndivision by zero\nZeroDivisionError --  division by zero\nMyException Zero Division\ndivision by zero\n",
                "For the common case where you need to throw an exception in response to some unexpected conditions, and that you never intend to catch, but simply to fail fast to enable you to debug from there if it ever happens \u2014 the most logical one seems to be AssertionError:AssertionErrorif 0 < distance <= RADIUS:\n    #Do something.\nelif RADIUS < distance:\n    #Do something.\nelse:\n    raise AssertionError(\"Unexpected value of 'distance'!\", distance)\nif 0 < distance <= RADIUS:\n    #Do something.\nelif RADIUS < distance:\n    #Do something.\nelse:\n    raise AssertionError(\"Unexpected value of 'distance'!\", distance)\n",
                "Read the existing answers first, this is just an addendum.Read the existing answers first, this is just an addendum.Notice that you can raise exceptions with or without arguments.Example:raise SystemExit\nraise SystemExit\nexits the program, but you might want to know what happened. So you can use this.raise SystemExit(\"program exited\")\nraise SystemExit(\"program exited\")\nThis will print \"program exited\" to standard error before closing the program.",
                "Just to note: there are times when you do want to handle generic exceptions. If you're processing a bunch of files and logging your errors, you might want to catch any error that occurs for a file, log it, and continue processing the rest of the files. In that case, adotry:\n    foo()\nexcept Exception as e:\n    print(e) # Print out handled error\ntry:\n    foo()\nexcept Exception as e:\n    print(e) # Print out handled error\nblock is a good way to do it. You'll still want to raise specific exceptions so you know what they mean, though.raise",
                "Another way to throw an exception is using assert. You can use assert to verify a condition is being fulfilled. If not, then it will raise AssertionError. For more details have a look here.assertassertassertAssertionErrorheredef avg(marks):\n    assert len(marks) != 0, \"List is empty.\"\n    return sum(marks)/len(marks)\n\nmark2 = [55,88,78,90,79]\nprint(\"Average of mark2:\", avg(mark2))\n\nmark1 = []\nprint(\"Average of mark1:\", avg(mark1))\ndef avg(marks):\n    assert len(marks) != 0, \"List is empty.\"\n    return sum(marks)/len(marks)\n\nmark2 = [55,88,78,90,79]\nprint(\"Average of mark2:\", avg(mark2))\n\nmark1 = []\nprint(\"Average of mark1:\", avg(mark1))\n",
                "You might also want to raise custom exceptions. For example, if you're writing a library, it's a very good practice to make a base exception class for your module, and then have custom sub-exceptions to be more specific.custom exceptionsYou can achieve that like this:class MyModuleBaseClass(Exception):\n    pass\n\nclass MoreSpecificException(MyModuleBaseClass):\n    pass\n\n\n# To raise custom exceptions, you can just\n# use the raise keyword\nraise MoreSpecificException\nraise MoreSpecificException('message')\nclass MyModuleBaseClass(Exception):\n    pass\n\nclass MoreSpecificException(MyModuleBaseClass):\n    pass\n\n\n# To raise custom exceptions, you can just\n# use the raise keyword\nraise MoreSpecificException\nraise MoreSpecificException('message')\nIf you're not interested in having a custom base class, you can just inherit your custom exception classes from an ordinary exception class like Exception, TypeError, ValueError, etc.ExceptionTypeErrorValueError",
                "If you don't care about which error to raise, you could use assert to raise an AssertionError:whichassertAssertionError>>> assert False, \"Manually raised error\"\nTraceback (most recent call last):\n  File \"<pyshell#24>\", line 1, in <module>\n    assert False, \"Manually raised error\"\nAssertionError: Manually raised error\n>>> \n>>> assert False, \"Manually raised error\"\nTraceback (most recent call last):\n  File \"<pyshell#24>\", line 1, in <module>\n    assert False, \"Manually raised error\"\nAssertionError: Manually raised error\n>>> \nThe assert keyword raises an AssertionError if the condition is False. In this case, we specified False directly, so it raises the error, but to have it have a text we want it to raise to, we add a comma and specify the error text we want. In this case, I wrote Manually raised error and this raises it with that text.assertAssertionErrorFalseFalseManually raised error",
                "You should learn the raise statement of Python for that.raiseIt should be kept inside the try block.Example -try:\n    raise TypeError            # Replace TypeError by any other error if you want\nexcept TypeError:\n    print('TypeError raised')\ntry:\n    raise TypeError            # Replace TypeError by any other error if you want\nexcept TypeError:\n    print('TypeError raised')\n",
                "If you don\u2019t care about the raised exception, do:def crash(): return 0/0\ndef crash(): return 0/0\nThe good old division by 0."
            ]
        },
        {
            "tag": "color_text",
            "patterns": [
                "How do I print colored text to the terminal?",
                "what can i do to print a colored text to a terminal?",
                "how do i print color text for a terminal?",
                "how can i print colored text onto the terminal?",
                "how do i print the colored text to the terminal?"
            ],
            "responses": [
                "This somewhat depends on what platform you are on. The most common way to do this is by printing ANSI escape sequences. For a simple example, here's some Python code from the Blender build scripts:Blender build scriptsclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\nclass bcolors:\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKCYAN = '\\033[96m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = '\\033[1m'\n    UNDERLINE = '\\033[4m'\nTo use code like this, you can do something like:print(bcolors.WARNING + \"Warning: No active frommets remain. Continue?\" + bcolors.ENDC)\nprint(bcolors.WARNING + \"Warning: No active frommets remain. Continue?\" + bcolors.ENDC)\nOr, with Python 3.6+:print(f\"{bcolors.WARNING}Warning: No active frommets remain. Continue?{bcolors.ENDC}\")\nprint(f\"{bcolors.WARNING}Warning: No active frommets remain. Continue?{bcolors.ENDC}\")\nThis will work on unixes including OS X, Linux and Windows (provided you use ANSICON, or in Windows 10 provided you enable VT100 emulation). There are ANSI codes for setting the color, moving the cursor, and more.ANSICONVT100 emulationIf you are going to get complicated with this (and it sounds like you are if you are writing a game), you should look into the \"curses\" module, which handles a lot of the complicated parts of this for you. The Python Curses HowTO is a good introduction.cursesPython Curses HowTOIf you are not using extended ASCII (i.e., not on a PC), you are stuck with the ASCII characters below 127, and '#' or '@' is probably your best bet for a block. If you can ensure your terminal is using a IBM extended ASCII character set, you have many more options. Characters 176, 177, 178 and 219 are the \"block characters\".extended ASCII character setSome modern text-based programs, such as \"Dwarf Fortress\", emulate text mode in a graphical mode, and use images of the classic PC font. You can find some of these bitmaps that you can use on the Dwarf Fortress Wiki see (user-made tilesets).Dwarf Fortress Wikiuser-made tilesetsThe Text Mode Demo Contest has more resources for doing graphics in text mode.Text Mode Demo Contest",
                "There is also the Python termcolor module. Usage is pretty simple:Python termcolor modulefrom termcolor import colored\n\nprint colored('hello', 'red'), colored('world', 'green')\nfrom termcolor import colored\n\nprint colored('hello', 'red'), colored('world', 'green')\nOr in Python 3:print(colored('hello', 'red'), colored('world', 'green'))\nprint(colored('hello', 'red'), colored('world', 'green'))\nIt may not be sophisticated enough, however, for game programming and the \"colored blocks\" that you want to do...To get the ANSI codes working on windows, first runos.system('color')\nos.system('color')\n",
                "The answer is Colorama for all cross-platform coloring in Python.ColoramaIt supports Python 3.5+ as well as Python 2.7.And as of January 2021 it is maintained.Example Code:from colorama import init as colorama_init\nfrom colorama import Fore\nfrom colorama import Style\n\ncolorama_init()\n\nprint(f\"This is {Fore.GREEN}color{Style.RESET_ALL}!\")\nfrom colorama import init as colorama_init\nfrom colorama import Fore\nfrom colorama import Style\n\ncolorama_init()\n\nprint(f\"This is {Fore.GREEN}color{Style.RESET_ALL}!\")\nExample Screenshot:\n",
                "Print a string that starts a color/style, then the string, and then end the color/style change with '\\x1b[0m':'\\x1b[0m'print('\\x1b[6;30;42m' + 'Success!' + '\\x1b[0m')\nprint('\\x1b[6;30;42m' + 'Success!' + '\\x1b[0m')\nGet a table of format options for shell text with the following code:def print_format_table():\n    \"\"\"\n    prints table of formatted text format options\n    \"\"\"\n    for style in range(8):\n        for fg in range(30,38):\n            s1 = ''\n            for bg in range(40,48):\n                format = ';'.join([str(style), str(fg), str(bg)])\n                s1 += '\\x1b[%sm %s \\x1b[0m' % (format, format)\n            print(s1)\n        print('\\n')\n\nprint_format_table()\ndef print_format_table():\n    \"\"\"\n    prints table of formatted text format options\n    \"\"\"\n    for style in range(8):\n        for fg in range(30,38):\n            s1 = ''\n            for bg in range(40,48):\n                format = ';'.join([str(style), str(fg), str(bg)])\n                s1 += '\\x1b[%sm %s \\x1b[0m' % (format, format)\n            print(s1)\n        print('\\n')\n\nprint_format_table()\nLight-on-dark example (complete)Dark-on-light example (partial)Reference: https://en.wikipedia.org/wiki/ANSI_escape_code#Colorshttps://en.wikipedia.org/wiki/ANSI_escape_code#Colors",
                "Define a string that starts a color and a string that ends the color. Then print your text with the start string at the front and the end string at the end.CRED = '\\033[91m'\nCEND = '\\033[0m'\nprint(CRED + \"Error, does not compute!\" + CEND)\nCRED = '\\033[91m'\nCEND = '\\033[0m'\nprint(CRED + \"Error, does not compute!\" + CEND)\nThis produces the following in Bash, in urxvt with a Zenburn-style color scheme:urxvtThrough experimentation, we can get more colors:Note: \\33[5m and \\33[6m are blinking.\\33[5m\\33[6mThis way we can create a full color collection:CEND      = '\\33[0m'\nCBOLD     = '\\33[1m'\nCITALIC   = '\\33[3m'\nCURL      = '\\33[4m'\nCBLINK    = '\\33[5m'\nCBLINK2   = '\\33[6m'\nCSELECTED = '\\33[7m'\n\nCBLACK  = '\\33[30m'\nCRED    = '\\33[31m'\nCGREEN  = '\\33[32m'\nCYELLOW = '\\33[33m'\nCBLUE   = '\\33[34m'\nCVIOLET = '\\33[35m'\nCBEIGE  = '\\33[36m'\nCWHITE  = '\\33[37m'\n\nCBLACKBG  = '\\33[40m'\nCREDBG    = '\\33[41m'\nCGREENBG  = '\\33[42m'\nCYELLOWBG = '\\33[43m'\nCBLUEBG   = '\\33[44m'\nCVIOLETBG = '\\33[45m'\nCBEIGEBG  = '\\33[46m'\nCWHITEBG  = '\\33[47m'\n\nCGREY    = '\\33[90m'\nCRED2    = '\\33[91m'\nCGREEN2  = '\\33[92m'\nCYELLOW2 = '\\33[93m'\nCBLUE2   = '\\33[94m'\nCVIOLET2 = '\\33[95m'\nCBEIGE2  = '\\33[96m'\nCWHITE2  = '\\33[97m'\n\nCGREYBG    = '\\33[100m'\nCREDBG2    = '\\33[101m'\nCGREENBG2  = '\\33[102m'\nCYELLOWBG2 = '\\33[103m'\nCBLUEBG2   = '\\33[104m'\nCVIOLETBG2 = '\\33[105m'\nCBEIGEBG2  = '\\33[106m'\nCWHITEBG2  = '\\33[107m'\nCEND      = '\\33[0m'\nCBOLD     = '\\33[1m'\nCITALIC   = '\\33[3m'\nCURL      = '\\33[4m'\nCBLINK    = '\\33[5m'\nCBLINK2   = '\\33[6m'\nCSELECTED = '\\33[7m'\n\nCBLACK  = '\\33[30m'\nCRED    = '\\33[31m'\nCGREEN  = '\\33[32m'\nCYELLOW = '\\33[33m'\nCBLUE   = '\\33[34m'\nCVIOLET = '\\33[35m'\nCBEIGE  = '\\33[36m'\nCWHITE  = '\\33[37m'\n\nCBLACKBG  = '\\33[40m'\nCREDBG    = '\\33[41m'\nCGREENBG  = '\\33[42m'\nCYELLOWBG = '\\33[43m'\nCBLUEBG   = '\\33[44m'\nCVIOLETBG = '\\33[45m'\nCBEIGEBG  = '\\33[46m'\nCWHITEBG  = '\\33[47m'\n\nCGREY    = '\\33[90m'\nCRED2    = '\\33[91m'\nCGREEN2  = '\\33[92m'\nCYELLOW2 = '\\33[93m'\nCBLUE2   = '\\33[94m'\nCVIOLET2 = '\\33[95m'\nCBEIGE2  = '\\33[96m'\nCWHITE2  = '\\33[97m'\n\nCGREYBG    = '\\33[100m'\nCREDBG2    = '\\33[101m'\nCGREENBG2  = '\\33[102m'\nCYELLOWBG2 = '\\33[103m'\nCBLUEBG2   = '\\33[104m'\nCVIOLETBG2 = '\\33[105m'\nCBEIGEBG2  = '\\33[106m'\nCWHITEBG2  = '\\33[107m'\nHere is the code to generate the test:x = 0\nfor i in range(24):\n  colors = \"\"\n  for j in range(5):\n    code = str(x+j)\n    colors = colors + \"\\33[\" + code + \"m\\\\33[\" + code + \"m\\033[0m \"\n  print(colors)\n  x = x + 5\nx = 0\nfor i in range(24):\n  colors = \"\"\n  for j in range(5):\n    code = str(x+j)\n    colors = colors + \"\\33[\" + code + \"m\\\\33[\" + code + \"m\\033[0m \"\n  print(colors)\n  x = x + 5\n",
                "Here's a solution that works on Windows 10 natively.Using a system call, such as os.system(\"\"), allows colours to be printed in Command Prompt and Powershell natively:os.system(\"\")import os\n\n# System call\nos.system(\"\")\n\n# Class of different styles\nclass style():\n    BLACK = '\\033[30m'\n    RED = '\\033[31m'\n    GREEN = '\\033[32m'\n    YELLOW = '\\033[33m'\n    BLUE = '\\033[34m'\n    MAGENTA = '\\033[35m'\n    CYAN = '\\033[36m'\n    WHITE = '\\033[37m'\n    UNDERLINE = '\\033[4m'\n    RESET = '\\033[0m'\n\nprint(style.YELLOW + \"Hello, World!\")\nimport os\n\n# System call\nos.system(\"\")\n\n# Class of different styles\nclass style():\n    BLACK = '\\033[30m'\n    RED = '\\033[31m'\n    GREEN = '\\033[32m'\n    YELLOW = '\\033[33m'\n    BLUE = '\\033[34m'\n    MAGENTA = '\\033[35m'\n    CYAN = '\\033[36m'\n    WHITE = '\\033[37m'\n    UNDERLINE = '\\033[4m'\n    RESET = '\\033[0m'\n\nprint(style.YELLOW + \"Hello, World!\")\nNote: Windows does not fully support ANSI codes, whether through system calls or modules. Not all text decoration is supported, and although the bright colours display, they are identical to the regular colours.Thanks to @j-l for finding an even shorter method.tl;dr: Add os.system(\"\")tl;dros.system(\"\")",
                "You want to learn about ANSI escape sequences. Here's a brief example:CSI = \"\\x1B[\"\nprint(CSI+\"31;40m\" + \"Colored Text\" + CSI + \"0m\")\nCSI = \"\\x1B[\"\nprint(CSI+\"31;40m\" + \"Colored Text\" + CSI + \"0m\")\nFor more information, see ANSI escape code.ANSI escape codeANSI escape codeFor a block character, try a Unicode character like \\u2588:print(u\"\\u2588\")\nprint(u\"\\u2588\")\nPutting it all together:print(CSI+\"31;40m\" + u\"\\u2588\" + CSI + \"0m\")\nprint(CSI+\"31;40m\" + u\"\\u2588\" + CSI + \"0m\")\n",
                "sty is similar to colorama, but it's less verbose, supports 8-bit and 24-bit (RGB) colors, supports all effects (bold, underline, etc.), allows you to register your own styles, is fully typed and high performant, supports muting, is not messing with globals such as sys.stdout, is really flexible, well documented and more...sty8-bit24-biteffectsregister your own stylesmutingsys.stdoutdocumentedExamples:Examples:from sty import fg, bg, ef, rs\n\nfoo = fg.red + 'This is red text!' + fg.rs\nbar = bg.blue + 'This has a blue background!' + bg.rs\nbaz = ef.italic + 'This is italic text' + rs.italic\nqux = fg(201) + 'This is pink text using 8bit colors' + fg.rs\nqui = fg(255, 10, 10) + 'This is red text using 24bit colors.' + fg.rs\n\n# Add custom colors:\n\nfrom sty import Style, RgbFg\n\nfg.orange = Style(RgbFg(255, 150, 50))\n\nbuf = fg.orange + 'Yay, Im orange.' + fg.rs\n\nprint(foo, bar, baz, qux, qui, buf, sep='\\n')\nfrom sty import fg, bg, ef, rs\n\nfoo = fg.red + 'This is red text!' + fg.rs\nbar = bg.blue + 'This has a blue background!' + bg.rs\nbaz = ef.italic + 'This is italic text' + rs.italic\nqux = fg(201) + 'This is pink text using 8bit colors' + fg.rs\nqui = fg(255, 10, 10) + 'This is red text using 24bit colors.' + fg.rs\n\n# Add custom colors:\n\nfrom sty import Style, RgbFg\n\nfg.orange = Style(RgbFg(255, 150, 50))\n\nbuf = fg.orange + 'Yay, Im orange.' + fg.rs\n\nprint(foo, bar, baz, qux, qui, buf, sep='\\n')\nprints:Demo:Demo:",
                "Rich is a relatively new Python library for working with color in the terminal.RichThere are a few ways of working with color in Rich. The quickest way to get started would be the rich print method which renders a BBCode-like syntax in to ANSI control codes:BBCodefrom rich import print\nprint(\"[red]Color[/] in the [bold magenta]Terminal[/]!\")\nfrom rich import print\nprint(\"[red]Color[/] in the [bold magenta]Terminal[/]!\")\nThere are other ways of applying color with Rich (regex, syntax) and related formatting features.",
                "My favorite way is with the Blessings library (full disclosure: I wrote it). For example:Blessingsfrom blessings import Terminal\n\nt = Terminal()\nprint t.red('This is red.')\nprint t.bold_bright_red_on_black('Bright red on black')\nfrom blessings import Terminal\n\nt = Terminal()\nprint t.red('This is red.')\nprint t.bold_bright_red_on_black('Bright red on black')\nTo print colored bricks, the most reliable way is to print spaces with background colors. I use this technique to draw the progress bar in nose-progressive:nose-progressiveprint t.on_green(' ')\nprint t.on_green(' ')\nYou can print in specific locations as well:with t.location(0, 5):\n    print t.on_yellow(' ')\nwith t.location(0, 5):\n    print t.on_yellow(' ')\nIf you have to muck with other terminal capabilities in the course of your game, you can do that as well. You can use Python's standard string formatting to keep it readable:print '{t.clear_eol}You just cleared a {t.bold}whole{t.normal} line!'.format(t=t)\nprint '{t.clear_eol}You just cleared a {t.bold}whole{t.normal} line!'.format(t=t)\nThe nice thing about Blessings is that it does its best to work on all sorts of terminals, not just the (overwhelmingly common) ANSI-color ones. It also keeps unreadable escape sequences out of your code while remaining concise to use. Have fun!",
                "I generated a class with all the colors using a for loop to iterate every combination of color up to 100, and then wrote a class with Python colors. Copy and paste as you will, GPLv2 by me:forclass colors:\n    '''Colors class:\n    Reset all colors with colors.reset\n    Two subclasses fg for foreground and bg for background.\n    Use as colors.subclass.colorname.\n    i.e. colors.fg.red or colors.bg.green\n    Also, the generic bold, disable, underline, reverse, strikethrough,\n    and invisible work with the main class\n    i.e. colors.bold\n    '''\n    reset='\\033[0m'\n    bold='\\033[01m'\n    disable='\\033[02m'\n    underline='\\033[04m'\n    reverse='\\033[07m'\n    strikethrough='\\033[09m'\n    invisible='\\033[08m'\n    class fg:\n        black='\\033[30m'\n        red='\\033[31m'\n        green='\\033[32m'\n        orange='\\033[33m'\n        blue='\\033[34m'\n        purple='\\033[35m'\n        cyan='\\033[36m'\n        lightgrey='\\033[37m'\n        darkgrey='\\033[90m'\n        lightred='\\033[91m'\n        lightgreen='\\033[92m'\n        yellow='\\033[93m'\n        lightblue='\\033[94m'\n        pink='\\033[95m'\n        lightcyan='\\033[96m'\n    class bg:\n        black='\\033[40m'\n        red='\\033[41m'\n        green='\\033[42m'\n        orange='\\033[43m'\n        blue='\\033[44m'\n        purple='\\033[45m'\n        cyan='\\033[46m'\n        lightgrey='\\033[47m'\nclass colors:\n    '''Colors class:\n    Reset all colors with colors.reset\n    Two subclasses fg for foreground and bg for background.\n    Use as colors.subclass.colorname.\n    i.e. colors.fg.red or colors.bg.green\n    Also, the generic bold, disable, underline, reverse, strikethrough,\n    and invisible work with the main class\n    i.e. colors.bold\n    '''\n    reset='\\033[0m'\n    bold='\\033[01m'\n    disable='\\033[02m'\n    underline='\\033[04m'\n    reverse='\\033[07m'\n    strikethrough='\\033[09m'\n    invisible='\\033[08m'\n    class fg:\n        black='\\033[30m'\n        red='\\033[31m'\n        green='\\033[32m'\n        orange='\\033[33m'\n        blue='\\033[34m'\n        purple='\\033[35m'\n        cyan='\\033[36m'\n        lightgrey='\\033[37m'\n        darkgrey='\\033[90m'\n        lightred='\\033[91m'\n        lightgreen='\\033[92m'\n        yellow='\\033[93m'\n        lightblue='\\033[94m'\n        pink='\\033[95m'\n        lightcyan='\\033[96m'\n    class bg:\n        black='\\033[40m'\n        red='\\033[41m'\n        green='\\033[42m'\n        orange='\\033[43m'\n        blue='\\033[44m'\n        purple='\\033[45m'\n        cyan='\\033[46m'\n        lightgrey='\\033[47m'\n",
                "This is, in my opinion, the easiest method. As long as you have the RGB values of the color you want, this should work:def colored(r, g, b, text):\n    return f\"\\033[38;2;{r};{g};{b}m{text}\\033[0m\"\ndef colored(r, g, b, text):\n    return f\"\\033[38;2;{r};{g};{b}m{text}\\033[0m\"\nAn example of printing red text:text = 'Hello, World!'\ncolored_text = colored(255, 0, 0, text)\nprint(colored_text)\n\n#or\n\nprint(colored(255, 0, 0, 'Hello, World!'))\ntext = 'Hello, World!'\ncolored_text = colored(255, 0, 0, text)\nprint(colored_text)\n\n#or\n\nprint(colored(255, 0, 0, 'Hello, World!'))\nMulti-colored texttext = colored(255, 0, 0, 'Hello, ') + colored(0, 255, 0, 'World')\nprint(text)\ntext = colored(255, 0, 0, 'Hello, ') + colored(0, 255, 0, 'World')\nprint(text)\n",
                "Try this simple codedef prRed(prt):\n    print(f\"\\033[91m{prt}\\033[00m\")\n\ndef prGreen(prt):\n    print(f\"\\033[92m{prt}\\033[00m\")\n\ndef prYellow(prt):\n    print(f\"\\033[93m{prt}\\033[00m\")\n\ndef prLightPurple(prt):\n    print(f\"\\033[94m{prt}\\033[00m\")\n\ndef prPurple(prt):\n    print(f\"\\033[95m{prt}\\033[00m\")\n\ndef prCyan(prt):\n    print(f\"\\033[96m{prt}\\033[00m\")\n\ndef prLightGray(prt):\n    print(f\"\\033[97m{prt}\\033[00m\")\n\ndef prBlack(prt):\n    print(f\"\\033[98m{prt}\\033[00m\")\n\ndef prReset(prt):\n    print(f\"\\033[0m{prt}\\033[00m\")\n\nprGreen(\"Hello, Green World!\")\nprBlack(\"Hello, Black World!\")\nprCyan(\"Hello, Cyan World!\")\nprGreen(\"Hello, Green World!\")\nprLightGray(\"Hello, Light Grey World!\")\nprLightPurple(\"Hello, Light Purple World!\")\nprPurple(\"Hello, Purple World!\")\nprRed(\"Hello, Red World!\")\nprYellow(\"Hello, Yellow World!\")\nprReset(\"Hello, Reset World!\")\ndef prRed(prt):\n    print(f\"\\033[91m{prt}\\033[00m\")\n\ndef prGreen(prt):\n    print(f\"\\033[92m{prt}\\033[00m\")\n\ndef prYellow(prt):\n    print(f\"\\033[93m{prt}\\033[00m\")\n\ndef prLightPurple(prt):\n    print(f\"\\033[94m{prt}\\033[00m\")\n\ndef prPurple(prt):\n    print(f\"\\033[95m{prt}\\033[00m\")\n\ndef prCyan(prt):\n    print(f\"\\033[96m{prt}\\033[00m\")\n\ndef prLightGray(prt):\n    print(f\"\\033[97m{prt}\\033[00m\")\n\ndef prBlack(prt):\n    print(f\"\\033[98m{prt}\\033[00m\")\n\ndef prReset(prt):\n    print(f\"\\033[0m{prt}\\033[00m\")\n\nprGreen(\"Hello, Green World!\")\nprBlack(\"Hello, Black World!\")\nprCyan(\"Hello, Cyan World!\")\nprGreen(\"Hello, Green World!\")\nprLightGray(\"Hello, Light Grey World!\")\nprLightPurple(\"Hello, Light Purple World!\")\nprPurple(\"Hello, Purple World!\")\nprRed(\"Hello, Red World!\")\nprYellow(\"Hello, Yellow World!\")\nprReset(\"Hello, Reset World!\")\nPython 3 Example\n# python2\n    def prRed(prt): print(\"\\033[91m {}\\033[00m\" .format(prt))\n    def prGreen(prt): print(\"\\033[92m {}\\033[00m\" .format(prt))\n    def prYellow(prt): print(\"\\033[93m {}\\033[00m\" .format(prt))\n    def prLightPurple(prt): print(\"\\033[94m {}\\033[00m\" .format(prt))\n    def prPurple(prt): print(\"\\033[95m {}\\033[00m\" .format(prt))\n    def prCyan(prt): print(\"\\033[96m {}\\033[00m\" .format(prt))\n    def prLightGray(prt): print(\"\\033[97m {}\\033[00m\" .format(prt))\n    def prBlack(prt): print(\"\\033[98m {}\\033[00m\" .format(prt))\n\n    prGreen(\"Hello, World!\")\n# python2\n    def prRed(prt): print(\"\\033[91m {}\\033[00m\" .format(prt))\n    def prGreen(prt): print(\"\\033[92m {}\\033[00m\" .format(prt))\n    def prYellow(prt): print(\"\\033[93m {}\\033[00m\" .format(prt))\n    def prLightPurple(prt): print(\"\\033[94m {}\\033[00m\" .format(prt))\n    def prPurple(prt): print(\"\\033[95m {}\\033[00m\" .format(prt))\n    def prCyan(prt): print(\"\\033[96m {}\\033[00m\" .format(prt))\n    def prLightGray(prt): print(\"\\033[97m {}\\033[00m\" .format(prt))\n    def prBlack(prt): print(\"\\033[98m {}\\033[00m\" .format(prt))\n\n    prGreen(\"Hello, World!\")\n",
                "# Pure Python 3.x demo, 256 colors\n# Works with bash under Linux and MacOS\n\nfg = lambda text, color: \"\\33[38;5;\" + str(color) + \"m\" + text + \"\\33[0m\"\nbg = lambda text, color: \"\\33[48;5;\" + str(color) + \"m\" + text + \"\\33[0m\"\n\ndef print_six(row, format, end=\"\\n\"):\n    for col in range(6):\n        color = row*6 + col - 2\n        if color>=0:\n            text = \"{:3d}\".format(color)\n            print (format(text,color), end=\" \")\n        else:\n            print(end=\"    \")   # four spaces\n    print(end=end)\n\nfor row in range(0, 43):\n    print_six(row, fg, \" \")\n    print_six(row, bg)\n\n# Simple usage: print(fg(\"text\", 160))\n# Pure Python 3.x demo, 256 colors\n# Works with bash under Linux and MacOS\n\nfg = lambda text, color: \"\\33[38;5;\" + str(color) + \"m\" + text + \"\\33[0m\"\nbg = lambda text, color: \"\\33[48;5;\" + str(color) + \"m\" + text + \"\\33[0m\"\n\ndef print_six(row, format, end=\"\\n\"):\n    for col in range(6):\n        color = row*6 + col - 2\n        if color>=0:\n            text = \"{:3d}\".format(color)\n            print (format(text,color), end=\" \")\n        else:\n            print(end=\"    \")   # four spaces\n    print(end=end)\n\nfor row in range(0, 43):\n    print_six(row, fg, \" \")\n    print_six(row, bg)\n\n# Simple usage: print(fg(\"text\", 160))\n\nTry it onlineTry it online",
                "I have a library called colorit. It is super simple.library called coloritHere are some examples:from colorit import *\n\n# Use this to ensure that ColorIt will be usable by certain command line interfaces\n# Note: This clears the terminal\ninit_colorit()\n\n# Foreground\nprint(color(\"This text is red\", Colors.red))\nprint(color(\"This text is orange\", Colors.orange))\nprint(color(\"This text is yellow\", Colors.yellow))\nprint(color(\"This text is green\", Colors.green))\nprint(color(\"This text is blue\", Colors.blue))\nprint(color(\"This text is purple\", Colors.purple))\nprint(color(\"This text is white\", Colors.white))\n\n# Background\nprint(background(\"This text has a background that is red\", Colors.red))\nprint(background(\"This text has a background that is orange\", Colors.orange))\nprint(background(\"This text has a background that is yellow\", Colors.yellow))\nprint(background(\"This text has a background that is green\", Colors.green))\nprint(background(\"This text has a background that is blue\", Colors.blue))\nprint(background(\"This text has a background that is purple\", Colors.purple))\nprint(background(\"This text has a background that is white\", Colors.white))\n\n# Custom\nprint(color(\"This color has a custom grey text color\", (150, 150, 150)))\nprint(background(\"This color has a custom grey background\", (150, 150, 150)))\n\n# Combination\nprint(\n    background(\n        color(\"This text is blue with a white background\", Colors.blue), Colors.white\n    )\n)\n\n# If you are using Windows Command Line, this is so that it doesn't close immediately\ninput()\nfrom colorit import *\n\n# Use this to ensure that ColorIt will be usable by certain command line interfaces\n# Note: This clears the terminal\ninit_colorit()\n\n# Foreground\nprint(color(\"This text is red\", Colors.red))\nprint(color(\"This text is orange\", Colors.orange))\nprint(color(\"This text is yellow\", Colors.yellow))\nprint(color(\"This text is green\", Colors.green))\nprint(color(\"This text is blue\", Colors.blue))\nprint(color(\"This text is purple\", Colors.purple))\nprint(color(\"This text is white\", Colors.white))\n\n# Background\nprint(background(\"This text has a background that is red\", Colors.red))\nprint(background(\"This text has a background that is orange\", Colors.orange))\nprint(background(\"This text has a background that is yellow\", Colors.yellow))\nprint(background(\"This text has a background that is green\", Colors.green))\nprint(background(\"This text has a background that is blue\", Colors.blue))\nprint(background(\"This text has a background that is purple\", Colors.purple))\nprint(background(\"This text has a background that is white\", Colors.white))\n\n# Custom\nprint(color(\"This color has a custom grey text color\", (150, 150, 150)))\nprint(background(\"This color has a custom grey background\", (150, 150, 150)))\n\n# Combination\nprint(\n    background(\n        color(\"This text is blue with a white background\", Colors.blue), Colors.white\n    )\n)\n\n# If you are using Windows Command Line, this is so that it doesn't close immediately\ninput()\nThis gives you:It's also worth noting that this is cross platform and has been tested on Mac, Linux, and Windows.You might want to try it out: https://github.com/SuperMaZingCoder/colorithttps://github.com/SuperMaZingCoder/coloritcolorit is now available to be installed with PyPi! You can install it with pip install color-it on Windows and pip3 install color-it on macOS and Linux.coloritpip install color-itpip3 install color-it",
                "On Windows you can use module 'win32console' (available in some Python distributions) or module 'ctypes' (Python 2.5 and up) to access the Win32 API.To see complete code that supports both ways, see the color console reporting code from Testoob.color console reporting codeTestoobctypes example:import ctypes\n\n# Constants from the Windows API\nSTD_OUTPUT_HANDLE = -11\nFOREGROUND_RED    = 0x0004 # text color contains red.\n\ndef get_csbi_attributes(handle):\n    # Based on IPython's winconsole.py, written by Alexander Belchenko\n    import struct\n    csbi = ctypes.create_string_buffer(22)\n    res = ctypes.windll.kernel32.GetConsoleScreenBufferInfo(handle, csbi)\n    assert res\n\n    (bufx, bufy, curx, cury, wattr,\n    left, top, right, bottom, maxx, maxy) = struct.unpack(\"hhhhHhhhhhh\", csbi.raw)\n    return wattr\n\n\nhandle = ctypes.windll.kernel32.GetStdHandle(STD_OUTPUT_HANDLE)\nreset = get_csbi_attributes(handle)\n\nctypes.windll.kernel32.SetConsoleTextAttribute(handle, FOREGROUND_RED)\nprint \"Cherry on top\"\nctypes.windll.kernel32.SetConsoleTextAttribute(handle, reset)\nimport ctypes\n\n# Constants from the Windows API\nSTD_OUTPUT_HANDLE = -11\nFOREGROUND_RED    = 0x0004 # text color contains red.\n\ndef get_csbi_attributes(handle):\n    # Based on IPython's winconsole.py, written by Alexander Belchenko\n    import struct\n    csbi = ctypes.create_string_buffer(22)\n    res = ctypes.windll.kernel32.GetConsoleScreenBufferInfo(handle, csbi)\n    assert res\n\n    (bufx, bufy, curx, cury, wattr,\n    left, top, right, bottom, maxx, maxy) = struct.unpack(\"hhhhHhhhhhh\", csbi.raw)\n    return wattr\n\n\nhandle = ctypes.windll.kernel32.GetStdHandle(STD_OUTPUT_HANDLE)\nreset = get_csbi_attributes(handle)\n\nctypes.windll.kernel32.SetConsoleTextAttribute(handle, FOREGROUND_RED)\nprint \"Cherry on top\"\nctypes.windll.kernel32.SetConsoleTextAttribute(handle, reset)\n",
                "I have wrapped joeld's answer into a module with global functions that I can use anywhere in my code.joeld's answerFile: log.pydef enable():\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = \"\\033[1m\"\n\ndef disable():\n    HEADER = ''\n    OKBLUE = ''\n    OKGREEN = ''\n    WARNING = ''\n    FAIL = ''\n    ENDC = ''\n\ndef infog(msg):\n    print(OKGREEN + msg + ENDC)\n\ndef info(msg):\n    print(OKBLUE + msg + ENDC)\n\ndef warn(msg):\n    print(WARNING + msg + ENDC)\n\ndef err(msg):\n    print(FAIL + msg + ENDC)\n\nenable()\ndef enable():\n    HEADER = '\\033[95m'\n    OKBLUE = '\\033[94m'\n    OKGREEN = '\\033[92m'\n    WARNING = '\\033[93m'\n    FAIL = '\\033[91m'\n    ENDC = '\\033[0m'\n    BOLD = \"\\033[1m\"\n\ndef disable():\n    HEADER = ''\n    OKBLUE = ''\n    OKGREEN = ''\n    WARNING = ''\n    FAIL = ''\n    ENDC = ''\n\ndef infog(msg):\n    print(OKGREEN + msg + ENDC)\n\ndef info(msg):\n    print(OKBLUE + msg + ENDC)\n\ndef warn(msg):\n    print(WARNING + msg + ENDC)\n\ndef err(msg):\n    print(FAIL + msg + ENDC)\n\nenable()\nUse as follows:import log\nlog.info(\"Hello, World!\")\nlog.err(\"System Error\")\nimport log\nlog.info(\"Hello, World!\")\nlog.err(\"System Error\")\n",
                "def black(text):\n    print('\\033[30m', text, '\\033[0m', sep='')\n\ndef red(text):\n    print('\\033[31m', text, '\\033[0m', sep='')\n\ndef green(text):\n    print('\\033[32m', text, '\\033[0m', sep='')\n\ndef yellow(text):\n    print('\\033[33m', text, '\\033[0m', sep='')\n\ndef blue(text):\n    print('\\033[34m', text, '\\033[0m', sep='')\n\ndef magenta(text):\n    print('\\033[35m', text, '\\033[0m', sep='')\n\ndef cyan(text):\n    print('\\033[36m', text, '\\033[0m', sep='')\n\ndef gray(text):\n    print('\\033[90m', text, '\\033[0m', sep='')\n\n\nblack(\"BLACK\")\nred(\"RED\")\ngreen(\"GREEN\")\nyellow(\"YELLOW\")\nblue(\"BLACK\")\nmagenta(\"MAGENTA\")\ncyan(\"CYAN\")\ngray(\"GRAY\")\ndef black(text):\n    print('\\033[30m', text, '\\033[0m', sep='')\n\ndef red(text):\n    print('\\033[31m', text, '\\033[0m', sep='')\n\ndef green(text):\n    print('\\033[32m', text, '\\033[0m', sep='')\n\ndef yellow(text):\n    print('\\033[33m', text, '\\033[0m', sep='')\n\ndef blue(text):\n    print('\\033[34m', text, '\\033[0m', sep='')\n\ndef magenta(text):\n    print('\\033[35m', text, '\\033[0m', sep='')\n\ndef cyan(text):\n    print('\\033[36m', text, '\\033[0m', sep='')\n\ndef gray(text):\n    print('\\033[90m', text, '\\033[0m', sep='')\n\n\nblack(\"BLACK\")\nred(\"RED\")\ngreen(\"GREEN\")\nyellow(\"YELLOW\")\nblue(\"BLACK\")\nmagenta(\"MAGENTA\")\ncyan(\"CYAN\")\ngray(\"GRAY\")\nTry online Try onlineTry online",
                "Here is my modern (2021) solution: yachalkyachalkIt is one of the few libraries that properly supports nested styles:Apart from that yachalk is auto-complete-friendly, has 256/truecolor support, comes with terminal-capability detection, and is fully typed.Here are some design decision you may consider for choosing your solution.High-level libraries vs low-level libraries / manual style handling?Many answers to this question demonstrate how to ANSI escape codes directly, or suggest low-level libraries that require manual style enabling/disabling.These approaches have subtle issues: Inserting on/off styles manually is\nmore verbose syntactically, because resets have to be specified explicitly,\nmore error prone, because you can accidentally forget to reset a style,\nfails to get edge cases right: For instance in some terminals it is necessary to reset styles before newlines, and re-activate them after the line break. Also, some terminal have problems with simply overriding mutually exclusive styles, and require inserting \"unnecessary\" reset codes. If a developer's local terminal doesn't have these quirks, the developer will not discover these quirks immediately. The issue will only be reported later by others or cause problems e.g. on CI terminals.\nmore verbose syntactically, because resets have to be specified explicitly,more error prone, because you can accidentally forget to reset a style,fails to get edge cases right: For instance in some terminals it is necessary to reset styles before newlines, and re-activate them after the line break. Also, some terminal have problems with simply overriding mutually exclusive styles, and require inserting \"unnecessary\" reset codes. If a developer's local terminal doesn't have these quirks, the developer will not discover these quirks immediately. The issue will only be reported later by others or cause problems e.g. on CI terminals.Therefore if compatibility with many terminals is a goal, it's best to use a high-level library that offers automatic handling of style resets. This allows the library to take care of all edge cases by inserting the \"spurious\" ANSI escape codes where needed.Why yet another library?In JavaScript the de-facto standard library for the task is chalk, and after using it for a while in JS projects, the solutions available in the Python world were lacking in comparison. Not only is the chalk API more convenient to use (fully auto-complete compatible), it also gets all the edge cases right.chalkThe idea of yachalk is to bring the same convenience to the Python ecosystem. If you're interested in a comparison to other libraries I've started feature comparison on the projects page. In addition, here is a long (but still incomplete) list of alternatives that came up during my research -- a lot to choose from :)yachalkfeature comparison\ncolored\nansicolors\ntermcolor\ncolorama\nsty\nblessings\nrich\ncolorit\ncolorprint\nconsole-color\npyfance\ncouleur\nstyle (formerly known as clr)\npychalk\nsimple-chalk\nchlk\nchalky\nconstyle\ncoloredcoloredansicolorsansicolorstermcolortermcolorcoloramacoloramastystyblessingsblessingsrichrichcoloritcoloritcolorprintcolorprintconsole-colorconsole-colorpyfancepyfancecouleurcouleurstyle (formerly known as clr)stylepychalkpychalksimple-chalksimple-chalkchlkchlkchalkychalkyconstyleconstyle",
                "I ended up doing this, and I felt it was cleanest:formatters = {\n    'RED': '\\033[91m',\n    'GREEN': '\\033[92m',\n    'END': '\\033[0m',\n}\n\nprint 'Master is currently {RED}red{END}!'.format(**formatters)\nprint 'Help make master {GREEN}green{END} again!'.format(**formatters)\nformatters = {\n    'RED': '\\033[91m',\n    'GREEN': '\\033[92m',\n    'END': '\\033[0m',\n}\n\nprint 'Master is currently {RED}red{END}!'.format(**formatters)\nprint 'Help make master {GREEN}green{END} again!'.format(**formatters)\n",
                "For Windows you cannot print to console with colors unless you're using the Win32 API.Win32For Linux it's as simple as using print, with the escape sequences outlined here:ColorsColorsFor the character to print like a box, it really depends on what font you are using for the console window. The pound symbol works well, but it depends on the font:#\n#\n",
                "Stupidly simple, based on joeld's answer:joeld's answerclass PrintInColor:\n    RED = '\\033[91m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'\n    LIGHT_PURPLE = '\\033[94m'\n    PURPLE = '\\033[95m'\n    END = '\\033[0m'\n\n    @classmethod\n    def red(cls, s, **kwargs):\n        print(cls.RED + s + cls.END, **kwargs)\n\n    @classmethod\n    def green(cls, s, **kwargs):\n        print(cls.GREEN + s + cls.END, **kwargs)\n\n    @classmethod\n    def yellow(cls, s, **kwargs):\n        print(cls.YELLOW + s + cls.END, **kwargs)\n\n    @classmethod\n    def lightPurple(cls, s, **kwargs):\n        print(cls.LIGHT_PURPLE + s + cls.END, **kwargs)\n\n    @classmethod\n    def purple(cls, s, **kwargs):\n        print(cls.PURPLE + s + cls.END, **kwargs)\nclass PrintInColor:\n    RED = '\\033[91m'\n    GREEN = '\\033[92m'\n    YELLOW = '\\033[93m'\n    LIGHT_PURPLE = '\\033[94m'\n    PURPLE = '\\033[95m'\n    END = '\\033[0m'\n\n    @classmethod\n    def red(cls, s, **kwargs):\n        print(cls.RED + s + cls.END, **kwargs)\n\n    @classmethod\n    def green(cls, s, **kwargs):\n        print(cls.GREEN + s + cls.END, **kwargs)\n\n    @classmethod\n    def yellow(cls, s, **kwargs):\n        print(cls.YELLOW + s + cls.END, **kwargs)\n\n    @classmethod\n    def lightPurple(cls, s, **kwargs):\n        print(cls.LIGHT_PURPLE + s + cls.END, **kwargs)\n\n    @classmethod\n    def purple(cls, s, **kwargs):\n        print(cls.PURPLE + s + cls.END, **kwargs)\nThen justPrintInColor.red('hello', end=' ')\nPrintInColor.green('world')\nPrintInColor.red('hello', end=' ')\nPrintInColor.green('world')\n",
                "Building on joeld's answer, using https://pypi.python.org/pypi/lazyme \npip install -U lazyme:joeld's answerhttps://pypi.python.org/pypi/lazymepip install -U lazymefrom lazyme.string import color_print\n>>> color_print('abc')\nabc\n>>> color_print('abc', color='pink')\nabc\n>>> color_print('abc', color='red')\nabc\n>>> color_print('abc', color='yellow')\nabc\n>>> color_print('abc', color='green')\nabc\n>>> color_print('abc', color='blue', underline=True)\nabc\n>>> color_print('abc', color='blue', underline=True, bold=True)\nabc\n>>> color_print('abc', color='pink', underline=True, bold=True)\nabc\nfrom lazyme.string import color_print\n>>> color_print('abc')\nabc\n>>> color_print('abc', color='pink')\nabc\n>>> color_print('abc', color='red')\nabc\n>>> color_print('abc', color='yellow')\nabc\n>>> color_print('abc', color='green')\nabc\n>>> color_print('abc', color='blue', underline=True)\nabc\n>>> color_print('abc', color='blue', underline=True, bold=True)\nabc\n>>> color_print('abc', color='pink', underline=True, bold=True)\nabc\nScreenshot:Some updates to the color_print with new formatters, e.g.:color_print>>> from lazyme.string import palette, highlighter, formatter\n>>> from lazyme.string import color_print\n>>> palette.keys() # Available colors.\n['pink', 'yellow', 'cyan', 'magenta', 'blue', 'gray', 'default', 'black', 'green', 'white', 'red']\n>>> highlighter.keys() # Available highlights.\n['blue', 'pink', 'gray', 'black', 'yellow', 'cyan', 'green', 'magenta', 'white', 'red']\n>>> formatter.keys() # Available formatter,\n['hide', 'bold', 'italic', 'default', 'fast_blinking', 'faint', 'strikethrough', 'underline', 'blinking', 'reverse']\n>>> from lazyme.string import palette, highlighter, formatter\n>>> from lazyme.string import color_print\n>>> palette.keys() # Available colors.\n['pink', 'yellow', 'cyan', 'magenta', 'blue', 'gray', 'default', 'black', 'green', 'white', 'red']\n>>> highlighter.keys() # Available highlights.\n['blue', 'pink', 'gray', 'black', 'yellow', 'cyan', 'green', 'magenta', 'white', 'red']\n>>> formatter.keys() # Available formatter,\n['hide', 'bold', 'italic', 'default', 'fast_blinking', 'faint', 'strikethrough', 'underline', 'blinking', 'reverse']\nNote: italic, fast blinking, and strikethrough may not work on all terminals, and they don't work on Mac and Ubuntu.italicfast blinkingstrikethroughE.g.,>>> color_print('foo bar', color='pink', highlight='white')\nfoo bar\n>>> color_print('foo bar', color='pink', highlight='white', reverse=True)\nfoo bar\n>>> color_print('foo bar', color='pink', highlight='white', bold=True)\nfoo bar\n>>> color_print('foo bar', color='pink', highlight='white', faint=True)\nfoo bar\n>>> color_print('foo bar', color='pink', highlight='white', faint=True, reverse=True)\nfoo bar\n>>> color_print('foo bar', color='pink', highlight='white', underline=True, reverse=True)\nfoo bar\n>>> color_print('foo bar', color='pink', highlight='white')\nfoo bar\n>>> color_print('foo bar', color='pink', highlight='white', reverse=True)\nfoo bar\n>>> color_print('foo bar', color='pink', highlight='white', bold=True)\nfoo bar\n>>> color_print('foo bar', color='pink', highlight='white', faint=True)\nfoo bar\n>>> color_print('foo bar', color='pink', highlight='white', faint=True, reverse=True)\nfoo bar\n>>> color_print('foo bar', color='pink', highlight='white', underline=True, reverse=True)\nfoo bar\nScreenshot:",
                "Note how well the with keyword mixes with modifiers like these that need to be reset (using Python 3 and Colorama):withfrom colorama import Fore, Style\nimport sys\n\nclass Highlight:\n  def __init__(self, clazz, color):\n    self.color = color\n    self.clazz = clazz\n  def __enter__(self):\n    print(self.color, end=\"\")\n  def __exit__(self, type, value, traceback):\n    if self.clazz == Fore:\n      print(Fore.RESET, end=\"\")\n    else:\n      assert self.clazz == Style\n      print(Style.RESET_ALL, end=\"\")\n    sys.stdout.flush()\n\nwith Highlight(Fore, Fore.GREEN):\n  print(\"this is highlighted\")\nprint(\"this is not\")\nfrom colorama import Fore, Style\nimport sys\n\nclass Highlight:\n  def __init__(self, clazz, color):\n    self.color = color\n    self.clazz = clazz\n  def __enter__(self):\n    print(self.color, end=\"\")\n  def __exit__(self, type, value, traceback):\n    if self.clazz == Fore:\n      print(Fore.RESET, end=\"\")\n    else:\n      assert self.clazz == Style\n      print(Style.RESET_ALL, end=\"\")\n    sys.stdout.flush()\n\nwith Highlight(Fore, Fore.GREEN):\n  print(\"this is highlighted\")\nprint(\"this is not\")\n",
                "You could use Clint:Clintfrom clint.textui import colored\nprint colored.red('some warning message')\nprint colored.green('nicely done!')\nfrom clint.textui import colored\nprint colored.red('some warning message')\nprint colored.green('nicely done!')\n",
                "You can use the Python implementation of the curses library:\ncurses \u2014 Terminal handling for character-cell displayscursescurses \u2014 Terminal handling for character-cell displayscurses \u2014 Terminal handling for character-cell displaysAlso, run this and you'll find your box:for i in range(255):\n    print i, chr(i)\nfor i in range(255):\n    print i, chr(i)\n",
                "EmojiYou can use colors for text as others mentioned in their answers to have colorful text with a background or foreground color.But you can use emojis instead! for example, you can use\u26a0\ufe0f for warning messages and \ud83d\uded1 for error messages.emojis\u26a0\ufe0f\ud83d\uded1Or simply use these notebooks as a color:\ud83d\udcd5: error message\n\ud83d\udcd9: warning message\n\ud83d\udcd7: ok status message\n\ud83d\udcd8: action message\n\ud83d\udcd3: canceled status message\n\ud83d\udcd4: Or anything you like and want to recognize immediately by color\n\n\ud83d\udcd5: error message\n\ud83d\udcd9: warning message\n\ud83d\udcd7: ok status message\n\ud83d\udcd8: action message\n\ud83d\udcd3: canceled status message\n\ud83d\udcd4: Or anything you like and want to recognize immediately by color\n\n\ud83c\udf81 Bonus:This method also helps you to quickly scan and find logs directly in the source code.directly in the source codeBut some operating systems (including some Linux distributions in some version with some window managers) default emoji font is not colorful by default and you may want to make them colorful, first.But some operating systems (including some Linux distributions in some version with some window managers) default emoji font is not colorful by default and you may want to make them colorful, first.How to open emoji picker?mac os: control + command + spacemac os:mac oscontrolcommandspacewindows: win + .windows:windowswin.linux: control + . or  control + ;linux:linuxcontrol.control;",
                "If you are programming a game perhaps you would like to change the background color and use only spaces? For example:print \" \"+ \"\\033[01;41m\" + \" \" +\"\\033[01;46m\"  + \"  \" + \"\\033[01;42m\"\nprint \" \"+ \"\\033[01;41m\" + \" \" +\"\\033[01;46m\"  + \"  \" + \"\\033[01;42m\"\n",
                "An easier option would be to use the cprint function from the termcolor package.cprinttermcolortermcolorIt also supports %s, %d format of printing:%s, %dResults can be terminal dependant, so review the Terminal Properties section of the package documentation.Terminal Properties\nWindows Command Prompt and Python IDLE don't work\nWindows Command Prompt and Python IDLE don't work\nJupyterLab notebook does work\nJupyterLab notebook does work",
                "YAY! Another versionWhile I find this answer useful, I modified it a bit. This GitHub Gist is the resultthis answerGitHub GistUsageprint colors.draw(\"i'm yellow\", bold=True, fg_yellow=True)\nprint colors.draw(\"i'm yellow\", bold=True, fg_yellow=True)\nIn addition, you can wrap common usages:print colors.error('sorry, ')\nprint colors.error('sorry, ')\nhttps://gist.github.com/Jossef/0ee20314577925b4027fhttps://gist.github.com/Jossef/0ee20314577925b4027f"
            ]
        },
        {
            "tag": "split_list",
            "patterns": [
                "How do I split a list into equally-sized chunks?",
                "tell me the best way to sort the lists into equally-sized chunks?",
                "what can i do to split a list into smaller chunks?",
                "how do i split a given list into equally-sized chunks?",
                "how do i break a list into equally-sized chunks?",
                "how do i split my list into equally-sized chunks?",
                "how do i split a list into equally-sized chunks?"
            ],
            "responses": [
                "Here's a generator that yields evenly-sized chunks:def chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in range(0, len(lst), n):\n        yield lst[i:i + n]\nimport pprint\npprint.pprint(list(chunks(range(10, 75), 10)))\n[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n [70, 71, 72, 73, 74]]\nimport pprint\npprint.pprint(list(chunks(range(10, 75), 10)))\n[[10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n [70, 71, 72, 73, 74]]\nFor Python 2, using xrange instead of range:xrangerangedef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in xrange(0, len(lst), n):\n        yield lst[i:i + n]\ndef chunks(lst, n):\n    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n    for i in xrange(0, len(lst), n):\n        yield lst[i:i + n]\nBelow is a list comprehension one-liner. The method above is preferable, though, since using named functions makes code easier to understand. For Python 3:[lst[i:i + n] for i in range(0, len(lst), n)]\n[lst[i:i + n] for i in range(0, len(lst), n)]\nFor Python 2:[lst[i:i + n] for i in xrange(0, len(lst), n)]\n[lst[i:i + n] for i in xrange(0, len(lst), n)]\n",
                "Something super simple:def chunks(xs, n):\n    n = max(1, n)\n    return (xs[i:i+n] for i in range(0, len(xs), n))\ndef chunks(xs, n):\n    n = max(1, n)\n    return (xs[i:i+n] for i in range(0, len(xs), n))\nFor Python 2, use xrange() instead of range().xrange()range()",
                "I know this is kind of old but nobody yet mentioned numpy.array_split:numpy.array_splitnumpy.array_splitimport numpy as np\n\nlst = range(50)\nnp.array_split(lst, 5)\nimport numpy as np\n\nlst = range(50)\nnp.array_split(lst, 5)\nResult:[array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),\n array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])]\n[array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n array([10, 11, 12, 13, 14, 15, 16, 17, 18, 19]),\n array([20, 21, 22, 23, 24, 25, 26, 27, 28, 29]),\n array([30, 31, 32, 33, 34, 35, 36, 37, 38, 39]),\n array([40, 41, 42, 43, 44, 45, 46, 47, 48, 49])]\n",
                "Directly from the (old) Python documentation (recipes for itertools):from itertools import izip, chain, repeat\n\ndef grouper(n, iterable, padvalue=None):\n    \"grouper(3, 'abcdefg', 'x') --> ('a','b','c'), ('d','e','f'), ('g','x','x')\"\n    return izip(*[chain(iterable, repeat(padvalue, n-1))]*n)\nfrom itertools import izip, chain, repeat\n\ndef grouper(n, iterable, padvalue=None):\n    \"grouper(3, 'abcdefg', 'x') --> ('a','b','c'), ('d','e','f'), ('g','x','x')\"\n    return izip(*[chain(iterable, repeat(padvalue, n-1))]*n)\nThe current version, as suggested by J.F.Sebastian:#from itertools import izip_longest as zip_longest # for Python 2.x\nfrom itertools import zip_longest # for Python 3.x\n#from six.moves import zip_longest # for both (uses the six compat library)\n\ndef grouper(n, iterable, padvalue=None):\n    \"grouper(3, 'abcdefg', 'x') --> ('a','b','c'), ('d','e','f'), ('g','x','x')\"\n    return zip_longest(*[iter(iterable)]*n, fillvalue=padvalue)\n#from itertools import izip_longest as zip_longest # for Python 2.x\nfrom itertools import zip_longest # for Python 3.x\n#from six.moves import zip_longest # for both (uses the six compat library)\n\ndef grouper(n, iterable, padvalue=None):\n    \"grouper(3, 'abcdefg', 'x') --> ('a','b','c'), ('d','e','f'), ('g','x','x')\"\n    return zip_longest(*[iter(iterable)]*n, fillvalue=padvalue)\nI guess Guido's time machine works\u2014worked\u2014will work\u2014will have worked\u2014was working again.These solutions work because [iter(iterable)]*n (or the equivalent in the earlier version) creates one iterator, repeated n times in the list. izip_longest then effectively performs a round-robin of \"each\" iterator; because this is the same iterator, it is advanced by each such call, resulting in each such zip-roundrobin generating one tuple of n items.[iter(iterable)]*nonenizip_longestn",
                "I'm surprised nobody has thought of using iter's two-argument form:itertwo-argument formfrom itertools import islice\n\ndef chunk(it, size):\n    it = iter(it)\n    return iter(lambda: tuple(islice(it, size)), ())\nfrom itertools import islice\n\ndef chunk(it, size):\n    it = iter(it)\n    return iter(lambda: tuple(islice(it, size)), ())\nDemo:>>> list(chunk(range(14), 3))\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13)]\n>>> list(chunk(range(14), 3))\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13)]\nThis works with any iterable and produces output lazily. It returns tuples rather than iterators, but I think it has a certain elegance nonetheless. It also doesn't pad; if you want padding, a simple variation on the above will suffice:from itertools import islice, chain, repeat\n\ndef chunk_pad(it, size, padval=None):\n    it = chain(iter(it), repeat(padval))\n    return iter(lambda: tuple(islice(it, size)), (padval,) * size)\nfrom itertools import islice, chain, repeat\n\ndef chunk_pad(it, size, padval=None):\n    it = chain(iter(it), repeat(padval))\n    return iter(lambda: tuple(islice(it, size)), (padval,) * size)\nDemo:>>> list(chunk_pad(range(14), 3))\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, None)]\n>>> list(chunk_pad(range(14), 3, 'a'))\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, 'a')]\n>>> list(chunk_pad(range(14), 3))\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, None)]\n>>> list(chunk_pad(range(14), 3, 'a'))\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, 'a')]\nLike the izip_longest-based solutions, the above always pads. As far as I know, there's no one- or two-line itertools recipe for a function that optionally pads. By combining the above two approaches, this one comes pretty close:izip_longestalwaysoptionally_no_padding = object()\n\ndef chunk(it, size, padval=_no_padding):\n    if padval == _no_padding:\n        it = iter(it)\n        sentinel = ()\n    else:\n        it = chain(iter(it), repeat(padval))\n        sentinel = (padval,) * size\n    return iter(lambda: tuple(islice(it, size)), sentinel)\n_no_padding = object()\n\ndef chunk(it, size, padval=_no_padding):\n    if padval == _no_padding:\n        it = iter(it)\n        sentinel = ()\n    else:\n        it = chain(iter(it), repeat(padval))\n        sentinel = (padval,) * size\n    return iter(lambda: tuple(islice(it, size)), sentinel)\nDemo:>>> list(chunk(range(14), 3))\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13)]\n>>> list(chunk(range(14), 3, None))\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, None)]\n>>> list(chunk(range(14), 3, 'a'))\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, 'a')]\n>>> list(chunk(range(14), 3))\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13)]\n>>> list(chunk(range(14), 3, None))\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, None)]\n>>> list(chunk(range(14), 3, 'a'))\n[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, 11), (12, 13, 'a')]\nI believe this is the shortest chunker proposed that offers optional padding.As Tomasz Gandor observed, the two padding chunkers will stop unexpectedly if they encounter a long sequence of pad values. Here's a final variation that works around that problem in a reasonable way:observed_no_padding = object()\ndef chunk(it, size, padval=_no_padding):\n    it = iter(it)\n    chunker = iter(lambda: tuple(islice(it, size)), ())\n    if padval == _no_padding:\n        yield from chunker\n    else:\n        for ch in chunker:\n            yield ch if len(ch) == size else ch + (padval,) * (size - len(ch))\n_no_padding = object()\ndef chunk(it, size, padval=_no_padding):\n    it = iter(it)\n    chunker = iter(lambda: tuple(islice(it, size)), ())\n    if padval == _no_padding:\n        yield from chunker\n    else:\n        for ch in chunker:\n            yield ch if len(ch) == size else ch + (padval,) * (size - len(ch))\nDemo:>>> list(chunk([1, 2, (), (), 5], 2))\n[(1, 2), ((), ()), (5,)]\n>>> list(chunk([1, 2, None, None, 5], 2, None))\n[(1, 2), (None, None), (5, None)]\n>>> list(chunk([1, 2, (), (), 5], 2))\n[(1, 2), ((), ()), (5,)]\n>>> list(chunk([1, 2, None, None, 5], 2, None))\n[(1, 2), (None, None), (5, None)]\n",
                "Here is a generator that work on arbitrary iterables:def split_seq(iterable, size):\n    it = iter(iterable)\n    item = list(itertools.islice(it, size))\n    while item:\n        yield item\n        item = list(itertools.islice(it, size))\ndef split_seq(iterable, size):\n    it = iter(iterable)\n    item = list(itertools.islice(it, size))\n    while item:\n        yield item\n        item = list(itertools.islice(it, size))\nExample:>>> import pprint\n>>> pprint.pprint(list(split_seq(xrange(75), 10)))\n[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n [70, 71, 72, 73, 74]]\n>>> import pprint\n>>> pprint.pprint(list(split_seq(xrange(75), 10)))\n[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n [70, 71, 72, 73, 74]]\n",
                "Simple yet elegantL = range(1, 1000)\nprint [L[x:x+10] for x in xrange(0, len(L), 10)]\nL = range(1, 1000)\nprint [L[x:x+10] for x in xrange(0, len(L), 10)]\nor if you prefer:def chunks(L, n): return [L[x: x+n] for x in xrange(0, len(L), n)]\nchunks(L, 10)\ndef chunks(L, n): return [L[x: x+n] for x in xrange(0, len(L), n)]\nchunks(L, 10)\n",
                "Don't reinvent the wheel.Don't reinvent the wheel.UPDATE: The upcoming Python 3.12 introduces itertools.batched, which solves this problem at last.  See below.UPDATE: The upcoming Python 3.12 introduces itertools.batched, which solves this problem at last.  See below.UPDATEintroduces itertools.batcheditertools.batchedGivenGivenimport itertools as it\nimport collections as ct\n\nimport more_itertools as mit\n\n\niterable = range(11)\nn = 3\nimport itertools as it\nimport collections as ct\n\nimport more_itertools as mit\n\n\niterable = range(11)\nn = 3\nCodeCodeitertools.batched++itertools.batcheditertools.batcheditertools.batched++list(it.batched(iterable, n))\n# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]\nlist(it.batched(iterable, n))\n# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]\nmore_itertools+more_itertoolsmore_itertoolsmore_itertools+list(mit.chunked(iterable, n))\n# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]\n\nlist(mit.sliced(iterable, n))\n# [range(0, 3), range(3, 6), range(6, 9), range(9, 11)]\n\nlist(mit.grouper(n, iterable))\n# [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, None)]\n\nlist(mit.windowed(iterable, len(iterable)//n, step=n))\n# [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, None)]\n\nlist(mit.chunked_even(iterable, n))\n# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]\nlist(mit.chunked(iterable, n))\n# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]\n\nlist(mit.sliced(iterable, n))\n# [range(0, 3), range(3, 6), range(6, 9), range(9, 11)]\n\nlist(mit.grouper(n, iterable))\n# [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, None)]\n\nlist(mit.windowed(iterable, len(iterable)//n, step=n))\n# [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, None)]\n\nlist(mit.chunked_even(iterable, n))\n# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]\n(or DIY, if you want)The Standard LibraryThe Standard Librarylist(it.zip_longest(*[iter(iterable)] * n))\n# [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, None)]\nlist(it.zip_longest(*[iter(iterable)] * n))\n# [(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, 10, None)]\nd = {}\nfor i, x in enumerate(iterable):\n    d.setdefault(i//n, []).append(x)\n    \n\nlist(d.values())\n# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]\nd = {}\nfor i, x in enumerate(iterable):\n    d.setdefault(i//n, []).append(x)\n    \n\nlist(d.values())\n# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]\ndd = ct.defaultdict(list)\nfor i, x in enumerate(iterable):\n    dd[i//n].append(x)\n    \n\nlist(dd.values())\n# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]\ndd = ct.defaultdict(list)\nfor i, x in enumerate(iterable):\n    dd[i//n].append(x)\n    \n\nlist(dd.values())\n# [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10]]\nReferencesReferences\nmore_itertools.chunked (related posted)\nmore_itertools.sliced\nmore_itertools.grouper (related post)\nmore_itertools.windowed (see also stagger, zip_offset)\nmore_itertools.chunked_even\nzip_longest (related post, related post)\nsetdefault (ordered results requires Python 3.6+)\ncollections.defaultdict  (ordered results requires Python 3.6+)\nmore_itertools.chunked (related posted)more_itertools.chunkedmore_itertools.chunkedrelated postedmore_itertools.slicedmore_itertools.slicedmore_itertools.slicedmore_itertools.grouper (related post)more_itertools.groupermore_itertools.grouperrelated postmore_itertools.windowed (see also stagger, zip_offset)more_itertools.windowedmore_itertools.windowedstaggerstaggerzip_offsetzip_offsetmore_itertools.chunked_evenmore_itertools.chunked_evenmore_itertools.chunked_evenzip_longest (related post, related post)zip_longestzip_longestrelated postrelated postsetdefault (ordered results requires Python 3.6+)setdefaultsetdefaultcollections.defaultdict  (ordered results requires Python 3.6+)collections.defaultdictcollections.defaultdict+ A third-party library that implements itertools recipes and more. > pip install more_itertools + A third-party library that implements itertools recipes and more. > pip install more_itertools +itertools recipes> pip install more_itertools++Included in Python Standard Library 3.12+.  batched is similar to more_itertools.chunked.++Included in Python Standard Library 3.12+.  batched is similar to more_itertools.chunked.++batchedmore_itertools.chunked",
                "def chunk(input, size):\n    return map(None, *([iter(input)] * size))\ndef chunk(input, size):\n    return map(None, *([iter(input)] * size))\n",
                "How do you split a list into evenly sized chunks?\"Evenly sized chunks\", to me, implies that they are all the same length, or barring that option, at minimal variance in length. E.g. 5 baskets for 21 items could have the following results:minimal variance>>> import statistics\n>>> statistics.variance([5,5,5,5,1]) \n3.2\n>>> statistics.variance([5,4,4,4,4]) \n0.19999999999999998\n>>> import statistics\n>>> statistics.variance([5,5,5,5,1]) \n3.2\n>>> statistics.variance([5,4,4,4,4]) \n0.19999999999999998\nA practical reason to prefer the latter result: if you were using these functions to distribute work, you've built-in the prospect of one likely finishing well before the others, so it would sit around doing nothing while the others continued working hard.Critique of other answers hereWhen I originally wrote this answer, none of the other answers were evenly sized chunks - they all leave a runt chunk at the end, so they're not well balanced, and have a higher than necessary variance of lengths.For example, the current top answer ends with:[60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n[70, 71, 72, 73, 74]]\n[60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n[70, 71, 72, 73, 74]]\nOthers, like list(grouper(3, range(7))), and chunk(range(7), 3) both return: [(0, 1, 2), (3, 4, 5), (6, None, None)]. The None's are just padding, and rather inelegant in my opinion. They are NOT evenly chunking the iterables.list(grouper(3, range(7)))chunk(range(7), 3)[(0, 1, 2), (3, 4, 5), (6, None, None)]NoneWhy can't we divide these better?Cycle SolutionA high-level balanced solution using itertools.cycle, which is the way I might do it today. Here's the setup:itertools.cyclefrom itertools import cycle\nitems = range(10, 75)\nnumber_of_baskets = 10\nfrom itertools import cycle\nitems = range(10, 75)\nnumber_of_baskets = 10\nNow we need our lists into which to populate the elements:baskets = [[] for _ in range(number_of_baskets)]\nbaskets = [[] for _ in range(number_of_baskets)]\nFinally, we zip the elements we're going to allocate together with a cycle of the baskets until we run out of elements, which, semantically, it exactly what we want:for element, basket in zip(items, cycle(baskets)):\n    basket.append(element)\nfor element, basket in zip(items, cycle(baskets)):\n    basket.append(element)\nHere's the result:>>> from pprint import pprint\n>>> pprint(baskets)\n[[10, 20, 30, 40, 50, 60, 70],\n [11, 21, 31, 41, 51, 61, 71],\n [12, 22, 32, 42, 52, 62, 72],\n [13, 23, 33, 43, 53, 63, 73],\n [14, 24, 34, 44, 54, 64, 74],\n [15, 25, 35, 45, 55, 65],\n [16, 26, 36, 46, 56, 66],\n [17, 27, 37, 47, 57, 67],\n [18, 28, 38, 48, 58, 68],\n [19, 29, 39, 49, 59, 69]]\n>>> from pprint import pprint\n>>> pprint(baskets)\n[[10, 20, 30, 40, 50, 60, 70],\n [11, 21, 31, 41, 51, 61, 71],\n [12, 22, 32, 42, 52, 62, 72],\n [13, 23, 33, 43, 53, 63, 73],\n [14, 24, 34, 44, 54, 64, 74],\n [15, 25, 35, 45, 55, 65],\n [16, 26, 36, 46, 56, 66],\n [17, 27, 37, 47, 57, 67],\n [18, 28, 38, 48, 58, 68],\n [19, 29, 39, 49, 59, 69]]\nTo productionize this solution, we write a function, and provide the type annotations:from itertools import cycle\nfrom typing import List, Any\n\ndef cycle_baskets(items: List[Any], maxbaskets: int) -> List[List[Any]]:\n    baskets = [[] for _ in range(min(maxbaskets, len(items)))]\n    for item, basket in zip(items, cycle(baskets)):\n        basket.append(item)\n    return baskets\nfrom itertools import cycle\nfrom typing import List, Any\n\ndef cycle_baskets(items: List[Any], maxbaskets: int) -> List[List[Any]]:\n    baskets = [[] for _ in range(min(maxbaskets, len(items)))]\n    for item, basket in zip(items, cycle(baskets)):\n        basket.append(item)\n    return baskets\nIn the above, we take our list of items, and the max number of baskets. We create a list of empty lists, in which to append each element, in a round-robin style.SlicesAnother elegant solution is to use slices - specifically the less-commonly used step argument to slices. i.e.:stepstart = 0\nstop = None\nstep = number_of_baskets\n\nfirst_basket = items[start:stop:step]\nstart = 0\nstop = None\nstep = number_of_baskets\n\nfirst_basket = items[start:stop:step]\nThis is especially elegant in that slices don't care how long the data are - the result, our first basket, is only as long as it needs to be. We'll only need to increment the starting point for each basket.In fact this could be a one-liner, but we'll go multiline for readability and to avoid an overlong line of code:from typing import List, Any\n\ndef slice_baskets(items: List[Any], maxbaskets: int) -> List[List[Any]]:\n    n_baskets = min(maxbaskets, len(items))\n    return [items[i::n_baskets] for i in range(n_baskets)]\nfrom typing import List, Any\n\ndef slice_baskets(items: List[Any], maxbaskets: int) -> List[List[Any]]:\n    n_baskets = min(maxbaskets, len(items))\n    return [items[i::n_baskets] for i in range(n_baskets)]\nAnd islice from the itertools module will provide a lazily iterating approach, like that which was originally asked for in the question.isliceI don't expect most use-cases to benefit very much, as the original data is already fully materialized in a list, but for large datasets, it could save nearly half the memory usage.from itertools import islice\nfrom typing import List, Any, Generator\n    \ndef yield_islice_baskets(items: List[Any], maxbaskets: int) -> Generator[List[Any], None, None]:\n    n_baskets = min(maxbaskets, len(items))\n    for i in range(n_baskets):\n        yield islice(items, i, None, n_baskets)\nfrom itertools import islice\nfrom typing import List, Any, Generator\n    \ndef yield_islice_baskets(items: List[Any], maxbaskets: int) -> Generator[List[Any], None, None]:\n    n_baskets = min(maxbaskets, len(items))\n    for i in range(n_baskets):\n        yield islice(items, i, None, n_baskets)\nView results with:from pprint import pprint\n\nitems = list(range(10, 75))\npprint(cycle_baskets(items, 10))\npprint(slice_baskets(items, 10))\npprint([list(s) for s in yield_islice_baskets(items, 10)])\nfrom pprint import pprint\n\nitems = list(range(10, 75))\npprint(cycle_baskets(items, 10))\npprint(slice_baskets(items, 10))\npprint([list(s) for s in yield_islice_baskets(items, 10)])\nUpdated prior solutionsHere's another balanced solution, adapted from a function I've used in production in the past, that uses the modulo operator:def baskets_from(items, maxbaskets=25):\n    baskets = [[] for _ in range(maxbaskets)]\n    for i, item in enumerate(items):\n        baskets[i % maxbaskets].append(item)\n    return filter(None, baskets) \ndef baskets_from(items, maxbaskets=25):\n    baskets = [[] for _ in range(maxbaskets)]\n    for i, item in enumerate(items):\n        baskets[i % maxbaskets].append(item)\n    return filter(None, baskets) \nAnd I created a generator that does the same if you put it into a list:def iter_baskets_from(items, maxbaskets=3):\n    '''generates evenly balanced baskets from indexable iterable'''\n    item_count = len(items)\n    baskets = min(item_count, maxbaskets)\n    for x_i in range(baskets):\n        yield [items[y_i] for y_i in range(x_i, item_count, baskets)]\n    \ndef iter_baskets_from(items, maxbaskets=3):\n    '''generates evenly balanced baskets from indexable iterable'''\n    item_count = len(items)\n    baskets = min(item_count, maxbaskets)\n    for x_i in range(baskets):\n        yield [items[y_i] for y_i in range(x_i, item_count, baskets)]\n    \nAnd finally, since I see that all of the above functions return elements in a contiguous order (as they were given):def iter_baskets_contiguous(items, maxbaskets=3, item_count=None):\n    '''\n    generates balanced baskets from iterable, contiguous contents\n    provide item_count if providing a iterator that doesn't support len()\n    '''\n    item_count = item_count or len(items)\n    baskets = min(item_count, maxbaskets)\n    items = iter(items)\n    floor = item_count // baskets \n    ceiling = floor + 1\n    stepdown = item_count % baskets\n    for x_i in range(baskets):\n        length = ceiling if x_i < stepdown else floor\n        yield [items.next() for _ in range(length)]\ndef iter_baskets_contiguous(items, maxbaskets=3, item_count=None):\n    '''\n    generates balanced baskets from iterable, contiguous contents\n    provide item_count if providing a iterator that doesn't support len()\n    '''\n    item_count = item_count or len(items)\n    baskets = min(item_count, maxbaskets)\n    items = iter(items)\n    floor = item_count // baskets \n    ceiling = floor + 1\n    stepdown = item_count % baskets\n    for x_i in range(baskets):\n        length = ceiling if x_i < stepdown else floor\n        yield [items.next() for _ in range(length)]\nOutputTo test them out:print(baskets_from(range(6), 8))\nprint(list(iter_baskets_from(range(6), 8)))\nprint(list(iter_baskets_contiguous(range(6), 8)))\nprint(baskets_from(range(22), 8))\nprint(list(iter_baskets_from(range(22), 8)))\nprint(list(iter_baskets_contiguous(range(22), 8)))\nprint(baskets_from('ABCDEFG', 3))\nprint(list(iter_baskets_from('ABCDEFG', 3)))\nprint(list(iter_baskets_contiguous('ABCDEFG', 3)))\nprint(baskets_from(range(26), 5))\nprint(list(iter_baskets_from(range(26), 5)))\nprint(list(iter_baskets_contiguous(range(26), 5)))\nprint(baskets_from(range(6), 8))\nprint(list(iter_baskets_from(range(6), 8)))\nprint(list(iter_baskets_contiguous(range(6), 8)))\nprint(baskets_from(range(22), 8))\nprint(list(iter_baskets_from(range(22), 8)))\nprint(list(iter_baskets_contiguous(range(22), 8)))\nprint(baskets_from('ABCDEFG', 3))\nprint(list(iter_baskets_from('ABCDEFG', 3)))\nprint(list(iter_baskets_contiguous('ABCDEFG', 3)))\nprint(baskets_from(range(26), 5))\nprint(list(iter_baskets_from(range(26), 5)))\nprint(list(iter_baskets_contiguous(range(26), 5)))\nWhich prints out:[[0], [1], [2], [3], [4], [5]]\n[[0], [1], [2], [3], [4], [5]]\n[[0], [1], [2], [3], [4], [5]]\n[[0, 8, 16], [1, 9, 17], [2, 10, 18], [3, 11, 19], [4, 12, 20], [5, 13, 21], [6, 14], [7, 15]]\n[[0, 8, 16], [1, 9, 17], [2, 10, 18], [3, 11, 19], [4, 12, 20], [5, 13, 21], [6, 14], [7, 15]]\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17], [18, 19], [20, 21]]\n[['A', 'D', 'G'], ['B', 'E'], ['C', 'F']]\n[['A', 'D', 'G'], ['B', 'E'], ['C', 'F']]\n[['A', 'B', 'C'], ['D', 'E'], ['F', 'G']]\n[[0, 5, 10, 15, 20, 25], [1, 6, 11, 16, 21], [2, 7, 12, 17, 22], [3, 8, 13, 18, 23], [4, 9, 14, 19, 24]]\n[[0, 5, 10, 15, 20, 25], [1, 6, 11, 16, 21], [2, 7, 12, 17, 22], [3, 8, 13, 18, 23], [4, 9, 14, 19, 24]]\n[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]\n[[0], [1], [2], [3], [4], [5]]\n[[0], [1], [2], [3], [4], [5]]\n[[0], [1], [2], [3], [4], [5]]\n[[0, 8, 16], [1, 9, 17], [2, 10, 18], [3, 11, 19], [4, 12, 20], [5, 13, 21], [6, 14], [7, 15]]\n[[0, 8, 16], [1, 9, 17], [2, 10, 18], [3, 11, 19], [4, 12, 20], [5, 13, 21], [6, 14], [7, 15]]\n[[0, 1, 2], [3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14], [15, 16, 17], [18, 19], [20, 21]]\n[['A', 'D', 'G'], ['B', 'E'], ['C', 'F']]\n[['A', 'D', 'G'], ['B', 'E'], ['C', 'F']]\n[['A', 'B', 'C'], ['D', 'E'], ['F', 'G']]\n[[0, 5, 10, 15, 20, 25], [1, 6, 11, 16, 21], [2, 7, 12, 17, 22], [3, 8, 13, 18, 23], [4, 9, 14, 19, 24]]\n[[0, 5, 10, 15, 20, 25], [1, 6, 11, 16, 21], [2, 7, 12, 17, 22], [3, 8, 13, 18, 23], [4, 9, 14, 19, 24]]\n[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13, 14, 15], [16, 17, 18, 19, 20], [21, 22, 23, 24, 25]]\nNotice that the contiguous generator provide chunks in the same length patterns as the other two, but the items are all in order, and they are as evenly divided as one may divide a list of discrete elements.",
                "If you know list size:def SplitList(mylist, chunk_size):\n    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\ndef SplitList(mylist, chunk_size):\n    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\nIf you don't (an iterator):def IterChunks(sequence, chunk_size):\n    res = []\n    for item in sequence:\n        res.append(item)\n        if len(res) >= chunk_size:\n            yield res\n            res = []\n    if res:\n        yield res  # yield the last, incomplete, portion\ndef IterChunks(sequence, chunk_size):\n    res = []\n    for item in sequence:\n        res.append(item)\n        if len(res) >= chunk_size:\n            yield res\n            res = []\n    if res:\n        yield res  # yield the last, incomplete, portion\nIn the latter case, it can be rephrased in a more beautiful way if you can be sure that the sequence always contains a whole number of chunks of given size (i.e. there is no incomplete last chunk).",
                "I saw the most awesome Python-ish answer in a duplicate of this question:duplicatefrom itertools import zip_longest\n\na = range(1, 16)\ni = iter(a)\nr = list(zip_longest(i, i, i))\n>>> print(r)\n[(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12), (13, 14, 15)]\nfrom itertools import zip_longest\n\na = range(1, 16)\ni = iter(a)\nr = list(zip_longest(i, i, i))\n>>> print(r)\n[(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12), (13, 14, 15)]\nYou can create n-tuple for any n. If a = range(1, 15), then the result will be:a = range(1, 15)[(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12), (13, 14, None)]\n[(1, 2, 3), (4, 5, 6), (7, 8, 9), (10, 11, 12), (13, 14, None)]\nIf the list is divided evenly, then you can replace zip_longest with zip, otherwise the triplet (13, 14, None) would be lost. Python 3 is used above. For Python 2, use izip_longest.zip_longestzip(13, 14, None)izip_longest",
                "[AA[i:i+SS] for i in range(len(AA))[::SS]]\n[AA[i:i+SS] for i in range(len(AA))[::SS]]\nWhere AA is array, SS is chunk size. For example:>>> AA=range(10,21);SS=3\n>>> [AA[i:i+SS] for i in range(len(AA))[::SS]]\n[[10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20]]\n# or [range(10, 13), range(13, 16), range(16, 19), range(19, 21)] in py3\n>>> AA=range(10,21);SS=3\n>>> [AA[i:i+SS] for i in range(len(AA))[::SS]]\n[[10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20]]\n# or [range(10, 13), range(13, 16), range(16, 19), range(19, 21)] in py3\nTo expand the ranges in py3 do(py3) >>> [list(AA[i:i+SS]) for i in range(len(AA))[::SS]]\n[[10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20]]\n(py3) >>> [list(AA[i:i+SS]) for i in range(len(AA))[::SS]]\n[[10, 11, 12], [13, 14, 15], [16, 17, 18], [19, 20]]\n",
                "With Assignment Expressions in Python 3.8 it becomes quite nice:Assignment Expressionsimport itertools\n\ndef batch(iterable, size):\n    it = iter(iterable)\n    while item := list(itertools.islice(it, size)):\n        yield item\nimport itertools\n\ndef batch(iterable, size):\n    it = iter(iterable)\n    while item := list(itertools.islice(it, size)):\n        yield item\nThis works on an arbitrary iterable, not just a list.>>> import pprint\n>>> pprint.pprint(list(batch(range(75), 10)))\n[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n [70, 71, 72, 73, 74]]\n>>> import pprint\n>>> pprint.pprint(list(batch(range(75), 10)))\n[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],\n [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],\n [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],\n [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],\n [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],\n [70, 71, 72, 73, 74]]\nUPDATEUPDATEStarting with Python 3.12, this exact implementation is available as itertools.batcheditertools.batched",
                "If you had a chunk size of 3 for example, you could do:zip(*[iterable[i::3] for i in range(3)]) \nzip(*[iterable[i::3] for i in range(3)]) \nsource:\nhttp://code.activestate.com/recipes/303060-group-a-list-into-sequential-n-tuples/http://code.activestate.com/recipes/303060-group-a-list-into-sequential-n-tuples/I would use this when my chunk size is fixed number I can type, e.g. '3', and would never change.",
                "The toolz library has the partition function for this:toolzpartitionfrom toolz.itertoolz.core import partition\n\nlist(partition(2, [1, 2, 3, 4]))\n[(1, 2), (3, 4)]\nfrom toolz.itertoolz.core import partition\n\nlist(partition(2, [1, 2, 3, 4]))\n[(1, 2), (3, 4)]\n",
                "I was curious about the performance of different approaches and here it is:Tested on Python 3.5.1import time\nbatch_size = 7\narr_len = 298937\n\n#---------slice-------------\n\nprint(\"\\r\\nslice\")\nstart = time.time()\narr = [i for i in range(0, arr_len)]\nwhile True:\n    if not arr:\n        break\n\n    tmp = arr[0:batch_size]\n    arr = arr[batch_size:-1]\nprint(time.time() - start)\n\n#-----------index-----------\n\nprint(\"\\r\\nindex\")\narr = [i for i in range(0, arr_len)]\nstart = time.time()\nfor i in range(0, round(len(arr) / batch_size + 1)):\n    tmp = arr[batch_size * i : batch_size * (i + 1)]\nprint(time.time() - start)\n\n#----------batches 1------------\n\ndef batch(iterable, n=1):\n    l = len(iterable)\n    for ndx in range(0, l, n):\n        yield iterable[ndx:min(ndx + n, l)]\n\nprint(\"\\r\\nbatches 1\")\narr = [i for i in range(0, arr_len)]\nstart = time.time()\nfor x in batch(arr, batch_size):\n    tmp = x\nprint(time.time() - start)\n\n#----------batches 2------------\n\nfrom itertools import islice, chain\n\ndef batch(iterable, size):\n    sourceiter = iter(iterable)\n    while True:\n        batchiter = islice(sourceiter, size)\n        yield chain([next(batchiter)], batchiter)\n\n\nprint(\"\\r\\nbatches 2\")\narr = [i for i in range(0, arr_len)]\nstart = time.time()\nfor x in batch(arr, batch_size):\n    tmp = x\nprint(time.time() - start)\n\n#---------chunks-------------\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\nprint(\"\\r\\nchunks\")\narr = [i for i in range(0, arr_len)]\nstart = time.time()\nfor x in chunks(arr, batch_size):\n    tmp = x\nprint(time.time() - start)\n\n#-----------grouper-----------\n\nfrom itertools import zip_longest # for Python 3.x\n#from six.moves import zip_longest # for both (uses the six compat library)\n\ndef grouper(iterable, n, padvalue=None):\n    \"grouper(3, 'abcdefg', 'x') --> ('a','b','c'), ('d','e','f'), ('g','x','x')\"\n    return zip_longest(*[iter(iterable)]*n, fillvalue=padvalue)\n\narr = [i for i in range(0, arr_len)]\nprint(\"\\r\\ngrouper\")\nstart = time.time()\nfor x in grouper(arr, batch_size):\n    tmp = x\nprint(time.time() - start)\nimport time\nbatch_size = 7\narr_len = 298937\n\n#---------slice-------------\n\nprint(\"\\r\\nslice\")\nstart = time.time()\narr = [i for i in range(0, arr_len)]\nwhile True:\n    if not arr:\n        break\n\n    tmp = arr[0:batch_size]\n    arr = arr[batch_size:-1]\nprint(time.time() - start)\n\n#-----------index-----------\n\nprint(\"\\r\\nindex\")\narr = [i for i in range(0, arr_len)]\nstart = time.time()\nfor i in range(0, round(len(arr) / batch_size + 1)):\n    tmp = arr[batch_size * i : batch_size * (i + 1)]\nprint(time.time() - start)\n\n#----------batches 1------------\n\ndef batch(iterable, n=1):\n    l = len(iterable)\n    for ndx in range(0, l, n):\n        yield iterable[ndx:min(ndx + n, l)]\n\nprint(\"\\r\\nbatches 1\")\narr = [i for i in range(0, arr_len)]\nstart = time.time()\nfor x in batch(arr, batch_size):\n    tmp = x\nprint(time.time() - start)\n\n#----------batches 2------------\n\nfrom itertools import islice, chain\n\ndef batch(iterable, size):\n    sourceiter = iter(iterable)\n    while True:\n        batchiter = islice(sourceiter, size)\n        yield chain([next(batchiter)], batchiter)\n\n\nprint(\"\\r\\nbatches 2\")\narr = [i for i in range(0, arr_len)]\nstart = time.time()\nfor x in batch(arr, batch_size):\n    tmp = x\nprint(time.time() - start)\n\n#---------chunks-------------\ndef chunks(l, n):\n    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n    for i in range(0, len(l), n):\n        yield l[i:i + n]\nprint(\"\\r\\nchunks\")\narr = [i for i in range(0, arr_len)]\nstart = time.time()\nfor x in chunks(arr, batch_size):\n    tmp = x\nprint(time.time() - start)\n\n#-----------grouper-----------\n\nfrom itertools import zip_longest # for Python 3.x\n#from six.moves import zip_longest # for both (uses the six compat library)\n\ndef grouper(iterable, n, padvalue=None):\n    \"grouper(3, 'abcdefg', 'x') --> ('a','b','c'), ('d','e','f'), ('g','x','x')\"\n    return zip_longest(*[iter(iterable)]*n, fillvalue=padvalue)\n\narr = [i for i in range(0, arr_len)]\nprint(\"\\r\\ngrouper\")\nstart = time.time()\nfor x in grouper(arr, batch_size):\n    tmp = x\nprint(time.time() - start)\nResults:Results:slice\n31.18285083770752\n\nindex\n0.02184295654296875\n\nbatches 1\n0.03503894805908203\n\nbatches 2\n0.22681021690368652\n\nchunks\n0.019841909408569336\n\ngrouper\n0.006506919860839844\nslice\n31.18285083770752\n\nindex\n0.02184295654296875\n\nbatches 1\n0.03503894805908203\n\nbatches 2\n0.22681021690368652\n\nchunks\n0.019841909408569336\n\ngrouper\n0.006506919860839844\n",
                "You may also use get_chunks function of utilspie library as:get_chunksget_chunksutilspieutilspie>>> from utilspie import iterutils\n>>> a = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n>>> list(iterutils.get_chunks(a, 5))\n[[1, 2, 3, 4, 5], [6, 7, 8, 9]]\n>>> from utilspie import iterutils\n>>> a = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n>>> list(iterutils.get_chunks(a, 5))\n[[1, 2, 3, 4, 5], [6, 7, 8, 9]]\nYou can install utilspie via pip:utilspieutilspiesudo pip install utilspie\nsudo pip install utilspie\nDisclaimer: I am the creator of utilspie library.Disclaimer: I am the creator of utilspie libraryutilspie",
                "I like the Python doc's version proposed by tzot and J.F.Sebastian a lot,\n but it has two shortcomings:\nit is not very explicit\nI usually don't want a fill value in the last chunk\nit is not very explicitI usually don't want a fill value in the last chunkI'm using this one a lot in my code:from itertools import islice\n\ndef chunks(n, iterable):\n    iterable = iter(iterable)\n    while True:\n        yield tuple(islice(iterable, n)) or iterable.next()\nfrom itertools import islice\n\ndef chunks(n, iterable):\n    iterable = iter(iterable)\n    while True:\n        yield tuple(islice(iterable, n)) or iterable.next()\nUPDATE: A lazy chunks version:from itertools import chain, islice\n\ndef chunks(n, iterable):\n   iterable = iter(iterable)\n   while True:\n       yield chain([next(iterable)], islice(iterable, n-1))\nfrom itertools import chain, islice\n\ndef chunks(n, iterable):\n   iterable = iter(iterable)\n   while True:\n       yield chain([next(iterable)], islice(iterable, n-1))\n",
                "code:def split_list(the_list, chunk_size):\n    result_list = []\n    while the_list:\n        result_list.append(the_list[:chunk_size])\n        the_list = the_list[chunk_size:]\n    return result_list\n\na_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nprint split_list(a_list, 3)\ndef split_list(the_list, chunk_size):\n    result_list = []\n    while the_list:\n        result_list.append(the_list[:chunk_size])\n        the_list = the_list[chunk_size:]\n    return result_list\n\na_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nprint split_list(a_list, 3)\nresult:[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]\n[[1, 2, 3], [4, 5, 6], [7, 8, 9], [10]]\n",
                "heh, one line versionIn [48]: chunk = lambda ulist, step:  map(lambda i: ulist[i:i+step],  xrange(0, len(ulist), step))\n\nIn [49]: chunk(range(1,100), 10)\nOut[49]: \n[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n [21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n [31, 32, 33, 34, 35, 36, 37, 38, 39, 40],\n [41, 42, 43, 44, 45, 46, 47, 48, 49, 50],\n [51, 52, 53, 54, 55, 56, 57, 58, 59, 60],\n [61, 62, 63, 64, 65, 66, 67, 68, 69, 70],\n [71, 72, 73, 74, 75, 76, 77, 78, 79, 80],\n [81, 82, 83, 84, 85, 86, 87, 88, 89, 90],\n [91, 92, 93, 94, 95, 96, 97, 98, 99]]\nIn [48]: chunk = lambda ulist, step:  map(lambda i: ulist[i:i+step],  xrange(0, len(ulist), step))\n\nIn [49]: chunk(range(1,100), 10)\nOut[49]: \n[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n [11, 12, 13, 14, 15, 16, 17, 18, 19, 20],\n [21, 22, 23, 24, 25, 26, 27, 28, 29, 30],\n [31, 32, 33, 34, 35, 36, 37, 38, 39, 40],\n [41, 42, 43, 44, 45, 46, 47, 48, 49, 50],\n [51, 52, 53, 54, 55, 56, 57, 58, 59, 60],\n [61, 62, 63, 64, 65, 66, 67, 68, 69, 70],\n [71, 72, 73, 74, 75, 76, 77, 78, 79, 80],\n [81, 82, 83, 84, 85, 86, 87, 88, 89, 90],\n [91, 92, 93, 94, 95, 96, 97, 98, 99]]\n",
                "Another more explicit version.def chunkList(initialList, chunkSize):\n    \"\"\"\n    This function chunks a list into sub lists \n    that have a length equals to chunkSize.\n\n    Example:\n    lst = [3, 4, 9, 7, 1, 1, 2, 3]\n    print(chunkList(lst, 3)) \n    returns\n    [[3, 4, 9], [7, 1, 1], [2, 3]]\n    \"\"\"\n    finalList = []\n    for i in range(0, len(initialList), chunkSize):\n        finalList.append(initialList[i:i+chunkSize])\n    return finalList\ndef chunkList(initialList, chunkSize):\n    \"\"\"\n    This function chunks a list into sub lists \n    that have a length equals to chunkSize.\n\n    Example:\n    lst = [3, 4, 9, 7, 1, 1, 2, 3]\n    print(chunkList(lst, 3)) \n    returns\n    [[3, 4, 9], [7, 1, 1], [2, 3]]\n    \"\"\"\n    finalList = []\n    for i in range(0, len(initialList), chunkSize):\n        finalList.append(initialList[i:i+chunkSize])\n    return finalList\n",
                "At this point, I think we need a recursive generator, just in case...recursive generatorIn python 2:def chunks(li, n):\n    if li == []:\n        return\n    yield li[:n]\n    for e in chunks(li[n:], n):\n        yield e\ndef chunks(li, n):\n    if li == []:\n        return\n    yield li[:n]\n    for e in chunks(li[n:], n):\n        yield e\nIn python 3:def chunks(li, n):\n    if li == []:\n        return\n    yield li[:n]\n    yield from chunks(li[n:], n)\ndef chunks(li, n):\n    if li == []:\n        return\n    yield li[:n]\n    yield from chunks(li[n:], n)\nAlso, in case of massive Alien invasion, a decorated recursive generator might become handy:decorated recursive generatordef dec(gen):\n    def new_gen(li, n):\n        for e in gen(li, n):\n            if e == []:\n                return\n            yield e\n    return new_gen\n\n@dec\ndef chunks(li, n):\n    yield li[:n]\n    for e in chunks(li[n:], n):\n        yield e\ndef dec(gen):\n    def new_gen(li, n):\n        for e in gen(li, n):\n            if e == []:\n                return\n            yield e\n    return new_gen\n\n@dec\ndef chunks(li, n):\n    yield li[:n]\n    for e in chunks(li[n:], n):\n        yield e\n",
                "Without calling len() which is good for large lists:def splitter(l, n):\n    i = 0\n    chunk = l[:n]\n    while chunk:\n        yield chunk\n        i += n\n        chunk = l[i:i+n]\ndef splitter(l, n):\n    i = 0\n    chunk = l[:n]\n    while chunk:\n        yield chunk\n        i += n\n        chunk = l[i:i+n]\nAnd this is for iterables:def isplitter(l, n):\n    l = iter(l)\n    chunk = list(islice(l, n))\n    while chunk:\n        yield chunk\n        chunk = list(islice(l, n))\ndef isplitter(l, n):\n    l = iter(l)\n    chunk = list(islice(l, n))\n    while chunk:\n        yield chunk\n        chunk = list(islice(l, n))\nThe functional flavour of the above:def isplitter2(l, n):\n    return takewhile(bool,\n                     (tuple(islice(start, n))\n                            for start in repeat(iter(l))))\ndef isplitter2(l, n):\n    return takewhile(bool,\n                     (tuple(islice(start, n))\n                            for start in repeat(iter(l))))\nOR:def chunks_gen_sentinel(n, seq):\n    continuous_slices = imap(islice, repeat(iter(seq)), repeat(0), repeat(n))\n    return iter(imap(tuple, continuous_slices).next,())\ndef chunks_gen_sentinel(n, seq):\n    continuous_slices = imap(islice, repeat(iter(seq)), repeat(0), repeat(n))\n    return iter(imap(tuple, continuous_slices).next,())\nOR:def chunks_gen_filter(n, seq):\n    continuous_slices = imap(islice, repeat(iter(seq)), repeat(0), repeat(n))\n    return takewhile(bool,imap(tuple, continuous_slices))\ndef chunks_gen_filter(n, seq):\n    continuous_slices = imap(islice, repeat(iter(seq)), repeat(0), repeat(n))\n    return takewhile(bool,imap(tuple, continuous_slices))\n",
                "def split_seq(seq, num_pieces):\n    start = 0\n    for i in xrange(num_pieces):\n        stop = start + len(seq[i::num_pieces])\n        yield seq[start:stop]\n        start = stop\ndef split_seq(seq, num_pieces):\n    start = 0\n    for i in xrange(num_pieces):\n        stop = start + len(seq[i::num_pieces])\n        yield seq[start:stop]\n        start = stop\nusage:seq = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nfor seq in split_seq(seq, 3):\n    print seq\nseq = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n\nfor seq in split_seq(seq, 3):\n    print seq\n",
                "See this referencethis reference>>> orange = range(1, 1001)\n>>> otuples = list( zip(*[iter(orange)]*10))\n>>> print(otuples)\n[(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), ... (991, 992, 993, 994, 995, 996, 997, 998, 999, 1000)]\n>>> olist = [list(i) for i in otuples]\n>>> print(olist)\n[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ..., [991, 992, 993, 994, 995, 996, 997, 998, 999, 1000]]\n>>> \n>>> orange = range(1, 1001)\n>>> otuples = list( zip(*[iter(orange)]*10))\n>>> print(otuples)\n[(1, 2, 3, 4, 5, 6, 7, 8, 9, 10), ... (991, 992, 993, 994, 995, 996, 997, 998, 999, 1000)]\n>>> olist = [list(i) for i in otuples]\n>>> print(olist)\n[[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], ..., [991, 992, 993, 994, 995, 996, 997, 998, 999, 1000]]\n>>> \nPython3",
                "def chunks(iterable,n):\n    \"\"\"assumes n is an integer>0\n    \"\"\"\n    iterable=iter(iterable)\n    while True:\n        result=[]\n        for i in range(n):\n            try:\n                a=next(iterable)\n            except StopIteration:\n                break\n            else:\n                result.append(a)\n        if result:\n            yield result\n        else:\n            break\n\ng1=(i*i for i in range(10))\ng2=chunks(g1,3)\nprint g2\n'<generator object chunks at 0x0337B9B8>'\nprint list(g2)\n'[[0, 1, 4], [9, 16, 25], [36, 49, 64], [81]]'\ndef chunks(iterable,n):\n    \"\"\"assumes n is an integer>0\n    \"\"\"\n    iterable=iter(iterable)\n    while True:\n        result=[]\n        for i in range(n):\n            try:\n                a=next(iterable)\n            except StopIteration:\n                break\n            else:\n                result.append(a)\n        if result:\n            yield result\n        else:\n            break\n\ng1=(i*i for i in range(10))\ng2=chunks(g1,3)\nprint g2\n'<generator object chunks at 0x0337B9B8>'\nprint list(g2)\n'[[0, 1, 4], [9, 16, 25], [36, 49, 64], [81]]'\n",
                "Since everybody here talking about iterators. boltons has perfect method for that, called iterutils.chunked_iter.boltonsboltonsiterutils.chunked_iteriterutils.chunked_iterfrom boltons import iterutils\n\nlist(iterutils.chunked_iter(list(range(50)), 11))\nfrom boltons import iterutils\n\nlist(iterutils.chunked_iter(list(range(50)), 11))\nOutput:[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43],\n [44, 45, 46, 47, 48, 49]]\n[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],\n [22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43],\n [44, 45, 46, 47, 48, 49]]\nBut if you don't want to be mercy on memory, you can use old-way and store the full list in the first place with iterutils.chunked.listiterutils.chunkediterutils.chunked",
                "Consider using matplotlib.cbook piecesmatplotlib.cbookfor example:import matplotlib.cbook as cbook\nsegments = cbook.pieces(np.arange(20), 3)\nfor s in segments:\n     print s\nimport matplotlib.cbook as cbook\nsegments = cbook.pieces(np.arange(20), 3)\nfor s in segments:\n     print s\n",
                "a = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nCHUNK = 4\n[a[i*CHUNK:(i+1)*CHUNK] for i in xrange((len(a) + CHUNK - 1) / CHUNK )]\na = [1, 2, 3, 4, 5, 6, 7, 8, 9]\nCHUNK = 4\n[a[i*CHUNK:(i+1)*CHUNK] for i in xrange((len(a) + CHUNK - 1) / CHUNK )]\n"
            ]
        },
        {
            "tag": "environment_var",
            "patterns": [
                "How can I access environment variables in Python?",
                "how do i manipulate environment variables on my python?",
                "how do i get environment variables in python?",
                "how do i access the environment variables in python?",
                "what can i do to access environment variables in python?",
                "how can you access environment variables in python?",
                "how do i access environment variables in python?"
            ],
            "responses": [
                "Environment variables are accessed through os.environ:os.environos.environimport os\nprint(os.environ['HOME'])\nimport os\nprint(os.environ['HOME'])\nTo see a list of all environment variables:print(os.environ)\nprint(os.environ)\nIf a key is not present, attempting to access it will raise a KeyError. To avoid this:KeyError# Returns `None` if the key doesn't exist\nprint(os.environ.get('KEY_THAT_MIGHT_EXIST'))\n\n# Returns `default_value` if the key doesn't exist\nprint(os.environ.get('KEY_THAT_MIGHT_EXIST', default_value))\n\n# Returns `default_value` if the key doesn't exist\nprint(os.getenv('KEY_THAT_MIGHT_EXIST', default_value))\n# Returns `None` if the key doesn't exist\nprint(os.environ.get('KEY_THAT_MIGHT_EXIST'))\n\n# Returns `default_value` if the key doesn't exist\nprint(os.environ.get('KEY_THAT_MIGHT_EXIST', default_value))\n\n# Returns `default_value` if the key doesn't exist\nprint(os.getenv('KEY_THAT_MIGHT_EXIST', default_value))\n",
                "To check if the key exists (returns True or False)TrueFalse'HOME' in os.environ\n'HOME' in os.environ\nYou can also use get() when printing the key; useful if you want to use a default.get()print(os.environ.get('HOME', '/home/username/'))\nprint(os.environ.get('HOME', '/home/username/'))\nwhere /home/username/ is the default/home/username/",
                "Actually it can be done this way:import os\n\nfor item, value in os.environ.items():\n    print('{}: {}'.format(item, value))\nimport os\n\nfor item, value in os.environ.items():\n    print('{}: {}'.format(item, value))\nOr simply:for i, j in os.environ.items():\n    print(i, j)\nfor i, j in os.environ.items():\n    print(i, j)\nFor viewing the value in the parameter:print(os.environ['HOME'])\nprint(os.environ['HOME'])\nOr:print(os.environ.get('HOME'))\nprint(os.environ.get('HOME'))\nTo set the value:os.environ['HOME'] = '/new/value'\nos.environ['HOME'] = '/new/value'\n",
                "Here's how to check if $FOO is set:$FOOtry:  \n   os.environ[\"FOO\"]\nexcept KeyError: \n   print \"Please set the environment variable FOO\"\n   sys.exit(1)\ntry:  \n   os.environ[\"FOO\"]\nexcept KeyError: \n   print \"Please set the environment variable FOO\"\n   sys.exit(1)\n",
                "You can access the environment variables usingimport os\nprint os.environ\nimport os\nprint os.environ\nTry to see the content of the PYTHONPATH or PYTHONHOME environment variables. Maybe this will be helpful for your second question.",
                "As for the environment variables:import os\nprint os.environ[\"HOME\"]\nimport os\nprint os.environ[\"HOME\"]\n",
                "import os\nfor a in os.environ:\n    print('Var: ', a, 'Value: ', os.getenv(a))\nprint(\"all done\")\nimport os\nfor a in os.environ:\n    print('Var: ', a, 'Value: ', os.getenv(a))\nprint(\"all done\")\nThat will print all of the environment variables along with their values.",
                "Import the os module:osimport os\nimport os\nTo get an environment variable:os.environ.get('Env_var')\nos.environ.get('Env_var')\nTo set an environment variable:# Set environment variables\nos.environ['Env_var'] = 'Some Value'\n# Set environment variables\nos.environ['Env_var'] = 'Some Value'\n",
                "If you are planning to use the code in a production web application code, using any web framework like Django and Flask, use projects like envparse. Using it, you can read the value as your defined type.DjangoFlaskenvparsefrom envparse import env\n# will read WHITE_LIST=hello,world,hi to white_list = [\"hello\", \"world\", \"hi\"]\nwhite_list = env.list(\"WHITE_LIST\", default=[])\n# Perfect for reading boolean\nDEBUG = env.bool(\"DEBUG\", default=False)\nfrom envparse import env\n# will read WHITE_LIST=hello,world,hi to white_list = [\"hello\", \"world\", \"hi\"]\nwhite_list = env.list(\"WHITE_LIST\", default=[])\n# Perfect for reading boolean\nDEBUG = env.bool(\"DEBUG\", default=False)\nNOTE: kennethreitz's autoenv is a recommended tool for making project-specific environment variables. For those who are using autoenv, please note to keep the .env file private (inaccessible to public).autoenvautoenv.env",
                "There are also a number of great libraries. Envs, for example, will allow you to parse objects out of your environment variables, which is rad. For example:Envsfrom envs import env\nenv('SECRET_KEY') # 'your_secret_key_here'\nenv('SERVER_NAMES',var_type='list') #['your', 'list', 'here']\nfrom envs import env\nenv('SECRET_KEY') # 'your_secret_key_here'\nenv('SERVER_NAMES',var_type='list') #['your', 'list', 'here']\n",
                "You can also try this:First, install python-decouplepython-decouplepip install python-decouple\npip install python-decouple\nImport it in your filefrom decouple import config\nfrom decouple import config\nThen get the environment variableSECRET_KEY=config('SECRET_KEY')\nSECRET_KEY=config('SECRET_KEY')\nRead more about the Python library here.here",
                "Edited - October 2021Edited - October 2021Following @Peter's comment, here's how you can test it:main.pymain.py#!/usr/bin/env python\n\n\nfrom os import environ\n\n# Initialize variables\nnum_of_vars = 50\nfor i in range(1, num_of_vars):\n    environ[f\"_BENCHMARK_{i}\"] = f\"BENCHMARK VALUE {i}\"  \n\ndef stopwatch(repeat=1, autorun=True):\n    \"\"\"\n    Source: https://stackoverflow.com/a/68660080/5285732\n    stopwatch decorator to calculate the total time of a function\n    \"\"\"\n    import timeit\n    import functools\n    \n    def outer_func(func):\n        @functools.wraps(func)\n        def time_func(*args, **kwargs):\n            t1 = timeit.default_timer()\n            for _ in range(repeat):\n                r = func(*args, **kwargs)\n            t2 = timeit.default_timer()\n            print(f\"Function={func.__name__}, Time={t2 - t1}\")\n            return r\n        \n        if autorun:\n            try:\n                time_func()\n            except TypeError:\n                raise Exception(f\"{time_func.__name__}: autorun only works with no parameters, you may want to use @stopwatch(autorun=False)\") from None\n        \n        return time_func\n    \n    if callable(repeat):\n        func = repeat\n        repeat = 1\n        return outer_func(func)\n    \n    return outer_func\n\n@stopwatch(repeat=10000)\ndef using_environ():\n    for item in environ:\n        pass\n\n@stopwatch\ndef using_dict(repeat=10000):\n    env_vars_dict = dict(environ)\n    for item in env_vars_dict:\n        pass\n#!/usr/bin/env python\n\n\nfrom os import environ\n\n# Initialize variables\nnum_of_vars = 50\nfor i in range(1, num_of_vars):\n    environ[f\"_BENCHMARK_{i}\"] = f\"BENCHMARK VALUE {i}\"  \n\ndef stopwatch(repeat=1, autorun=True):\n    \"\"\"\n    Source: https://stackoverflow.com/a/68660080/5285732\n    stopwatch decorator to calculate the total time of a function\n    \"\"\"\n    import timeit\n    import functools\n    \n    def outer_func(func):\n        @functools.wraps(func)\n        def time_func(*args, **kwargs):\n            t1 = timeit.default_timer()\n            for _ in range(repeat):\n                r = func(*args, **kwargs)\n            t2 = timeit.default_timer()\n            print(f\"Function={func.__name__}, Time={t2 - t1}\")\n            return r\n        \n        if autorun:\n            try:\n                time_func()\n            except TypeError:\n                raise Exception(f\"{time_func.__name__}: autorun only works with no parameters, you may want to use @stopwatch(autorun=False)\") from None\n        \n        return time_func\n    \n    if callable(repeat):\n        func = repeat\n        repeat = 1\n        return outer_func(func)\n    \n    return outer_func\n\n@stopwatch(repeat=10000)\ndef using_environ():\n    for item in environ:\n        pass\n\n@stopwatch\ndef using_dict(repeat=10000):\n    env_vars_dict = dict(environ)\n    for item in env_vars_dict:\n        pass\npython \"main.py\"\n\n# Output\nFunction=using_environ, Time=0.216224731\nFunction=using_dict, Time=0.00014206099999999888\npython \"main.py\"\n\n# Output\nFunction=using_environ, Time=0.216224731\nFunction=using_dict, Time=0.00014206099999999888\nIf this is true ... It's 1500x faster to use a dict() instead of accessing environ directly.dict()environA performance-driven approach - calling environ is expensive, so it's better to call it once and save it to a dictionary. Full example:environfrom os import environ\n\n\n# Slower\nprint(environ[\"USER\"], environ[\"NAME\"])\n\n# Faster\nenv_dict = dict(environ)\nprint(env_dict[\"USER\"], env_dict[\"NAME\"])\nfrom os import environ\n\n\n# Slower\nprint(environ[\"USER\"], environ[\"NAME\"])\n\n# Faster\nenv_dict = dict(environ)\nprint(env_dict[\"USER\"], env_dict[\"NAME\"])\nP.S- if you worry about exposing private environment variables, then sanitize env_dict after the assignment.env_dict",
                "For Django, see Django-environ.Django-environ$ pip install django-environ\n\nimport environ\n\nenv = environ.Env(\n    # set casting, default value\n    DEBUG=(bool, False)\n)\n# reading .env file\nenviron.Env.read_env()\n\n# False if not in os.environ\nDEBUG = env('DEBUG')\n\n# Raises Django's ImproperlyConfigured exception if SECRET_KEY not in os.environ\nSECRET_KEY = env('SECRET_KEY')\n$ pip install django-environ\n\nimport environ\n\nenv = environ.Env(\n    # set casting, default value\n    DEBUG=(bool, False)\n)\n# reading .env file\nenviron.Env.read_env()\n\n# False if not in os.environ\nDEBUG = env('DEBUG')\n\n# Raises Django's ImproperlyConfigured exception if SECRET_KEY not in os.environ\nSECRET_KEY = env('SECRET_KEY')\n",
                "You should first import os usingimport os\nimport os\nand then actually print the environment variable valueprint(os.environ['yourvariable'])\nprint(os.environ['yourvariable'])\nof course, replace yourvariable as the variable you want to access.",
                "The tricky part of using nested for-loops in one-liners is that you have to use list comprehension. So in order to print all your environment variables, without having to import a foreign library, you can use:for-loopshaveallpython -c \"import os;L=[f'{k}={v}' for k,v in os.environ.items()]; print('\\n'.join(L))\"\npython -c \"import os;L=[f'{k}={v}' for k,v in os.environ.items()]; print('\\n'.join(L))\"\n"
            ]
        },
        {
            "tag": "datetime",
            "patterns": [
                "Convert string \"Jun 1 2005 1:33PM\" into datetime"
            ],
            "responses": [
                "datetime.strptime parses an input string in the user-specified format into a timezone-naive datetime object:datetime.strptimedatetime.strptimetimezone-naivedatetimedatetime>>> from datetime import datetime\n>>> datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ndatetime.datetime(2005, 6, 1, 13, 33)\n>>> from datetime import datetime\n>>> datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ndatetime.datetime(2005, 6, 1, 13, 33)\nTo obtain a date object using an existing datetime object, convert it using .date():datedatedatetime.date()>>> datetime.strptime('Jun 1 2005', '%b %d %Y').date()\ndate(2005, 6, 1)\n>>> datetime.strptime('Jun 1 2005', '%b %d %Y').date()\ndate(2005, 6, 1)\nLinks:Links:\nstrptime docs: Python 2, Python 3\n\nstrptime/strftime format string docs: Python 2, Python 3\n\nstrftime.org format string cheatsheet\n\nstrptime docs: Python 2, Python 3\nstrptime docs: Python 2, Python 3strptimePython 2Python 3strptime/strftime format string docs: Python 2, Python 3\nstrptime/strftime format string docs: Python 2, Python 3strptimestrftimePython 2Python 3strftime.org format string cheatsheet\nstrftime.org format string cheatsheetstrftime.orgNotes:Notes:\nstrptime = \"string parse time\"\nstrftime = \"string format time\"\nstrptime = \"string parse time\"strptimestrftime = \"string format time\"strftime",
                "Use the third-party dateutil library:dateutildateutilfrom dateutil import parser\nparser.parse(\"Aug 28 1999 12:00AM\")  # datetime.datetime(1999, 8, 28, 0, 0)\nfrom dateutil import parser\nparser.parse(\"Aug 28 1999 12:00AM\")  # datetime.datetime(1999, 8, 28, 0, 0)\nIt can handle most date formats and is more convenient than strptime since it usually guesses the correct format. It is also very useful for writing tests, where readability is more important than performance.strptimeInstall it with:pip install python-dateutil\npip install python-dateutil\n",
                "Check out strptime in the time module.  It is the inverse of strftime.strptimetimestrftime$ python\n>>> import time\n>>> my_time = time.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ntime.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,\n                 tm_hour=13, tm_min=33, tm_sec=0,\n                 tm_wday=2, tm_yday=152, tm_isdst=-1)\n\ntimestamp = time.mktime(my_time)\n# convert time object to datetime\nfrom datetime import datetime\nmy_datetime = datetime.fromtimestamp(timestamp)\n# convert time object to date\nfrom datetime import date\nmy_date = date.fromtimestamp(timestamp)\n$ python\n>>> import time\n>>> my_time = time.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\ntime.struct_time(tm_year=2005, tm_mon=6, tm_mday=1,\n                 tm_hour=13, tm_min=33, tm_sec=0,\n                 tm_wday=2, tm_yday=152, tm_isdst=-1)\n\ntimestamp = time.mktime(my_time)\n# convert time object to datetime\nfrom datetime import datetime\nmy_datetime = datetime.fromtimestamp(timestamp)\n# convert time object to date\nfrom datetime import date\nmy_date = date.fromtimestamp(timestamp)\n",
                "Python >= 3.7To convert a YYYY-MM-DD string to a datetime object, datetime.fromisoformat could be used.YYYY-MM-DDdatetime.fromisoformatfrom datetime import datetime\n\ndate_string = \"2012-12-12 10:10:10\"\nprint (datetime.fromisoformat(date_string))\n2012-12-12 10:10:10\nfrom datetime import datetime\n\ndate_string = \"2012-12-12 10:10:10\"\nprint (datetime.fromisoformat(date_string))\n2012-12-12 10:10:10\nCaution from the documentation:\nThis does not support parsing arbitrary ISO 8601 strings - it is only intended as the inverse operation of datetime.isoformat(). A more full-featured ISO 8601 parser, dateutil.parser.isoparse is available in the third-party package dateutil.\nThis does not support parsing arbitrary ISO 8601 strings - it is only intended as the inverse operation of datetime.isoformat(). A more full-featured ISO 8601 parser, dateutil.parser.isoparse is available in the third-party package dateutil.notdatetime.isoformat()datetime.isoformat()dateutil.parser.isoparsedateutildateutil",
                "I have put together a project that can convert some really neat expressions. Check out timestring.timestringtimestringHere are some examples below:pip install timestring>>> import timestring\n>>> timestring.Date('monday, aug 15th 2015 at 8:40 pm')\n<timestring.Date 2015-08-15 20:40:00 4491909392>\n>>> timestring.Date('monday, aug 15th 2015 at 8:40 pm').date\ndatetime.datetime(2015, 8, 15, 20, 40)\n>>> timestring.Range('next week')\n<timestring.Range From 03/10/14 00:00:00 to 03/03/14 00:00:00 4496004880>\n>>> (timestring.Range('next week').start.date, timestring.Range('next week').end.date)\n(datetime.datetime(2014, 3, 10, 0, 0), datetime.datetime(2014, 3, 14, 0, 0))\n>>> import timestring\n>>> timestring.Date('monday, aug 15th 2015 at 8:40 pm')\n<timestring.Date 2015-08-15 20:40:00 4491909392>\n>>> timestring.Date('monday, aug 15th 2015 at 8:40 pm').date\ndatetime.datetime(2015, 8, 15, 20, 40)\n>>> timestring.Range('next week')\n<timestring.Range From 03/10/14 00:00:00 to 03/03/14 00:00:00 4496004880>\n>>> (timestring.Range('next week').start.date, timestring.Range('next week').end.date)\n(datetime.datetime(2014, 3, 10, 0, 0), datetime.datetime(2014, 3, 14, 0, 0))\n",
                "Remember this and you didn't need to get confused in datetime conversion again.String to datetime object = strptimestrptimedatetime object to other formats = strftimestrftimeJun 1 2005  1:33PMJun 1 2005  1:33PMis equals to%b %d %Y %I:%M%p%b %d %Y %I:%M%p\n%b    Month as locale\u2019s abbreviated name(Jun)\n%d    Day of the month as a zero-padded decimal number(1)\n%Y    Year with century as a decimal number(2015)\n%I    Hour (12-hour clock) as a zero-padded decimal number(01)\n%M    Minute as a zero-padded decimal number(33)\n%p    Locale\u2019s equivalent of either AM or PM(PM)\n%b    Month as locale\u2019s abbreviated name(Jun)%d    Day of the month as a zero-padded decimal number(1)%Y    Year with century as a decimal number(2015)%I    Hour (12-hour clock) as a zero-padded decimal number(01)%M    Minute as a zero-padded decimal number(33)%p    Locale\u2019s equivalent of either AM or PM(PM)so you need strptime i-e converting string to string>>> dates = []\n>>> dates.append('Jun 1 2005  1:33PM')\n>>> dates.append('Aug 28 1999 12:00AM')\n>>> from datetime import datetime\n>>> for d in dates:\n...     date = datetime.strptime(d, '%b %d %Y %I:%M%p')\n...     print type(date)\n...     print date\n... \n>>> dates = []\n>>> dates.append('Jun 1 2005  1:33PM')\n>>> dates.append('Aug 28 1999 12:00AM')\n>>> from datetime import datetime\n>>> for d in dates:\n...     date = datetime.strptime(d, '%b %d %Y %I:%M%p')\n...     print type(date)\n...     print date\n... \nOutput<type 'datetime.datetime'>\n2005-06-01 13:33:00\n<type 'datetime.datetime'>\n1999-08-28 00:00:00\n<type 'datetime.datetime'>\n2005-06-01 13:33:00\n<type 'datetime.datetime'>\n1999-08-28 00:00:00\nWhat if you have different format of dates you can use panda or dateutil.parse>>> import dateutil\n>>> dates = []\n>>> dates.append('12 1 2017')\n>>> dates.append('1 1 2017')\n>>> dates.append('1 12 2017')\n>>> dates.append('June 1 2017 1:30:00AM')\n>>> [parser.parse(x) for x in dates]\n>>> import dateutil\n>>> dates = []\n>>> dates.append('12 1 2017')\n>>> dates.append('1 1 2017')\n>>> dates.append('1 12 2017')\n>>> dates.append('June 1 2017 1:30:00AM')\n>>> [parser.parse(x) for x in dates]\nOutPut[datetime.datetime(2017, 12, 1, 0, 0), datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 12, 0, 0), datetime.datetime(2017, 6, 1, 1, 30)]\n[datetime.datetime(2017, 12, 1, 0, 0), datetime.datetime(2017, 1, 1, 0, 0), datetime.datetime(2017, 1, 12, 0, 0), datetime.datetime(2017, 6, 1, 1, 30)]\n",
                "Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.Python 3.2+:>>> datetime.datetime.strptime(\n...     \"March 5, 2014, 20:13:50\", \"%B %d, %Y, %H:%M:%S\"\n... ).replace(tzinfo=datetime.timezone(datetime.timedelta(hours=-3)))\n>>> datetime.datetime.strptime(\n...     \"March 5, 2014, 20:13:50\", \"%B %d, %Y, %H:%M:%S\"\n... ).replace(tzinfo=datetime.timezone(datetime.timedelta(hours=-3)))\nThis assumes you know the offset. If you don't, but you know e.g. the location, you can use the pytz package to query the IANA time zone database for the offset. I'll use Tehran here as an example because it has a half-hour offset:pytzIANA time zone database>>> tehran = pytz.timezone(\"Asia/Tehran\")\n>>> local_time = tehran.localize(\n...   datetime.datetime.strptime(\"March 5, 2014, 20:13:50\",\n...                              \"%B %d, %Y, %H:%M:%S\")\n... )\n>>> local_time\ndatetime.datetime(2014, 3, 5, 20, 13, 50, tzinfo=<DstTzInfo 'Asia/Tehran' +0330+3:30:00 STD>)\n>>> tehran = pytz.timezone(\"Asia/Tehran\")\n>>> local_time = tehran.localize(\n...   datetime.datetime.strptime(\"March 5, 2014, 20:13:50\",\n...                              \"%B %d, %Y, %H:%M:%S\")\n... )\n>>> local_time\ndatetime.datetime(2014, 3, 5, 20, 13, 50, tzinfo=<DstTzInfo 'Asia/Tehran' +0330+3:30:00 STD>)\nAs you can see, pytz has determined that the offset was +3:30 at that particular date. You can now convert this to UTC time, and it will apply the offset:pytz>>> utc_time = local_time.astimezone(pytz.utc)\n>>> utc_time\ndatetime.datetime(2014, 3, 5, 16, 43, 50, tzinfo=<UTC>)\n>>> utc_time = local_time.astimezone(pytz.utc)\n>>> utc_time\ndatetime.datetime(2014, 3, 5, 16, 43, 50, tzinfo=<UTC>)\nNote that dates before the adoption of timezones will give you weird offsets. This is because the IANA has decided to use Local Mean Time:Local Mean Time>>> chicago = pytz.timezone(\"America/Chicago\")\n>>> weird_time = chicago.localize(\n...   datetime.datetime.strptime(\"November 18, 1883, 11:00:00\",\n...                              \"%B %d, %Y, %H:%M:%S\")\n... )\n>>> weird_time.astimezone(pytz.utc)\ndatetime.datetime(1883, 11, 18, 7, 34, tzinfo=<UTC>)\n>>> chicago = pytz.timezone(\"America/Chicago\")\n>>> weird_time = chicago.localize(\n...   datetime.datetime.strptime(\"November 18, 1883, 11:00:00\",\n...                              \"%B %d, %Y, %H:%M:%S\")\n... )\n>>> weird_time.astimezone(pytz.utc)\ndatetime.datetime(1883, 11, 18, 7, 34, tzinfo=<UTC>)\nThe weird \"7 hours and 34 minutes\" are derived from the longitude of Chicago. I used this timestamp because it is right before standardized time was adopted in Chicago.standardized time was adopted in Chicago",
                "If your string is in ISO 8601 format and you have Python 3.7+, you can use the following simple code:ISO 8601import datetime\n\naDate = datetime.date.fromisoformat('2020-10-04')\nimport datetime\n\naDate = datetime.date.fromisoformat('2020-10-04')\nfor dates andimport datetime\n\naDateTime = datetime.datetime.fromisoformat('2020-10-04 22:47:00')\nimport datetime\n\naDateTime = datetime.datetime.fromisoformat('2020-10-04 22:47:00')\nfor strings containing date and time. If timestamps are included, the function datetime.datetime.isoformat() supports the following format:datetime.datetime.isoformat()YYYY-MM-DD[*HH[:MM[:SS[.fff[fff]]]][+HH:MM[:SS[.ffffff]]]]\nYYYY-MM-DD[*HH[:MM[:SS[.fff[fff]]]][+HH:MM[:SS[.ffffff]]]]\nWhere * matches any single character. See also here and here.*herehere",
                "Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.import pandas as pd\n\ndates = ['2015-12-25', '2015-12-26']\n\n# 1) Use a list comprehension.\n>>> [d.date() for d in pd.to_datetime(dates)]\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n\n# 2) Convert the dates to a DatetimeIndex and extract the python dates.\n>>> pd.DatetimeIndex(dates).date.tolist()\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\nimport pandas as pd\n\ndates = ['2015-12-25', '2015-12-26']\n\n# 1) Use a list comprehension.\n>>> [d.date() for d in pd.to_datetime(dates)]\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\n\n# 2) Convert the dates to a DatetimeIndex and extract the python dates.\n>>> pd.DatetimeIndex(dates).date.tolist()\n[datetime.date(2015, 12, 25), datetime.date(2015, 12, 26)]\nTimingsTimingsdates = pd.DatetimeIndex(start='2000-1-1', end='2010-1-1', freq='d').date.tolist()\n\n>>> %timeit [d.date() for d in pd.to_datetime(dates)]\n# 100 loops, best of 3: 3.11 ms per loop\n\n>>> %timeit pd.DatetimeIndex(dates).date.tolist()\n# 100 loops, best of 3: 6.85 ms per loop\ndates = pd.DatetimeIndex(start='2000-1-1', end='2010-1-1', freq='d').date.tolist()\n\n>>> %timeit [d.date() for d in pd.to_datetime(dates)]\n# 100 loops, best of 3: 3.11 ms per loop\n\n>>> %timeit pd.DatetimeIndex(dates).date.tolist()\n# 100 loops, best of 3: 6.85 ms per loop\nAnd here is how to convert the OP's original date-time examples:datetimes = ['Jun 1 2005  1:33PM', 'Aug 28 1999 12:00AM']\n\n>>> pd.to_datetime(datetimes).to_pydatetime().tolist()\n[datetime.datetime(2005, 6, 1, 13, 33), \n datetime.datetime(1999, 8, 28, 0, 0)]\ndatetimes = ['Jun 1 2005  1:33PM', 'Aug 28 1999 12:00AM']\n\n>>> pd.to_datetime(datetimes).to_pydatetime().tolist()\n[datetime.datetime(2005, 6, 1, 13, 33), \n datetime.datetime(1999, 8, 28, 0, 0)]\nThere are many options for converting from the strings to Pandas Timestamps using to_datetime, so check the docs if you need anything special.to_datetimedocsLikewise, Timestamps have many properties and methods that can be accessed in addition to .dateproperties and methods.date",
                "I personally like the solution using the parser module, which is the second answer to this question and is beautiful, as you don't have to construct any string literals to get it working. But, one downside is that it is 90% slower than the accepted answer with strptime.parserBut90% slowerstrptimefrom dateutil import parser\nfrom datetime import datetime\nimport timeit\n\ndef dt():\n    dt = parser.parse(\"Jun 1 2005  1:33PM\")\ndef strptime():\n    datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n\nprint(timeit.timeit(stmt=dt, number=10**5))\nprint(timeit.timeit(stmt=strptime, number=10**5))\nfrom dateutil import parser\nfrom datetime import datetime\nimport timeit\n\ndef dt():\n    dt = parser.parse(\"Jun 1 2005  1:33PM\")\ndef strptime():\n    datetime_object = datetime.strptime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n\nprint(timeit.timeit(stmt=dt, number=10**5))\nprint(timeit.timeit(stmt=strptime, number=10**5))\nOutput:\n10.70296801342902 \n1.3627995655316933\n10.70296801342902 \n1.3627995655316933As long as you are not doing this a million times over and over again, I still think the parser method is more convenient and will handle most of the time formats automatically.a millionparser",
                "Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.import time\n\ndef num_suffix(n):\n    '''\n    Returns the suffix for any given int\n    '''\n    suf = ('th','st', 'nd', 'rd')\n    n = abs(n) # wise guy\n    tens = int(str(n)[-2:])\n    units = n % 10\n    if tens > 10 and tens < 20:\n        return suf[0] # teens with 'th'\n    elif units <= 3:\n        return suf[units]\n    else:\n        return suf[0] # 'th'\n\ndef day_suffix(t):\n    '''\n    Returns the suffix of the given struct_time day\n    '''\n    return num_suffix(t.tm_mday)\n\n# Examples\nprint num_suffix(123)\nprint num_suffix(3431)\nprint num_suffix(1234)\nprint ''\nprint day_suffix(time.strptime(\"1 Dec 00\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"2 Nov 01\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"3 Oct 02\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"4 Sep 03\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"13 Nov 90\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"14 Oct 10\", \"%d %b %y\"))\u200b\u200b\u200b\u200b\u200b\u200b\u200b\nimport time\n\ndef num_suffix(n):\n    '''\n    Returns the suffix for any given int\n    '''\n    suf = ('th','st', 'nd', 'rd')\n    n = abs(n) # wise guy\n    tens = int(str(n)[-2:])\n    units = n % 10\n    if tens > 10 and tens < 20:\n        return suf[0] # teens with 'th'\n    elif units <= 3:\n        return suf[units]\n    else:\n        return suf[0] # 'th'\n\ndef day_suffix(t):\n    '''\n    Returns the suffix of the given struct_time day\n    '''\n    return num_suffix(t.tm_mday)\n\n# Examples\nprint num_suffix(123)\nprint num_suffix(3431)\nprint num_suffix(1234)\nprint ''\nprint day_suffix(time.strptime(\"1 Dec 00\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"2 Nov 01\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"3 Oct 02\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"4 Sep 03\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"13 Nov 90\", \"%d %b %y\"))\nprint day_suffix(time.strptime(\"14 Oct 10\", \"%d %b %y\"))\u200b\u200b\u200b\u200b\u200b\u200b\u200b\n",
                "In [34]: import datetime\n\nIn [35]: _now = datetime.datetime.now()\n\nIn [36]: _now\nOut[36]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [37]: print _now\n2016-01-19 09:47:00.432000\n\nIn [38]: _parsed = datetime.datetime.strptime(str(_now),\"%Y-%m-%d %H:%M:%S.%f\")\n\nIn [39]: _parsed\nOut[39]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [40]: assert _now == _parsed\nIn [34]: import datetime\n\nIn [35]: _now = datetime.datetime.now()\n\nIn [36]: _now\nOut[36]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [37]: print _now\n2016-01-19 09:47:00.432000\n\nIn [38]: _parsed = datetime.datetime.strptime(str(_now),\"%Y-%m-%d %H:%M:%S.%f\")\n\nIn [39]: _parsed\nOut[39]: datetime.datetime(2016, 1, 19, 9, 47, 0, 432000)\n\nIn [40]: assert _now == _parsed\n",
                "Django Timezone aware datetime object example.import datetime\nfrom django.utils.timezone import get_current_timezone\ntz = get_current_timezone()\n\nformat = '%b %d %Y %I:%M%p'\ndate_object = datetime.datetime.strptime('Jun 1 2005  1:33PM', format)\ndate_obj = tz.localize(date_object)\nimport datetime\nfrom django.utils.timezone import get_current_timezone\ntz = get_current_timezone()\n\nformat = '%b %d %Y %I:%M%p'\ndate_object = datetime.datetime.strptime('Jun 1 2005  1:33PM', format)\ndate_obj = tz.localize(date_object)\nThis conversion is very important for Django and Python when you have USE_TZ = True:USE_TZ = TrueRuntimeWarning: DateTimeField MyModel.created received a naive datetime (2016-03-04 00:00:00) while time zone support is active.\nRuntimeWarning: DateTimeField MyModel.created received a naive datetime (2016-03-04 00:00:00) while time zone support is active.\n",
                "Create a small utility function like:def date(datestr=\"\", format=\"%Y-%m-%d\"):\n    from datetime import datetime\n    if not datestr:\n        return datetime.today().date()\n    return datetime.strptime(datestr, format).date()\ndef date(datestr=\"\", format=\"%Y-%m-%d\"):\n    from datetime import datetime\n    if not datestr:\n        return datetime.today().date()\n    return datetime.strptime(datestr, format).date()\nThis is versatile enough:\nIf you don't pass any arguments it will return today's date.\nThere's a date format as default that you can override.\nYou can easily modify it to return a datetime.\nIf you don't pass any arguments it will return today's date.There's a date format as default that you can override.You can easily modify it to return a datetime.",
                "This would be helpful for converting a string to datetime and also with a time zone:def convert_string_to_time(date_string, timezone):\n\n    from datetime import datetime\n    import pytz\n\n    date_time_obj = datetime.strptime(date_string[:26], '%Y-%m-%d %H:%M:%S.%f')\n    date_time_obj_timezone = pytz.timezone(timezone).localize(date_time_obj)\n\n    return date_time_obj_timezone\n\ndate = '2018-08-14 13:09:24.543953+00:00'\nTIME_ZONE = 'UTC'\ndate_time_obj_timezone = convert_string_to_time(date, TIME_ZONE)\ndef convert_string_to_time(date_string, timezone):\n\n    from datetime import datetime\n    import pytz\n\n    date_time_obj = datetime.strptime(date_string[:26], '%Y-%m-%d %H:%M:%S.%f')\n    date_time_obj_timezone = pytz.timezone(timezone).localize(date_time_obj)\n\n    return date_time_obj_timezone\n\ndate = '2018-08-14 13:09:24.543953+00:00'\nTIME_ZONE = 'UTC'\ndate_time_obj_timezone = convert_string_to_time(date, TIME_ZONE)\n",
                "arrow offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.arrow>>> import arrow\n>>> dateStrings = [ 'Jun 1  2005 1:33PM', 'Aug 28 1999 12:00AM' ]\n>>> for dateString in dateStrings:\n...     dateString\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').datetime\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').format('ddd, Do MMM YYYY HH:mm')\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').humanize(locale='de')\n...\n'Jun 1  2005 1:33PM'\ndatetime.datetime(2005, 6, 1, 13, 33, tzinfo=tzutc())\n'Wed, 1st Jun 2005 13:33'\n'vor 11 Jahren'\n'Aug 28 1999 12:00AM'\ndatetime.datetime(1999, 8, 28, 0, 0, tzinfo=tzutc())\n'Sat, 28th Aug 1999 00:00'\n'vor 17 Jahren'\n>>> import arrow\n>>> dateStrings = [ 'Jun 1  2005 1:33PM', 'Aug 28 1999 12:00AM' ]\n>>> for dateString in dateStrings:\n...     dateString\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').datetime\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').format('ddd, Do MMM YYYY HH:mm')\n...     arrow.get(dateString.replace('  ',' '), 'MMM D YYYY H:mmA').humanize(locale='de')\n...\n'Jun 1  2005 1:33PM'\ndatetime.datetime(2005, 6, 1, 13, 33, tzinfo=tzutc())\n'Wed, 1st Jun 2005 13:33'\n'vor 11 Jahren'\n'Aug 28 1999 12:00AM'\ndatetime.datetime(1999, 8, 28, 0, 0, tzinfo=tzutc())\n'Sat, 28th Aug 1999 00:00'\n'vor 17 Jahren'\nSee http://arrow.readthedocs.io/en/latest/ for more.http://arrow.readthedocs.io/en/latest/",
                "You can also check out dateparser:dateparserdateparser\ndateparser provides modules to easily parse localized dates in almost\nany string formats commonly found on web pages.\ndateparser provides modules to easily parse localized dates in almost\nany string formats commonly found on web pages.dateparserdateparserInstall:pip install dateparser\npip install dateparser\nThis is, I think, the easiest way you can parse dates.\nThe most straightforward way is to use the dateparser.parse function,\nthat wraps around most of the functionality in the module.\nThe most straightforward way is to use the dateparser.parse function,\nthat wraps around most of the functionality in the module.dateparser.parseSample code:import dateparser\n\nt1 = 'Jun 1 2005  1:33PM'\nt2 = 'Aug 28 1999 12:00AM'\n\ndt1 = dateparser.parse(t1)\ndt2 = dateparser.parse(t2)\n\nprint(dt1)\nprint(dt2)\nimport dateparser\n\nt1 = 'Jun 1 2005  1:33PM'\nt2 = 'Aug 28 1999 12:00AM'\n\ndt1 = dateparser.parse(t1)\ndt2 = dateparser.parse(t2)\n\nprint(dt1)\nprint(dt2)\nOutput:2005-06-01 13:33:00\n1999-08-28 00:00:00\n2005-06-01 13:33:00\n1999-08-28 00:00:00\n",
                "You can use easy_date to make it easy:easy_dateimport date_converter\nconverted_date = date_converter.string_to_datetime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\nimport date_converter\nconverted_date = date_converter.string_to_datetime('Jun 1 2005  1:33PM', '%b %d %Y %I:%M%p')\n",
                "If you want only date format then you can manually convert it by passing your individual fields like:>>> import datetime\n>>> date = datetime.date(int('2017'),int('12'),int('21'))\n>>> date\ndatetime.date(2017, 12, 21)\n>>> type(date)\n<type 'datetime.date'>\n>>> import datetime\n>>> date = datetime.date(int('2017'),int('12'),int('21'))\n>>> date\ndatetime.date(2017, 12, 21)\n>>> type(date)\n<type 'datetime.date'>\nYou can pass your split string values to convert it into date type like:selected_month_rec = '2017-09-01'\ndate_formate = datetime.date(int(selected_month_rec.split('-')[0]),int(selected_month_rec.split('-')[1]),int(selected_month_rec.split('-')[2]))\nselected_month_rec = '2017-09-01'\ndate_formate = datetime.date(int(selected_month_rec.split('-')[0]),int(selected_month_rec.split('-')[1]),int(selected_month_rec.split('-')[2]))\nYou will get the resulting value in date format.",
                "Similar to Javed's answer, I just wanted date from string - so combining Simon's and Javed's logic, we get:Javed's answerSimon'sfrom dateutil import parser\nimport datetime\n\ns = '2021-03-04'\n\nparser.parse(s).date()\nfrom dateutil import parser\nimport datetime\n\ns = '2021-03-04'\n\nparser.parse(s).date()\nOutput\ndatetime.date(2021, 3, 4)\ndatetime.date(2021, 3, 4)datetime.date(2021, 3, 4)",
                "It seems using pandas Timestamp is the fastest:pandas Timestampimport pandas as pd\n\nN = 1000\n\nl = ['Jun 1 2005  1:33PM'] * N\n\nlist(pd.to_datetime(l, format=format))\n\n%timeit _ = list(pd.to_datetime(l, format=format))\n1.58 ms \u00b1 21.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\nimport pandas as pd\n\nN = 1000\n\nl = ['Jun 1 2005  1:33PM'] * N\n\nlist(pd.to_datetime(l, format=format))\n\n%timeit _ = list(pd.to_datetime(l, format=format))\n1.58 ms \u00b1 21.6 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\nOther solutionsfrom datetime import datetime\n%timeit _ = list(map(lambda x: datetime.strptime(x, format), l))\n9.41 ms \u00b1 95.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nfrom dateutil.parser import parse\n%timeit _ = list(map(lambda x: parse(x), l))\n73.8 ms \u00b1 1.14 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\nfrom datetime import datetime\n%timeit _ = list(map(lambda x: datetime.strptime(x, format), l))\n9.41 ms \u00b1 95.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n\nfrom dateutil.parser import parse\n%timeit _ = list(map(lambda x: parse(x), l))\n73.8 ms \u00b1 1.14 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\nIf the string is an ISO\u00a08601 string, please use csio8601:ISO\u00a08601csio8601import ciso8601\n\nl = ['2014-01-09'] * N\n\n%timeit _ = list(map(lambda x: ciso8601.parse_datetime(x), l))\n186 \u00b5s \u00b1 4.13 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\nimport ciso8601\n\nl = ['2014-01-09'] * N\n\n%timeit _ = list(map(lambda x: ciso8601.parse_datetime(x), l))\n186 \u00b5s \u00b1 4.13 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n",
                "If you don't want to explicitly specify which format your string is in with respect to the date time format, you can use this hack to by pass that step:don'tfrom dateutil.parser import parse\n\n# Function that'll guess the format and convert it into the python datetime format\ndef update_event(start_datetime=None, end_datetime=None, description=None):\n    if start_datetime is not None:\n        new_start_time = parse(start_datetime)\n\n        return new_start_time\n\n# Sample input dates in different formats\nd = ['06/07/2021 06:40:23.277000', '06/07/2021 06:40', '06/07/2021']\n\nnew = [update_event(i) for i in d]\n\nfor date in new:\n    print(date)\n    # Sample output dates in Python datetime object\n    #   2014-04-23 00:00:00\n    #   2013-04-24 00:00:00\n    #   2014-04-25 00:00:00\nfrom dateutil.parser import parse\n\n# Function that'll guess the format and convert it into the python datetime format\ndef update_event(start_datetime=None, end_datetime=None, description=None):\n    if start_datetime is not None:\n        new_start_time = parse(start_datetime)\n\n        return new_start_time\n\n# Sample input dates in different formats\nd = ['06/07/2021 06:40:23.277000', '06/07/2021 06:40', '06/07/2021']\n\nnew = [update_event(i) for i in d]\n\nfor date in new:\n    print(date)\n    # Sample output dates in Python datetime object\n    #   2014-04-23 00:00:00\n    #   2013-04-24 00:00:00\n    #   2014-04-25 00:00:00\nIf you want to convert it into some other datetime format, just modify the last line with the format you like for example something like date.strftime('%Y/%m/%d %H:%M:%S.%f'):date.strftime('%Y/%m/%d %H:%M:%S.%f')from dateutil.parser import parse\n\ndef update_event(start_datetime=None, end_datetime=None, description=None):\n    if start_datetime is not None:\n        new_start_time = parse(start_datetime)\n\n        return new_start_time\n\n# Sample input dates in different formats\nd = ['06/07/2021 06:40:23.277000', '06/07/2021 06:40', '06/07/2021']\n\n# Passing the dates one by one through the function\nnew = [update_event(i) for i in d]\n\nfor date in new:\n    print(date.strftime('%Y/%m/%d %H:%M:%S.%f'))\n    # Sample output dates in required Python datetime object\n    #   2021/06/07 06:40:23.277000\n    #   2021/06/07 06:40:00.000000\n    #   2021/06/07 00:00:00.000000\nfrom dateutil.parser import parse\n\ndef update_event(start_datetime=None, end_datetime=None, description=None):\n    if start_datetime is not None:\n        new_start_time = parse(start_datetime)\n\n        return new_start_time\n\n# Sample input dates in different formats\nd = ['06/07/2021 06:40:23.277000', '06/07/2021 06:40', '06/07/2021']\n\n# Passing the dates one by one through the function\nnew = [update_event(i) for i in d]\n\nfor date in new:\n    print(date.strftime('%Y/%m/%d %H:%M:%S.%f'))\n    # Sample output dates in required Python datetime object\n    #   2021/06/07 06:40:23.277000\n    #   2021/06/07 06:40:00.000000\n    #   2021/06/07 00:00:00.000000\nTry running the above snippet to have a better clarity.",
                "See my answer.my answerIn real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from strptime()) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From my solutionstrptime()my solutiondef try_strptime(s, fmts=['%d-%b-%y','%m/%d/%Y']):\n    for fmt in fmts:\n        try:\n            return datetime.strptime(s, fmt)\n        except:\n            continue\n\n    return None # or reraise the ValueError if no format matched, if you prefer\ndef try_strptime(s, fmts=['%d-%b-%y','%m/%d/%Y']):\n    for fmt in fmts:\n        try:\n            return datetime.strptime(s, fmt)\n        except:\n            continue\n\n    return None # or reraise the ValueError if no format matched, if you prefer\n",
                "A short sample mapping a yyyy-mm-dd date string to a datetime.date object:from datetime import date\ndate_from_yyyy_mm_dd = lambda \u03b4 : date(*[int(_) for _ in \u03b4.split('-')])\ndate_object = date_from_yyyy_mm_dd('2021-02-15')\nfrom datetime import date\ndate_from_yyyy_mm_dd = lambda \u03b4 : date(*[int(_) for _ in \u03b4.split('-')])\ndate_object = date_from_yyyy_mm_dd('2021-02-15')\n",
                "Use:emp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\")\nemp.info()\nemp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\")\nemp.info()\nIt shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame:<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n\n    Start Date           1000 non-null object\n\n    Last Login Time      1000 non-null object\n\nSalary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: float64(1), int64(1), object(6)\nmemory usage: 62.6+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n\n    Start Date           1000 non-null object\n\n    Last Login Time      1000 non-null object\n\nSalary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: float64(1), int64(1), object(6)\nmemory usage: 62.6+ KB\nBy using the parse_dates option in read_csv mention, you can convert your string datetime into the pandas datetime format.parse_datesread_csvemp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\", parse_dates=[\"Start Date\", \"Last Login Time\"])\nemp.info()\nemp = pd.read_csv(\"C:\\\\py\\\\programs\\\\pandas_2\\\\pandas\\\\employees.csv\", parse_dates=[\"Start Date\", \"Last Login Time\"])\nemp.info()\nOutput:<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n\n     Start Date           1000 non-null datetime64[ns]\n     Last Login Time      1000 non-null datetime64[ns]\n\nSalary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: datetime64[ns](2), float64(1), int64(1), object(4)\nmemory usage: 62.6+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 8 columns):\nFirst Name           933 non-null object\nGender               855 non-null object\n\n     Start Date           1000 non-null datetime64[ns]\n     Last Login Time      1000 non-null datetime64[ns]\n\nSalary               1000 non-null int64\nBonus %              1000 non-null float64\nSenior Management    933 non-null object\nTeam                 957 non-null object\ndtypes: datetime64[ns](2), float64(1), int64(1), object(4)\nmemory usage: 62.6+ KB\n",
                "#Convert String to datetime\n>>> x=datetime.strptime('Jun 1 2005', '%b %d %Y').date()\n>>> print(x,type(x))\n2005-06-01 00:00:00 <class 'datetime.datetime'>\n\n\n#Convert datetime to String (Reverse above process)\n>>> y=x.strftime('%b %d %Y')\n>>> print(y,type(y))\nJun 01 2005 <class 'str'>\n#Convert String to datetime\n>>> x=datetime.strptime('Jun 1 2005', '%b %d %Y').date()\n>>> print(x,type(x))\n2005-06-01 00:00:00 <class 'datetime.datetime'>\n\n\n#Convert datetime to String (Reverse above process)\n>>> y=x.strftime('%b %d %Y')\n>>> print(y,type(y))\nJun 01 2005 <class 'str'>\n"
            ]
        },
        {
            "tag": "python_speed",
            "patterns": [
                "Why is \"1000000000000000 in range(1000000000000001)\" so fast in Python 3?",
                "what causes 1000000000000000 in range 1000000000001 to be so fast in python?",
                "why is 1000000000000000 in the range1000000000000001 really fast in python 3?"
            ],
            "responses": [
                "The Python 3 range() object doesn't produce numbers immediately; it is a smart sequence object that produces numbers on demand. All it contains is your start, stop and step values, then as you iterate over the object the next integer is calculated each iteration.range()sequence objecton demandThe object also implements the object.__contains__ hook, and calculates if your number is part of its range. Calculating is a (near) constant time operation *. There is never a need to scan through all possible integers in the range.object.__contains__ hookobject.__contains__calculates*From the range() object documentation:range() object documentationrange()\nThe advantage of the range type over a regular list or tuple is that a range object will always take the same (small) amount of memory, no matter the size of the range it represents (as it only stores the start, stop and step values, calculating individual items and subranges as needed).\nThe advantage of the range type over a regular list or tuple is that a range object will always take the same (small) amount of memory, no matter the size of the range it represents (as it only stores the start, stop and step values, calculating individual items and subranges as needed).rangelisttuplestartstopstepSo at a minimum, your range() object would do:range()class my_range:\n    def __init__(self, start, stop=None, step=1, /):\n        if stop is None:\n            start, stop = 0, start\n        self.start, self.stop, self.step = start, stop, step\n        if step < 0:\n            lo, hi, step = stop, start, -step\n        else:\n            lo, hi = start, stop\n        self.length = 0 if lo > hi else ((hi - lo - 1) // step) + 1\n\n    def __iter__(self):\n        current = self.start\n        if self.step < 0:\n            while current > self.stop:\n                yield current\n                current += self.step\n        else:\n            while current < self.stop:\n                yield current\n                current += self.step\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, i):\n        if i < 0:\n            i += self.length\n        if 0 <= i < self.length:\n            return self.start + i * self.step\n        raise IndexError('my_range object index out of range')\n\n    def __contains__(self, num):\n        if self.step < 0:\n            if not (self.stop < num <= self.start):\n                return False\n        else:\n            if not (self.start <= num < self.stop):\n                return False\n        return (num - self.start) % self.step == 0\nclass my_range:\n    def __init__(self, start, stop=None, step=1, /):\n        if stop is None:\n            start, stop = 0, start\n        self.start, self.stop, self.step = start, stop, step\n        if step < 0:\n            lo, hi, step = stop, start, -step\n        else:\n            lo, hi = start, stop\n        self.length = 0 if lo > hi else ((hi - lo - 1) // step) + 1\n\n    def __iter__(self):\n        current = self.start\n        if self.step < 0:\n            while current > self.stop:\n                yield current\n                current += self.step\n        else:\n            while current < self.stop:\n                yield current\n                current += self.step\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, i):\n        if i < 0:\n            i += self.length\n        if 0 <= i < self.length:\n            return self.start + i * self.step\n        raise IndexError('my_range object index out of range')\n\n    def __contains__(self, num):\n        if self.step < 0:\n            if not (self.stop < num <= self.start):\n                return False\n        else:\n            if not (self.start <= num < self.stop):\n                return False\n        return (num - self.start) % self.step == 0\nThis is still missing several things that a real range() supports (such as the .index() or .count() methods, hashing, equality testing, or slicing), but should give you an idea.range().index().count()I also simplified the __contains__ implementation to only focus on integer tests; if you give a real range() object a non-integer value (including subclasses of int), a slow scan is initiated to see if there is a match, just as if you use a containment test against a list of all the contained values. This was done to continue to support other numeric types that just happen to support equality testing with integers but are not expected to support integer arithmetic as well. See the original Python issue that implemented the containment test.__contains__range()intPython issue* Near constant time because Python integers are unbounded and so math operations also grow in time as N grows, making this a O(log N) operation. Since it\u2019s all executed in optimised C code and Python stores integer values in 30-bit chunks, you\u2019d run out of memory before you saw any performance impact due to the size of the integers involved here.Near",
                "The fundamental misunderstanding here is in thinking that range is a generator. It's not. In fact, it's not any kind of iterator.rangeYou can tell this pretty easily:>>> a = range(5)\n>>> print(list(a))\n[0, 1, 2, 3, 4]\n>>> print(list(a))\n[0, 1, 2, 3, 4]\n>>> a = range(5)\n>>> print(list(a))\n[0, 1, 2, 3, 4]\n>>> print(list(a))\n[0, 1, 2, 3, 4]\nIf it were a generator, iterating it once would exhaust it:>>> b = my_crappy_range(5)\n>>> print(list(b))\n[0, 1, 2, 3, 4]\n>>> print(list(b))\n[]\n>>> b = my_crappy_range(5)\n>>> print(list(b))\n[0, 1, 2, 3, 4]\n>>> print(list(b))\n[]\nWhat range actually is, is a sequence, just like a list. You can even test this:range>>> import collections.abc\n>>> isinstance(a, collections.abc.Sequence)\nTrue\n>>> import collections.abc\n>>> isinstance(a, collections.abc.Sequence)\nTrue\nThis means it has to follow all the rules of being a sequence:>>> a[3]         # indexable\n3\n>>> len(a)       # sized\n5\n>>> 3 in a       # membership\nTrue\n>>> reversed(a)  # reversible\n<range_iterator at 0x101cd2360>\n>>> a.index(3)   # implements 'index'\n3\n>>> a.count(3)   # implements 'count'\n1\n>>> a[3]         # indexable\n3\n>>> len(a)       # sized\n5\n>>> 3 in a       # membership\nTrue\n>>> reversed(a)  # reversible\n<range_iterator at 0x101cd2360>\n>>> a.index(3)   # implements 'index'\n3\n>>> a.count(3)   # implements 'count'\n1\nThe difference between a range and a list is that a range is a lazy or dynamic sequence; it doesn't remember all of its values, it just remembers its start, stop, and step, and creates the values on demand on __getitem__.rangelistrangelazydynamicstartstopstep__getitem__(As a side note, if you print(iter(a)), you'll notice that range uses the same listiterator type as list. How does that work? A listiterator doesn't use anything special about list except for the fact that it provides a C implementation of __getitem__, so it works fine for range too.)print(iter(a))rangelistiteratorlistlistiteratorlist__getitem__rangeNow, there's nothing that says that Sequence.__contains__ has to be constant time\u2014in fact, for obvious examples of sequences like list, it isn't. But there's nothing that says it can't be. And it's easier to implement range.__contains__ to just check it mathematically ((val - start) % step, but with some extra complexity to deal with negative steps) than to actually generate and test all the values, so why shouldn't it do it the better way?Sequence.__contains__listcan'trange.__contains__(val - start) % stepshouldn'tBut there doesn't seem to be anything in the language that guarantees this will happen. As Ashwini Chaudhari points out, if you give it a non-integral value, instead of converting to integer and doing the mathematical test, it will fall back to iterating all the values and comparing them one by one. And just because CPython 3.2+ and PyPy 3.x versions happen to contain this optimization, and it's an obvious good idea and easy to do, there's no reason that IronPython or NewKickAssPython 3.x couldn't leave it out. (And in fact, CPython 3.0-3.1 didn't include it.)guaranteesdidn'tIf range actually were a generator, like my_crappy_range, then it wouldn't make sense to test __contains__ this way, or at least the way it makes sense wouldn't be obvious. If you'd already iterated the first 3 values, is 1 still in the generator? Should testing for 1 cause it to iterate and consume all the values up to 1 (or up to the first value >= 1)?rangemy_crappy_range__contains__1in11>= 1",
                "Use the source, Luke!sourceIn CPython, range(...).__contains__ (a method wrapper) will eventually delegate to a simple calculation which checks if the value can possibly be in the range.  The reason for the speed here is we're using mathematical reasoning about the bounds, rather than a direct iteration of the range object.  To explain the logic used:range(...).__contains__mathematical reasoning about the bounds, rather than a direct iteration of the range object\nCheck that the number is between start and stop, and\nCheck that the stride value doesn't \"step over\" our number.\nCheck that the number is between start and stop, andstartstopCheck that the stride value doesn't \"step over\" our number.For example, 994 is in range(4, 1000, 2) because:994range(4, 1000, 2)\n4 <= 994 < 1000, and\n(994 - 4) % 2 == 0.\n4 <= 994 < 1000, and4 <= 994 < 1000(994 - 4) % 2 == 0.(994 - 4) % 2 == 0The full C code is included below, which is a bit more verbose because of memory management and reference counting details, but the basic idea is there:static int\nrange_contains_long(rangeobject *r, PyObject *ob)\n{\n    int cmp1, cmp2, cmp3;\n    PyObject *tmp1 = NULL;\n    PyObject *tmp2 = NULL;\n    PyObject *zero = NULL;\n    int result = -1;\n\n    zero = PyLong_FromLong(0);\n    if (zero == NULL) /* MemoryError in int(0) */\n        goto end;\n\n    /* Check if the value can possibly be in the range. */\n\n    cmp1 = PyObject_RichCompareBool(r->step, zero, Py_GT);\n    if (cmp1 == -1)\n        goto end;\n    if (cmp1 == 1) { /* positive steps: start <= ob < stop */\n        cmp2 = PyObject_RichCompareBool(r->start, ob, Py_LE);\n        cmp3 = PyObject_RichCompareBool(ob, r->stop, Py_LT);\n    }\n    else { /* negative steps: stop < ob <= start */\n        cmp2 = PyObject_RichCompareBool(ob, r->start, Py_LE);\n        cmp3 = PyObject_RichCompareBool(r->stop, ob, Py_LT);\n    }\n\n    if (cmp2 == -1 || cmp3 == -1) /* TypeError */\n        goto end;\n    if (cmp2 == 0 || cmp3 == 0) { /* ob outside of range */\n        result = 0;\n        goto end;\n    }\n\n    /* Check that the stride does not invalidate ob's membership. */\n    tmp1 = PyNumber_Subtract(ob, r->start);\n    if (tmp1 == NULL)\n        goto end;\n    tmp2 = PyNumber_Remainder(tmp1, r->step);\n    if (tmp2 == NULL)\n        goto end;\n    /* result = ((int(ob) - start) % step) == 0 */\n    result = PyObject_RichCompareBool(tmp2, zero, Py_EQ);\n  end:\n    Py_XDECREF(tmp1);\n    Py_XDECREF(tmp2);\n    Py_XDECREF(zero);\n    return result;\n}\n\nstatic int\nrange_contains(rangeobject *r, PyObject *ob)\n{\n    if (PyLong_CheckExact(ob) || PyBool_Check(ob))\n        return range_contains_long(r, ob);\n\n    return (int)_PySequence_IterSearch((PyObject*)r, ob,\n                                       PY_ITERSEARCH_CONTAINS);\n}\nstatic int\nrange_contains_long(rangeobject *r, PyObject *ob)\n{\n    int cmp1, cmp2, cmp3;\n    PyObject *tmp1 = NULL;\n    PyObject *tmp2 = NULL;\n    PyObject *zero = NULL;\n    int result = -1;\n\n    zero = PyLong_FromLong(0);\n    if (zero == NULL) /* MemoryError in int(0) */\n        goto end;\n\n    /* Check if the value can possibly be in the range. */\n\n    cmp1 = PyObject_RichCompareBool(r->step, zero, Py_GT);\n    if (cmp1 == -1)\n        goto end;\n    if (cmp1 == 1) { /* positive steps: start <= ob < stop */\n        cmp2 = PyObject_RichCompareBool(r->start, ob, Py_LE);\n        cmp3 = PyObject_RichCompareBool(ob, r->stop, Py_LT);\n    }\n    else { /* negative steps: stop < ob <= start */\n        cmp2 = PyObject_RichCompareBool(ob, r->start, Py_LE);\n        cmp3 = PyObject_RichCompareBool(r->stop, ob, Py_LT);\n    }\n\n    if (cmp2 == -1 || cmp3 == -1) /* TypeError */\n        goto end;\n    if (cmp2 == 0 || cmp3 == 0) { /* ob outside of range */\n        result = 0;\n        goto end;\n    }\n\n    /* Check that the stride does not invalidate ob's membership. */\n    tmp1 = PyNumber_Subtract(ob, r->start);\n    if (tmp1 == NULL)\n        goto end;\n    tmp2 = PyNumber_Remainder(tmp1, r->step);\n    if (tmp2 == NULL)\n        goto end;\n    /* result = ((int(ob) - start) % step) == 0 */\n    result = PyObject_RichCompareBool(tmp2, zero, Py_EQ);\n  end:\n    Py_XDECREF(tmp1);\n    Py_XDECREF(tmp2);\n    Py_XDECREF(zero);\n    return result;\n}\n\nstatic int\nrange_contains(rangeobject *r, PyObject *ob)\n{\n    if (PyLong_CheckExact(ob) || PyBool_Check(ob))\n        return range_contains_long(r, ob);\n\n    return (int)_PySequence_IterSearch((PyObject*)r, ob,\n                                       PY_ITERSEARCH_CONTAINS);\n}\nThe \"meat\" of the idea is mentioned in the comment lines:/* positive steps: start <= ob < stop */\n/* negative steps: stop < ob <= start */\n/* result = ((int(ob) - start) % step) == 0 */ \n/* positive steps: start <= ob < stop */\n/* negative steps: stop < ob <= start */\n/* result = ((int(ob) - start) % step) == 0 */ \nAs a final note - look at the range_contains function at the bottom of the code snippet.  If the exact type check fails then we don't use the clever algorithm described, instead falling back to a dumb iteration search of the range using _PySequence_IterSearch!  You can check this behaviour in the interpreter (I'm using v3.5.0 here):range_contains_PySequence_IterSearch>>> x, r = 1000000000000000, range(1000000000000001)\n>>> class MyInt(int):\n...     pass\n... \n>>> x_ = MyInt(x)\n>>> x in r  # calculates immediately :) \nTrue\n>>> x_ in r  # iterates for ages.. :( \n^\\Quit (core dumped)\n>>> x, r = 1000000000000000, range(1000000000000001)\n>>> class MyInt(int):\n...     pass\n... \n>>> x_ = MyInt(x)\n>>> x in r  # calculates immediately :) \nTrue\n>>> x_ in r  # iterates for ages.. :( \n^\\Quit (core dumped)\n",
                "To add to Martijn\u2019s answer, this is the relevant part of the source (in C, as the range object is written in native code):the sourcestatic int\nrange_contains(rangeobject *r, PyObject *ob)\n{\n    if (PyLong_CheckExact(ob) || PyBool_Check(ob))\n        return range_contains_long(r, ob);\n\n    return (int)_PySequence_IterSearch((PyObject*)r, ob,\n                                       PY_ITERSEARCH_CONTAINS);\n}\nstatic int\nrange_contains(rangeobject *r, PyObject *ob)\n{\n    if (PyLong_CheckExact(ob) || PyBool_Check(ob))\n        return range_contains_long(r, ob);\n\n    return (int)_PySequence_IterSearch((PyObject*)r, ob,\n                                       PY_ITERSEARCH_CONTAINS);\n}\nSo for PyLong objects (which is int in Python 3), it will use the range_contains_long function to determine the result. And that function essentially checks if ob is in the specified range (although it looks a bit more complex in C).PyLongintrange_contains_longobIf it\u2019s not an int object, it falls back to iterating until it finds the value (or not).intThe whole logic could be translated to pseudo-Python like this:def range_contains (rangeObj, obj):\n    if isinstance(obj, int):\n        return range_contains_long(rangeObj, obj)\n\n    # default logic by iterating\n    return any(obj == x for x in rangeObj)\n\ndef range_contains_long (r, num):\n    if r.step > 0:\n        # positive step: r.start <= num < r.stop\n        cmp2 = r.start <= num\n        cmp3 = num < r.stop\n    else:\n        # negative step: r.start >= num > r.stop\n        cmp2 = num <= r.start\n        cmp3 = r.stop < num\n\n    # outside of the range boundaries\n    if not cmp2 or not cmp3:\n        return False\n\n    # num must be on a valid step inside the boundaries\n    return (num - r.start) % r.step == 0\ndef range_contains (rangeObj, obj):\n    if isinstance(obj, int):\n        return range_contains_long(rangeObj, obj)\n\n    # default logic by iterating\n    return any(obj == x for x in rangeObj)\n\ndef range_contains_long (r, num):\n    if r.step > 0:\n        # positive step: r.start <= num < r.stop\n        cmp2 = r.start <= num\n        cmp3 = num < r.stop\n    else:\n        # negative step: r.start >= num > r.stop\n        cmp2 = num <= r.start\n        cmp3 = r.stop < num\n\n    # outside of the range boundaries\n    if not cmp2 or not cmp3:\n        return False\n\n    # num must be on a valid step inside the boundaries\n    return (num - r.start) % r.step == 0\n",
                "If you're wondering why this optimization was added to range.__contains__, and why it wasn't added to xrange.__contains__ in 2.7:whyrange.__contains__wasn'txrange.__contains__First, as Ashwini Chaudhary discovered, issue 1766304 was opened explicitly to optimize [x]range.__contains__. A patch for this was accepted and checked in for 3.2, but not backported to 2.7 because \"xrange has behaved like this for such a long time that I don't see what it buys us to commit the patch this late.\" (2.7 was nearly out at that point.)issue 1766304[x]range.__contains__accepted and checked in for 3.2xrangeMeanwhile:Originally, xrange was a not-quite-sequence object. As the 3.1 docs say:xrangethe 3.1 docs\nRange objects have very little behavior: they only support indexing, iteration, and the len function.\nRange objects have very little behavior: they only support indexing, iteration, and the len function.lenThis wasn't quite true; an xrange object actually supported a few other things that come automatically with indexing and len,* including __contains__ (via linear search). But nobody thought it was worth making them full sequences at the time.xrangelen*__contains__Then, as part of implementing the Abstract Base Classes PEP, it was important to figure out which builtin types should be marked as implementing which ABCs, and xrange/range claimed to implement collections.Sequence, even though it still only handled the same \"very little behavior\". Nobody noticed that problem until issue 9213. The patch for that issue not only added index and count to 3.2's range, it also re-worked the optimized __contains__ (which shares the same math with index, and is directly used by count).** This change went in for 3.2 as well, and was not backported to 2.x, because \"it's a bugfix that adds new methods\". (At this point, 2.7 was already past rc status.)Abstract Base Classesxrangerangecollections.Sequenceissue 9213indexcountrange__contains__indexcount**This changeSo, there were two chances to get this optimization backported to 2.7, but they were both rejected.* In fact, you even get iteration for free with indexing alone, but in 2.3 xrange objects got a custom iterator.* In fact, you even get iteration for free with indexing alone, but in 2.3 xrange objects got a custom iterator.in 2.3xrange** The first version actually reimplemented it, and got the details wrong\u2014e.g., it would give you MyIntSubclass(2) in range(5) == False. But Daniel Stutzbach's updated version of the patch restored most of the previous code, including the fallback to the generic, slow _PySequence_IterSearch that pre-3.2 range.__contains__ was implicitly using when the optimization doesn't apply.** The first version actually reimplemented it, and got the details wrong\u2014e.g., it would give you MyIntSubclass(2) in range(5) == False. But Daniel Stutzbach's updated version of the patch restored most of the previous code, including the fallback to the generic, slow _PySequence_IterSearch that pre-3.2 range.__contains__ was implicitly using when the optimization doesn't apply.MyIntSubclass(2) in range(5) == False_PySequence_IterSearchrange.__contains__",
                "The other answers explained it well already, but I'd like to offer another experiment illustrating the nature of range objects:>>> r = range(5)\n>>> for i in r:\n        print(i, 2 in r, list(r))\n        \n0 True [0, 1, 2, 3, 4]\n1 True [0, 1, 2, 3, 4]\n2 True [0, 1, 2, 3, 4]\n3 True [0, 1, 2, 3, 4]\n4 True [0, 1, 2, 3, 4]\n>>> r = range(5)\n>>> for i in r:\n        print(i, 2 in r, list(r))\n        \n0 True [0, 1, 2, 3, 4]\n1 True [0, 1, 2, 3, 4]\n2 True [0, 1, 2, 3, 4]\n3 True [0, 1, 2, 3, 4]\n4 True [0, 1, 2, 3, 4]\nAs you can see, a range object is an object that remembers its range and can be used many times (even while iterating over it), not just a one-time generator.range",
                "It's all about a lazy approach to the evaluation and some extra optimization of range.\nValues in ranges don't need to be computed until real use, or even further due to extra optimization.lazy approachextra optimizationrangeBy the way, your integer is not such big, consider sys.maxsizesys.maxsizesys.maxsize in range(sys.maxsize) is pretty fastsys.maxsize in range(sys.maxsize)is pretty fastdue to optimization - it's easy to compare given integers just with min and max of range.but:Decimal(sys.maxsize) in range(sys.maxsize) is pretty slow.Decimal(sys.maxsize) in range(sys.maxsize)is pretty slow(in this case, there is no optimization in range, so if python receives unexpected Decimal, python will compare all numbers)rangeYou should be aware of an implementation detail but should not be relied upon, because this may change in the future.",
                "TL;DRThe object returned by range() is actually a range object. This object implements the iterator interface so you can iterate over its values sequentially, just like a generator, list, or tuple.range()rangeBut it also implements the __contains__ interface which is actually what gets called when an object appears on the right-hand side of the in operator. The __contains__() method returns a bool of whether or not the item on the left-hand side of the in is in the object. Since range objects know their bounds and stride, this is very easy to implement in O(1).also__contains____contains__in__contains__()boolinrange",
                "\nDue to optimization, it is very easy to compare given integers just with min and max range.\nThe reason that the range() function is so fast in Python3 is that here we use mathematical reasoning for the bounds, rather than a direct iteration of the range object.\nSo for explaining the logic here:\nDue to optimization, it is very easy to compare given integers just with min and max range.The reason that the range() function is so fast in Python3 is that here we use mathematical reasoning for the bounds, rather than a direct iteration of the range object.range()So for explaining the logic here:\nCheck whether the number is between the start and stop.\nCheck whether the step precision value doesn't go over our number.\nCheck whether the number is between the start and stop.Check whether the step precision value doesn't go over our number.\nTake an example, 997 is in range(4, 1000, 3) because:\n4 <= 997 < 1000, and (997 - 4) % 3 == 0.\n\nTake an example, 997 is in range(4, 1000, 3) because:\n4 <= 997 < 1000, and (997 - 4) % 3 == 0.\nTake an example, 997 is in range(4, 1000, 3) because:997 is in range(4, 1000, 3)4 <= 997 < 1000, and (997 - 4) % 3 == 0.4 <= 997 < 1000, and (997 - 4) % 3 == 0.",
                "Try x-1 in (i for i in range(x)) for large x values, which uses a generator comprehension to avoid invoking the range.__contains__ optimisation.x-1 in (i for i in range(x))xrange.__contains__",
                "TLDR;\nthe range is an arithmetic series so it can very easily calculate whether the object is there. It could even get the index of it if it were list like really quickly.TLDR;range",
                "__contains__ method compares directly with the start and end of the range__contains__"
            ]
        },
        {
            "tag": "find_directory",
            "patterns": [
                "Find the current directory and file's directory [duplicate]",
                "Find the current directory and file's directory [duplicate]", "How to find directory"
            ],
            "responses": [
                "To get the full path to the directory a Python file is contained in, write this in that file:import os \ndir_path = os.path.dirname(os.path.realpath(__file__))\nimport os \ndir_path = os.path.dirname(os.path.realpath(__file__))\n(Note that the incantation above won't work if you've already used os.chdir() to change your current working directory, since the value of the __file__ constant is relative to the current working directory and is not changed by an os.chdir() call.)os.chdir()__file__os.chdir()To get the current working directory use import os\ncwd = os.getcwd()\nimport os\ncwd = os.getcwd()\nDocumentation references for the modules, constants and functions used above:\nThe os and os.path modules.\nThe __file__ constant\nos.path.realpath(path) (returns \"the canonical path of the specified filename, eliminating any symbolic links encountered in the path\")\nos.path.dirname(path) (returns \"the directory name of pathname path\")\nos.getcwd() (returns \"a string representing the current working directory\")\nos.chdir(path) (\"change the current working directory to path\")\nThe os and os.path modules.ososos.pathos.pathThe __file__ constant__file____file__os.path.realpath(path) (returns \"the canonical path of the specified filename, eliminating any symbolic links encountered in the path\")os.path.realpath(path)os.path.realpath(path)\"the canonical path of the specified filename, eliminating any symbolic links encountered in the path\"os.path.dirname(path) (returns \"the directory name of pathname path\")os.path.dirname(path)os.path.dirname(path)\"the directory name of pathname path\"pathos.getcwd() (returns \"a string representing the current working directory\")os.getcwd()os.getcwd()\"a string representing the current working directory\"os.chdir(path) (\"change the current working directory to path\")os.chdir(path)os.chdir(path)\"change the current working directory to path\"path",
                "Current working directory:  os.getcwd()Current working directoryos.getcwd()os.getcwd()And the __file__ attribute can help you find out where the file you are executing is located. This Stack\u00a0Overflow post explains everything:  How do I get the path of the current executed file in Python?__file__ attribute__file__How do I get the path of the current executed file in Python?How do I get the path of the current executed file in Python?",
                "You may find this useful as a reference:import os\n\nprint(\"Path at terminal when executing this file\")\nprint(os.getcwd() + \"\\n\")\n\nprint(\"This file path, relative to os.getcwd()\")\nprint(__file__ + \"\\n\")\n\nprint(\"This file full path (following symlinks)\")\nfull_path = os.path.realpath(__file__)\nprint(full_path + \"\\n\")\n\nprint(\"This file directory and name\")\npath, filename = os.path.split(full_path)\nprint(path + ' --> ' + filename + \"\\n\")\n\nprint(\"This file directory only\")\nprint(os.path.dirname(full_path))\nimport os\n\nprint(\"Path at terminal when executing this file\")\nprint(os.getcwd() + \"\\n\")\n\nprint(\"This file path, relative to os.getcwd()\")\nprint(__file__ + \"\\n\")\n\nprint(\"This file full path (following symlinks)\")\nfull_path = os.path.realpath(__file__)\nprint(full_path + \"\\n\")\n\nprint(\"This file directory and name\")\npath, filename = os.path.split(full_path)\nprint(path + ' --> ' + filename + \"\\n\")\n\nprint(\"This file directory only\")\nprint(os.path.dirname(full_path))\n",
                "The pathlib module, introduced in Python 3.4 (PEP 428 \u2014 The pathlib module \u2014 object-oriented filesystem paths), makes the path-related experience much much better.pathlibpathlibintroduced in Python 3.4PEP 428 \u2014 The pathlib module \u2014 object-oriented filesystem pathsPEP 428 \u2014 The pathlib module \u2014 object-oriented filesystem pathspwd\n\n/home/skovorodkin/stack\n\ntree\n\n.\n\u2514\u2500\u2500 scripts\n    \u251c\u2500\u2500 1.py\n    \u2514\u2500\u2500 2.py\npwd\n\n/home/skovorodkin/stack\n\ntree\n\n.\n\u2514\u2500\u2500 scripts\n    \u251c\u2500\u2500 1.py\n    \u2514\u2500\u2500 2.py\nIn order to get the current working directory, use Path.cwd():Path.cwd()Path.cwd()from pathlib import Path\n\nprint(Path.cwd())  # /home/skovorodkin/stack\nfrom pathlib import Path\n\nprint(Path.cwd())  # /home/skovorodkin/stack\nTo get an absolute path to your script file, use the Path.resolve() method:Path.resolve()Path.resolve()print(Path(__file__).resolve())  # /home/skovorodkin/stack/scripts/1.py\nprint(Path(__file__).resolve())  # /home/skovorodkin/stack/scripts/1.py\nAnd to get the path of a directory where your script is located, access .parent (it is recommended to call .resolve() before .parent):.parent.parent.resolve().parentprint(Path(__file__).resolve().parent)  # /home/skovorodkin/stack/scripts\nprint(Path(__file__).resolve().parent)  # /home/skovorodkin/stack/scripts\nRemember that __file__ is not reliable in some situations: How do I get the path of the current executed file in Python?.__file__How do I get the path of the current executed file in Python?How do I get the path of the current executed file in Python?\nPlease note, that Path.cwd(), Path.resolve() and other Path methods return path objects (PosixPath in my case), not strings. In Python 3.4 and 3.5 that caused some pain, because open built-in function could only work with string or bytes objects, and did not support Path objects, so you had to convert Path objects to strings or use the Path.open() method, but the latter option required you to change old code:\nFile scripts/2.py\nfrom pathlib import Path\n\np = Path(__file__).resolve()\n\nwith p.open() as f: pass\nwith open(str(p)) as f: pass\nwith open(p) as f: pass\n\nprint('OK')\n\nOutput\npython3.5 scripts/2.py\n\nTraceback (most recent call last):\n  File \"scripts/2.py\", line 11, in <module>\n    with open(p) as f:\nTypeError: invalid file: PosixPath('/home/skovorodkin/stack/scripts/2.py')\n\nAs you can see, open(p) does not work with Python 3.5.\nPEP 519 \u2014 Adding a file system path protocol, implemented in Python 3.6, adds support of PathLike objects to the open function, so now you can pass Path objects to the open function directly:\npython3.6 scripts/2.py\n\nOK\n\nPlease note, that Path.cwd(), Path.resolve() and other Path methods return path objects (PosixPath in my case), not strings. In Python 3.4 and 3.5 that caused some pain, because open built-in function could only work with string or bytes objects, and did not support Path objects, so you had to convert Path objects to strings or use the Path.open() method, but the latter option required you to change old code:Path.cwd()Path.resolve()PathPosixPathPosixPathopenopenPathPathPath.open()Path.open()File scripts/2.pyscripts/2.pyfrom pathlib import Path\n\np = Path(__file__).resolve()\n\nwith p.open() as f: pass\nwith open(str(p)) as f: pass\nwith open(p) as f: pass\n\nprint('OK')\nfrom pathlib import Path\n\np = Path(__file__).resolve()\n\nwith p.open() as f: pass\nwith open(str(p)) as f: pass\nwith open(p) as f: pass\n\nprint('OK')\nOutputpython3.5 scripts/2.py\n\nTraceback (most recent call last):\n  File \"scripts/2.py\", line 11, in <module>\n    with open(p) as f:\nTypeError: invalid file: PosixPath('/home/skovorodkin/stack/scripts/2.py')\npython3.5 scripts/2.py\n\nTraceback (most recent call last):\n  File \"scripts/2.py\", line 11, in <module>\n    with open(p) as f:\nTypeError: invalid file: PosixPath('/home/skovorodkin/stack/scripts/2.py')\nAs you can see, open(p) does not work with Python 3.5.open(p)PEP 519 \u2014 Adding a file system path protocol, implemented in Python 3.6, adds support of PathLike objects to the open function, so now you can pass Path objects to the open function directly:PEP 519 \u2014 Adding a file system path protocolPEP 519 \u2014 Adding a file system path protocolPathLikePathLikeopenopenPathopenpython3.6 scripts/2.py\n\nOK\npython3.6 scripts/2.py\n\nOK\n",
                "\nTo get the current directory full path\n>>import os\n>>print os.getcwd()\n\nOutput: \"C :\\Users\\admin\\myfolder\"\n\nTo get the current directory folder name alone\n>>import os\n>>str1=os.getcwd()\n>>str2=str1.split('\\\\')\n>>n=len(str2)\n>>print str2[n-1]\n\nOutput: \"myfolder\"\n\nTo get the current directory full path\n>>import os\n>>print os.getcwd()\n\nOutput: \"C :\\Users\\admin\\myfolder\"\nTo get the current directory full path>>import os\n>>print os.getcwd()\n>>import os\n>>print os.getcwd()\nOutput: \"C :\\Users\\admin\\myfolder\"To get the current directory folder name alone\n>>import os\n>>str1=os.getcwd()\n>>str2=str1.split('\\\\')\n>>n=len(str2)\n>>print str2[n-1]\n\nOutput: \"myfolder\"\nTo get the current directory folder name alone>>import os\n>>str1=os.getcwd()\n>>str2=str1.split('\\\\')\n>>n=len(str2)\n>>print str2[n-1]\n>>import os\n>>str1=os.getcwd()\n>>str2=str1.split('\\\\')\n>>n=len(str2)\n>>print str2[n-1]\nOutput: \"myfolder\"",
                "Pathlib can be used this way to get the directory containing the current script:Pathlibimport pathlib\nfilepath = pathlib.Path(__file__).resolve().parent\nimport pathlib\nfilepath = pathlib.Path(__file__).resolve().parent\n",
                "If you are trying to find the current directory of the file you are currently in:OS agnostic way:dirname, filename = os.path.split(os.path.abspath(__file__))\ndirname, filename = os.path.split(os.path.abspath(__file__))\n",
                "If you're using Python 3.4, there is the brand new higher-level pathlib module which allows you to conveniently call pathlib.Path.cwd() to get a Path object representing your current working directory, along with many other new features.pathlibpathlib.Path.cwd()PathMore info on this new API can be found here.here",
                "To get the current directory full path:os.path.realpath('.')\nos.path.realpath('.')\n",
                "Answer to #1:Answer to #1:If you want the current directory, do this:import os\nos.getcwd()\nimport os\nos.getcwd()\nIf you want just any folder name and you have the path to that folder, do this:def get_folder_name(folder):\n    '''\n    Returns the folder name, given a full folder path\n    '''\n    return folder.split(os.sep)[-1]\ndef get_folder_name(folder):\n    '''\n    Returns the folder name, given a full folder path\n    '''\n    return folder.split(os.sep)[-1]\nAnswer to #2:Answer to #2:import os\nprint os.path.abspath(__file__)\nimport os\nprint os.path.abspath(__file__)\n",
                "I think the most succinct way to find just the name of your current execution context would be:current_folder_path, current_folder_name = os.path.split(os.getcwd())\ncurrent_folder_path, current_folder_name = os.path.split(os.getcwd())\n",
                "If you're searching for the location of the currently executed script, you can use sys.argv[0] to get the full path.sys.argv[0]",
                "For question 1, use os.getcwd() # Get working directory and os.chdir(r'D:\\Steam\\steamapps\\common') # Set working directoryos.getcwd() # Get working directoryos.chdir(r'D:\\Steam\\steamapps\\common') # Set working directory\nI recommend using sys.argv[0] for question 2 because sys.argv is immutable and therefore always returns the current file (module object path) and not affected by os.chdir(). Also you can do like this:\nimport os\nthis_py_file = os.path.realpath(__file__)\n\n# vvv Below comes your code vvv #\n\nBut that snippet and sys.argv[0] will not work or will work weird when compiled by PyInstaller, because magic properties are not set in __main__ level and sys.argv[0] is the way your executable was called (it means that it becomes affected by the working directory).\nI recommend using sys.argv[0] for question 2 because sys.argv is immutable and therefore always returns the current file (module object path) and not affected by os.chdir(). Also you can do like this:sys.argv[0]sys.argvos.chdir()import os\nthis_py_file = os.path.realpath(__file__)\n\n# vvv Below comes your code vvv #\nimport os\nthis_py_file = os.path.realpath(__file__)\n\n# vvv Below comes your code vvv #\nBut that snippet and sys.argv[0] will not work or will work weird when compiled by PyInstaller, because magic properties are not set in __main__ level and sys.argv[0] is the way your executable was called (it means that it becomes affected by the working directory).sys.argv[0]__main__sys.argv[0]"
            ]
        },
        {
            "tag": "Pandas",
            "patterns": [
                "Renaming column names in Pandas",
                "Renaming column names in Pandas"
            ],
            "responses": [
                "Rename Specific ColumnsUse the df.rename() function and refer the columns to be renamed. Not all the columns have to be renamed:df.rename()df.rename()df = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})\n# Or rename the existing DataFrame (rather than creating a copy) \ndf.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\ndf = df.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'})\n# Or rename the existing DataFrame (rather than creating a copy) \ndf.rename(columns={'oldName1': 'newName1', 'oldName2': 'newName2'}, inplace=True)\nMinimal Code ExampleMinimal Code Exampledf = pd.DataFrame('x', index=range(3), columns=list('abcde'))\ndf\n\n   a  b  c  d  e\n0  x  x  x  x  x\n1  x  x  x  x  x\n2  x  x  x  x  x\ndf = pd.DataFrame('x', index=range(3), columns=list('abcde'))\ndf\n\n   a  b  c  d  e\n0  x  x  x  x  x\n1  x  x  x  x  x\n2  x  x  x  x  x\nThe following methods all work and produce the same output:df2 = df.rename({'a': 'X', 'b': 'Y'}, axis=1)  # new method\ndf2 = df.rename({'a': 'X', 'b': 'Y'}, axis='columns')\ndf2 = df.rename(columns={'a': 'X', 'b': 'Y'})  # old method  \n\ndf2\n\n   X  Y  c  d  e\n0  x  x  x  x  x\n1  x  x  x  x  x\n2  x  x  x  x  x\ndf2 = df.rename({'a': 'X', 'b': 'Y'}, axis=1)  # new method\ndf2 = df.rename({'a': 'X', 'b': 'Y'}, axis='columns')\ndf2 = df.rename(columns={'a': 'X', 'b': 'Y'})  # old method  \n\ndf2\n\n   X  Y  c  d  e\n0  x  x  x  x  x\n1  x  x  x  x  x\n2  x  x  x  x  x\nRemember to assign the result back, as the modification is not-inplace. Alternatively, specify inplace=True:inplace=Truedf.rename({'a': 'X', 'b': 'Y'}, axis=1, inplace=True)\ndf\n\n   X  Y  c  d  e\n0  x  x  x  x  x\n1  x  x  x  x  x\n2  x  x  x  x  x\n \ndf.rename({'a': 'X', 'b': 'Y'}, axis=1, inplace=True)\ndf\n\n   X  Y  c  d  e\n0  x  x  x  x  x\n1  x  x  x  x  x\n2  x  x  x  x  x\n \nFrom v0.25, you can also specify errors='raise' to raise errors if an invalid column-to-rename is specified. See v0.25 rename() docs.errors='raise'v0.25 rename() docsrename()Reassign Column HeadersUse df.set_axis() with axis=1 and inplace=False (to return a copy).df.set_axis()df.set_axis()axis=1inplace=Falsedf2 = df.set_axis(['V', 'W', 'X', 'Y', 'Z'], axis=1, inplace=False)\ndf2\n\n   V  W  X  Y  Z\n0  x  x  x  x  x\n1  x  x  x  x  x\n2  x  x  x  x  x\ndf2 = df.set_axis(['V', 'W', 'X', 'Y', 'Z'], axis=1, inplace=False)\ndf2\n\n   V  W  X  Y  Z\n0  x  x  x  x  x\n1  x  x  x  x  x\n2  x  x  x  x  x\nThis returns a copy, but you can modify the DataFrame in-place by setting inplace=True (this is the default behaviour for versions <=0.24 but is likely to change in the future).inplace=TrueYou can also assign headers directly:df.columns = ['V', 'W', 'X', 'Y', 'Z']\ndf\n\n   V  W  X  Y  Z\n0  x  x  x  x  x\n1  x  x  x  x  x\n2  x  x  x  x  x\ndf.columns = ['V', 'W', 'X', 'Y', 'Z']\ndf\n\n   V  W  X  Y  Z\n0  x  x  x  x  x\n1  x  x  x  x  x\n2  x  x  x  x  x\n",
                "Just assign it to the .columns attribute:.columns>>> df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\n>>> df\n   $a  $b\n0   1  10\n1   2  20\n\n>>> df.columns = ['a', 'b']\n>>> df\n   a   b\n0  1  10\n1  2  20\n>>> df = pd.DataFrame({'$a':[1,2], '$b': [10,20]})\n>>> df\n   $a  $b\n0   1  10\n1   2  20\n\n>>> df.columns = ['a', 'b']\n>>> df\n   a   b\n0  1  10\n1  2  20\n",
                "The rename method can take a function, for example:renamerenamefunctionfunctionIn [11]: df.columns\nOut[11]: Index([u'$a', u'$b', u'$c', u'$d', u'$e'], dtype=object)\n\nIn [12]: df.rename(columns=lambda x: x[1:], inplace=True)\n\nIn [13]: df.columns\nOut[13]: Index([u'a', u'b', u'c', u'd', u'e'], dtype=object)\nIn [11]: df.columns\nOut[11]: Index([u'$a', u'$b', u'$c', u'$d', u'$e'], dtype=object)\n\nIn [12]: df.rename(columns=lambda x: x[1:], inplace=True)\n\nIn [13]: df.columns\nOut[13]: Index([u'a', u'b', u'c', u'd', u'e'], dtype=object)\n",
                "As documented in Working with text data:Working with text dataWorking with text datadf.columns = df.columns.str.replace('$', '')\ndf.columns = df.columns.str.replace('$', '')\n",
                "Pandas 0.21+ AnswerThere have been some significant updates to column renaming in version 0.21. \nThe rename method has added the axis parameter which may be set to columns or 1. This update makes this method match the rest of the pandas API. It still has the index and columns parameters but you are no longer forced to use them. \nThe set_axis method with the inplace set to False enables you to rename all the index or column labels with a list.\nThe rename method has added the axis parameter which may be set to columns or 1. This update makes this method match the rest of the pandas API. It still has the index and columns parameters but you are no longer forced to use them. rename methodrenameaxiscolumns1indexcolumnsThe set_axis method with the inplace set to False enables you to rename all the index or column labels with a list.set_axis methodset_axisinplaceFalseExamples for Pandas 0.21+Construct sample DataFrame:df = pd.DataFrame({'$a':[1,2], '$b': [3,4], \n                   '$c':[5,6], '$d':[7,8], \n                   '$e':[9,10]})\n\n   $a  $b  $c  $d  $e\n0   1   3   5   7   9\n1   2   4   6   8  10\ndf = pd.DataFrame({'$a':[1,2], '$b': [3,4], \n                   '$c':[5,6], '$d':[7,8], \n                   '$e':[9,10]})\n\n   $a  $b  $c  $d  $e\n0   1   3   5   7   9\n1   2   4   6   8  10\nUsing rename with axis='columns' or axis=1renameaxis='columns'axis=1df.rename({'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'}, axis='columns')\ndf.rename({'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'}, axis='columns')\nor df.rename({'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'}, axis=1)\ndf.rename({'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'}, axis=1)\nBoth result in the following:   a  b  c  d   e\n0  1  3  5  7   9\n1  2  4  6  8  10\n   a  b  c  d   e\n0  1  3  5  7   9\n1  2  4  6  8  10\nIt is still possible to use the old method signature:df.rename(columns={'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'})\ndf.rename(columns={'$a':'a', '$b':'b', '$c':'c', '$d':'d', '$e':'e'})\nThe rename function also accepts functions that will be applied to each column name.renamedf.rename(lambda x: x[1:], axis='columns')\ndf.rename(lambda x: x[1:], axis='columns')\nordf.rename(lambda x: x[1:], axis=1)\ndf.rename(lambda x: x[1:], axis=1)\nUsing set_axis with a list and inplace=Falseset_axisinplace=FalseYou can supply a list to the set_axis method that is equal in length to the number of columns (or index). Currently, inplace defaults to True, but inplace will be defaulted to False in future releases.set_axisinplaceTrueinplaceFalsedf.set_axis(['a', 'b', 'c', 'd', 'e'], axis='columns', inplace=False)\ndf.set_axis(['a', 'b', 'c', 'd', 'e'], axis='columns', inplace=False)\nordf.set_axis(['a', 'b', 'c', 'd', 'e'], axis=1, inplace=False)\ndf.set_axis(['a', 'b', 'c', 'd', 'e'], axis=1, inplace=False)\nWhy not use df.columns = ['a', 'b', 'c', 'd', 'e']?df.columns = ['a', 'b', 'c', 'd', 'e']There is nothing wrong with assigning columns directly like this. It is a perfectly good solution. The advantage of using set_axis is that it can be used as part of a method chain and that it returns a new copy of the DataFrame. Without it, you would have to store your intermediate steps of the chain to another variable before reassigning the columns.set_axis# new for pandas 0.21+\ndf.some_method1()\n  .some_method2()\n  .set_axis()\n  .some_method3()\n\n# old way\ndf1 = df.some_method1()\n        .some_method2()\ndf1.columns = columns\ndf1.some_method3()\n# new for pandas 0.21+\ndf.some_method1()\n  .some_method2()\n  .set_axis()\n  .some_method3()\n\n# old way\ndf1 = df.some_method1()\n        .some_method2()\ndf1.columns = columns\ndf1.some_method3()\n",
                "Since you only want to remove the $ sign in all column names, you could just do:df = df.rename(columns=lambda x: x.replace('$', ''))\ndf = df.rename(columns=lambda x: x.replace('$', ''))\nORdf.rename(columns=lambda x: x.replace('$', ''), inplace=True)\ndf.rename(columns=lambda x: x.replace('$', ''), inplace=True)\n",
                "Renaming columns in Pandas is an easy task.df.rename(columns={'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}, inplace=True)\ndf.rename(columns={'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}, inplace=True)\n",
                "df.columns = ['a', 'b', 'c', 'd', 'e']\ndf.columns = ['a', 'b', 'c', 'd', 'e']\nIt will replace the existing names with the names you provide, in the order you provide.",
                "Use:old_names = ['$a', '$b', '$c', '$d', '$e'] \nnew_names = ['a', 'b', 'c', 'd', 'e']\ndf.rename(columns=dict(zip(old_names, new_names)), inplace=True)\nold_names = ['$a', '$b', '$c', '$d', '$e'] \nnew_names = ['a', 'b', 'c', 'd', 'e']\ndf.rename(columns=dict(zip(old_names, new_names)), inplace=True)\nThis way you can manually edit the new_names as you wish. It works great when you need to rename only a few columns to correct misspellings, accents, remove special characters, etc.new_names",
                "One line or Pipeline solutionsI'll focus on two things:\nOP clearly states\n\nI have the edited column names stored it in a list, but I don't know how to replace the column names.\n\nI do not want to solve the problem of how to replace '$' or strip the first character off of each column header.  OP has already done this step.  Instead I want to focus on replacing the existing columns object with a new one given a list of replacement column names.\n\ndf.columns = new where new is the list of new columns names is as simple as it gets.  The drawback of this approach is that it requires editing the existing dataframe's columns attribute and it isn't done inline.  I'll show a few ways to perform this via pipelining without editing the existing dataframe.\n\nOP clearly states\n\nI have the edited column names stored it in a list, but I don't know how to replace the column names.\n\nI do not want to solve the problem of how to replace '$' or strip the first character off of each column header.  OP has already done this step.  Instead I want to focus on replacing the existing columns object with a new one given a list of replacement column names.\nOP clearly states\nI have the edited column names stored it in a list, but I don't know how to replace the column names.\nI have the edited column names stored it in a list, but I don't know how to replace the column names.I do not want to solve the problem of how to replace '$' or strip the first character off of each column header.  OP has already done this step.  Instead I want to focus on replacing the existing columns object with a new one given a list of replacement column names.'$'columnsdf.columns = new where new is the list of new columns names is as simple as it gets.  The drawback of this approach is that it requires editing the existing dataframe's columns attribute and it isn't done inline.  I'll show a few ways to perform this via pipelining without editing the existing dataframe.\ndf.columns = new where new is the list of new columns names is as simple as it gets.  The drawback of this approach is that it requires editing the existing dataframe's columns attribute and it isn't done inline.  I'll show a few ways to perform this via pipelining without editing the existing dataframe.df.columns = newnewcolumns\nSetup 1\nTo focus on the need to rename of replace column names with a pre-existing list, I'll create a new sample dataframe df with initial column names and unrelated new column names.\ndf = pd.DataFrame({'Jack': [1, 2], 'Mahesh': [3, 4], 'Xin': [5, 6]})\nnew = ['x098', 'y765', 'z432']\n\ndf\n\n   Jack  Mahesh  Xin\n0     1       3    5\n1     2       4    6\n\n\nSolution 1\npd.DataFrame.rename\nIt has been said already that if you had a dictionary mapping the old column names to new column names, you could use pd.DataFrame.rename.\nd = {'Jack': 'x098', 'Mahesh': 'y765', 'Xin': 'z432'}\ndf.rename(columns=d)\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\nHowever, you can easily create that dictionary and include it in the call to rename.  The following takes advantage of the fact that when iterating over df, we iterate over each column name.\n# Given just a list of new column names\ndf.rename(columns=dict(zip(df, new)))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\nThis works great if your original column names are unique.  But if they are not, then this breaks down.\n\nSetup 2\nNon-unique columns\ndf = pd.DataFrame(\n    [[1, 3, 5], [2, 4, 6]],\n    columns=['Mahesh', 'Mahesh', 'Xin']\n)\nnew = ['x098', 'y765', 'z432']\n\ndf\n\n   Mahesh  Mahesh  Xin\n0       1       3    5\n1       2       4    6\n\n\nSolution 2\npd.concat using the keys argument\nFirst, notice what happens when we attempt to use solution 1:\ndf.rename(columns=dict(zip(df, new)))\n\n   y765  y765  z432\n0     1     3     5\n1     2     4     6\n\nWe didn't map the new list as the column names.  We ended up repeating y765.  Instead, we can use the keys argument of the pd.concat function while iterating through the columns of df.\npd.concat([c for _, c in df.items()], axis=1, keys=new) \n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\n\nSolution 3\nReconstruct.  This should only be used if you have a single dtype for all columns.  Otherwise, you'll end up with dtype object for all columns and converting them back requires more dictionary work.\nSingle dtype\npd.DataFrame(df.values, df.index, new)\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\nMixed dtype\npd.DataFrame(df.values, df.index, new).astype(dict(zip(new, df.dtypes)))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\n\nSolution 4\nThis is a gimmicky trick with transpose and set_index.  pd.DataFrame.set_index allows us to set an index inline, but there is no corresponding set_columns.  So we can transpose, then set_index, and transpose back.  However, the same single dtype versus mixed dtype caveat from solution 3 applies here.\nSingle dtype\ndf.T.set_index(np.asarray(new)).T\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\nMixed dtype\ndf.T.set_index(np.asarray(new)).T.astype(dict(zip(new, df.dtypes)))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\n\nSolution 5\nUse a lambda in pd.DataFrame.rename that cycles through each element of new.\nIn this solution, we pass a lambda that takes x but then ignores it.  It also takes a y but doesn't expect it.  Instead, an iterator is given as a default value and I can then use that to cycle through one at a time without regard to what the value of x is.\ndf.rename(columns=lambda x, y=iter(new): next(y))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\nAnd as pointed out to me by the folks in sopython chat, if I add a * in between x and y, I can protect my y variable.  Though, in this context I don't believe it needs protecting.  It is still worth mentioning.\ndf.rename(columns=lambda x, *, y=iter(new): next(y))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\nSetup 1\nTo focus on the need to rename of replace column names with a pre-existing list, I'll create a new sample dataframe df with initial column names and unrelated new column names.Setup 1dfdf = pd.DataFrame({'Jack': [1, 2], 'Mahesh': [3, 4], 'Xin': [5, 6]})\nnew = ['x098', 'y765', 'z432']\n\ndf\n\n   Jack  Mahesh  Xin\n0     1       3    5\n1     2       4    6\ndf = pd.DataFrame({'Jack': [1, 2], 'Mahesh': [3, 4], 'Xin': [5, 6]})\nnew = ['x098', 'y765', 'z432']\n\ndf\n\n   Jack  Mahesh  Xin\n0     1       3    5\n1     2       4    6\n\nSolution 1\npd.DataFrame.rename\nIt has been said already that if you had a dictionary mapping the old column names to new column names, you could use pd.DataFrame.rename.\nd = {'Jack': 'x098', 'Mahesh': 'y765', 'Xin': 'z432'}\ndf.rename(columns=d)\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\nHowever, you can easily create that dictionary and include it in the call to rename.  The following takes advantage of the fact that when iterating over df, we iterate over each column name.\n# Given just a list of new column names\ndf.rename(columns=dict(zip(df, new)))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\nThis works great if your original column names are unique.  But if they are not, then this breaks down.\n\nSetup 2\nNon-unique columns\ndf = pd.DataFrame(\n    [[1, 3, 5], [2, 4, 6]],\n    columns=['Mahesh', 'Mahesh', 'Xin']\n)\nnew = ['x098', 'y765', 'z432']\n\ndf\n\n   Mahesh  Mahesh  Xin\n0       1       3    5\n1       2       4    6\n\n\nSolution 2\npd.concat using the keys argument\nFirst, notice what happens when we attempt to use solution 1:\ndf.rename(columns=dict(zip(df, new)))\n\n   y765  y765  z432\n0     1     3     5\n1     2     4     6\n\nWe didn't map the new list as the column names.  We ended up repeating y765.  Instead, we can use the keys argument of the pd.concat function while iterating through the columns of df.\npd.concat([c for _, c in df.items()], axis=1, keys=new) \n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\n\nSolution 3\nReconstruct.  This should only be used if you have a single dtype for all columns.  Otherwise, you'll end up with dtype object for all columns and converting them back requires more dictionary work.\nSingle dtype\npd.DataFrame(df.values, df.index, new)\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\nMixed dtype\npd.DataFrame(df.values, df.index, new).astype(dict(zip(new, df.dtypes)))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\n\nSolution 4\nThis is a gimmicky trick with transpose and set_index.  pd.DataFrame.set_index allows us to set an index inline, but there is no corresponding set_columns.  So we can transpose, then set_index, and transpose back.  However, the same single dtype versus mixed dtype caveat from solution 3 applies here.\nSingle dtype\ndf.T.set_index(np.asarray(new)).T\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\nMixed dtype\ndf.T.set_index(np.asarray(new)).T.astype(dict(zip(new, df.dtypes)))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\n\nSolution 5\nUse a lambda in pd.DataFrame.rename that cycles through each element of new.\nIn this solution, we pass a lambda that takes x but then ignores it.  It also takes a y but doesn't expect it.  Instead, an iterator is given as a default value and I can then use that to cycle through one at a time without regard to what the value of x is.\ndf.rename(columns=lambda x, y=iter(new): next(y))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\nAnd as pointed out to me by the folks in sopython chat, if I add a * in between x and y, I can protect my y variable.  Though, in this context I don't believe it needs protecting.  It is still worth mentioning.\ndf.rename(columns=lambda x, *, y=iter(new): next(y))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n\nSolution 1\npd.DataFrame.renameSolution 1pd.DataFrame.renamepd.DataFrame.renamepd.DataFrame.renameIt has been said already that if you had a dictionary mapping the old column names to new column names, you could use pd.DataFrame.rename.ifpd.DataFrame.renamed = {'Jack': 'x098', 'Mahesh': 'y765', 'Xin': 'z432'}\ndf.rename(columns=d)\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\nd = {'Jack': 'x098', 'Mahesh': 'y765', 'Xin': 'z432'}\ndf.rename(columns=d)\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\nHowever, you can easily create that dictionary and include it in the call to rename.  The following takes advantage of the fact that when iterating over df, we iterate over each column name.renamedf# Given just a list of new column names\ndf.rename(columns=dict(zip(df, new)))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n# Given just a list of new column names\ndf.rename(columns=dict(zip(df, new)))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\nThis works great if your original column names are unique.  But if they are not, then this breaks down.Setup 2\nNon-unique columnsSetup 2df = pd.DataFrame(\n    [[1, 3, 5], [2, 4, 6]],\n    columns=['Mahesh', 'Mahesh', 'Xin']\n)\nnew = ['x098', 'y765', 'z432']\n\ndf\n\n   Mahesh  Mahesh  Xin\n0       1       3    5\n1       2       4    6\ndf = pd.DataFrame(\n    [[1, 3, 5], [2, 4, 6]],\n    columns=['Mahesh', 'Mahesh', 'Xin']\n)\nnew = ['x098', 'y765', 'z432']\n\ndf\n\n   Mahesh  Mahesh  Xin\n0       1       3    5\n1       2       4    6\nSolution 2\npd.concat using the keys argumentSolution 2pd.concatpd.concatpd.concatkeysFirst, notice what happens when we attempt to use solution 1:df.rename(columns=dict(zip(df, new)))\n\n   y765  y765  z432\n0     1     3     5\n1     2     4     6\ndf.rename(columns=dict(zip(df, new)))\n\n   y765  y765  z432\n0     1     3     5\n1     2     4     6\nWe didn't map the new list as the column names.  We ended up repeating y765.  Instead, we can use the keys argument of the pd.concat function while iterating through the columns of df.newy765keyspd.concatdfpd.concat([c for _, c in df.items()], axis=1, keys=new) \n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\npd.concat([c for _, c in df.items()], axis=1, keys=new) \n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\nSolution 3\nReconstruct.  This should only be used if you have a single dtype for all columns.  Otherwise, you'll end up with dtype object for all columns and converting them back requires more dictionary work.Solution 3dtypedtypeobjectSingle dtypeSingle dtypedtypepd.DataFrame(df.values, df.index, new)\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\npd.DataFrame(df.values, df.index, new)\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\nMixed dtypeMixed dtypedtypepd.DataFrame(df.values, df.index, new).astype(dict(zip(new, df.dtypes)))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\npd.DataFrame(df.values, df.index, new).astype(dict(zip(new, df.dtypes)))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\nSolution 4\nThis is a gimmicky trick with transpose and set_index.  pd.DataFrame.set_index allows us to set an index inline, but there is no corresponding set_columns.  So we can transpose, then set_index, and transpose back.  However, the same single dtype versus mixed dtype caveat from solution 3 applies here.Solution 4transposeset_indexpd.DataFrame.set_indexpd.DataFrame.set_indexpd.DataFrame.set_indexset_columnsset_indexdtypedtypeSingle dtypeSingle dtypedtypedf.T.set_index(np.asarray(new)).T\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\ndf.T.set_index(np.asarray(new)).T\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\nMixed dtypeMixed dtypedtypedf.T.set_index(np.asarray(new)).T.astype(dict(zip(new, df.dtypes)))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\ndf.T.set_index(np.asarray(new)).T.astype(dict(zip(new, df.dtypes)))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\nSolution 5\nUse a lambda in pd.DataFrame.rename that cycles through each element of new.\nIn this solution, we pass a lambda that takes x but then ignores it.  It also takes a y but doesn't expect it.  Instead, an iterator is given as a default value and I can then use that to cycle through one at a time without regard to what the value of x is.Solution 5lambdapd.DataFrame.renamenewxyxdf.rename(columns=lambda x, y=iter(new): next(y))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\ndf.rename(columns=lambda x, y=iter(new): next(y))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\nAnd as pointed out to me by the folks in sopython chat, if I add a * in between x and y, I can protect my y variable.  Though, in this context I don't believe it needs protecting.  It is still worth mentioning.sopython chatsopython*xyydf.rename(columns=lambda x, *, y=iter(new): next(y))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\ndf.rename(columns=lambda x, *, y=iter(new): next(y))\n\n   x098  y765  z432\n0     1     3     5\n1     2     4     6\n",
                "Column names vs Names of SeriesI would like to explain a bit what happens behind the scenes.Dataframes are a set of Series.Series in turn are an extension of a numpy.array.numpy.arraynumpy.arrays have a property .name.numpy.array.nameThis is the name of the series. It is seldom that Pandas respects this attribute, but it lingers in places and can be used to hack some Pandas behaviors.Naming the list of columnsA lot of answers here talks about the df.columns attribute being a list when in fact it is a Series. This means it has a .name attribute.df.columnslistSeries.nameThis is what happens if you decide to fill in the name of the columns Series:Seriesdf.columns = ['column_one', 'column_two']\ndf.columns.names = ['name of the list of columns']\ndf.index.names = ['name of the index']\n\nname of the list of columns     column_one  column_two\nname of the index\n0                                    4           1\n1                                    5           2\n2                                    6           3\ndf.columns = ['column_one', 'column_two']\ndf.columns.names = ['name of the list of columns']\ndf.index.names = ['name of the index']\n\nname of the list of columns     column_one  column_two\nname of the index\n0                                    4           1\n1                                    5           2\n2                                    6           3\nNote that the name of the index always comes one column lower.Artefacts that lingerThe .name attribute lingers on sometimes. If you set df.columns = ['one', 'two'] then the df.one.name will be 'one'..namedf.columns = ['one', 'two']df.one.name'one'If you set df.one.name = 'three' then df.columns will still give you ['one', 'two'], and df.one.name will give you 'three'.df.one.name = 'three'df.columns['one', 'two']df.one.name'three'BUTpd.DataFrame(df.one) will returnpd.DataFrame(df.one)    three\n0       1\n1       2\n2       3\n    three\n0       1\n1       2\n2       3\nBecause Pandas reuses the .name of the already defined Series..nameSeriesMulti-level column namesPandas has ways of doing multi-layered column names. There is not so much magic involved, but I wanted to cover this in my answer too since I don't see anyone picking up on this here.    |one            |\n    |one      |two  |\n0   |  4      |  1  |\n1   |  5      |  2  |\n2   |  6      |  3  |\n    |one            |\n    |one      |two  |\n0   |  4      |  1  |\n1   |  5      |  2  |\n2   |  6      |  3  |\nThis is easily achievable by setting columns to lists, like this:df.columns = [['one', 'one'], ['one', 'two']]\ndf.columns = [['one', 'one'], ['one', 'two']]\n",
                "Let's understand renaming by a small example...understand\nRenaming columns using mapping:\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]}) # Creating a df with column name A and B\n df.rename({\"A\": \"new_a\", \"B\": \"new_b\"}, axis='columns', inplace =True) # Renaming column A with 'new_a' and B with 'new_b'\n\n Output:\n\n    new_a  new_b\n 0  1       4\n 1  2       5\n 2  3       6\n\n\nRenaming index/Row_Name using mapping:\n df.rename({0: \"x\", 1: \"y\", 2: \"z\"}, axis='index', inplace =True) # Row name are getting replaced by 'x', 'y', and 'z'.\n\n Output:\n\n        new_a  new_b\n     x  1       4\n     y  2       5\n     z  3       6\n\n\nRenaming columns using mapping:\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]}) # Creating a df with column name A and B\n df.rename({\"A\": \"new_a\", \"B\": \"new_b\"}, axis='columns', inplace =True) # Renaming column A with 'new_a' and B with 'new_b'\n\n Output:\n\n    new_a  new_b\n 0  1       4\n 1  2       5\n 2  3       6\n\nRenaming columns using mapping: df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]}) # Creating a df with column name A and B\n df.rename({\"A\": \"new_a\", \"B\": \"new_b\"}, axis='columns', inplace =True) # Renaming column A with 'new_a' and B with 'new_b'\n\n Output:\n\n    new_a  new_b\n 0  1       4\n 1  2       5\n 2  3       6\n df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]}) # Creating a df with column name A and B\n df.rename({\"A\": \"new_a\", \"B\": \"new_b\"}, axis='columns', inplace =True) # Renaming column A with 'new_a' and B with 'new_b'\n\n Output:\n\n    new_a  new_b\n 0  1       4\n 1  2       5\n 2  3       6\nRenaming index/Row_Name using mapping:\n df.rename({0: \"x\", 1: \"y\", 2: \"z\"}, axis='index', inplace =True) # Row name are getting replaced by 'x', 'y', and 'z'.\n\n Output:\n\n        new_a  new_b\n     x  1       4\n     y  2       5\n     z  3       6\n\nRenaming index/Row_Name using mapping: df.rename({0: \"x\", 1: \"y\", 2: \"z\"}, axis='index', inplace =True) # Row name are getting replaced by 'x', 'y', and 'z'.\n\n Output:\n\n        new_a  new_b\n     x  1       4\n     y  2       5\n     z  3       6\n df.rename({0: \"x\", 1: \"y\", 2: \"z\"}, axis='index', inplace =True) # Row name are getting replaced by 'x', 'y', and 'z'.\n\n Output:\n\n        new_a  new_b\n     x  1       4\n     y  2       5\n     z  3       6\n",
                "Many of pandas functions have an inplace parameter. When setting it True, the transformation applies directly to the dataframe that you are calling it on. For example:df = pd.DataFrame({'$a':[1,2], '$b': [3,4]})\ndf.rename(columns={'$a': 'a'}, inplace=True)\ndf.columns\n\n>>> Index(['a', '$b'], dtype='object')\ndf = pd.DataFrame({'$a':[1,2], '$b': [3,4]})\ndf.rename(columns={'$a': 'a'}, inplace=True)\ndf.columns\n\n>>> Index(['a', '$b'], dtype='object')\nAlternatively, there are cases where you want to preserve the original dataframe. I have often seen people fall into this case if creating the dataframe is an expensive task. For example, if creating the dataframe required querying a snowflake database. In this case, just make sure the the inplace parameter is set to False.df = pd.DataFrame({'$a':[1,2], '$b': [3,4]})\ndf2 = df.rename(columns={'$a': 'a'}, inplace=False)\ndf.columns\n\n>>> Index(['$a', '$b'], dtype='object')\n\ndf2.columns\n\n>>> Index(['a', '$b'], dtype='object')\ndf = pd.DataFrame({'$a':[1,2], '$b': [3,4]})\ndf2 = df.rename(columns={'$a': 'a'}, inplace=False)\ndf.columns\n\n>>> Index(['$a', '$b'], dtype='object')\n\ndf2.columns\n\n>>> Index(['a', '$b'], dtype='object')\nIf these types of transformations are something that you do often, you could also look into a number of different pandas GUI tools. I'm the creator of one called Mito. It\u2019s a spreadsheet that automatically converts your edits to python code.Mito",
                "Suppose your dataset name is df, and df has.df = ['$a', '$b', '$c', '$d', '$e']`\ndf = ['$a', '$b', '$c', '$d', '$e']`\nSo, to rename these, we would simply do.df.columns = ['a','b','c','d','e']\ndf.columns = ['a','b','c','d','e']\n",
                "Let's say this is your dataframe.You can rename the columns using two methods.\nUsing dataframe.columns=[#list]\ndf.columns=['a','b','c','d','e']\n\n\nThe limitation of this method is that if one column has to be changed, full column list has to be passed. Also, this method is not applicable on index labels.\nFor example, if you passed this:\ndf.columns = ['a','b','c','d']\n\nThis will throw an error. Length mismatch: Expected axis has 5 elements, new values have 4 elements.\nAnother method is the Pandas rename() method which is used to rename any index, column or row\ndf = df.rename(columns={'$a':'a'})\n\n\nUsing dataframe.columns=[#list]\ndf.columns=['a','b','c','d','e']\n\n\nThe limitation of this method is that if one column has to be changed, full column list has to be passed. Also, this method is not applicable on index labels.\nFor example, if you passed this:\ndf.columns = ['a','b','c','d']\n\nThis will throw an error. Length mismatch: Expected axis has 5 elements, new values have 4 elements.Using dataframe.columns=[#list]dataframe.columns=[#list]df.columns=['a','b','c','d','e']\ndf.columns=['a','b','c','d','e']\nThe limitation of this method is that if one column has to be changed, full column list has to be passed. Also, this method is not applicable on index labels.\nFor example, if you passed this:df.columns = ['a','b','c','d']\ndf.columns = ['a','b','c','d']\nThis will throw an error. Length mismatch: Expected axis has 5 elements, new values have 4 elements.Another method is the Pandas rename() method which is used to rename any index, column or row\ndf = df.rename(columns={'$a':'a'})\n\nAnother method is the Pandas rename() method which is used to rename any index, column or rowrename()df = df.rename(columns={'$a':'a'})\ndf = df.rename(columns={'$a':'a'})\nSimilarly, you can change any rows or columns.",
                "If you've got the dataframe, df.columns dumps everything into a list you can manipulate and then reassign into your dataframe as the names of columns...columns = df.columns\ncolumns = [row.replace(\"$\", \"\") for row in columns]\ndf.rename(columns=dict(zip(columns, things)), inplace=True)\ndf.head() # To validate the output\ncolumns = df.columns\ncolumns = [row.replace(\"$\", \"\") for row in columns]\ndf.rename(columns=dict(zip(columns, things)), inplace=True)\ndf.head() # To validate the output\nBest way? I don't know. A way - yes.A better way of evaluating all the main techniques put forward in the answers to the question is below using cProfile to gage memory and execution time. @kadee, @kaitlyn, and @eumiro had the functions with the fastest execution times - though these functions are so fast we're comparing the rounding of 0.000 and 0.001 seconds for all the answers. Moral: my answer above likely isn't the 'best' way.import pandas as pd\nimport cProfile, pstats, re\n\nold_names = ['$a', '$b', '$c', '$d', '$e']\nnew_names = ['a', 'b', 'c', 'd', 'e']\ncol_dict = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}\n\ndf = pd.DataFrame({'$a':[1, 2], '$b': [10, 20], '$c': ['bleep', 'blorp'], '$d': [1, 2], '$e': ['texa$', '']})\n\ndf.head()\n\ndef eumiro(df, nn):\n    df.columns = nn\n    # This direct renaming approach is duplicated in methodology in several other answers:\n    return df\n\ndef lexual1(df):\n    return df.rename(columns=col_dict)\n\ndef lexual2(df, col_dict):\n    return df.rename(columns=col_dict, inplace=True)\n\ndef Panda_Master_Hayden(df):\n    return df.rename(columns=lambda x: x[1:], inplace=True)\n\ndef paulo1(df):\n    return df.rename(columns=lambda x: x.replace('$', ''))\n\ndef paulo2(df):\n    return df.rename(columns=lambda x: x.replace('$', ''), inplace=True)\n\ndef migloo(df, on, nn):\n    return df.rename(columns=dict(zip(on, nn)), inplace=True)\n\ndef kadee(df):\n    return df.columns.str.replace('$', '')\n\ndef awo(df):\n    columns = df.columns\n    columns = [row.replace(\"$\", \"\") for row in columns]\n    return df.rename(columns=dict(zip(columns, '')), inplace=True)\n\ndef kaitlyn(df):\n    df.columns = [col.strip('$') for col in df.columns]\n    return df\n\nprint 'eumiro'\ncProfile.run('eumiro(df, new_names)')\nprint 'lexual1'\ncProfile.run('lexual1(df)')\nprint 'lexual2'\ncProfile.run('lexual2(df, col_dict)')\nprint 'andy hayden'\ncProfile.run('Panda_Master_Hayden(df)')\nprint 'paulo1'\ncProfile.run('paulo1(df)')\nprint 'paulo2'\ncProfile.run('paulo2(df)')\nprint 'migloo'\ncProfile.run('migloo(df, old_names, new_names)')\nprint 'kadee'\ncProfile.run('kadee(df)')\nprint 'awo'\ncProfile.run('awo(df)')\nprint 'kaitlyn'\ncProfile.run('kaitlyn(df)')\nimport pandas as pd\nimport cProfile, pstats, re\n\nold_names = ['$a', '$b', '$c', '$d', '$e']\nnew_names = ['a', 'b', 'c', 'd', 'e']\ncol_dict = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}\n\ndf = pd.DataFrame({'$a':[1, 2], '$b': [10, 20], '$c': ['bleep', 'blorp'], '$d': [1, 2], '$e': ['texa$', '']})\n\ndf.head()\n\ndef eumiro(df, nn):\n    df.columns = nn\n    # This direct renaming approach is duplicated in methodology in several other answers:\n    return df\n\ndef lexual1(df):\n    return df.rename(columns=col_dict)\n\ndef lexual2(df, col_dict):\n    return df.rename(columns=col_dict, inplace=True)\n\ndef Panda_Master_Hayden(df):\n    return df.rename(columns=lambda x: x[1:], inplace=True)\n\ndef paulo1(df):\n    return df.rename(columns=lambda x: x.replace('$', ''))\n\ndef paulo2(df):\n    return df.rename(columns=lambda x: x.replace('$', ''), inplace=True)\n\ndef migloo(df, on, nn):\n    return df.rename(columns=dict(zip(on, nn)), inplace=True)\n\ndef kadee(df):\n    return df.columns.str.replace('$', '')\n\ndef awo(df):\n    columns = df.columns\n    columns = [row.replace(\"$\", \"\") for row in columns]\n    return df.rename(columns=dict(zip(columns, '')), inplace=True)\n\ndef kaitlyn(df):\n    df.columns = [col.strip('$') for col in df.columns]\n    return df\n\nprint 'eumiro'\ncProfile.run('eumiro(df, new_names)')\nprint 'lexual1'\ncProfile.run('lexual1(df)')\nprint 'lexual2'\ncProfile.run('lexual2(df, col_dict)')\nprint 'andy hayden'\ncProfile.run('Panda_Master_Hayden(df)')\nprint 'paulo1'\ncProfile.run('paulo1(df)')\nprint 'paulo2'\ncProfile.run('paulo2(df)')\nprint 'migloo'\ncProfile.run('migloo(df, old_names, new_names)')\nprint 'kadee'\ncProfile.run('kadee(df)')\nprint 'awo'\ncProfile.run('awo(df)')\nprint 'kaitlyn'\ncProfile.run('kaitlyn(df)')\n",
                "df = pd.DataFrame({'$a': [1], '$b': [1], '$c': [1], '$d': [1], '$e': [1]})\ndf = pd.DataFrame({'$a': [1], '$b': [1], '$c': [1], '$d': [1], '$e': [1]})\nIf your new list of columns is in the same order as the existing columns, the assignment is simple:new_cols = ['a', 'b', 'c', 'd', 'e']\ndf.columns = new_cols\n>>> df\n   a  b  c  d  e\n0  1  1  1  1  1\nnew_cols = ['a', 'b', 'c', 'd', 'e']\ndf.columns = new_cols\n>>> df\n   a  b  c  d  e\n0  1  1  1  1  1\nIf you had a dictionary keyed on old column names to new column names, you could do the following:d = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}\ndf.columns = df.columns.map(lambda col: d[col])  # Or `.map(d.get)` as pointed out by @PiRSquared.\n>>> df\n   a  b  c  d  e\n0  1  1  1  1  1\nd = {'$a': 'a', '$b': 'b', '$c': 'c', '$d': 'd', '$e': 'e'}\ndf.columns = df.columns.map(lambda col: d[col])  # Or `.map(d.get)` as pointed out by @PiRSquared.\n>>> df\n   a  b  c  d  e\n0  1  1  1  1  1\nIf you don't have a list or dictionary mapping, you could strip the leading $ symbol via a list comprehension:$df.columns = [col[1:] if col[0] == '$' else col for col in df]\ndf.columns = [col[1:] if col[0] == '$' else col for col in df]\n",
                "df.rename(index=str, columns={'A':'a', 'B':'b'})\ndf.rename(index=str, columns={'A':'a', 'B':'b'})\npandas.DataFrame.renamepandas.DataFrame.renamepandas.DataFrame.rename",
                "If you already have a list for the new column names, you can try this:new_cols = ['a', 'b', 'c', 'd', 'e']\nnew_names_map = {df.columns[i]:new_cols[i] for i in range(len(new_cols))}\n\ndf.rename(new_names_map, axis=1, inplace=True)\nnew_cols = ['a', 'b', 'c', 'd', 'e']\nnew_names_map = {df.columns[i]:new_cols[i] for i in range(len(new_cols))}\n\ndf.rename(new_names_map, axis=1, inplace=True)\n",
                "Another way we could replace the original column labels is by stripping the unwanted characters (here '$') from the original column labels.This could have been done by running a for loop over df.columns and appending the stripped columns to df.columns.forInstead, we can do this neatly in a single statement by using list comprehension like below:df.columns = [col.strip('$') for col in df.columns]\ndf.columns = [col.strip('$') for col in df.columns]\n(strip method in Python strips the given character from beginning and end of the string.)strip",
                "It is real simple. Just use:df.columns = ['Name1', 'Name2', 'Name3'...]\ndf.columns = ['Name1', 'Name2', 'Name3'...]\nAnd it will assign the column names by the order you put them in.",
                "# This way it will work\nimport pandas as pd\n\n# Define a dictionary \nrankings = {'test': ['a'],\n        'odi': ['E'],\n        't20': ['P']}\n\n# Convert the dictionary into DataFrame\nrankings_pd = pd.DataFrame(rankings)\n\n# Before renaming the columns\nprint(rankings_pd)\n\nrankings_pd.rename(columns = {'test':'TEST'}, inplace = True)\n# This way it will work\nimport pandas as pd\n\n# Define a dictionary \nrankings = {'test': ['a'],\n        'odi': ['E'],\n        't20': ['P']}\n\n# Convert the dictionary into DataFrame\nrankings_pd = pd.DataFrame(rankings)\n\n# Before renaming the columns\nprint(rankings_pd)\n\nrankings_pd.rename(columns = {'test':'TEST'}, inplace = True)\n",
                "You could use str.slice for that:str.slicestr.slicedf.columns = df.columns.str.slice(1)\ndf.columns = df.columns.str.slice(1)\n",
                "Another option is to rename using a regular expression:import pandas as pd\nimport re\n\ndf = pd.DataFrame({'$a':[1,2], '$b':[3,4], '$c':[5,6]})\n\ndf = df.rename(columns=lambda x: re.sub('\\$','',x))\n>>> df\n   a  b  c\n0  1  3  5\n1  2  4  6\nimport pandas as pd\nimport re\n\ndf = pd.DataFrame({'$a':[1,2], '$b':[3,4], '$c':[5,6]})\n\ndf = df.rename(columns=lambda x: re.sub('\\$','',x))\n>>> df\n   a  b  c\n0  1  3  5\n1  2  4  6\n",
                "My method is generic wherein you can add additional delimiters by comma separating delimiters= variable and future-proof it.delimiters=Working Code:Working Code:import pandas as pd\nimport re\n\n\ndf = pd.DataFrame({'$a':[1,2], '$b': [3,4],'$c':[5,6], '$d': [7,8], '$e': [9,10]})\n\ndelimiters = '$'\nmatchPattern = '|'.join(map(re.escape, delimiters))\ndf.columns = [re.split(matchPattern, i)[1] for i in df.columns ]\nimport pandas as pd\nimport re\n\n\ndf = pd.DataFrame({'$a':[1,2], '$b': [3,4],'$c':[5,6], '$d': [7,8], '$e': [9,10]})\n\ndelimiters = '$'\nmatchPattern = '|'.join(map(re.escape, delimiters))\ndf.columns = [re.split(matchPattern, i)[1] for i in df.columns ]\nOutput:Output:>>> df\n   $a  $b  $c  $d  $e\n0   1   3   5   7   9\n1   2   4   6   8  10\n\n>>> df\n   a  b  c  d   e\n0  1  3  5  7   9\n1  2  4  6  8  10\n>>> df\n   $a  $b  $c  $d  $e\n0   1   3   5   7   9\n1   2   4   6   8  10\n\n>>> df\n   a  b  c  d   e\n0  1  3  5  7   9\n1  2  4  6  8  10\n",
                "Note that the approaches in previous answers do not work for a MultiIndex. For a MultiIndex, you need to do something like the following:MultiIndexMultiIndex>>> df = pd.DataFrame({('$a','$x'):[1,2], ('$b','$y'): [3,4], ('e','f'):[5,6]})\n>>> df\n   $a $b  e\n   $x $y  f\n0  1  3  5\n1  2  4  6\n>>> rename = {('$a','$x'):('a','x'), ('$b','$y'):('b','y')}\n>>> df.columns = pandas.MultiIndex.from_tuples([\n        rename.get(item, item) for item in df.columns.tolist()])\n>>> df\n   a  b  e\n   x  y  f\n0  1  3  5\n1  2  4  6\n>>> df = pd.DataFrame({('$a','$x'):[1,2], ('$b','$y'): [3,4], ('e','f'):[5,6]})\n>>> df\n   $a $b  e\n   $x $y  f\n0  1  3  5\n1  2  4  6\n>>> rename = {('$a','$x'):('a','x'), ('$b','$y'):('b','y')}\n>>> df.columns = pandas.MultiIndex.from_tuples([\n        rename.get(item, item) for item in df.columns.tolist()])\n>>> df\n   a  b  e\n   x  y  f\n0  1  3  5\n1  2  4  6\n",
                "If you have to deal with loads of columns named by the providing system out of your control, I came up with the following approach that is a combination of a general approach and specific replacements in one go.First create a dictionary from the dataframe column names using regular expressions in order to throw away certain appendixes of column names and then add specific replacements to the dictionary to name core columns as expected later in the receiving database.This is then applied to the dataframe in one go.dict = dict(zip(df.columns, df.columns.str.replace('(:S$|:C1$|:L$|:D$|\\.Serial:L$)', '')))\ndict['brand_timeseries:C1'] = 'BTS'\ndict['respid:L'] = 'RespID'\ndict['country:C1'] = 'CountryID'\ndict['pim1:D'] = 'pim_actual'\ndf.rename(columns=dict, inplace=True)\ndict = dict(zip(df.columns, df.columns.str.replace('(:S$|:C1$|:L$|:D$|\\.Serial:L$)', '')))\ndict['brand_timeseries:C1'] = 'BTS'\ndict['respid:L'] = 'RespID'\ndict['country:C1'] = 'CountryID'\ndict['pim1:D'] = 'pim_actual'\ndf.rename(columns=dict, inplace=True)\n",
                "If you just want to remove the '$' sign then use the below codedf.columns = pd.Series(df.columns.str.replace(\"$\", \"\"))\ndf.columns = pd.Series(df.columns.str.replace(\"$\", \"\"))\n",
                "In addition to the solution already provided, you can replace all the columns while you are reading the file. We can use names and header=0 to do that.namesheader=0First, we create a list of the names that we like to use as our column names:import pandas as pd\n\nufo_cols = ['city', 'color reported', 'shape reported', 'state', 'time']\nufo.columns = ufo_cols\n\nufo = pd.read_csv('link to the file you are using', names = ufo_cols, header = 0)\nimport pandas as pd\n\nufo_cols = ['city', 'color reported', 'shape reported', 'state', 'time']\nufo.columns = ufo_cols\n\nufo = pd.read_csv('link to the file you are using', names = ufo_cols, header = 0)\nIn this case, all the column names will be replaced with the names you have in your list.",
                "Here's a nifty little function I like to use to cut down on typing:def rename(data, oldnames, newname):\n    if type(oldnames) == str: # Input can be a string or list of strings\n        oldnames = [oldnames] # When renaming multiple columns\n        newname = [newname] # Make sure you pass the corresponding list of new names\n    i = 0\n    for name in oldnames:\n        oldvar = [c for c in data.columns if name in c]\n        if len(oldvar) == 0:\n            raise ValueError(\"Sorry, couldn't find that column in the dataset\")\n        if len(oldvar) > 1: # Doesn't have to be an exact match\n            print(\"Found multiple columns that matched \" + str(name) + \": \")\n            for c in oldvar:\n                print(str(oldvar.index(c)) + \": \" + str(c))\n            ind = input('Please enter the index of the column you would like to rename: ')\n            oldvar = oldvar[int(ind)]\n        if len(oldvar) == 1:\n            oldvar = oldvar[0]\n        data = data.rename(columns = {oldvar : newname[i]})\n        i += 1\n    return data\ndef rename(data, oldnames, newname):\n    if type(oldnames) == str: # Input can be a string or list of strings\n        oldnames = [oldnames] # When renaming multiple columns\n        newname = [newname] # Make sure you pass the corresponding list of new names\n    i = 0\n    for name in oldnames:\n        oldvar = [c for c in data.columns if name in c]\n        if len(oldvar) == 0:\n            raise ValueError(\"Sorry, couldn't find that column in the dataset\")\n        if len(oldvar) > 1: # Doesn't have to be an exact match\n            print(\"Found multiple columns that matched \" + str(name) + \": \")\n            for c in oldvar:\n                print(str(oldvar.index(c)) + \": \" + str(c))\n            ind = input('Please enter the index of the column you would like to rename: ')\n            oldvar = oldvar[int(ind)]\n        if len(oldvar) == 1:\n            oldvar = oldvar[0]\n        data = data.rename(columns = {oldvar : newname[i]})\n        i += 1\n    return data\nHere is an example of how it works:In [2]: df = pd.DataFrame(np.random.randint(0, 10, size=(10, 4)), columns = ['col1', 'col2', 'omg', 'idk'])\n# First list = existing variables\n# Second list = new names for those variables\nIn [3]: df = rename(df, ['col', 'omg'],['first', 'ohmy'])\nFound multiple columns that matched col:\n0: col1\n1: col2\n\nPlease enter the index of the column you would like to rename: 0\n\nIn [4]: df.columns\nOut[5]: Index(['first', 'col2', 'ohmy', 'idk'], dtype='object')\nIn [2]: df = pd.DataFrame(np.random.randint(0, 10, size=(10, 4)), columns = ['col1', 'col2', 'omg', 'idk'])\n# First list = existing variables\n# Second list = new names for those variables\nIn [3]: df = rename(df, ['col', 'omg'],['first', 'ohmy'])\nFound multiple columns that matched col:\n0: col1\n1: col2\n\nPlease enter the index of the column you would like to rename: 0\n\nIn [4]: df.columns\nOut[5]: Index(['first', 'col2', 'ohmy', 'idk'], dtype='object')\n"
            ]
        },
        {
            "tag": "remove_key",
            "patterns": [
                "How can I remove a key from a Python dictionary?",
                "how do i get a key out of a python dictionary?",
                "how do i delete a key in a python dictionary?",
                "how do i remove a given key from a python dictionary?",
                "how do i remove a key from the python dictionary?",
                "how do i remove the key from a python dictionary?",
                "how do you remove a key from a python dictionary?",
                "how can i delete a key from a python dictionary?",
                "how can you remove a key from a python dictionary?"
            ],
            "responses": [
                "To delete a key regardless of whether it is in the dictionary, use the two-argument form of dict.pop():dict.pop()dict.pop()my_dict.pop('key', None)\nmy_dict.pop('key', None)\nThis will return my_dict[key] if key exists in the dictionary, and None otherwise. If the second parameter is not specified (i.e. my_dict.pop('key')) and key does not exist, a KeyError is raised.my_dict[key]keyNonemy_dict.pop('key')keyKeyErrorTo delete a key that is guaranteed to exist, you can also use:del my_dict['key']\ndel my_dict['key']\nThis will raise a KeyError if the key is not in the dictionary.KeyError",
                "Specifically to answer \"is there a one line way of doing this?\"if 'key' in my_dict: del my_dict['key']\nif 'key' in my_dict: del my_dict['key']\n...well, you asked ;-)askedYou should consider, though, that this way of deleting an object from a dict is not atomic\u2014it is possible that 'key' may be in my_dict during the if statement, but may be deleted before del is executed, in which case del will fail with a KeyError.  Given this, it would be safest to either use dict.pop or something along the lines ofdictnot atomic'key'my_dictifdeldelKeyErroruse dict.popdict.poptry:\n    del my_dict['key']\nexcept KeyError:\n    pass\ntry:\n    del my_dict['key']\nexcept KeyError:\n    pass\nwhich, of course, is definitely not a one-liner.not",
                "It took me some time to figure out what exactly my_dict.pop(\"key\", None) is doing. So I'll add this as an answer to save others googling time:my_dict.pop(\"key\", None)\npop(key[, default])\nIf key is in the dictionary, remove it and return its value, else\nreturn default. If default is not given and key is not in the\ndictionary, a KeyError is raised.\npop(key[, default])pop(key[, default])If key is in the dictionary, remove it and return its value, else\nreturn default. If default is not given and key is not in the\ndictionary, a KeyError is raised.keydefaultdefaultkeyKeyErrorDocumentationDocumentation",
                "del my_dict[key] is slightly faster than my_dict.pop(key) for removing a key from a dictionary when the key existsdel my_dict[key]my_dict.pop(key)>>> import timeit\n>>> setup = \"d = {i: i for i in range(100000)}\"\n\n>>> timeit.timeit(\"del d[3]\", setup=setup, number=1)\n1.79e-06\n>>> timeit.timeit(\"d.pop(3)\", setup=setup, number=1)\n2.09e-06\n>>> timeit.timeit(\"d2 = {key: val for key, val in d.items() if key != 3}\", setup=setup, number=1)\n0.00786\n>>> import timeit\n>>> setup = \"d = {i: i for i in range(100000)}\"\n\n>>> timeit.timeit(\"del d[3]\", setup=setup, number=1)\n1.79e-06\n>>> timeit.timeit(\"d.pop(3)\", setup=setup, number=1)\n2.09e-06\n>>> timeit.timeit(\"d2 = {key: val for key, val in d.items() if key != 3}\", setup=setup, number=1)\n0.00786\nBut when the key doesn't exist if key in my_dict: del my_dict[key] is slightly faster than my_dict.pop(key, None). Both are at least three times faster than del in a try/except statement:if key in my_dict: del my_dict[key]my_dict.pop(key, None)deltryexcept>>> timeit.timeit(\"if 'missing key' in d: del d['missing key']\", setup=setup)\n0.0229\n>>> timeit.timeit(\"d.pop('missing key', None)\", setup=setup)\n0.0426\n>>> try_except = \"\"\"\n... try:\n...     del d['missing key']\n... except KeyError:\n...     pass\n... \"\"\"\n>>> timeit.timeit(try_except, setup=setup)\n0.133\n>>> timeit.timeit(\"if 'missing key' in d: del d['missing key']\", setup=setup)\n0.0229\n>>> timeit.timeit(\"d.pop('missing key', None)\", setup=setup)\n0.0426\n>>> try_except = \"\"\"\n... try:\n...     del d['missing key']\n... except KeyError:\n...     pass\n... \"\"\"\n>>> timeit.timeit(try_except, setup=setup)\n0.133\n",
                "If you need to remove a lot of keys from a dictionary in one line of code, I think using map() is quite succinct and Pythonic readable:myDict = {'a':1,'b':2,'c':3,'d':4}\nmap(myDict.pop, ['a','c']) # The list of keys to remove\n>>> myDict\n{'b': 2, 'd': 4}\nmyDict = {'a':1,'b':2,'c':3,'d':4}\nmap(myDict.pop, ['a','c']) # The list of keys to remove\n>>> myDict\n{'b': 2, 'd': 4}\nAnd if you need to catch errors where you pop a value that isn't in the dictionary, use lambda inside map() like this:map(lambda x: myDict.pop(x,None), ['a', 'c', 'e'])\n[1, 3, None] # pop returns\n>>> myDict\n{'b': 2, 'd': 4}\nmap(lambda x: myDict.pop(x,None), ['a', 'c', 'e'])\n[1, 3, None] # pop returns\n>>> myDict\n{'b': 2, 'd': 4}\nor in python3, you must use a list comprehension instead:python3[myDict.pop(x, None) for x in ['a', 'c', 'e']]\n[myDict.pop(x, None) for x in ['a', 'c', 'e']]\nIt works. And 'e' did not cause an error, even though myDict did not have an 'e' key.",
                "You can use a dictionary comprehension to create a new dictionary with that key removed:dictionary comprehension>>> my_dict = {k: v for k, v in my_dict.items() if k != 'key'}\n>>> my_dict = {k: v for k, v in my_dict.items() if k != 'key'}\nYou can delete by conditions. No error if key doesn't exist.key",
                "We can delete a key from a Python dictionary by the some of the following approaches.Using the del keyword; it's almost the same approach like you did though -del myDict = {'one': 100, 'two': 200, 'three': 300 }\n print(myDict)  # {'one': 100, 'two': 200, 'three': 300}\n if myDict.get('one') : del myDict['one']\n print(myDict)  # {'two': 200, 'three': 300}\n myDict = {'one': 100, 'two': 200, 'three': 300 }\n print(myDict)  # {'one': 100, 'two': 200, 'three': 300}\n if myDict.get('one') : del myDict['one']\n print(myDict)  # {'two': 200, 'three': 300}\nOrOrWe can do like the following:But one should keep in mind that, in this process actually it won't delete any key from the dictionary rather than making a specific key excluded from that dictionary. In addition, I observed that it returned a dictionary which was not ordered the same as myDict.deleteexcludedmyDictmyDict = {'one': 100, 'two': 200, 'three': 300, 'four': 400, 'five': 500}\n{key:value for key, value in myDict.items() if key != 'one'}\nmyDict = {'one': 100, 'two': 200, 'three': 300, 'four': 400, 'five': 500}\n{key:value for key, value in myDict.items() if key != 'one'}\nIf we run it in the shell, it'll execute something like {'five': 500, 'four': 400, 'three': 300, 'two': 200} - notice that it's not the same ordered as myDict. Again if we try to print myDict, then we can see all keys including which we excluded from the dictionary by this approach. However, we can make a new dictionary by assigning the following statement into a variable:{'five': 500, 'four': 400, 'three': 300, 'two': 200}myDictmyDictvar = {key:value for key, value in myDict.items() if key != 'one'}\nvar = {key:value for key, value in myDict.items() if key != 'one'}\nNow if we try to print it, then it'll follow the parent order:print(var) # {'two': 200, 'three': 300, 'four': 400, 'five': 500}\nprint(var) # {'two': 200, 'three': 300, 'four': 400, 'five': 500}\nOrOrUsing the pop() method.pop()myDict = {'one': 100, 'two': 200, 'three': 300}\nprint(myDict)\n\nif myDict.get('one') : myDict.pop('one')\nprint(myDict)  # {'two': 200, 'three': 300}\nmyDict = {'one': 100, 'two': 200, 'three': 300}\nprint(myDict)\n\nif myDict.get('one') : myDict.pop('one')\nprint(myDict)  # {'two': 200, 'three': 300}\nThe difference between del and pop is that, using pop() method, we can actually store the key's value if needed, like the following:delpoppop()key's valuemyDict = {'one': 100, 'two': 200, 'three': 300}\nif myDict.get('one') : var = myDict.pop('one')\nprint(myDict) # {'two': 200, 'three': 300}\nprint(var)    # 100\nmyDict = {'one': 100, 'two': 200, 'three': 300}\nif myDict.get('one') : var = myDict.pop('one')\nprint(myDict) # {'two': 200, 'three': 300}\nprint(var)    # 100\nFork this gist for future reference, if you find this useful.this gist",
                "You can use exception handling if you want to be very verbose:try: \n    del dict[key]\n\nexcept KeyError: pass\ntry: \n    del dict[key]\n\nexcept KeyError: pass\nThis is slower, however, than the pop() method, if the key doesn't exist.pop()my_dict.pop('key', None)\nmy_dict.pop('key', None)\nIt won't matter for a few keys, but if you're doing this repeatedly, then the latter method is a better bet.The fastest approach is this:if 'key' in dict: \n    del myDict['key']\nif 'key' in dict: \n    del myDict['key']\nBut this method is dangerous because if 'key' is removed in between the two lines, a KeyError will be raised.'key'KeyError",
                "I prefer the immutable versionfoo = {\n    1:1,\n    2:2,\n    3:3\n}\nremoveKeys = [1,2]\ndef woKeys(dct, keyIter):\n    return {\n        k:v\n        for k,v in dct.items() if k not in keyIter\n    }\n\n>>> print(woKeys(foo, removeKeys))\n{3: 3}\n>>> print(foo)\n{1: 1, 2: 2, 3: 3}\nfoo = {\n    1:1,\n    2:2,\n    3:3\n}\nremoveKeys = [1,2]\ndef woKeys(dct, keyIter):\n    return {\n        k:v\n        for k,v in dct.items() if k not in keyIter\n    }\n\n>>> print(woKeys(foo, removeKeys))\n{3: 3}\n>>> print(foo)\n{1: 1, 2: 2, 3: 3}\n",
                "Another way is by using items() + dict comprehension.items() coupled with dict comprehension can also help us achieve the task of key-value pair deletion, but it has the drawback of not being an in place dict technique. Actually a new dict if created except for the key we don\u2019t wish to include.test_dict = {\"sai\" : 22, \"kiran\" : 21, \"vinod\" : 21, \"sangam\" : 21}\n\n# Printing dictionary before removal\nprint (\"dictionary before performing remove is : \" + str(test_dict))\n\n# Using items() + dict comprehension to remove a dict. pair\n# removes  vinod\nnew_dict = {key:val for key, val in test_dict.items() if key != 'vinod'}\n\n# Printing dictionary after removal\nprint (\"dictionary after remove is : \" + str(new_dict))\ntest_dict = {\"sai\" : 22, \"kiran\" : 21, \"vinod\" : 21, \"sangam\" : 21}\n\n# Printing dictionary before removal\nprint (\"dictionary before performing remove is : \" + str(test_dict))\n\n# Using items() + dict comprehension to remove a dict. pair\n# removes  vinod\nnew_dict = {key:val for key, val in test_dict.items() if key != 'vinod'}\n\n# Printing dictionary after removal\nprint (\"dictionary after remove is : \" + str(new_dict))\nOutput:dictionary before performing remove is : {'sai': 22, 'kiran': 21, 'vinod': 21, 'sangam': 21}\ndictionary after remove is : {'sai': 22, 'kiran': 21, 'sangam': 21}\ndictionary before performing remove is : {'sai': 22, 'kiran': 21, 'vinod': 21, 'sangam': 21}\ndictionary after remove is : {'sai': 22, 'kiran': 21, 'sangam': 21}\n",
                "If you want to do that without KeyError, you can declare a temporary class and set it as default value in dict.get, if the value is equal to that class if means that key does not existKeyErrorKeys and Dicta={\"Assemblyisscary\":3637}\nkey1=\"Assemblyisscary\"\nkey2=\"Binarylangisnightmare\"\na={\"Assemblyisscary\":3637}\nkey1=\"Assemblyisscary\"\nkey2=\"Binarylangisnightmare\"\nFunction for deletingdef delkey(dictionary,key):\n    class temp:pass\n    if temp==dictionary.get(key,temp):return False\n    else: del dictionary[key];return True\ndef delkey(dictionary,key):\n    class temp:pass\n    if temp==dictionary.get(key,temp):return False\n    else: del dictionary[key];return True\nTestingdelete(a,key2)\ndelete(a,key1)\ndelete(a,key2)\ndelete(a,key1)\nOutputFalse\nTrue\nFalse\nTrue\n"
            ]
        },
        {
            "tag": "check_key",
            "patterns": [
                "Check if a given key already exists in a dictionary",
                "check if a given key exists in a dictionary",
                "check if a given key already exists in the dictionary",
                "check if a given key already exists in a dictionary"
            ],
            "responses": [
                "in tests for the existence of a key in a dict:inindictdictd = {\"key1\": 10, \"key2\": 23}\n\nif \"key1\" in d:\n    print(\"this will execute\")\n\nif \"nonexistent key\" in d:\n    print(\"this will not\")\nd = {\"key1\": 10, \"key2\": 23}\n\nif \"key1\" in d:\n    print(\"this will execute\")\n\nif \"nonexistent key\" in d:\n    print(\"this will not\")\nUse dict.get() to provide a default value when the key does not exist:dict.get()dict.get()d = {}\n\nfor i in range(10):\n    d[i] = d.get(i, 0) + 1\nd = {}\n\nfor i in range(10):\n    d[i] = d.get(i, 0) + 1\nTo provide a default value for every key, either use dict.setdefault() on each assignment:everydict.setdefault()dict.setdefault()d = {}\n\nfor i in range(10):\n    d[i] = d.setdefault(i, 0) + 1\nd = {}\n\nfor i in range(10):\n    d[i] = d.setdefault(i, 0) + 1\nor use defaultdict from the collections module:defaultdictdefaultdictcollectionscollectionsfrom collections import defaultdict\n\nd = defaultdict(int)\n\nfor i in range(10):\n    d[i] += 1\nfrom collections import defaultdict\n\nd = defaultdict(int)\n\nfor i in range(10):\n    d[i] += 1\n",
                "Use key in my_dict directly instead of key in my_dict.keys():key in my_dictkey in my_dict.keys()if 'key1' in my_dict:\n    print(\"blah\")\nelse:\n    print(\"boo\")\nif 'key1' in my_dict:\n    print(\"blah\")\nelse:\n    print(\"boo\")\nThat will be much faster as it uses the dictionary's O(1) hashing as opposed to doing an O(n) linear search on a list of keys.faster",
                "You can test for the presence of a key in a dictionary, using the in keyword:ind = {'a': 1, 'b': 2}\n'a' in d # <== evaluates to True\n'c' in d # <== evaluates to False\nd = {'a': 1, 'b': 2}\n'a' in d # <== evaluates to True\n'c' in d # <== evaluates to False\nA common use for checking the existence of a key in a dictionary before mutating it is to default-initialize the value (e.g. if your values are lists, for example, and you want to ensure that there is an empty list to which you can append when inserting the first value for a key). In cases such as those, you may find the collections.defaultdict() type to be of interest.collections.defaultdict()collections.defaultdict()In older code, you may also find some uses of has_key(), a deprecated method for checking the existence of keys in dictionaries (just use key_name in dict_name, instead).has_key()key_name in dict_name",
                "You can shorten your code to this:if 'key1' in my_dict:\n    ...\nif 'key1' in my_dict:\n    ...\nHowever, this is at best a cosmetic improvement. Why do you believe this is not the best way?",
                "For additional information on speed execution of the accepted answer's proposed methods (10\u00a0million loops):accepted answer's\n'key' in mydict elapsed time 1.07 seconds\nmydict.get('key') elapsed time 1.84 seconds\nmydefaultdict['key'] elapsed time 1.07 seconds\n'key' in mydict elapsed time 1.07 seconds'key' in mydictmydict.get('key') elapsed time 1.84 secondsmydict.get('key')mydefaultdict['key'] elapsed time 1.07 secondsmydefaultdict['key']Therefore using in or defaultdict are recommended against get.indefaultdictget",
                "I would recommend using the setdefault method instead.  It sounds like it will do everything you want.setdefault>>> d = {'foo':'bar'}\n>>> q = d.setdefault('foo','baz') #Do not override the existing key\n>>> print q #The value takes what was originally in the dictionary\nbar\n>>> print d\n{'foo': 'bar'}\n>>> r = d.setdefault('baz',18) #baz was never in the dictionary\n>>> print r #Now r has the value supplied above\n18\n>>> print d #The dictionary's been updated\n{'foo': 'bar', 'baz': 18}\n>>> d = {'foo':'bar'}\n>>> q = d.setdefault('foo','baz') #Do not override the existing key\n>>> print q #The value takes what was originally in the dictionary\nbar\n>>> print d\n{'foo': 'bar'}\n>>> r = d.setdefault('baz',18) #baz was never in the dictionary\n>>> print r #Now r has the value supplied above\n18\n>>> print d #The dictionary's been updated\n{'foo': 'bar', 'baz': 18}\n",
                "A dictionary in Python has a get('key', default) method. So you can just set a default value in case there isn't any key.get('key', default)values = {...}\nmyValue = values.get('Key', None)\nvalues = {...}\nmyValue = values.get('Key', None)\n",
                "Using the Python ternary operator:Python ternary operatormessage = \"blah\" if 'key1' in my_dict else \"booh\"\nprint(message)\nmessage = \"blah\" if 'key1' in my_dict else \"booh\"\nprint(message)\n",
                "Use EAFP (easier to ask forgiveness than permission):try:\n   blah = dict[\"mykey\"]\n   # key exists in dict\nexcept KeyError:\n   # key doesn't exist in dict\ntry:\n   blah = dict[\"mykey\"]\n   # key exists in dict\nexcept KeyError:\n   # key doesn't exist in dict\nSee other Stack Overflow posts:\nUsing 'try' vs. 'if' in Python\nChecking for member existence in Python\nUsing 'try' vs. 'if' in PythonUsing 'try' vs. 'if' in PythonUsing 'try' vs. 'if' in PythonChecking for member existence in PythonChecking for member existence in PythonChecking for member existence in Python",
                "\nCheck if a given key already exists in a dictionary\nCheck if a given key already exists in a dictionaryTo get the idea how to do that we first inspect what methods we can call on dictionary.Here are the methods:d={'clear':0, 'copy':1, 'fromkeys':2, 'get':3, 'items':4, 'keys':5, 'pop':6, 'popitem':7, 'setdefault':8, 'update':9, 'values':10}\nd={'clear':0, 'copy':1, 'fromkeys':2, 'get':3, 'items':4, 'keys':5, 'pop':6, 'popitem':7, 'setdefault':8, 'update':9, 'values':10}\nPython Dictionary clear()        Removes all Items\nPython Dictionary copy()         Returns Shallow Copy of a Dictionary\nPython Dictionary fromkeys()     Creates dictionary from given sequence\nPython Dictionary get()          Returns Value of The Key\nPython Dictionary items()        Returns view of dictionary (key, value) pair\nPython Dictionary keys()         Returns View Object of All Keys\nPython Dictionary pop()          Removes and returns element having given key\nPython Dictionary popitem()      Returns & Removes Element From Dictionary\nPython Dictionary setdefault()   Inserts Key With a Value if Key is not Present\nPython Dictionary update()       Updates the Dictionary\nPython Dictionary values()       Returns view of all values in dictionary\nPython Dictionary clear()        Removes all Items\nPython Dictionary copy()         Returns Shallow Copy of a Dictionary\nPython Dictionary fromkeys()     Creates dictionary from given sequence\nPython Dictionary get()          Returns Value of The Key\nPython Dictionary items()        Returns view of dictionary (key, value) pair\nPython Dictionary keys()         Returns View Object of All Keys\nPython Dictionary pop()          Removes and returns element having given key\nPython Dictionary popitem()      Returns & Removes Element From Dictionary\nPython Dictionary setdefault()   Inserts Key With a Value if Key is not Present\nPython Dictionary update()       Updates the Dictionary\nPython Dictionary values()       Returns view of all values in dictionary\nThe brutal method to check if the key already exists may be the get() method:get()d.get(\"key\")\nd.get(\"key\")\nThe other two interesting methods items() and keys() sounds like too much of work. So let's examine if get() is the right method for us. We have our dict d:interestingitems()keys()get()dd= {'clear':0, 'copy':1, 'fromkeys':2, 'get':3, 'items':4, 'keys':5, 'pop':6, 'popitem':7, 'setdefault':8, 'update':9, 'values':10}\nd= {'clear':0, 'copy':1, 'fromkeys':2, 'get':3, 'items':4, 'keys':5, 'pop':6, 'popitem':7, 'setdefault':8, 'update':9, 'values':10}\nPrinting shows the key we don't have will return None:Noneprint(d.get('key')) #None\nprint(d.get('clear')) #0\nprint(d.get('copy')) #1\nprint(d.get('key')) #None\nprint(d.get('clear')) #0\nprint(d.get('copy')) #1\nWe use that to get the information if the key is present or no.\nBut consider this if we create a dict with a single key:None:key:Noned= {'key':None}\nprint(d.get('key')) #None\nprint(d.get('key2')) #None\nd= {'key':None}\nprint(d.get('key')) #None\nprint(d.get('key2')) #None\nLeading that get() method is not reliable in case some values may be None.get()NoneThis story should have a happier ending. If we use the in comparator:inprint('key' in d) #True\nprint('key2' in d) #False\nprint('key' in d) #True\nprint('key2' in d) #False\nWe get the correct results.We may examine the Python byte code:import dis\ndis.dis(\"'key' in d\")\n#   1           0 LOAD_CONST               0 ('key')\n#               2 LOAD_NAME                0 (d)\n#               4 COMPARE_OP               6 (in)\n#               6 RETURN_VALUE\n\ndis.dis(\"d.get('key2')\")\n#   1           0 LOAD_NAME                0 (d)\n#               2 LOAD_METHOD              1 (get)\n#               4 LOAD_CONST               0 ('key2')\n#               6 CALL_METHOD              1\n#               8 RETURN_VALUE\nimport dis\ndis.dis(\"'key' in d\")\n#   1           0 LOAD_CONST               0 ('key')\n#               2 LOAD_NAME                0 (d)\n#               4 COMPARE_OP               6 (in)\n#               6 RETURN_VALUE\n\ndis.dis(\"d.get('key2')\")\n#   1           0 LOAD_NAME                0 (d)\n#               2 LOAD_METHOD              1 (get)\n#               4 LOAD_CONST               0 ('key2')\n#               6 CALL_METHOD              1\n#               8 RETURN_VALUE\nThis shows that in compare operator is not just more reliable, but even faster than get().inget()",
                "The ways in which you can get the results are:The ways in which you can get the results are:\nif your_dict.has_key(key) Removed in Python 3\nif key in your_dict\ntry/except block\nif your_dict.has_key(key) Removed in Python 3Removed in Python 3if key in your_dicttry/except blockWhich is better is dependent on 3 things:Which is better is dependent on 3 things:\nDoes the dictionary 'normally has the key' or 'normally does not have the key'.\nDo you intend to use conditions like if...else...elseif...else?\nHow big is dictionary?\nDoes the dictionary 'normally has the key' or 'normally does not have the key'.Do you intend to use conditions like if...else...elseif...else?How big is dictionary?Read More: http://paltman.com/try-except-performance-in-python-a-simple-test/http://paltman.com/try-except-performance-in-python-a-simple-test/Use of try/block instead of 'in' or 'if':try:\n    my_dict_of_items[key_i_want_to_check]\nexcept KeyError:\n    # Do the operation you wanted to do for \"key not present in dict\".\nelse:\n    # Do the operation you wanted to do with \"key present in dict.\"\ntry:\n    my_dict_of_items[key_i_want_to_check]\nexcept KeyError:\n    # Do the operation you wanted to do for \"key not present in dict\".\nelse:\n    # Do the operation you wanted to do with \"key present in dict.\"\n",
                "Python 2 only: (and Python\u00a02.7 supports `in` already)You can use the has_key() method:has_key()if dict.has_key('xyz')==1:\n    # Update the value for the key\nelse:\n    pass\nif dict.has_key('xyz')==1:\n    # Update the value for the key\nelse:\n    pass\n",
                "Just an FYI adding to Chris. B's (best) answer:Chris. B's (best) answerd = defaultdict(int)\nd = defaultdict(int)\nWorks as well; the reason is that calling int() returns 0 which is what defaultdict does behind the scenes (when constructing a dictionary), hence the name \"Factory Function\" in the documentation.int()0defaultdict",
                "A Python dictionary has the method called __contains__. This method will return True if the dictionary has the key, else it returns False.__contains__>>> temp = {}\n\n>>> help(temp.__contains__)\n\nHelp on built-in function __contains__:\n\n__contains__(key, /) method of builtins.dict instance\n    True if D has a key k, else False.\n>>> temp = {}\n\n>>> help(temp.__contains__)\n\nHelp on built-in function __contains__:\n\n__contains__(key, /) method of builtins.dict instance\n    True if D has a key k, else False.\n",
                "Another way of checking if a key exists using Boolean operators:d = {'a': 1, 'b':2}\nkeys = 'abcd'\n\nfor k in keys:\n    x = (k in d and 'blah') or 'boo'\n    print(x)\nd = {'a': 1, 'b':2}\nkeys = 'abcd'\n\nfor k in keys:\n    x = (k in d and 'blah') or 'boo'\n    print(x)\nThis returns>>> blah\n>>> blah\n>>> boo\n>>> boo\n>>> blah\n>>> blah\n>>> boo\n>>> boo\nExplanationExplanationFirst, you should know that in Python, 0, None, or objects with zero length evaluate to False. Everything else evaluates to True. Boolean operations are evaluated left to right and return the operand not True or False.0NoneFalseTrueLet's see an example:>>> 'Some string' or 1/0\n'Some string'\n>>>\n>>> 'Some string' or 1/0\n'Some string'\n>>>\nSince 'Some string' evaluates to True, the rest of the or is not evaluated and there is no division by zero error raised.'Some string'TrueorBut if we switch the order 1/0 is evaluated first and raises an exception:1/0>>> 1/0 or 'Some string'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nZeroDivisionError: division by zero\n>>>\n>>> 1/0 or 'Some string'\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nZeroDivisionError: division by zero\n>>>\nWe can use this for pattern for checking if a key exists.(k in d and 'blah')\n(k in d and 'blah')\ndoes the same asif k in d:\n    'blah'\nelse:\n    False\nif k in d:\n    'blah'\nelse:\n    False\nThis already returns the correct result if the key exists, but we want it to print 'boo' when it doesn't. So, we take the result and or it with 'boo'or'boo'>>> False or 'boo'\n'boo'\n>>> 'blah' or 'boo'\n'blah'\n>>>\n>>> False or 'boo'\n'boo'\n>>> 'blah' or 'boo'\n'blah'\n>>>\n",
                "You can use a for loop to iterate over the dictionary and get the name of key you want to find in the dictionary. After that, check if it exist or not using if condition:forifdic = {'first' : 12, 'second' : 123}\nfor each in dic:\n    if each == 'second':\n        print('the key exists and the corresponding value can be updated in the dictionary')\ndic = {'first' : 12, 'second' : 123}\nfor each in dic:\n    if each == 'second':\n        print('the key exists and the corresponding value can be updated in the dictionary')\n"
            ]
        },
        {
            "tag": "parse_string",
            "patterns": [
                "How do I parse a string to a float or int?",
                "how do i convert a string to a variable like float?",
                "tell me the best way to parse strings for a float or int format?",
                "how do i parse strings to float?",
                "how can i parse a string to float?",
                "how do i convert a string to a floating point?",
                "how do i parse a string for a float?",
                "how do i parse a string to an int?",
                "how do i parse a string to a float or int?"
            ],
            "responses": [
                ">>> a = \"545.2222\"\n>>> float(a)\n545.22220000000004\n>>> int(float(a))\n545\n>>> a = \"545.2222\"\n>>> float(a)\n545.22220000000004\n>>> int(float(a))\n545\n",
                "Python2 method to check if a string is a float:def is_float(value):\n  if value is None:\n      return False\n  try:\n      float(value)\n      return True\n  except:\n      return False\ndef is_float(value):\n  if value is None:\n      return False\n  try:\n      float(value)\n      return True\n  except:\n      return False\nFor the Python3 version of is_float see: Checking if a string can be converted to float in PythonChecking if a string can be converted to float in PythonA longer and more accurate name for this function could be: is_convertible_to_float(value)is_convertible_to_float(value)What is, and is not a float in Python may surprise you:PythonThe below unit tests were done using python2.  Check it that Python3 has different behavior for what strings are convertable to float.  One confounding difference is that any number of interior underscores are now allowed:  (float(\"1_3.4\") == float(13.4)) is True(float(\"1_3.4\") == float(13.4))val                   is_float(val) Note\n--------------------  ----------   --------------------------------\n\"\"                    False        Blank string\n\"127\"                 True         Passed string\nTrue                  True         Pure sweet Truth\n\"True\"                False        Vile contemptible lie\nFalse                 True         So false it becomes true\n\"123.456\"             True         Decimal\n\"      -127    \"      True         Spaces trimmed\n\"\\t\\n12\\r\\n\"          True         whitespace ignored\n\"NaN\"                 True         Not a number\n\"NaNanananaBATMAN\"    False        I am Batman\n\"-iNF\"                True         Negative infinity\n\"123.E4\"              True         Exponential notation\n\".1\"                  True         mantissa only\n\"1_2_3.4\"             False        Underscores not allowed\n\"12 34\"               False        Spaces not allowed on interior\n\"1,234\"               False        Commas gtfo\nu'\\x30'               True         Unicode is fine.\n\"NULL\"                False        Null is not special\n0x3fade               True         Hexadecimal\n\"6e7777777777777\"     True         Shrunk to infinity\n\"1.797693e+308\"       True         This is max value\n\"infinity\"            True         Same as inf\n\"infinityandBEYOND\"   False        Extra characters wreck it\n\"12.34.56\"            False        Only one dot allowed\nu'\u56db'                 False        Japanese '4' is not a float.\n\"#56\"                 False        Pound sign\n\"56%\"                 False        Percent of what?\n\"0E0\"                 True         Exponential, move dot 0 places\n0**0                  True         0___0  Exponentiation\n\"-5e-5\"               True         Raise to a negative number\n\"+1e1\"                True         Plus is OK with exponent\n\"+1e1^5\"              False        Fancy exponent not interpreted\n\"+1e1.3\"              False        No decimals in exponent\n\"-+1\"                 False        Make up your mind\n\"(1)\"                 False        Parenthesis is bad\nval                   is_float(val) Note\n--------------------  ----------   --------------------------------\n\"\"                    False        Blank string\n\"127\"                 True         Passed string\nTrue                  True         Pure sweet Truth\n\"True\"                False        Vile contemptible lie\nFalse                 True         So false it becomes true\n\"123.456\"             True         Decimal\n\"      -127    \"      True         Spaces trimmed\n\"\\t\\n12\\r\\n\"          True         whitespace ignored\n\"NaN\"                 True         Not a number\n\"NaNanananaBATMAN\"    False        I am Batman\n\"-iNF\"                True         Negative infinity\n\"123.E4\"              True         Exponential notation\n\".1\"                  True         mantissa only\n\"1_2_3.4\"             False        Underscores not allowed\n\"12 34\"               False        Spaces not allowed on interior\n\"1,234\"               False        Commas gtfo\nu'\\x30'               True         Unicode is fine.\n\"NULL\"                False        Null is not special\n0x3fade               True         Hexadecimal\n\"6e7777777777777\"     True         Shrunk to infinity\n\"1.797693e+308\"       True         This is max value\n\"infinity\"            True         Same as inf\n\"infinityandBEYOND\"   False        Extra characters wreck it\n\"12.34.56\"            False        Only one dot allowed\nu'\u56db'                 False        Japanese '4' is not a float.\n\"#56\"                 False        Pound sign\n\"56%\"                 False        Percent of what?\n\"0E0\"                 True         Exponential, move dot 0 places\n0**0                  True         0___0  Exponentiation\n\"-5e-5\"               True         Raise to a negative number\n\"+1e1\"                True         Plus is OK with exponent\n\"+1e1^5\"              False        Fancy exponent not interpreted\n\"+1e1.3\"              False        No decimals in exponent\n\"-+1\"                 False        Make up your mind\n\"(1)\"                 False        Parenthesis is bad\nYou think you know what numbers are? You are not so good as you think! Not big surprise.Don't use this code on life-critical software!Catching broad exceptions this way, killing canaries and gobbling the exception creates a tiny chance that a valid float as string will return false.  The float(...) line of code can failed for any of a thousand reasons that have nothing to do with the contents of the string.  But if you're writing life-critical software in a duck-typing prototype language like Python, then you've got much larger problems.float(...)",
                "def num(s):\n    try:\n        return int(s)\n    except ValueError:\n        return float(s)\ndef num(s):\n    try:\n        return int(s)\n    except ValueError:\n        return float(s)\n",
                "This is another method which deserves to be mentioned here, ast.literal_eval:ast.literal_eval\nThis can be used for safely evaluating strings containing Python expressions from untrusted sources without the need to parse the values oneself.\nThis can be used for safely evaluating strings containing Python expressions from untrusted sources without the need to parse the values oneself.That is, a safe 'eval'>>> import ast\n>>> ast.literal_eval(\"545.2222\")\n545.2222\n>>> ast.literal_eval(\"31\")\n31\n>>> import ast\n>>> ast.literal_eval(\"545.2222\")\n545.2222\n>>> ast.literal_eval(\"31\")\n31\n",
                "Localization and commasYou should consider the possibility of commas in the string representation of a number, for cases like  float(\"545,545.2222\") which throws an exception. Instead, use methods in locale to convert the strings to numbers and interpret commas correctly. The locale.atof method converts to a float in one step once the locale has been set for the desired number convention.float(\"545,545.2222\")localelocale.atofExample 1 -- United States number conventions Example 1 -- United States number conventionsIn the United States and the UK, commas can be used as a thousands separator.  In this example with American locale, the comma is handled properly as a separator:>>> import locale\n>>> a = u'545,545.2222'\n>>> locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n'en_US.UTF-8'\n>>> locale.atof(a)\n545545.2222\n>>> int(locale.atof(a))\n545545\n>>>\n>>> import locale\n>>> a = u'545,545.2222'\n>>> locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')\n'en_US.UTF-8'\n>>> locale.atof(a)\n545545.2222\n>>> int(locale.atof(a))\n545545\n>>>\nExample 2 -- European number conventionsExample 2 -- European number conventionsIn the majority of countries of the world,  commas are used for decimal marks instead of periods.  In this example with French locale, the comma is correctly handled as a decimal mark:majority of countries of the world>>> import locale\n>>> b = u'545,2222'\n>>> locale.setlocale(locale.LC_ALL, 'fr_FR')\n'fr_FR'\n>>> locale.atof(b)\n545.2222\n>>> import locale\n>>> b = u'545,2222'\n>>> locale.setlocale(locale.LC_ALL, 'fr_FR')\n'fr_FR'\n>>> locale.atof(b)\n545.2222\nThe method locale.atoi is also available, but the argument should be an integer.locale.atoi",
                "float(x) if '.' in x else int(x)\nfloat(x) if '.' in x else int(x)\n",
                "If you aren't averse to third-party modules, you could check out the fastnumbers module. It provides a function called fast_real that does exactly what this question is asking for and does it faster than a pure-Python implementation:fastnumbersfast_real>>> from fastnumbers import fast_real\n>>> fast_real(\"545.2222\")\n545.2222\n>>> type(fast_real(\"545.2222\"))\nfloat\n>>> fast_real(\"31\")\n31\n>>> type(fast_real(\"31\"))\nint\n>>> from fastnumbers import fast_real\n>>> fast_real(\"545.2222\")\n545.2222\n>>> type(fast_real(\"545.2222\"))\nfloat\n>>> fast_real(\"31\")\n31\n>>> type(fast_real(\"31\"))\nint\n",
                "Users codelogic and harley are correct, but keep in mind if you know the string is an integer (for example, 545) you can call int(\"545\") without first casting to float.codelogicharleyIf your strings are in a list, you could use the map function as well. >>> x = [\"545.0\", \"545.6\", \"999.2\"]\n>>> map(float, x)\n[545.0, 545.60000000000002, 999.20000000000005]\n>>>\n>>> x = [\"545.0\", \"545.6\", \"999.2\"]\n>>> map(float, x)\n[545.0, 545.60000000000002, 999.20000000000005]\n>>>\nIt is only good if they're all the same type.",
                "\nIn Python, how can I parse a numeric string like \"545.2222\" to its corresponding float value, 542.2222? Or parse the string \"31\" to an integer, 31?\n  I just want to know how to parse a float string to a float, and (separately) an int string to an int.\nIn Python, how can I parse a numeric string like \"545.2222\" to its corresponding float value, 542.2222? Or parse the string \"31\" to an integer, 31?\n  I just want to know how to parse a float string to a float, and (separately) an int string to an int.In Python, how can I parse a numeric string like \"545.2222\" to its corresponding float value, 542.2222? Or parse the string \"31\" to an integer, 31?It's good that you ask to do these separately. If you're mixing them, you may be setting yourself up for problems later. The simple answer is:\"545.2222\" to float:\"545.2222\" to float:\"545.2222\">>> float(\"545.2222\")\n545.2222\n>>> float(\"545.2222\")\n545.2222\n\"31\" to an integer:\"31\" to an integer:\"31\">>> int(\"31\")\n31\n>>> int(\"31\")\n31\nOther conversions, ints to and from strings and literals:Conversions from various bases, and you should know the base in advance (10 is the default). Note you can prefix them with what Python expects for its literals (see below) or remove the prefix:>>> int(\"0b11111\", 2)\n31\n>>> int(\"11111\", 2)\n31\n>>> int('0o37', 8)\n31\n>>> int('37', 8)\n31\n>>> int('0x1f', 16)\n31\n>>> int('1f', 16)\n31\n>>> int(\"0b11111\", 2)\n31\n>>> int(\"11111\", 2)\n31\n>>> int('0o37', 8)\n31\n>>> int('37', 8)\n31\n>>> int('0x1f', 16)\n31\n>>> int('1f', 16)\n31\nIf you don't know the base in advance, but you do know they will have the correct prefix, Python can infer this for you if you pass 0 as the base:0>>> int(\"0b11111\", 0)\n31\n>>> int('0o37', 0)\n31\n>>> int('0x1f', 0)\n31\n>>> int(\"0b11111\", 0)\n31\n>>> int('0o37', 0)\n31\n>>> int('0x1f', 0)\n31\nNon-Decimal (i.e. Integer) Literals from other BasesIf your motivation is to have your own code clearly represent hard-coded specific values, however, you may not need to convert from the bases - you can let Python do it for you automatically with the correct syntax.You can use the apropos prefixes to get automatic conversion to integers with the following literals. These are valid for Python 2 and 3:the following literalsBinary, prefix 0b0b>>> 0b11111\n31\n>>> 0b11111\n31\nOctal, prefix 0o0o>>> 0o37\n31\n>>> 0o37\n31\nHexadecimal, prefix 0x0x>>> 0x1f\n31\n>>> 0x1f\n31\nThis can be useful when describing binary flags, file permissions in code, or hex values for colors - for example, note no quotes:>>> 0b10101 # binary flags\n21\n>>> 0o755 # read, write, execute perms for owner, read & ex for group & others\n493\n>>> 0xffffff # the color, white, max values for red, green, and blue\n16777215\n>>> 0b10101 # binary flags\n21\n>>> 0o755 # read, write, execute perms for owner, read & ex for group & others\n493\n>>> 0xffffff # the color, white, max values for red, green, and blue\n16777215\nMaking ambiguous Python 2 octals compatible with Python 3If you see an integer that starts with a 0, in Python 2, this is (deprecated) octal syntax.>>> 037\n31\n>>> 037\n31\nIt is bad because it looks like the value should be 37. So in Python 3, it now raises a SyntaxError:37SyntaxError>>> 037\n  File \"<stdin>\", line 1\n    037\n      ^\nSyntaxError: invalid token\n>>> 037\n  File \"<stdin>\", line 1\n    037\n      ^\nSyntaxError: invalid token\nConvert your Python 2 octals to octals that work in both 2 and 3 with the 0o prefix:0o>>> 0o37\n31\n>>> 0o37\n31\n",
                "The question seems a little bit old. But let me suggest a function, parseStr, which makes something similar, that is, returns integer or float and if a given ASCII string cannot be converted to none of them it returns it untouched. The code of course might be adjusted to do only what you want:   >>> import string\n   >>> parseStr = lambda x: x.isalpha() and x or x.isdigit() and \\\n   ...                      int(x) or x.isalnum() and x or \\\n   ...                      len(set(string.punctuation).intersection(x)) == 1 and \\\n   ...                      x.count('.') == 1 and float(x) or x\n   >>> parseStr('123')\n   123\n   >>> parseStr('123.3')\n   123.3\n   >>> parseStr('3HC1')\n   '3HC1'\n   >>> parseStr('12.e5')\n   1200000.0\n   >>> parseStr('12$5')\n   '12$5'\n   >>> parseStr('12.2.2')\n   '12.2.2'\n   >>> import string\n   >>> parseStr = lambda x: x.isalpha() and x or x.isdigit() and \\\n   ...                      int(x) or x.isalnum() and x or \\\n   ...                      len(set(string.punctuation).intersection(x)) == 1 and \\\n   ...                      x.count('.') == 1 and float(x) or x\n   >>> parseStr('123')\n   123\n   >>> parseStr('123.3')\n   123.3\n   >>> parseStr('3HC1')\n   '3HC1'\n   >>> parseStr('12.e5')\n   1200000.0\n   >>> parseStr('12$5')\n   '12$5'\n   >>> parseStr('12.2.2')\n   '12.2.2'\n",
                "float(\"545.2222\") and int(float(\"545.2222\"))float(\"545.2222\")int(float(\"545.2222\"))",
                "The YAML parser can help you figure out what datatype your string is. Use yaml.load(), and then you can use type(result) to test for type:YAMLyaml.load()type(result)>>> import yaml\n\n>>> a = \"545.2222\"\n>>> result = yaml.load(a)\n>>> result\n545.22220000000004\n>>> type(result)\n<type 'float'>\n\n>>> b = \"31\"\n>>> result = yaml.load(b)\n>>> result\n31\n>>> type(result)\n<type 'int'>\n\n>>> c = \"HI\"\n>>> result = yaml.load(c)\n>>> result\n'HI'\n>>> type(result)\n<type 'str'>\n>>> import yaml\n\n>>> a = \"545.2222\"\n>>> result = yaml.load(a)\n>>> result\n545.22220000000004\n>>> type(result)\n<type 'float'>\n\n>>> b = \"31\"\n>>> result = yaml.load(b)\n>>> result\n31\n>>> type(result)\n<type 'int'>\n\n>>> c = \"HI\"\n>>> result = yaml.load(c)\n>>> result\n'HI'\n>>> type(result)\n<type 'str'>\n",
                "I use this function for thatimport ast\n\ndef parse_str(s):\n   try:\n      return ast.literal_eval(str(s))\n   except:\n      return\nimport ast\n\ndef parse_str(s):\n   try:\n      return ast.literal_eval(str(s))\n   except:\n      return\nIt will convert the string to its typevalue = parse_str('1')  # Returns Integer\nvalue = parse_str('1.5')  # Returns Float\nvalue = parse_str('1')  # Returns Integer\nvalue = parse_str('1.5')  # Returns Float\n",
                "def get_int_or_float(v):\n    number_as_float = float(v)\n    number_as_int = int(number_as_float)\n    return number_as_int if number_as_float == number_as_int else number_as_float\ndef get_int_or_float(v):\n    number_as_float = float(v)\n    number_as_int = int(number_as_float)\n    return number_as_int if number_as_float == number_as_int else number_as_float\n",
                "def num(s):\n    \"\"\"num(s)\n    num(3),num(3.7)-->3\n    num('3')-->3, num('3.7')-->3.7\n    num('3,700')-->ValueError\n    num('3a'),num('a3'),-->ValueError\n    num('3e4') --> 30000.0\n    \"\"\"\n    try:\n        return int(s)\n    except ValueError:\n        try:\n            return float(s)\n        except ValueError:\n            raise ValueError('argument is not a string of number')\ndef num(s):\n    \"\"\"num(s)\n    num(3),num(3.7)-->3\n    num('3')-->3, num('3.7')-->3.7\n    num('3,700')-->ValueError\n    num('3a'),num('a3'),-->ValueError\n    num('3e4') --> 30000.0\n    \"\"\"\n    try:\n        return int(s)\n    except ValueError:\n        try:\n            return float(s)\n        except ValueError:\n            raise ValueError('argument is not a string of number')\n",
                "You could use json.loads:json.loads>>> import json\n>>> json.loads('123.456')\n123.456\n>>> type(_)\n<class 'float'>\n>>> \n>>> import json\n>>> json.loads('123.456')\n123.456\n>>> type(_)\n<class 'float'>\n>>> \nAs you can see it becomes a type of float.float",
                "You need to take into account rounding to do this properly.i.e. - int(5.1) => 5\nint(5.6) => 5  -- wrong, should be 6 so we do int(5.6 + 0.5) => 6int(5.1)int(5.6)int(5.6 + 0.5)def convert(n):\n    try:\n        return int(n)\n    except ValueError:\n        return float(n + 0.5)\ndef convert(n):\n    try:\n        return int(n)\n    except ValueError:\n        return float(n + 0.5)\n",
                "To typecast in Python use the constructor functions of the type, passing the string (or whatever value you are trying to cast) as a parameter.typecastFor example:>>>float(\"23.333\")\n   23.333\n>>>float(\"23.333\")\n   23.333\nBehind the scenes, Python is calling the objects __float__ method, which should return a float representation of the parameter. This is especially powerful, as you can define your own types (using classes) with a __float__ method so that it can be casted into a float using float(myobject).__float____float__float(myobject)",
                "Handles hex, octal, binary, decimal, and floatHandles hex, octal, binary, decimal, and floatThis solution will handle all of the string conventions for numbers (all that I know about).def to_number(n):\n    ''' Convert any number representation to a number\n    This covers: float, decimal, hex, and octal numbers.\n    '''\n\n    try:\n        return int(str(n), 0)\n    except:\n        try:\n            # Python 3 doesn't accept \"010\" as a valid octal.  You must use the\n            # '0o' prefix\n            return int('0o' + n, 0)\n        except:\n            return float(n)\ndef to_number(n):\n    ''' Convert any number representation to a number\n    This covers: float, decimal, hex, and octal numbers.\n    '''\n\n    try:\n        return int(str(n), 0)\n    except:\n        try:\n            # Python 3 doesn't accept \"010\" as a valid octal.  You must use the\n            # '0o' prefix\n            return int('0o' + n, 0)\n        except:\n            return float(n)\nThis test case output illustrates what I'm talking about.======================== CAPTURED OUTPUT =========================\nto_number(3735928559)   = 3735928559 == 3735928559\nto_number(\"0xFEEDFACE\") = 4277009102 == 4277009102\nto_number(\"0x0\")        =          0 ==          0\nto_number(100)          =        100 ==        100\nto_number(\"42\")         =         42 ==         42\nto_number(8)            =          8 ==          8\nto_number(\"0o20\")       =         16 ==         16\nto_number(\"020\")        =         16 ==         16\nto_number(3.14)         =       3.14 ==       3.14\nto_number(\"2.72\")       =       2.72 ==       2.72\nto_number(\"1e3\")        =     1000.0 ==       1000\nto_number(0.001)        =      0.001 ==      0.001\nto_number(\"0xA\")        =         10 ==         10\nto_number(\"012\")        =         10 ==         10\nto_number(\"0o12\")       =         10 ==         10\nto_number(\"0b01010\")    =         10 ==         10\nto_number(\"10\")         =         10 ==         10\nto_number(\"10.0\")       =       10.0 ==         10\nto_number(\"1e1\")        =       10.0 ==         10\n======================== CAPTURED OUTPUT =========================\nto_number(3735928559)   = 3735928559 == 3735928559\nto_number(\"0xFEEDFACE\") = 4277009102 == 4277009102\nto_number(\"0x0\")        =          0 ==          0\nto_number(100)          =        100 ==        100\nto_number(\"42\")         =         42 ==         42\nto_number(8)            =          8 ==          8\nto_number(\"0o20\")       =         16 ==         16\nto_number(\"020\")        =         16 ==         16\nto_number(3.14)         =       3.14 ==       3.14\nto_number(\"2.72\")       =       2.72 ==       2.72\nto_number(\"1e3\")        =     1000.0 ==       1000\nto_number(0.001)        =      0.001 ==      0.001\nto_number(\"0xA\")        =         10 ==         10\nto_number(\"012\")        =         10 ==         10\nto_number(\"0o12\")       =         10 ==         10\nto_number(\"0b01010\")    =         10 ==         10\nto_number(\"10\")         =         10 ==         10\nto_number(\"10.0\")       =       10.0 ==         10\nto_number(\"1e1\")        =       10.0 ==         10\nHere is the test:class test_to_number(unittest.TestCase):\n\n    def test_hex(self):\n        # All of the following should be converted to an integer\n        #\n        values = [\n\n                 #          HEX\n                 # ----------------------\n                 # Input     |   Expected\n                 # ----------------------\n                (0xDEADBEEF  , 3735928559), # Hex\n                (\"0xFEEDFACE\", 4277009102), # Hex\n                (\"0x0\"       ,          0), # Hex\n\n                 #        Decimals\n                 # ----------------------\n                 # Input     |   Expected\n                 # ----------------------\n                (100         ,        100), # Decimal\n                (\"42\"        ,         42), # Decimal\n            ]\n\n\n\n        values += [\n                 #        Octals\n                 # ----------------------\n                 # Input     |   Expected\n                 # ----------------------\n                (0o10        ,          8), # Octal\n                (\"0o20\"      ,         16), # Octal\n                (\"020\"       ,         16), # Octal\n            ]\n\n\n        values += [\n                 #        Floats\n                 # ----------------------\n                 # Input     |   Expected\n                 # ----------------------\n                (3.14        ,       3.14), # Float\n                (\"2.72\"      ,       2.72), # Float\n                (\"1e3\"       ,       1000), # Float\n                (1e-3        ,      0.001), # Float\n            ]\n\n        values += [\n                 #        All ints\n                 # ----------------------\n                 # Input     |   Expected\n                 # ----------------------\n                (\"0xA\"       ,         10),\n                (\"012\"       ,         10),\n                (\"0o12\"      ,         10),\n                (\"0b01010\"   ,         10),\n                (\"10\"        ,         10),\n                (\"10.0\"      ,         10),\n                (\"1e1\"       ,         10),\n            ]\n\n        for _input, expected in values:\n            value = to_number(_input)\n\n            if isinstance(_input, str):\n                cmd = 'to_number(\"{}\")'.format(_input)\n            else:\n                cmd = 'to_number({})'.format(_input)\n\n            print(\"{:23} = {:10} == {:10}\".format(cmd, value, expected))\n            self.assertEqual(value, expected)\nclass test_to_number(unittest.TestCase):\n\n    def test_hex(self):\n        # All of the following should be converted to an integer\n        #\n        values = [\n\n                 #          HEX\n                 # ----------------------\n                 # Input     |   Expected\n                 # ----------------------\n                (0xDEADBEEF  , 3735928559), # Hex\n                (\"0xFEEDFACE\", 4277009102), # Hex\n                (\"0x0\"       ,          0), # Hex\n\n                 #        Decimals\n                 # ----------------------\n                 # Input     |   Expected\n                 # ----------------------\n                (100         ,        100), # Decimal\n                (\"42\"        ,         42), # Decimal\n            ]\n\n\n\n        values += [\n                 #        Octals\n                 # ----------------------\n                 # Input     |   Expected\n                 # ----------------------\n                (0o10        ,          8), # Octal\n                (\"0o20\"      ,         16), # Octal\n                (\"020\"       ,         16), # Octal\n            ]\n\n\n        values += [\n                 #        Floats\n                 # ----------------------\n                 # Input     |   Expected\n                 # ----------------------\n                (3.14        ,       3.14), # Float\n                (\"2.72\"      ,       2.72), # Float\n                (\"1e3\"       ,       1000), # Float\n                (1e-3        ,      0.001), # Float\n            ]\n\n        values += [\n                 #        All ints\n                 # ----------------------\n                 # Input     |   Expected\n                 # ----------------------\n                (\"0xA\"       ,         10),\n                (\"012\"       ,         10),\n                (\"0o12\"      ,         10),\n                (\"0b01010\"   ,         10),\n                (\"10\"        ,         10),\n                (\"10.0\"      ,         10),\n                (\"1e1\"       ,         10),\n            ]\n\n        for _input, expected in values:\n            value = to_number(_input)\n\n            if isinstance(_input, str):\n                cmd = 'to_number(\"{}\")'.format(_input)\n            else:\n                cmd = 'to_number({})'.format(_input)\n\n            print(\"{:23} = {:10} == {:10}\".format(cmd, value, expected))\n            self.assertEqual(value, expected)\n",
                "Pass your string to this function:def string_to_number(str):\n  if(\".\" in str):\n    try:\n      res = float(str)\n    except:\n      res = str\n  elif(str.isdigit()):\n    res = int(str)\n  else:\n    res = str\n  return(res)\ndef string_to_number(str):\n  if(\".\" in str):\n    try:\n      res = float(str)\n    except:\n      res = str\n  elif(str.isdigit()):\n    res = int(str)\n  else:\n    res = str\n  return(res)\nIt will return int, float or string depending on what was passed.String that is an intprint(type(string_to_number(\"124\")))\n<class 'int'>\nprint(type(string_to_number(\"124\")))\n<class 'int'>\nString that is a floatprint(type(string_to_number(\"12.4\")))\n<class 'float'>\nprint(type(string_to_number(\"12.4\")))\n<class 'float'>\nString that is a stringprint(type(string_to_number(\"hello\")))\n<class 'str'>\nprint(type(string_to_number(\"hello\")))\n<class 'str'>\nString that looks like a floatprint(type(string_to_number(\"hel.lo\")))\n<class 'str'>\nprint(type(string_to_number(\"hel.lo\")))\n<class 'str'>\n",
                "There is also regex, because sometimes string must be prepared and normalized before casting to a number:import re\n\ndef parseNumber(value, as_int=False):\n    try:\n        number = float(re.sub('[^.\\-\\d]', '', value))\n        if as_int:\n            return int(number + 0.5)\n        else:\n            return number\n    except ValueError:\n        return float('nan')  # or None if you wish\nimport re\n\ndef parseNumber(value, as_int=False):\n    try:\n        number = float(re.sub('[^.\\-\\d]', '', value))\n        if as_int:\n            return int(number + 0.5)\n        else:\n            return number\n    except ValueError:\n        return float('nan')  # or None if you wish\nUsage:parseNumber('13,345')\n> 13345.0\n\nparseNumber('- 123 000')\n> -123000.0\n\nparseNumber('99999\\n')\n> 99999.0\nparseNumber('13,345')\n> 13345.0\n\nparseNumber('- 123 000')\n> -123000.0\n\nparseNumber('99999\\n')\n> 99999.0\nAnd by the way, something to verify you have a number:import numbers\ndef is_number(value):\n    return isinstance(value, numbers.Number)\n    # Will work with int, float, long, Decimal\nimport numbers\ndef is_number(value):\n    return isinstance(value, numbers.Number)\n    # Will work with int, float, long, Decimal\n",
                "a = int(float(a)) if int(float(a)) == float(a) else float(a)\na = int(float(a)) if int(float(a)) == float(a) else float(a)\n",
                "This is a corrected version of Totoro's answer.This is a corrected versionTotoro's answerThis will try to parse a string and return either int or float depending on what the string represents. It might rise parsing exceptions or have some unexpected behaviour.intfloathave some unexpected behaviour  def get_int_or_float(v):\n        number_as_float = float(v)\n        number_as_int = int(number_as_float)\n        return number_as_int if number_as_float == number_as_int else\n        number_as_float\n  def get_int_or_float(v):\n        number_as_float = float(v)\n        number_as_int = int(number_as_float)\n        return number_as_int if number_as_float == number_as_int else\n        number_as_float\n",
                "If you are dealing with mixed integers and floats and want a consistent way to deal with your mixed data, here is my solution with the proper docstring:docstringdef parse_num(candidate):\n    \"\"\"Parse string to number if possible\n    It work equally well with negative and positive numbers, integers and floats.\n\n    Args:\n        candidate (str): string to convert\n\n    Returns:\n        float | int | None: float or int if possible otherwise None\n    \"\"\"\n    try:\n        float_value = float(candidate)\n    except ValueError:\n        return None\n\n    # Optional part if you prefer int to float when decimal part is 0\n    if float_value.is_integer():\n        return int(float_value)\n    # end of the optional part\n\n    return float_value\n\n# Test\ncandidates = ['34.77', '-13', 'jh', '8990', '76_3234_54']\nres_list = list(map(parse_num, candidates))\nprint('Before:')\nprint(candidates)\nprint('After:')\nprint(res_list)\ndef parse_num(candidate):\n    \"\"\"Parse string to number if possible\n    It work equally well with negative and positive numbers, integers and floats.\n\n    Args:\n        candidate (str): string to convert\n\n    Returns:\n        float | int | None: float or int if possible otherwise None\n    \"\"\"\n    try:\n        float_value = float(candidate)\n    except ValueError:\n        return None\n\n    # Optional part if you prefer int to float when decimal part is 0\n    if float_value.is_integer():\n        return int(float_value)\n    # end of the optional part\n\n    return float_value\n\n# Test\ncandidates = ['34.77', '-13', 'jh', '8990', '76_3234_54']\nres_list = list(map(parse_num, candidates))\nprint('Before:')\nprint(candidates)\nprint('After:')\nprint(res_list)\nOutput:Before:\n['34.77', '-13', 'jh', '8990', '76_3234_54']\n\nAfter:\n[34.77, -13, None, 8990, 76323454]\nBefore:\n['34.77', '-13', 'jh', '8990', '76_3234_54']\n\nAfter:\n[34.77, -13, None, 8990, 76323454]\n",
                "Use:def num(s):\n    try:\n        for each in s:\n            yield int(each)\n    except ValueError:\n        yield float(each)\na = num([\"123.55\",\"345\",\"44\"])\nprint a.next()\nprint a.next()\ndef num(s):\n    try:\n        for each in s:\n            yield int(each)\n    except ValueError:\n        yield float(each)\na = num([\"123.55\",\"345\",\"44\"])\nprint a.next()\nprint a.next()\nThis is the most Pythonic way I could come up with. ",
                "You can simply do this bys = '542.22'\n\nf = float(s) # This converts string data to float data with a decimal point\nprint(f) \n\ni = int(f) # This converts string data to integer data by just taking the whole number part of it\nprint(i) \ns = '542.22'\n\nf = float(s) # This converts string data to float data with a decimal point\nprint(f) \n\ni = int(f) # This converts string data to integer data by just taking the whole number part of it\nprint(i) \nFor more information on parsing of data types check on python documentation!",
                "This is a function which will convert any object (not just str) to int or float, based on if the actual string supplied looks like int or float. Further if it's an object which has both __float and __int__ methods, it defaults to using __float__objectstrintfloatlooks likeintfloat__float__int____float__def conv_to_num(x, num_type='asis'):\n    '''Converts an object to a number if possible.\n    num_type: int, float, 'asis'\n    Defaults to floating point in case of ambiguity.\n    '''\n    import numbers\n\n    is_num, is_str, is_other = [False]*3\n\n    if isinstance(x, numbers.Number):\n        is_num = True\n    elif isinstance(x, str):\n        is_str = True\n\n    is_other = not any([is_num, is_str])\n\n    if is_num:\n        res = x\n    elif is_str:\n        is_float, is_int, is_char = [False]*3\n        try:\n            res = float(x)\n            if '.' in x:\n                is_float = True\n            else:\n                is_int = True\n        except ValueError:\n            res = x\n            is_char = True\n\n    else:\n        if num_type == 'asis':\n            funcs = [int, float]\n        else:\n            funcs = [num_type]\n\n        for func in funcs:\n            try:\n                res = func(x)\n                break\n            except TypeError:\n                continue\n        else:\n            res = x\ndef conv_to_num(x, num_type='asis'):\n    '''Converts an object to a number if possible.\n    num_type: int, float, 'asis'\n    Defaults to floating point in case of ambiguity.\n    '''\n    import numbers\n\n    is_num, is_str, is_other = [False]*3\n\n    if isinstance(x, numbers.Number):\n        is_num = True\n    elif isinstance(x, str):\n        is_str = True\n\n    is_other = not any([is_num, is_str])\n\n    if is_num:\n        res = x\n    elif is_str:\n        is_float, is_int, is_char = [False]*3\n        try:\n            res = float(x)\n            if '.' in x:\n                is_float = True\n            else:\n                is_int = True\n        except ValueError:\n            res = x\n            is_char = True\n\n    else:\n        if num_type == 'asis':\n            funcs = [int, float]\n        else:\n            funcs = [num_type]\n\n        for func in funcs:\n            try:\n                res = func(x)\n                break\n            except TypeError:\n                continue\n        else:\n            res = x\n",
                "By using int and float methods we can convert a string to integer and floats.s=\"45.8\"\nprint(float(s))\n\ny='67'\nprint(int(y))\ns=\"45.8\"\nprint(float(s))\n\ny='67'\nprint(int(y))\n",
                "For numbers and characters together:string_for_int = \"498 results should get\"\nstring_for_float = \"498.45645765 results should get\"\nstring_for_int = \"498 results should get\"\nstring_for_float = \"498.45645765 results should get\"\nFirst import re:re import re\n\n # For getting the integer part:\n print(int(re.search(r'\\d+', string_for_int).group())) #498\n\n # For getting the float part:\n print(float(re.search(r'\\d+\\.\\d+', string_for_float).group())) #498.45645765\n import re\n\n # For getting the integer part:\n print(int(re.search(r'\\d+', string_for_int).group())) #498\n\n # For getting the float part:\n print(float(re.search(r'\\d+\\.\\d+', string_for_float).group())) #498.45645765\nFor easy model:value1 = \"10\"\nvalue2 = \"10.2\"\nprint(int(value1)) # 10\nprint(float(value2)) # 10.2\nvalue1 = \"10\"\nvalue2 = \"10.2\"\nprint(int(value1)) # 10\nprint(float(value2)) # 10.2\n",
                "If you don't want to use third party modules the following might be the most robust solution:def string_to_int_or_float(s):\n    try:\n        f = float(s) # replace s with str(s) if you are not sure that s is a string\n    except ValueError:\n        print(\"Provided string '\" + s + \"' is not interpretable as a literal number.\")\n        raise\n    try:\n        i = int(str(f).rstrip('0').rstrip('.'))\n    except:\n        return f\n    return i\ndef string_to_int_or_float(s):\n    try:\n        f = float(s) # replace s with str(s) if you are not sure that s is a string\n    except ValueError:\n        print(\"Provided string '\" + s + \"' is not interpretable as a literal number.\")\n        raise\n    try:\n        i = int(str(f).rstrip('0').rstrip('.'))\n    except:\n        return f\n    return i\nIt might not be the fastest, but it handles correctly literal numbers where many other solutions fail, such as:>>> string_to_int_or_float('789.')\n789\n>>> string_to_int_or_float('789.0')\n789\n>>> string_to_int_or_float('12.3e2')\n1230\n>>> string_to_int_or_float('12.3e-2')\n0.123\n>>> string_to_int_or_float('4560e-1')\n456\n>>> string_to_int_or_float('4560e-2')\n45.6\n>>> string_to_int_or_float('789.')\n789\n>>> string_to_int_or_float('789.0')\n789\n>>> string_to_int_or_float('12.3e2')\n1230\n>>> string_to_int_or_float('12.3e-2')\n0.123\n>>> string_to_int_or_float('4560e-1')\n456\n>>> string_to_int_or_float('4560e-2')\n45.6\n"
            ]
        },
        {
            "tag": "regex_no_words",
            "patterns": [
                "Regular expression to match a line that doesn't contain a word",
                "How to write a regular expression for no words",
                "get a line without words using regular expressions"
            ],
            "responses": [
                "The notion that regex doesn't support inverse matching is not entirely true. You can mimic this behavior by using negative look-arounds:Non-capturing variant:The regex above will match any string, or line without a line break, not containing the (sub)string 'hede'. As mentioned, this is not something regex is \"good\" at (or should do), but still, it is possible.And if you need to match line break chars as well, use the DOT-ALL modifier (the trailing s in the following pattern):or use it inline:(where the /.../ are the regex delimiters, i.e., not part of the pattern)If the DOT-ALL modifier is not available, you can mimic the same behavior with the character class [\\s\\S]:A string is just a list of n characters. Before, and after each character, there's an empty string. So a list of n characters will have n+1 empty strings. Consider the string \"ABhedeCD\":where the e's are the empty strings. The regex (?!hede). looks ahead to see if there's no substring \"hede\" to be seen, and if that is the case (so something else is seen), then the . (dot) will match any character except a line break. Look-arounds are also called zero-width-assertions because they don't consume any characters. They only assert/validate something.So, in my example, every empty string is first validated to see if there's no \"hede\" up ahead, before a character is consumed by the . (dot). The regex (?!hede). will do that only once, so it is wrapped in a group, and repeated zero or more times: ((?!hede).)*. Finally, the start- and end-of-input are anchored to make sure the entire input is consumed: ^((?!hede).)*$As you can see, the input \"ABhedeCD\" will fail because on e3, the regex (?!hede) fails (there is \"hede\" up ahead!).",
                "Note that the solution to does not start with \u201chede\u201d:is generally much more efficient than the solution to does not contain \u201chede\u201d:The former checks for \u201chede\u201d only at the input string\u2019s first position, rather than at every position.",
                "If you're just using it for grep, you can use grep -v hede to get all lines which do not contain hede.ETA Oh, rereading the question, grep -v is probably what you meant by \"tools options\".",
                "Answer:Explanation:^the beginning of the string,\n( group and capture to \\1 (0 or more times (matching the most amount possible)),\n(?! look ahead to see if there is not,hede your string,) end of look-ahead, \n. any character except \\n,\n)* end of \\1   (Note: because you are using a quantifier on this capture, only the LAST repetition of the captured pattern will be stored in \\1)\n$ before an optional \\n, and the end of the string",
                "The given answers are perfectly fine, just an academic point:Regular Expressions in the meaning of theoretical computer sciences ARE NOT ABLE do it like this. For them it had to look something like this:This only does a FULL match. Doing it for sub-matches would even be more awkward.",
                "If you want the regex test to only fail if the entire string matches, the following will work:e.g. -- If you want to allow all values except \"foo\" (i.e. \"foofoo\", \"barfoo\", and \"foobar\" will pass, but \"foo\" will fail), use: ^(?!foo$).*Of course, if you're checking for exact equality, a better general solution in this case is to check for string equality, i.e.You could even put the negation outside the test if you need any regex features (here, case insensitivity and range matching):The regex solution at the top of this answer may be helpful, however, in situations where a positive regex test is required (perhaps by an API).",
                "FWIW, since regular languages (aka rational languages) are closed under complementation, it's always possible to find a regular expression (aka rational expression) that negates another expression. But not many tools implement this.Vcsn supports this operator (which it denotes {c}, postfix).You first define the type of your expressions: labels are letter (lal_char) to pick from a to z for instance (defining the alphabet when working with complementation is, of course, very important), and the \"value\" computed for each word is just a Boolean: true the word is accepted, false, rejected.In Python:then you enter your expression:convert this expression to an automaton:finally, convert this automaton back to a simple expression.where + is usually denoted |, \\e denotes the empty word, and [^] is usually written . (any character).  So, with a bit of rewriting ()|h(ed?)?|([^h]|h([^e]|e([^d]|d([^e]|e.)))).*.You can see this example here, and try Vcsn online there.",
                "Here's a good explanation of why it's not easy to negate an arbitrary regex. I have to agree with the other answers, though: if this is anything other than a hypothetical question, then a regex is not the right choice here.",
                "With negative lookahead, regular expression can match something not contains specific pattern. This is answered and explained by Bart Kiers. Great explanation!However, with Bart Kiers' answer, the lookahead part will test 1 to 4 characters ahead while matching any single character. We can avoid this and let the lookahead part check out the whole text, ensure there is no 'hede', and then the normal part (.*) can eat the whole text all at one time.Here is the improved regex:Note the (*?) lazy quantifier in the negative lookahead part is optional, you can use (*) greedy quantifier instead, depending on your data: if 'hede' does present and in the beginning half of the text, the lazy quantifier can be faster; otherwise, the greedy quantifier be faster. However if 'hede' does not present, both would be equal slow.Here is the demo code.For more information about lookahead, please check out the great article: Mastering Lookahead and Lookbehind.Also, please check out RegexGen.js, a JavaScript Regular Expression Generator that helps to construct complex regular expressions. With RegexGen.js, you can construct the regex in a more readable way:",
                "I decided to evaluate some of the presented Options and compare their performance, as well as use some new Features.\nBenchmarking on .NET Regex Engine: http://regexhero.net/tester/The first 7 lines should not match, since they contain the searched Expression, while the lower 7 lines should match!Results are Iterations per second as the median of 3 runs - Bigger Number = BetterSince .NET doesn't support action Verbs (*FAIL, etc.) I couldn't test the solutions P1 and P2.The overall most readable and performance-wise fastest solution seems to be 03 with a simple negative lookahead. This is also the fastest solution for JavaScript, since JS does not support the more advanced Regex Features for the other solutions.",
                "Not regex, but I've found it logical and useful to use serial greps with pipe to eliminate noise.eg.  search an apache config file without all the comments-andThe logic of serial grep's is (not a comment) and (matches dir)",
                "with this, you avoid to test a lookahead on each positions:equivalent to (for .net):Old answer:",
                "Since no one else has given a direct answer to the question that was asked, I'll do it.The answer is that with POSIX grep, it's impossible to literally satisfy this request:The reason is that POSIX grep is only required to work with Basic Regular Expressions, which are simply not powerful enough for accomplishing that task (they are not capable of parsing all regular languages, because of lack of alternation).However, GNU grep implements extensions that allow it. In particular, \\| is the alternation operator in GNU's implementation of BREs. If your regular expression engine supports alternation, parentheses and the Kleene star, and is able to anchor to the beginning and end of the string, that's all you need for this approach. Note however that negative sets [^ ... ] are very convenient in addition to those, because otherwise, you need to replace them with an expression of the form (a|b|c| ... ) that lists every character that is not in the set, which is extremely tedious and overly long, even more so if the whole character set is Unicode.Thanks to formal language theory, we get to see how such an expression looks like. With GNU grep, the answer would be something like:(found with Grail and some further optimizations made by hand).You can also use a tool that implements Extended Regular Expressions, like egrep, to get rid of the backslashes:Here's a script to test it (note it generates a file testinput.txt in the current directory). Several of the expressions presented fail this test.In my system it prints:as expected.For those interested in the details, the technique employed is to convert the regular expression that matches the word into a finite automaton, then invert the automaton by changing every acceptance state to non-acceptance and vice versa, and then converting the resulting FA back to a regular expression.As everyone has noted, if your regular expression engine supports negative lookahead, the regular expression is much simpler. For example, with GNU grep:However, this approach has the disadvantage that it requires a backtracking regular expression engine. This makes it unsuitable in installations that are using secure regular expression engines like RE2, which is one reason to prefer the generated approach in some circumstances.Using Kendall Hopkins' excellent FormalTheory library, written in PHP, which provides a functionality similar to Grail, and a simplifier written by myself, I've been able to write an online generator of negative regular expressions given an input phrase (only alphanumeric and space characters currently supported): http://www.formauri.es/personal/pgimeno/misc/non-match-regex/For hede it outputs:which is equivalent to the above.",
                "Aforementioned (?:(?!hede).)* is great because it can be anchored.But the following would suffice in this case:This simplification is ready to have \"AND\" clauses added:",
                "An, in my opinon, more readable variant of the top answer:Basically, \"match at the beginning of the line if and only if it does not have 'hede' in it\" - so the requirement translated almost directly into regex.Of course, it's possible to have multiple failure requirements:Details: The ^ anchor ensures the regex engine doesn't retry the match at every location in the string, which would match every string.The ^ anchor in the beginning is meant to represent the beginning of the line. The grep tool matches each line one at a time, in contexts where you're working with a multiline string, you can use the \"m\" flag:or",
                "Here's how I'd do it:Accurate and more efficient than the other answers. It implements Friedl's \"unrolling-the-loop\" efficiency technique and requires much less backtracking.",
                "Another option is that to add a positive look-ahead and check if hede is anywhere in the input line, then we would negate that, with an expression similar to:with word boundaries.The expression is explained on the top right panel of regex101.com, if you wish to explore/simplify/modify it, and in this link, you can watch how it would match against some sample inputs, if you like.jex.im visualizes regular expressions:",
                "If you want to match a character to negate a word similar to negate character class:For example, a string:Do not use:Use:Notice \"(?!bbb).\" is neither lookbehind nor lookahead, it's lookcurrent, for example:",
                "The OP did not specify or Tag the post to indicate the context (programming language, editor, tool) the Regex will be used within.For me, I sometimes need to do this while editing a file using Textpad.Textpad supports some Regex, but does not support lookahead or lookbehind, so it takes a few steps.If I am looking to retain all lines that Do NOT contain the string hede, I would do it like this:1. Search/replace the entire file to add a unique \"Tag\" to the beginning of each line containing any text.2. Delete all lines that contain the string hede (replacement string is empty):3. At this point, all remaining lines Do NOT contain the string hede. Remove the unique \"Tag\" from all lines (replacement string is empty):Now you have the original text with all lines containing the string hede removed.If I am looking to Do Something Else to only lines that Do NOT contain the string hede, I would do it like this:1. Search/replace the entire file to add a unique \"Tag\" to the beginning of each line containing any text.2. For all lines that contain the string hede, remove the unique \"Tag\":3. At this point, all lines that begin with the unique \"Tag\", Do NOT contain the string hede. I can now do my Something Else to only those lines.4. When I am done, I remove the unique \"Tag\" from all lines (replacement string is empty):",
                "Since the introduction of ruby-2.4.1, we can use the new Absent Operator in Ruby\u2019s Regular Expressionsfrom the official docThus, in your case ^(?~hede)$ does the job for you",
                "Through PCRE verb (*SKIP)(*F)This would completely skips the line which contains the exact string hede and matches all the remaining lines.DEMOExecution of the parts:Let us consider the above regex by splitting it into two parts.Part before the | symbol. Part shouldn't be matched.Part after the | symbol. Part should be matched.PART 1Regex engine will start its execution from the first part.Explanation:So the line which contains the string hede would be matched. Once the regex engine sees the following (*SKIP)(*F) (Note: You could write (*F) as (*FAIL)) verb, it skips and make the match to fail. | called alteration or logical OR operator added next to the PCRE verb which inturn matches all the boundaries exists between each and every character on all the lines except the line contains the exact string hede. See the demo here. That is, it tries to match the characters from the remaining string. Now the regex in the second part would be executed.PART 2Explanation:.* In the Multiline mode, . would match any character except newline or carriage return characters. And * would repeat the previous character zero or more times. So .* would match the whole line. See the demo here.Hey why you added .* instead of .+ ?Because .* would match a blank line but .+ won't match a blank. We want to match all the lines except hede , there may be a possibility of blank lines also in the input . so you must use .* instead of .+ . .+ would repeat the previous character one or more times. See .* matches a blank line here.$ End of the line anchor is not necessary here.",
                "The TXR Language supports regex negation.A more complicated example: match all lines that start with a and end with z, but do not contain the substring hede:Regex negation is not particularly useful on its own but when you also have intersection, things get interesting, since you have a full set of boolean set operations: you can express \"the set which matches this, except for things which match that\".",
                "It may be more maintainable to two regexes in your code, one to do the first match, and then if it matches run the second regex to check for outlier cases you wish to block for example ^.*(hede).* then have appropriate logic in your code.OK, I admit this is not really an answer to the posted question posted and it may also use slightly more processing than a single regex. But for developers who came here looking for a fast emergency fix for an outlier case then this solution should not be overlooked.",
                "The below function will help you get your desired output",
                "I wanted to add another example for if you are trying to match an entire line that contains string X, but does not also contain string Y.For example, let's say we want to check if our URL / string contains \"tasty-treats\", so long as it does not also contain \"chocolate\" anywhere.This regex pattern would work (works in JavaScript too)(global, multiline flags in example)Interactive Example: https://regexr.com/53gv4(These urls contain \"tasty-treats\" and also do not contain \"chocolate\")(These urls contain \"chocolate\" somewhere - so they won't match even though they contain \"tasty-treats\")",
                "As long as you are dealing with lines, simply mark the negative matches and target the rest.In fact, I use this trick with sed because ^((?!hede).)*$ looks not supported by it.Mark the negative match: (e.g. lines with hede), using a character not included in the whole text at all. An emoji could probably be a good choice for this purpose.Target the rest (the unmarked strings: e.g. lines without hede). Suppose you want to keep only the target and delete the rest (as you want):Suppose you want to delete the target:Mark the negative match: (e.g. lines with hede), using a character not included in the whole text at all. An emoji could probably be a good choice for this purpose.Target the rest (the unmarked strings: e.g. lines without hede). Suppose you want to delete the target:Remove the mark:",
                "^((?!hede).)*$ is an elegant solution, except since it consumes characters you won't be able to combine it with other criteria. For instance, say you wanted to check for the non-presence of \"hede\" and the presence of \"haha.\" This solution would work because it won't consume characters:",
                "Here's a method that I haven't seen used before:First, it tries to find \"hede\" somewhere in the line. If successful, at this point, (*COMMIT) tells the engine to, not only not backtrack in the event of a failure, but also not to attempt any further matching in that case. Then, we try to match something that cannot possibly match (in this case, ^).If a line does not contain \"hede\" then the second alternative, an empty subpattern, successfully matches the subject string.This method is no more efficient than a negative lookahead, but I figured I'd just throw it on here in case someone finds it nifty and finds a use for it for other, more interesting applications.",
                "A simpler solution is to use the not operator !Your if statement will need to match \"contains\" and not match \"excludes\".I believe the designers of RegEx anticipated the use of not operators.",
                "Maybe you'll find this on Google while trying to write a regex that is able to match segments of a line (as opposed to entire lines) which do not contain a substring. Tooke me a while to figure out, so I'll share:Given a string: \n\n<span class=\"good\">bar</span><span class=\"bad\">foo</span><span class=\"ugly\">baz</span>I want to match <span> tags which do not contain the substring \"bad\"./<span(?:(?!bad).)*?> will match <span class=\\\"good\\\"> and <span class=\\\"ugly\\\">.Notice that there are two sets (layers) of parentheses:Demo in Ruby:"
            ]
        }
    ]
}