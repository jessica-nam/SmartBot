{
    "intents": [
        {
            "tag": "python",
            "question": [
                "What does the \"yield\" keyword do?"
            ],
            "votes": "12447",
            "answer": "['To understand what yield does, you must understand what generators are. And before you can understand generators, you must understand iterables.When you create a list, you can read its items one by one. Reading its items one by one is called iteration:mylist is an iterable. When you use a list comprehension, you create a list, and so an iterable:Everything you can use \"for... in...\" on is an iterable; lists, strings, files...These iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values.Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly:It is just the same except you used () instead of []. BUT, you cannot perform for i in mygenerator a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one.yield is a keyword that is used like return, except the function will return a generator.Here it\\'s a useless example, but it\\'s handy when you know your function will return a huge set of values that you will only need to read once.To master yield, you must understand that when you call the function, the code you have written in the function body does not run. The function only returns the generator object, this is a bit tricky.Then, your code will continue from where it left off each time for uses the generator.Now the hard part:The first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it\\'ll return the first value of the loop. Then, each subsequent call will run another iteration of the loop you have written in the function and return the next value. This will continue until the generator is considered empty, which happens when the function runs without hitting yield. That can be because the loop has come to an end, or because you no longer satisfy an \"if/else\".Generator:Caller:This code contains several smart parts:The loop iterates on a list, but the list expands while the loop is being iterated. It\\'s a concise way to go through all these nested data even if it\\'s a bit dangerous since you can end up with an infinite loop. In this case, candidates.extend(node._get_child_candidates(distance, min_dist, max_dist)) exhausts all the values of the generator, but while keeps creating new generator objects which will produce different values from the previous ones since it\\'s not applied on the same node.The extend() method is a list object method that expects an iterable and adds its values to the list.Usually, we pass a list to it:But in your code, it gets a generator, which is good because:And it works because Python does not care if the argument of a method is a list or not. Python expects iterables so it will work with strings, lists, tuples, and generators! This is called duck typing and is one of the reasons why Python is so cool. But this is another story, for another question...You can stop here, or read a little bit to see an advanced use of a generator:Note: For Python 3, useprint(corner_street_atm.__next__()) or print(next(corner_street_atm))It can be useful for various things like controlling access to a resource.The itertools module contains special functions to manipulate iterables. Ever wish to duplicate a generator?\\nChain two generators? Group values in a nested list with a one-liner? Map / Zip without creating another list?Then just import itertools.An example? Let\\'s see the possible orders of arrival for a four-horse race:Iteration is a process implying iterables (implementing the __iter__() method) and iterators (implementing the __next__() method).\\nIterables are any objects you can get an iterator from. Iterators are objects that let you iterate on iterables.There is more about it in this article about how for loops work.', \"When you see a function with yield statements, apply this easy trick to understand what will happen:This trick may give you an idea of the logic behind the function, but what actually happens with yield is significantly different than what happens in the list-based approach. In many cases, the yield approach will be a lot more memory efficient and faster too. In other cases, this trick will get you stuck in an infinite loop, even though the original function works just fine. Read on to learn more...First, the iterator protocol - when you writePython performs the following two steps:Gets an iterator for mylist:Call iter(mylist) -> this returns an object with a next() method (or __next__() in Python 3).[This is the step most people forget to tell you about]Uses the iterator to loop over items:Keep calling the next() method on the iterator returned from step 1. The return value from next() is assigned to x and the loop body is executed. If an exception StopIteration is raised from within next(), it means there are no more values in the iterator and the loop is exited.The truth is Python performs the above two steps anytime it wants to loop over the contents of an object - so it could be a for loop, but it could also be code like otherlist.extend(mylist) (where otherlist is a Python list).Here mylist is an iterable because it implements the iterator protocol. In a user-defined class, you can implement the __iter__() method to make instances of your class iterable. This method should return an iterator. An iterator is an object with a next() method. It is possible to implement both __iter__() and next() on the same class, and have __iter__() return self. This will work for simple cases, but not when you want two iterators looping over the same object at the same time.So that's the iterator protocol, many objects implement this protocol:Note that a for loop doesn't know what kind of object it's dealing with - it just follows the iterator protocol, and is happy to get item after item as it calls next(). Built-in lists return their items one by one, dictionaries return the keys one by one, files return the lines one by one, etc. And generators return... well that's where yield comes in:Instead of yield statements, if you had three return statements in f123() only the first would get executed, and the function would exit. But f123() is no ordinary function. When f123() is called, it does not return any of the values in the yield statements! It returns a generator object. Also, the function does not really exit - it goes into a suspended state. When the for loop tries to loop over the generator object, the function resumes from its suspended state at the very next line after the yield it previously returned from, executes the next line of code, in this case, a yield statement, and returns that as the next item. This happens until the function exits, at which point the generator raises StopIteration, and the loop exits.So the generator object is sort of like an adapter - at one end it exhibits the iterator protocol, by exposing __iter__() and next() methods to keep the for loop happy. At the other end, however, it runs the function just enough to get the next value out of it, and puts it back in suspended mode.Usually, you can write code that doesn't use generators but implements the same logic. One option is to use the temporary list 'trick' I mentioned before. That will not work in all cases, for e.g. if you have infinite loops, or it may make inefficient use of memory when you have a really long list. The other approach is to implement a new iterable class SomethingIter that keeps the state in instance members and performs the next logical step in its next() (or __next__() in Python 3) method. Depending on the logic, the code inside the next() method may end up looking very complex and prone to bugs. Here generators provide a clean and easy solution.\", \"Think of it this way:An iterator is just a fancy sounding term for an object that has a next() method.  So a yield-ed function ends up being something like this:Original version:This is basically what the Python interpreter does with the above code:For more insight as to what's happening behind the scenes, the for loop can be rewritten to this:Does that make more sense or just confuse you more?  :)I should note that this is an oversimplification for illustrative purposes. :)\", 'The yield keyword is reduced to two simple facts:In a nutshell: Most commonly, a generator is a lazy, incrementally-pending list, and yield statements allow you to use function notation to program the list values the generator should incrementally spit out. Furthermore, advanced usage lets you use generators as coroutines (see below).Basically, whenever the yield statement is encountered, the function pauses and saves its state, then emits \"the next return value in the \\'list\\'\" according to the python iterator protocol (to some syntactic construct like a for-loop that repeatedly calls next() and catches a StopIteration exception, etc.). You might have encountered generators with generator expressions; generator functions are more powerful because you can pass arguments back into the paused generator function, using them to implement coroutines. More on that later.Let\\'s define a function makeRange that\\'s just like Python\\'s range. Calling makeRange(n) RETURNS A GENERATOR:To force the generator to immediately return its pending values, you can pass it into list() (just like you could any iterable):The above example can be thought of as merely creating a list which you append to and return:There is one major difference, though; see the last section.An iterable is the last part of a list comprehension, and all generators are iterable, so they\\'re often used like so:To get a better feel for generators, you can play around with the itertools module (be sure to use chain.from_iterable rather than chain when warranted). For example, you might even use generators to implement infinitely-long lazy lists like itertools.count(). You could implement your own def enumerate(iterable): zip(count(), iterable), or alternatively do so with the yield keyword in a while-loop.Please note: generators can actually be used for many more things, such as implementing coroutines or non-deterministic programming or other elegant things. However, the \"lazy lists\" viewpoint I present here is the most common use you will find.This is how the \"Python iteration protocol\" works. That is, what is going on when you do list(makeRange(5)). This is what I describe earlier as a \"lazy, incremental list\".The built-in function next() just calls the objects .__next__() function, which is a part of the \"iteration protocol\" and is found on all iterators. You can manually use the next() function (and other parts of the iteration protocol) to implement fancy things, usually at the expense of readability, so try to avoid doing that...Coroutine example:A coroutine (generators which generally accept input via the yield keyword e.g. nextInput = yield nextOutput, as a form of two-way communication) is basically a computation which is allowed to pause itself and request input (e.g. to what it should do next). When the coroutine pauses itself (when the running coroutine eventually hits a yield keyword), the computation is paused and control is inverted (yielded) back to the \\'calling\\' function (the frame which requested the next value of the computation). The paused generator/coroutine remains paused until another invoking function (possibly a different function/context) requests the next value to unpause it (usually passing input data to direct the paused logic interior to the coroutine\\'s code).You can think of python coroutines as lazy incrementally-pending lists, where the next element doesn\\'t just depend on the previous computation, but also on input you may opt to inject during the generation process.Normally, most people would not care about the following distinctions and probably want to stop reading here.In Python-speak, an iterable is any object which \"understands the concept of a for-loop\" like a list [1,2,3], and an iterator is a specific instance of the requested for-loop like [1,2,3].__iter__(). A generator is exactly the same as any iterator, except for the way it was written (with function syntax).When you request an iterator from a list, it creates a new iterator. However, when you request an iterator from an iterator (which you would rarely do), it just gives you a copy of itself.Thus, in the unlikely event that you are failing to do something like this...... then remember that a generator is an iterator; that is, it is one-time-use. If you want to reuse it, you should call myRange(...) again. If you need to use the result twice, convert the result to a list and store it in a variable x = list(myRange(5)). Those who absolutely need to clone a generator (for example, who are doing terrifyingly hackish metaprogramming) can use itertools.tee (still works in Python 3) if absolutely necessary, since the copyable iterator Python PEP standards proposal has been deferred.', 'What does the yield keyword do in Python?yield is only legal inside of a function definition, and the inclusion of yield in a function definition makes it return a generator.The idea for generators comes from other languages (see footnote 1) with varying implementations. In Python\\'s Generators, the execution of the code is frozen at the point of the yield. When the generator is called (methods are discussed below) execution resumes and then freezes at the next yield.yield provides an\\neasy way of implementing the iterator protocol, defined by the following two methods:\\n__iter__ and __next__.  Both of those methods\\nmake an object an iterator that you could type-check with the Iterator Abstract Base\\nClass from the collections module.Let\\'s do some introspection:The generator type is a sub-type of iterator:And if necessary, we can type-check like this:A feature of an Iterator is that once exhausted, you can\\'t reuse or reset it:You\\'ll have to make another if you want to use its functionality again (see footnote 2):One can yield data programmatically, for example:The above simple generator is also equivalent to the below - as of Python 3.3 you can use yield from:However, yield from also allows for delegation to subgenerators,\\nwhich will be explained in the following section on cooperative delegation with sub-coroutines.yield forms an expression that allows data to be sent into the generator (see footnote 3)Here is an example, take note of the received variable, which will point to the data that is sent to the generator:First, we must queue up the generator with the builtin function, next. It will\\ncall the appropriate next or __next__ method, depending on the version of\\nPython you are using:And now we can send data into the generator. (Sending None is\\nthe same as calling next.) :Now, recall that yield from is available in Python 3. This allows us to delegate coroutines to a subcoroutine:And now we can delegate functionality to a sub-generator and it can be used\\nby a generator just as above:Now simulate adding another 1,000 to the account plus the return on the account (60.0):You can read more about the precise semantics of yield from in PEP 380.The close method raises GeneratorExit at the point the function\\nexecution was frozen. This will also be called by __del__ so you\\ncan put any cleanup code where you handle the GeneratorExit:You can also throw an exception which can be handled in the generator\\nor propagated back to the user:Raises:I believe I have covered all aspects of the following question:What does the yield keyword do in Python?It turns out that yield does a lot. I\\'m sure I could add even more\\nthorough examples to this. If you want more or have some constructive criticism, let me know by commenting\\nbelow.The top/accepted answer is a very incomplete answer.The grammar currently allows any expression in a list comprehension.Since yield is an expression, it has been touted by some as interesting to use it in comprehensions or generator expression - in spite of citing no particularly good use-case.The CPython core developers are discussing deprecating its allowance.\\nHere\\'s a relevant post from the mailing list:On 30 January 2017 at 19:05, Brett Cannon  wrote:On Sun, 29 Jan 2017 at 16:39 Craig Rodrigues  wrote:I\\'m OK with either approach.  Leaving things the way they are in Python 3\\nis no good, IMHO.My vote is it be a SyntaxError since you\\'re not getting what you expect from\\nthe syntax.I\\'d agree that\\'s a sensible place for us to end up, as any code\\nrelying on the current behaviour is really too clever to be\\nmaintainable.In terms of getting there, we\\'ll likely want:Cheers, Nick.--  Nick Coghlan   |   ncoghlan at gmail.com   |   Brisbane, AustraliaFurther, there is an outstanding issue (10544) which seems to be pointing in the direction of this never being a good idea (PyPy, a Python implementation written in Python, is already raising syntax warnings.)Bottom line, until the developers of CPython tell us otherwise: Don\\'t put yield in a generator expression or comprehension.In Python 3:In a generator function, the return statement indicates that the generator is done and will cause StopIteration to be raised. The returned value (if any) is used as an argument to construct StopIteration and becomes the StopIteration.value attribute.Historical note, in Python 2:\\n\"In a generator function, the return statement is not allowed to include an expression_list. In that context, a bare return indicates that the generator is done and will cause StopIteration to be raised.\"\\nAn expression_list is basically any number of expressions separated by commas - essentially, in Python 2, you can stop the generator with return, but you can\\'t return a value.The languages CLU, Sather, and Icon were referenced in the proposal\\nto introduce the concept of generators to Python. The general idea is\\nthat a function can maintain internal state and yield intermediate\\ndata points on demand by the user. This promised to be superior in performance\\nto other approaches, including Python threading, which isn\\'t even available on some systems.This means, for example, that range objects aren\\'t Iterators, even though they are iterable, because they can be reused. Like lists, their __iter__ methods return iterator objects.yield was originally introduced as a statement, meaning that it\\ncould only appear at the beginning of a line in a code block.\\nNow yield creates a yield expression.\\nhttps://docs.python.org/2/reference/simple_stmts.html#grammar-token-yield_stmt\\nThis change was proposed to allow a user to send data into the generator just as\\none might receive it. To send data, one must be able to assign it to something, and\\nfor that, a statement just won\\'t work.', \"yield is just like return - it returns whatever you tell it to (as a generator). The difference is that the next time you call the generator, execution starts from the last call to the yield statement. Unlike return, the stack frame is not cleaned up when a yield occurs, however control is transferred back to the caller, so its state will resume the next time the function is called.In the case of your code, the function get_child_candidates is acting like an iterator so that when you extend your list, it adds one element at a time to the new list.list.extend calls an iterator until it's exhausted. In the case of the code sample you posted, it would be much clearer to just return a tuple and append that to the list.\", \"There's one extra thing to mention: a function that yields doesn't actually have to terminate. I've written code like this:Then I can use it in other code like this:It really helps simplify some problems, and makes some things easier to work with.\", 'For those who prefer a minimal working example, meditate on this interactive Python session:', 'TL;DRWhenever you find yourself building a list from scratch, yield each piece instead.This was my first \"aha\" moment with yield.yield is a sugary way to saybuild a series of stuffSame behavior:Different behavior:Yield is single-pass: you can only iterate through once. When a function has a yield in it we call it a generator function. And an iterator is what it returns. Those terms are revealing. We lose the convenience of a container, but gain the power of a series that\\'s computed as needed, and arbitrarily long.Yield is lazy, it puts off computation. A function with a yield in it doesn\\'t actually execute at all when you call it. It returns an iterator object that remembers where it left off. Each time you call next() on the iterator (this happens in a for-loop) execution inches forward to the next yield. return raises StopIteration and ends the series (this is the natural end of a for-loop).Yield is versatile. Data doesn\\'t have to be stored all together, it can be made available one at a time. It can be infinite.If you need multiple passes and the series isn\\'t too long, just call list() on it:Brilliant choice of the word yield because both meanings apply:yield \u2014 produce or provide (as in agriculture)...provide the next data in the series.yield \u2014 give way or relinquish (as in political power)...relinquish CPU execution until the iterator advances.', 'Yield gives you a generator.As you can see, in the first case foo holds the entire list in memory at once. It\\'s not a big deal for a list with 5 elements, but what if you want a list of 5 million? Not only is this a huge memory eater, it also costs a lot of time to build at the time that the function is called.In the second case, bar just gives you a generator. A generator is an iterable--which means you can use it in a for loop, etc, but each value can only be accessed once. All the values are also not stored in memory at the same time; the generator object \"remembers\" where it was in the looping the last time you called it--this way, if you\\'re using an iterable to (say) count to 50 billion, you don\\'t have to count to 50 billion all at once and store the 50 billion numbers to count through.Again, this is a pretty contrived example, you probably would use itertools if you really wanted to count to 50 billion. :)This is the most simple use case of generators. As you said, it can be used to write efficient permutations, using yield to push things up through the call stack instead of using some sort of stack variable. Generators can also be used for specialized tree traversal, and all manner of other things.', 'It\\'s returning a generator. I\\'m not particularly familiar with Python, but I believe it\\'s the same kind of thing as C#\\'s iterator blocks if you\\'re familiar with those.The key idea is that the compiler/interpreter/whatever does some trickery so that as far as the caller is concerned, they can keep calling next() and it will keep returning values - as if the generator method was paused. Now obviously you can\\'t really \"pause\" a method, so the compiler builds a state machine for you to remember where you currently are and what the local variables etc look like. This is much easier than writing an iterator yourself.', \"There is one type of answer that I don't feel has been given yet, among the many great answers that describe how to use generators. Here is the programming language theory answer:The yield statement in Python returns a generator. A generator in Python is a function that returns continuations (and specifically a type of coroutine, but continuations represent the more general mechanism to understand what is going on).Continuations in programming languages theory are a much more fundamental kind of computation, but they are not often used, because they are extremely hard to reason about and also very difficult to implement. But the idea of what a continuation is, is straightforward: it is the state of a computation that has not yet finished. In this state, the current values of variables, the operations that have yet to be performed, and so on, are saved. Then at some point later in the program the continuation can be invoked, such that the program's variables are reset to that state and the operations that were saved are carried out.Continuations, in this more general form, can be implemented in two ways. In the call/cc way, the program's stack is literally saved and then when the continuation is invoked, the stack is restored.In continuation passing style (CPS), continuations are just normal functions (only in languages where functions are first class) which the programmer explicitly manages and passes around to subroutines. In this style, program state is represented by closures (and the variables that happen to be encoded in them) rather than variables that reside somewhere on the stack. Functions that manage control flow accept continuation as arguments (in some variations of CPS, functions may accept multiple continuations) and manipulate control flow by invoking them by simply calling them and returning afterwards. A very simple example of continuation passing style is as follows:In this (very simplistic) example, the programmer saves the operation of actually writing the file into a continuation (which can potentially be a very complex operation with many details to write out), and then passes that continuation (i.e, as a first-class closure) to another operator which does some more processing, and then calls it if necessary. (I use this design pattern a lot in actual GUI programming, either because it saves me lines of code or, more importantly, to manage control flow after GUI events trigger.)The rest of this post will, without loss of generality, conceptualize continuations as CPS, because it is a hell of a lot easier to understand and read.Now let's talk about generators in Python. Generators are a specific subtype of continuation. Whereas continuations are able in general to save the state of a computation (i.e., the program's call stack), generators are only able to save the state of iteration over an iterator. Although, this definition is slightly misleading for certain use cases of generators. For instance:This is clearly a reasonable iterable whose behavior is well defined -- each time the generator iterates over it, it returns 4 (and does so forever). But it isn't probably the prototypical type of iterable that comes to mind when thinking of iterators (i.e., for x in collection: do_something(x)). This example illustrates the power of generators: if anything is an iterator, a generator can save the state of its iteration.To reiterate: Continuations can save the state of a program's stack and generators can save the state of iteration. This means that continuations are more a lot powerful than generators, but also that generators are a lot, lot easier. They are easier for the language designer to implement, and they are easier for the programmer to use (if you have some time to burn, try to read and understand this page about continuations and call/cc).But you could easily implement (and conceptualize) generators as a simple, specific case of continuation passing style:Whenever yield is called, it tells the function to return a continuation.  When the function is called again, it starts from wherever it left off. So, in pseudo-pseudocode (i.e., not pseudocode, but not code) the generator's next method is basically as follows:where the yield keyword is actually syntactic sugar for the real generator function, basically something like:Remember that this is just pseudocode and the actual implementation of generators in Python is more complex. But as an exercise to understand what is going on, try to use continuation passing style to implement generator objects without use of the yield keyword.\", \"Here is an example in plain language. I will provide a correspondence between high-level human concepts to low-level Python concepts.I want to operate on a sequence of numbers, but I don't want to bother my self with the creation of that sequence, I want only to focus on the operation I want to do. So, I do the following:This is what a generator does (a function that contains a yield); it starts executing on the first next(), pauses whenever it does a yield, and when asked for the next() value it continues from the point it was last. It fits perfectly by design with the iterator protocol of Python, which describes how to sequentially request values.The most famous user of the iterator protocol is the for command in Python. So, whenever you do a:it doesn't matter if sequence is a list, a string, a dictionary or a generator object like described above; the result is the same: you read items off a sequence one by one.Note that defining a function which contains a yield keyword is not the only way to create a generator; it's just the easiest way to create one.For more accurate information, read about iterator types, the yield statement and generators in the Python documentation.\", \"While a lot of answers show why you'd use a yield to create a generator, there are more uses for yield.  It's quite easy to make a coroutine, which enables the passing of information between two blocks of code.  I won't repeat any of the fine examples that have already been given about using yield to create a generator.To help understand what a yield does in the following code, you can use your finger to trace the cycle through any code that has a yield.  Every time your finger hits the yield, you have to wait for a next or a send to be entered.  When a next is called, you trace through the code until you hit the yield\u2026 the code on the right of the yield is evaluated and returned to the caller\u2026 then you wait.  When next is called again, you perform another loop through the code.  However, you'll note that in a coroutine, yield can also be used with a send\u2026 which will send a value from the caller into the yielding function. If a send is given, then yield receives the value sent, and spits it out the left hand side\u2026 then the trace through the code progresses until you hit the yield again (returning the value at the end, as if next was called).For example:\", \"There is another yield use and meaning (since Python 3.3):From PEP 380 -- Syntax for Delegating to a Subgenerator:A syntax is proposed for a generator to delegate part of its operations to another generator. This allows a section of code containing 'yield' to be factored out and placed in another generator. Additionally, the subgenerator is allowed to return with a value, and the value is made available to the delegating generator.The new syntax also opens up some opportunities for optimisation when one generator re-yields values produced by another.Moreover this will introduce (since Python 3.5):to avoid coroutines being confused with a regular generator (today yield is used in both).\", \"All great answers, however a bit difficult for newbies.I assume you have learned the return statement.As an analogy, return and yield are twins. return means 'return and stop' whereas 'yield` means 'return, but continue'Run it:See, you get only a single number rather than a list of them. return never allows you prevail happily, just implements once and quit.Replace return with yield:Now, you win to get all the numbers.Comparing to return which runs once and stops, yield runs times you planed.\\nYou can interpret return as return one of them, and yield as return all of them. This is called iterable.It's the core about yield.The difference between a list return outputs and the object yield output is:You will always get [0, 1, 2] from a list object but only could retrieve them from 'the object yield output' once. So, it has a new name generator object as displayed in Out[11]: <generator object num_list at 0x10327c990>.In conclusion, as a metaphor to grok it:\", 'From a programming viewpoint, the iterators are implemented as thunks.To implement iterators, generators, and thread pools for concurrent execution, etc. as thunks, one uses messages sent to a closure object, which has a dispatcher, and the dispatcher answers to \"messages\".\"next\" is a message sent to a closure, created by the \"iter\" call.There are lots of ways to implement this computation. I used mutation, but it is possible to do this kind of computation without mutation, by returning the current value and the next yielder (making it referential transparent).  Racket uses a sequence of transformations of the initial program in some intermediary languages, one of such rewriting making the yield operator to be transformed in some language with simpler operators.Here is a demonstration of how yield could be rewritten, which uses the structure of R6RS, but the semantics is identical to Python\\'s. It\\'s the same model of computation, and only a change in syntax is required to rewrite it using yield of Python.', 'Here are some Python examples of how to actually implement generators as if Python did not provide syntactic sugar for them:As a Python generator:Using lexical closures instead of generatorsUsing object closures instead of generators (because ClosuresAndObjectsAreEquivalent)', 'I was going to post \"read page 19 of Beazley\\'s \\'Python: Essential Reference\\' for a quick description of generators\", but so many others have posted good descriptions already.Also, note that yield can be used in coroutines as the dual of their use in generator functions.  Although it isn\\'t the same use as your code snippet, (yield) can be used as an expression in a function.  When a caller sends a value to the method using the send() method, then the coroutine will execute until the next (yield) statement is encountered.Generators and coroutines are a cool way to set up data-flow type applications.  I thought it would be worthwhile knowing about the other use of the yield statement in functions.', 'Here is a simple example:Output:I am not a Python developer, but it looks to me yield holds the position of program flow and the next loop start from \"yield\" position. It seems like it is waiting at that position, and just before that, returning a value outside, and next time continues to work.It seems to be an interesting and nice ability :D', \"Here is a mental image of what yield does.I like to think of a thread as having a stack (even when it's not implemented that way).When a normal function is called, it puts its local variables on the stack, does some computation, then clears the stack and returns. The values of its local variables are never seen again.With a yield function, when its code begins to run (i.e. after the function is called, returning a generator object, whose next() method is then invoked), it similarly puts its local variables onto the stack and computes for a while. But then, when it hits the yield statement, before clearing its part of the stack and returning, it takes a snapshot of its local variables and stores them in the generator object. It also writes down the place where it's currently up to in its code (i.e. the particular yield statement).So it's a kind of a frozen function that the generator is hanging onto.When next() is called subsequently, it retrieves the function's belongings onto the stack and re-animates it. The function continues to compute from where it left off, oblivious to the fact that it had just spent an eternity in cold storage.Compare the following examples:When we call the second function, it behaves very differently to the first. The yield statement might be unreachable, but if it's present anywhere, it changes the nature of what we're dealing with.Calling yielderFunction() doesn't run its code, but makes a generator out of the code. (Maybe it's a good idea to name such things with the yielder prefix for readability.)The gi_code and gi_frame fields are where the frozen state is stored. Exploring them with dir(..), we can confirm that our mental model above is credible.\", \"Imagine that you have created a remarkable machine that is capable of generating thousands and thousands of lightbulbs per day. The machine generates these lightbulbs in boxes with a unique serial number. You don't have enough space to store all of these lightbulbs at the same time, so you would like to adjust it to generate lightbulbs on-demand.Python generators don't differ much from this concept. Imagine that you have a function called barcode_generator that generates unique serial numbers for the boxes. Obviously, you can have a huge number of such barcodes returned by the function, subject to the hardware (RAM) limitations. A wiser, and space efficient, option is to generate those serial numbers on-demand.Machine's code:Note the next(barcode) bit.As you can see, we have a self-contained \u201cfunction\u201d to generate the next unique serial number each time. This function returns a generator! As you can see, we are not calling the function each time we need a new serial number, but instead we are using next() given the generator to obtain the next serial number.To be more precise, this generator is a lazy iterator! An iterator is an object that helps us traverse a sequence of objects. It's called lazy because it does not load all the items of the sequence in memory until they are needed. The use of next in the previous example is the explicit way to obtain the next item from the iterator. The implicit way is using for loops:This will print barcodes infinitely, yet you will not run out of memory.In other words, a generator looks like a function but behaves like an iterator.Finally, real-world applications? They are usually useful when you work with big sequences. Imagine reading a huge file from disk with billions of records. Reading the entire file in memory, before you can work with its content, will probably be infeasible (i.e., you will run out of memory).\", 'An easy example to understand what it is: yieldThe output is:', \"Like every answer suggests, yield is used for creating a sequence generator. It's used for generating some sequence dynamically. For example, while reading a file line by line on a network, you can use the yield function as follows:You can use it in your code as follows:Execution Control Transfer gotchaThe execution control will be transferred from getNextLines() to the for loop when yield is executed. Thus, every time getNextLines() is invoked, execution begins from the point where it was paused last time.Thus in short, a function with the following codewill print\", \"(My below answer only speaks from the perspective of using Python generator, not the underlying implementation of generator mechanism, which involves some tricks of stack and heap manipulation.)When yield is used instead of a return in a python function, that function is turned into something special called generator function. That function will return an object of generator type. The yield keyword is a flag to notify the python compiler to treat such function specially. Normal functions will terminate once some value is returned from it. But with the help of the compiler, the generator function can be thought of as resumable. That is, the execution context will be restored and the execution will continue from last run. Until you explicitly call return, which will raise a StopIteration exception (which is also part of the iterator protocol), or reach the end of the function. I found a lot of references about generator but this one from the functional programming perspective is the most digestable.(Now I want to talk about the rationale behind generator, and the iterator based on my own understanding. I hope this can help you grasp the essential motivation of iterator and generator. Such concept shows up in other languages as well such as C#.)As I understand, when we want to process a bunch of data, we usually first store the data somewhere and then process it one by one. But this naive approach is problematic. If the data volume is huge, it's expensive to store them as a whole beforehand. So instead of storing the data itself directly, why not store some kind of metadata indirectly, i.e. the logic how the data is computed.There are 2 approaches to wrap such metadata.Either way, an iterator is created, i.e. some object that can give you the data you want. The OO approach may be a bit complex. Anyway, which one to use is up to you.\", \"In summary, the yield statement transforms your function into a factory that produces a special object called a generator which wraps around the body of your original function. When the generator is iterated, it executes your function  until it reaches the next yield then suspends execution and evaluates to the value passed to yield. It repeats this process on each iteration until the path of execution exits the function. For instance,simply outputsThe power comes from using the generator with a loop that calculates a sequence, the generator executes the loop stopping each time to 'yield' the next result of the calculation, in this way it calculates a list on the fly, the benefit being the memory saved for especially large calculationsSay you wanted to create a your own range function that produces an iterable range of numbers, you could do it like so,and use it like this;But this is inefficient becauseLuckily Guido and his team were generous enough to develop generators so we could just do this;Now upon each iteration a function on the generator called next() executes the function until it either reaches a 'yield' statement in which it stops and  'yields' the value or reaches the end of the function. In this case on the first call, next() executes up to the yield statement and yield 'n', on the next call it will execute the  increment statement, jump back to the 'while', evaluate it, and if true, it will stop and yield 'n' again, it will continue that way until the while condition returns false and the generator jumps to the end of the function.\", \"Yield is an objectA return in a function will return a single value.If you want a function to return a huge set of values, use yield.More importantly, yield is a barrier.like barrier in the CUDA language, it will not transfer control until it gets\\n  completed.That is, it will run the code in your function from the beginning until it hits yield. Then, it\u2019ll return the first value of the loop.Then, every other call will run the loop you have written in the function one more time, returning the next value until there isn't any value to return.\", 'Many people use return rather than yield, but in some cases yield can be more efficient and easier to work with.Here is an example which yield is definitely best for:return (in function)yield (in function)Calling functionsBoth functions do the same thing, but yield uses three lines instead of five and has one less variable to worry about.This is the result from the code:As you can see both functions do the same thing. The only difference is return_dates() gives a list and yield_dates() gives a generator.A real life example would be something like reading a file line by line or if you just want to make a generator.', 'The yield keyword simply collects returning results. Think of yield like return +=', \"yield is like a return element for a function. The difference is, that the yield element turns a function into a generator. A generator behaves just like a function until something is 'yielded'. The generator stops until it is next called, and continues from exactly the same point as it started. You can get a sequence of all the 'yielded' values in one, by calling list(generator()).\"]",
            "url": "https://stackoverflow.com/questions/231767"
        },
        {
            "tag": "python",
            "question": [
                "What does if __name__ == \"__main__\": do?"
            ],
            "votes": "7867",
            "answer": "['It\\'s boilerplate code that protects users from accidentally invoking the script when they didn\\'t intend to. Here are some common problems when the guard is omitted from a script:If you import the guardless script in another script (e.g. import my_script_without_a_name_eq_main_guard), then the latter script will trigger the former to run at import time and using the second script\\'s command line arguments. This is almost always a mistake.If you have a custom class in the guardless script and save it to a pickle file, then unpickling it in another script will trigger an import of the guardless script, with the same problems outlined in the previous bullet.To better understand why and how this matters, we need to take a step back to understand how Python initializes scripts and how this interacts with its module import mechanism.Whenever the Python interpreter reads a source file, it does two things:it sets a few special variables like __name__, and thenit executes all of the code found in the file.Let\\'s see how this works and how it relates to your question about the __name__ checks we always see in Python scripts.Let\\'s use a slightly different code sample to explore how imports and scripts work.  Suppose the following is in a file called foo.py.When the Python interpreter reads a source file, it first defines a few special variables. In this case, we care about the __name__ variable.When Your Module Is the Main ProgramIf you are running your module (the source file) as the main program, e.g.the interpreter will assign the hard-coded string \"__main__\" to the __name__ variable, i.e.When Your Module Is Imported By AnotherOn the other hand, suppose some other module is the main program and it imports your module. This means there\\'s a statement like this in the main program, or in some other module the main program imports:The interpreter will search for your foo.py file (along with searching for a few other variants), and prior to executing that module, it will assign the name \"foo\" from the import statement to the __name__ variable, i.e.After the special variables are set up, the interpreter executes all the code in the module, one statement at a time. You may want to open another window on the side with the code sample so you can follow along with this explanation.AlwaysIt prints the string \"before import\" (without quotes).It loads the math module and assigns it to a variable called math. This is equivalent to replacing import math with the following (note that __import__ is a low-level function in Python that takes a string and triggers the actual import):It prints the string \"before function_a\".It executes the def block, creating a function object, then assigning that function object to a variable called function_a.It prints the string \"before function_b\".It executes the second def block, creating another function object, then assigning it to a variable called function_b.It prints the string \"before __name__ guard\".Only When Your Module Is the Main ProgramOnly When Your Module Is Imported by AnotherAlwaysSummaryIn summary, here\\'s what\\'d be printed in the two cases:You might naturally wonder why anybody would want this.  Well, sometimes you want to write a .py file that can be both used by other programs and/or modules as a module, and can also be run as the main program itself.  Examples:Your module is a library, but you want to have a script mode where it runs some unit tests or a demo.Your module is only used as a main program, but it has some unit tests, and the testing framework works by importing .py files like your script and running special test functions. You don\\'t want it to try running the script just because it\\'s importing the module.Your module is mostly used as a main program, but it also provides a programmer-friendly API for advanced users.Beyond those examples, it\\'s elegant that running a script in Python is just setting up a few magic variables and importing the script. \"Running\" the script is a side effect of importing the script\\'s module.Question: Can I have multiple __name__ checking blocks?  Answer: it\\'s strange to do so, but the language won\\'t stop you.Suppose the following is in foo2.py.  What happens if you say python foo2.py on the command-line? Why?', 'When your script is run by passing it as a command to the Python interpreter,all of the code that is at indentation level 0 gets executed.  Functions and classes that are defined are, well, defined, but none of their code gets run.  Unlike other languages, there\\'s no main() function that gets run automatically - the main() function is implicitly all the code at the top level.In this case, the top-level code is an if block.  __name__ is a built-in variable which evaluates to the name of the current module.  However, if a module is being run directly (as in myscript.py above), then __name__ instead is set to the string \"__main__\".  Thus, you can test whether your script is being run directly or being imported by something else by testingIf your script is being imported into another module, its various function and class definitions will be imported and its top-level code will be executed, but the code in the then-body of the if clause above won\\'t get run as the condition is not met. As a basic example, consider the following two scripts:Now, if you invoke the interpreter asThe output will beIf you run two.py instead:You getThus, when module one gets loaded, its __name__ equals \"one\" instead of \"__main__\".', 'Create the following two files:Now run each file individually.Running python a.py:When a.py is executed, it imports the module b. This causes all the code inside b to run. Python sets globals()[\\'__name__\\'] in the b module to the module\\'s name, b.Running python b.py:When only the file b.py is executed, Python sets globals()[\\'__name__\\'] in this file to \"__main__\". Therefore, the if statement evaluates to True this time.', 'To outline the basics:The global variable, __name__, in the module that is the entry point to your program, is \\'__main__\\'. Otherwise, it\\'s the name you import the module by.So, code under the if block will only run if the module is the entry point to your program.It allows the code in the module to be importable by other modules, without executing the code block beneath on import.Why do we need this?Say you\\'re writing a Python script designed to be used as a module:You could test the module by adding this call of the function to the bottom:and running it (on a command prompt) with something like:However, if you want to import the module to another script:On import, the do_important function would be called, so you\\'d probably comment out your function call, do_important(), at the bottom.And then you\\'ll have to remember whether or not you\\'ve commented out your test function call. And this extra complexity would mean you\\'re likely to forget, making your development process more troublesome.The __name__ variable points to the namespace wherever the Python interpreter happens to be at the moment.Inside an imported module, it\\'s the name of that module.But inside the primary module (or an interactive Python session, i.e. the interpreter\\'s Read, Eval, Print Loop, or REPL) you are running everything from its \"__main__\".So if you check before executing:With the above, your code will only execute when you\\'re running it as the primary module (or intentionally call it from another script).There\\'s a Pythonic way to improve on this, though.What if we want to run this business process from outside the module?If we put the code we want to exercise as we develop and test in a function like this and then do our check for \\'__main__\\' immediately after:We now have a final function for the end of our module that will run if we run the module as the primary module.It will allow the module and its functions and classes to be imported into other scripts without running the main function, and will also allow the module (and its functions and classes) to be called when running from a different \\'__main__\\' module, i.e.This idiom can also be found in the Python documentation in an explanation of the __main__ module. That text states:This module represents the (otherwise anonymous) scope in which the\\n  interpreter\u2019s main program executes \u2014 commands read either from\\n  standard input, from a script file, or from an interactive prompt. It\\n  is this environment in which the idiomatic \u201cconditional script\u201d stanza\\n  causes a script to run:', 'if __name__ == \"__main__\" is the part that runs when the script is run from (say) the command line using a command like python myscript.py.', \"__name__ is a global variable (in Python, global actually means on the module level) that exists in all namespaces. It is typically the module's name (as a str type).As the only special case, however, in whatever Python process you run, as in mycode.py:the otherwise anonymous global namespace is assigned the value of '__main__' to its __name__.Thus, including the final lineswill cause your script's uniquely defined main function to run.Another benefit of using this construct: you can also import your code as a module in another script and then run the main function if and when your program decides:\", 'There are lots of different takes here on the mechanics of the code in question, the \"How\", but for me none of it made sense until I understood the \"Why\". This should be especially helpful for new programmers.Take file \"ab.py\":And a second file \"xy.py\":What is this code actually doing?When you execute xy.py, you import ab. The import statement runs the module immediately on import, so ab\\'s operations get executed before the remainder of xy\\'s. Once finished with ab, it continues with xy.The interpreter keeps track of which scripts are running with __name__. When you run a script - no matter what you\\'ve named it - the interpreter calls it \"__main__\", making it the master or \\'home\\' script that gets returned to after running an external script.Any other script that\\'s called from this \"__main__\" script is assigned its filename as its __name__ (e.g., __name__ == \"ab.py\"). Hence, the line if __name__ == \"__main__\": is the interpreter\\'s test to determine if it\\'s interpreting/parsing the \\'home\\' script that was initially executed, or if it\\'s temporarily peeking into another (external) script. This gives the programmer flexibility to have the script behave differently if it\\'s executed directly vs. called externally.Let\\'s step through the above code to understand what\\'s happening, focusing first on the unindented lines and the order they appear in the scripts. Remember that function - or def - blocks don\\'t do anything by themselves until they\\'re called. What the interpreter might say if mumbled to itself:The bottom two lines mean: \"If this is the \"__main__\" or \\'home\\' script, execute the function called main()\". That\\'s why you\\'ll see a def main(): block up top, which contains the main flow of the script\\'s functionality.Why implement this?Remember what I said earlier about import statements? When you import a module it doesn\\'t just \\'recognize\\' it and wait for further instructions - it actually runs all the executable operations contained within the script. So, putting the meat of your script into the main() function effectively quarantines it, putting it in isolation so that it won\\'t immediately run when imported by another script.Again, there will be exceptions, but common practice is that main() doesn\\'t usually get called externally. So you may be wondering one more thing: if we\\'re not calling main(), why are we calling the script at all? It\\'s because many people structure their scripts with standalone functions that are built to be run independent of the rest of the code in the file. They\\'re then later called somewhere else in the body of the script. Which brings me to this:But the code works without itYes, that\\'s right. These separate functions can be called from an in-line script that\\'s not contained inside a main() function. If you\\'re accustomed (as I am, in my early learning stages of programming) to building in-line scripts that do exactly what you need, and you\\'ll try to figure it out again if you ever need that operation again ... well, you\\'re not used to this kind of internal structure to your code, because it\\'s more complicated to build and it\\'s not as intuitive to read.But that\\'s a script that probably can\\'t have its functions called externally, because if it did it would immediately start calculating and assigning variables. And chances are if you\\'re trying to re-use a function, your new script is related closely enough to the old one that there will be conflicting variables.In splitting out independent functions, you gain the ability to re-use your previous work by calling them into another script. For example, \"example.py\" might import \"xy.py\" and call x(), making use of the \\'x\\' function from \"xy.py\". (Maybe it\\'s capitalizing the third word of a given text string; creating a NumPy array from a list of numbers and squaring them; or detrending a 3D surface. The possibilities are limitless.)(As an aside, this question contains an answer by @kindall that finally helped me to understand - the why, not the how. Unfortunately it\\'s been marked as a duplicate of this one, which I think is a mistake.)', \"The code under if __name__ == '__main__': will only be executed if the module is invoked as a script.As an example, consider the following module my_test_module.py:First possibility: Import my_test_module.py in another moduleNow if you invoke main.py:Note that only the top-level print() statement in my_test_module is executed.Second possibility: Invoke my_test_module.py as a scriptNow if you run my_test_module.py as a Python script, both print() statements will be executed:For a more comprehensive explanation, you can read What does if __name__ == '__main__' do in Python.\", 'When there are certain statements in our module (M.py) we want to be executed when it\\'ll be running as main (not imported), we can place those statements (test-cases, print statements) under this if block.As by default (when module running as main, not imported) the __name__ variable is set to \"__main__\", and when it\\'ll be imported the __name__ variable will get a different value, most probably the name of the module (\\'M\\').\\nThis is helpful in running different variants of a modules together, and separating their specific input & output statements and also if there are any test-cases.In short, use this \\'if __name__ == \"main\" \\' block to prevent (certain) code from being run when the module is imported.', 'Put simply, __name__ is a variable defined for each script that defines whether the script is being run as the main module or it is being run as an imported module.So if we have two scripts;andThe output from executing script1 isAnd the output from executing script2 is:As you can see, __name__ tells us which code is the \\'main\\' module.\\nThis is great, because you can just write code and not have to worry about structural issues like in C/C++, where, if a file does not implement a \\'main\\' function then it cannot be compiled as an executable and if it does, it cannot then be used as a library.Say you write a Python script that does something great and you implement a boatload of functions that are useful for other purposes. If I want to use them I can just import your script and use them without executing your program (given that your code only executes within the  if __name__ == \"__main__\": context). Whereas in C/C++ you would have to portion out those pieces into a separate module that then includes the file. Picture the situation below;The arrows are import links. For three modules each trying to include the previous modules code there are six files (nine, counting the implementation files) and five links. This makes it difficult to include other code into a C project unless it is compiled specifically as a library. Now picture it for Python:You write a module, and if someone wants to use your code they just import it and the __name__ variable can help to separate the executable portion of the program from the library part.', \"To be short, you need to know several points:import a action actually runs all that can be run in a.py, meaning each line in a.pyBecause of point 1, you may not want everything to be run in a.py when importing itTo solve the problem in point 2, Python allows you to use a condition check__name__ is an implicit variable in all .py modules:The important thing that Python is special at is point 4! The rest is just basic logic.I've been reading so much throughout the answers on this page. I would say, if you know the thing, for sure you will understand those answers, otherwise, you are still confused.\", \"Let's look at the answer in a more abstract way:Suppose we have this code in x.py:Blocks A and B are run when we are running x.py.But just block A (and not B) is run when we are running another module, y.py for example, in which x.py is imported and the code is run from there (like when a function in x.py is called from y.py).\", 'When you run Python interactively the local __name__ variable is assigned a value of __main__. Likewise, when you execute a Python module from the command line, rather than importing it into another module, its __name__ attribute is assigned a value of __main__, rather than the actual name of the module. In this way, modules can look at their own __name__ value to determine for themselves how they are being used, whether as support for another program or as the main application executed from the command line. Thus, the following idiom is quite common in Python modules:', 'Consider:It checks if the __name__ attribute of the Python script is \"__main__\". In other words, if the program itself is executed, the attribute will be __main__, so the program will be executed (in this case the main() function).However, if your Python script is used by a module, any code outside of the if statement will be executed, so if __name__ == \"__main__\" is used just to check if the program is used as a module or not, and therefore decides whether to run the code.', 'Before explaining anything about if __name__ == \\'__main__\\' it is important to understand what __name__ is and what it does.__name__ is a DunderAlias - can be thought of as a global variable (accessible from modules) and works in a similar way to global.It is a string (global as mentioned above) as indicated by type(__name__) (yielding <class \\'str\\'>), and is an inbuilt standard for both Python 3 and Python 2 versions.It can not only be used in scripts but can also be found in both the interpreter and modules/packages.test_file.py:Resulting in __main__somefile.py:test_file.py:Resulting in somefileNotice that when used in a package or module, __name__ takes the name of the file.  The path of the actual module or package path is not given, but has its own DunderAlias __file__, that allows for this.You should see that, where __name__, where it is the main file (or program) will always return __main__, and if it is a module/package, or anything that is running off some other Python script, will return the name of the file where it has originated from.Being a variable means that it\\'s value can be overwritten (\"can\" does not mean \"should\"), overwriting the value of __name__ will result in a lack of readability.  So do not do it, for any reason.  If you need a variable define a new variable.It is always assumed that the value of __name__ to be __main__ or the name of the file.  Once again changing this default value will cause more confusion that it will do good, causing problems further down the line.It is considered good practice in general to include the if __name__ == \\'__main__\\' in scripts.Now we know the behaviour of __name__ things become clearer:An if is a flow control statement that contains the block of code will execute if the value given is true. We have seen that __name__ can take either\\n__main__ or the file name it has been imported from.This means that if __name__ is equal to __main__ then the file must be the main file and must actually be running (or it is the interpreter), not a module or package imported into the script.If indeed __name__ does take the value of __main__ then whatever is in that block of code will execute.This tells us that if the file running is the main file (or you are running from the interpreter directly) then that condition must execute.  If it is a package then it should not, and the value will not be __main__.__name__ can also be used in modules to define the name of a moduleIt is also possible to do other, less common but useful things with __name__, some I will show here:You can also use it to provide runnable help functions/utilities on packages and modules without the elaborate use of libraries.It also allows modules to be run from the command line as main scripts, which can be also very useful.', \"I think it's best to break the answer in depth and in simple words:__name__: Every module in Python has a special attribute called __name__.\\nIt is a built-in variable that returns the name of the module.__main__: Like other programming languages, Python too has an execution entry point, i.e., main. '__main__' is the name of the scope in which top-level code executes. Basically you have two ways of using a Python module: Run it directly as a script, or import it. When a module is run as a script, its __name__ is set to __main__.Thus, the value of the __name__ attribute is set to __main__ when the module is run as the main program. Otherwise the value of __name__  is set to contain the name of the module.\", 'It is a special for when a Python file is called from the command line. This is typically used to call a \"main()\" function or execute other appropriate startup code, like commandline arguments handling for instance.It could be written in several ways. Another is:I am not saying you should use this in production code, but it serves to illustrate that there is nothing \"magical\" about if __name__ == \\'__main__\\'.It just a convention for invoking a main function in Python files.', 'There are a number of variables that the system (Python interpreter) provides for source files (modules).  You can get their values anytime you want, so, let us focus on the __name__ variable/attribute:When Python loads a source code file, it executes all of the code found in it. (Note that it doesn\\'t call all of the methods and functions defined in the file, but it does define them.)Before the interpreter executes the source code file though, it defines a few special variables for that file; __name__ is one of those special variables that Python automatically defines for each source code file.If Python is loading this source code file as the main program (i.e. the file you run), then it sets the special __name__ variable for this file to have a value \"__main__\".If this is being imported from another module, __name__ will be set to that module\\'s name.So, in your example in part:means that the code block:will be executed only when you run the module directly; the code block will not execute if another module is calling/importing it because the value of __name__ will not equal to \"main\" in that particular instance.Hope this helps out.', 'if __name__ == \"__main__\": is basically the top-level script environment, and it specifies the interpreter that (\\'I have the highest priority to be executed first\\').\\'__main__\\' is the name of the scope in which top-level code executes. A module\u2019s __name__ is set equal to \\'__main__\\' when read from standard input, a script, or from an interactive prompt.', 'Consider:The output for the above is __main__.The above statement is true and prints \"direct method\". Suppose if they imported this class in another class it doesn\\'t print \"direct method\" because, while importing, it will set __name__ equal to \"first model name\".', 'In simple words:The code you see under if __name__ == \"__main__\": will only get called upon when your Python file is executed as python example1.pyHowever, if you wish to import your Python file example1.py as a module to work with another Python file, say example2.py, the code under if __name__ == \"__main__\": will not run or take any effect.', 'You can make the file usable as a script as well as an importable module.fibo.py (a module named fibo)Reference: https://docs.python.org/3.5/tutorial/modules.html', 'The reason foris primarily to avoid the import lock problems that would arise from having code directly imported. You want main() to run if your file was directly invoked (that\\'s the __name__ == \"__main__\" case), but if your code was imported then the importer has to enter your code from the true main module to avoid import lock problems.A side-effect is that you automatically sign on to a methodology that supports multiple entry points. You can run your program using main() as the entry point, but you don\\'t have to. While setup.py expects main(), other tools use alternate entry points. For example, to run your file as a gunicorn process, you define an app() function instead of a main(). Just as with setup.py, gunicorn imports your code so you don\\'t want it do do anything while it\\'s being imported (because of the import lock issue).', 'If you are a beginner, probably the only answer you need right now is that this code is unnecessary for a simple script. It is only useful if you want to be able to import your script (or unpickle etc; see the other answers here for some other non-beginner scenarios).In slightly different words, the if __name__ guard is a mechanism for hiding code from other code. If you don\\'t have a specific reason to hide something, don\\'t: If you don\\'t need to hide some code from import, don\\'t put it behind this guard, and if you do, hide as little as possible.In slightly more detail, let\\'s say you have a simple script fib.py (adapted from this answer):Now, if you simply run python fib.py it works fine. But __name__ will always be \"__main__\" in this scenario, so the condition is actually unnecessary. The script could be simplified to justNow, you can\\'t import fib with the new version, but if you didn\\'t plan to do that in the first place, this version is actually better, because it\\'s simpler and clearer.If you do want to be able to import fib, the first version is useless, too, because the useful code is in a section which will not run when you import this file (in which case __name__ will not be \"__main__\"). The proper design in that case would be to refactor the code so that the useful parts are in a function you can run when you want to after you have imported it.Now, if you import fib, the call to main() will not be executed; but when you run python fib.py, it will.Actually, a better design still would be to isolate the reusable part (the actual calculation) from the user-visible input/output:Now, you can from fib import fibn and call the fibn() function from the code which performs this import.(I called the function fibn() just to make it clearer what is what in this example. In real life, you might call it fib() and do from fib import fib.)Similarly, you could import and call the main function if you wanted to reuse it.Returning to the code in the question, I would similarly move the code from the if into a function as well, so that callers can invoke that function if they want to.This changes the scope of the lock variable; if the surrounding code needs access to it, you will need to make it global (or, perhaps, better, refactor main to return lock, and have the caller capture the value in a local variable of its own).(Unlike in languages like C, the name main has no specific meaning to Python; but it\\'s a common convention to use it as the name of the thing which will be run. You still have to actually explicitly call it, like main(), unlike in C.)', 'Every module in Python has an attribute called __name__. The value of __name__  attribute is  __main__ when the module is run directly, like python my_module.py. Otherwise (like when you say import my_module) the value of __name__  is the name of the module.Small example to explain in short.We can execute this directly asOutputNow suppose we call the above script from another script:When you execute this,OutputSo, the above is self-explanatory that when you call test from another script, if loop __name__ in test.py will not execute.', \"This answer is for Java programmers learning Python.\\nEvery Java file typically contains one public class. You can use that class in two ways:Call the class from other files. You just have to import it in the calling program.Run the class stand alone, for testing purposes.For the latter case, the class should contain a public static void main() method. In Python this purpose is served by the globally defined label '__main__'.\", 'If this .py file are imported by other .py files, the code under the if statement will not be executed.If this .py are run by python this_py.py under shell, or double clicked in Windows. the code under the if statement will be executed.It is usually written for testing.', \"We see if __name__ == '__main__': quite often.It checks if a module is being imported or not.In other words, the code within the if block will be executed only when the code runs directly. Here directly means not imported.Let's see what it does using a simple code that prints the name of the module:If we run the code directly via python test.py, the module name is __main__:\", 'If the Python interpreter is running a particular module then the __name__ global variable will have the value \"__main__\":When you run this script, it prints:If you import this file, say A to file B, and execute the file B then if __name__ == \"__main__\" in file A becomes False, so it prints:', \"All the answers have pretty much explained the functionality. But I will provide one example of its usage which might help clearing out the concept further.Assume that you have two Python files, a.py and b.py. Now, a.py imports b.py. We run the a.py file, where the import b.py code is executed first. Before the rest of the a.py code runs, the code in the file b.py must run completely.In the b.py code, there is some code that is exclusive to that file b.py and we don't want any other file (other than the b.py file), that has imported the b.py file, to run it.So that is what this line of code checks. If it is the main file (i.e., b.py) running the code, which in this case it is not (a.py is the main file running), then only the code gets executed.\"]",
            "url": "https://stackoverflow.com/questions/419163"
        },
        {
            "tag": "python",
            "question": [
                "Does Python have a ternary conditional operator?"
            ],
            "votes": "7586",
            "answer": "['Yes, it was added in version 2.5. The expression syntax is:First condition is evaluated, then exactly one of either a or b is evaluated and returned based on the Boolean value of condition. If condition evaluates to True, then a is evaluated and returned but b is ignored, or else when b is evaluated and returned but a is ignored.This allows short-circuiting because when condition is true only a is evaluated and b is not evaluated at all, but when condition is false only b is evaluated and a is not evaluated at all.For example:Note that conditionals are an expression, not a statement. This means you can\\'t use statements such as pass, or assignments with = (or \"augmented\" assignments like +=), within a conditional expression:(In 3.8 and above, the := \"walrus\" operator allows simple assignment of values as an expression, which is then compatible with this syntax. But please don\\'t write code like that; it will quickly become very difficult to understand.)Similarly, because it is an expression, the else part is mandatory:You can, however, use conditional expressions to assign a variable like so:Or for example to return a value:Think of the conditional expression as switching between two values. We can use it when we are in a \\'one value or another\\' situation, where we will do the same thing with the result, regardless of whether the condition is met. We use the expression to compute the value, and then do something with it. If you need to do something different depending on the condition, then use a normal if statement instead.Keep in mind that it\\'s frowned upon by some Pythonistas for several reasons:If you\\'re having trouble remembering the order, then remember that when read aloud, you (almost) say what you mean. For example, x = 4 if b > 8 else 9 is read aloud as x will be 4 if b is greater than 8 otherwise 9.Official documentation:', 'You can index into a tuple:test needs to return True or False.\\nIt might be safer to always implement it as:or you can use the built-in bool() to assure a Boolean value:', \"For versions prior to 2.5, there's the trick:It can give wrong results when on_true has a false Boolean value.1Although it does have the benefit of evaluating expressions left to right, which is clearer in my opinion.1. Is there an equivalent of C\u2019s \u201d?:\u201d ternary operator?\", '<expression 1> if <condition> else <expression 2>', 'From the documentation:Conditional expressions (sometimes called a \u201cternary operator\u201d) have the lowest priority of all Python operations.The expression x if C else y first evaluates the condition, C (not x); if C is true, x is evaluated and its value is returned; otherwise, y is evaluated and its value is returned.See PEP 308 for more details about conditional expressions.New since version 2.5.', \"An operator for a conditional expression in Python was added in 2006 as part of Python Enhancement Proposal 308. Its form differ from common ?: operator and it's:which is equivalent to:Here is an example:Another syntax which can be used (compatible with versions before 2.5):where operands are lazily evaluated.Another way is by indexing a tuple (which isn't consistent with the conditional operator of most other languages):or explicitly constructed dictionary:Another (less reliable), but simpler method is to use and and or operators:however this won't work if x would be False.A possible workaround is to make x and y lists or tuples as in the following:or:If you're working with dictionaries, instead of using a ternary conditional, you can take advantage of get(key, default), for example:Source: ?: in Python at Wikipedia\", 'Unfortunately, thesolution doesn\\'t have short-circuit behaviour; thus both falseValue and trueValue are evaluated regardless of the condition. This could be suboptimal or even buggy (i.e. both trueValue and falseValue could be methods and have side effects).One solution to this would be(execution delayed until the winner is known ;)), but it introduces inconsistency between callable and non-callable objects. In addition, it doesn\\'t solve the case when using properties.And so the story goes - choosing between three mentioned solutions is a trade-off between having the short-circuit feature, using at least Python 2.5 (IMHO, not a problem anymore) and not being prone to \"trueValue-evaluates-to-false\" errors.', 'Here I just try to show some important differences in the ternary operator between a couple of programming languages.', \"For Python 2.5 and newer there is a specific syntax:In older Pythons a ternary operator is not implemented but it's possible to simulate it.Though, there is a potential problem, which if cond evaluates to True and on_true evaluates to False then on_false is returned instead of on_true. If you want this behavior the method is OK, otherwise use this:which can be wrapped by:and used this way:It is compatible with all Python versions.\", 'You might often findbut this leads to a problem when on_true == 0Where you would expect this result for a normal ternary operator:', \"Yes. From the grammar file:The part of interest is:So, a ternary conditional operation is of the form:expression3 will be lazily evaluated (that is, evaluated only if expression2 is false in a boolean context). And because of the recursive definition, you can chain them indefinitely (though it may considered bad style.)Note that every if must be followed with an else. People learning list comprehensions and generator expressions may find this to be a difficult lesson to learn - the following will not work, as Python expects a third expression for an else:which raises a SyntaxError: invalid syntax.\\nSo the above is either an incomplete piece of logic (perhaps the user expects a no-op in the false condition) or what may be intended is to use expression2 as a filter - notes that the following is legal Python:expression2 works as a filter for the list comprehension, and is not a ternary conditional operator.You may find it somewhat painful to write the following:expression1 will have to be evaluated twice with the above usage. It can limit redundancy if it is simply a local variable. However, a common and performant Pythonic idiom for this use-case is to use or's shortcutting behavior:which is equivalent in semantics. Note that some style-guides may limit this usage on the grounds of clarity - it does pack a lot of meaning into very little syntax.\", \"One of the alternatives to Python's conditional expressionis the following:which has the following nice extension:The shortest alternative remainswhich works because issubclass(bool, int).Careful, though: the alternative tois notbutThis works fine as long as no and yes are to be called with exactly the same parameters. If they are not, like inor inthen a similar alternative either does not exist (1) or is hardly viable (2). (In rare cases, depending on the context, something likecould make sense.)Thanks to Radek Roj\u00edk for his comment\", 'As already answered, yes, there is a ternary operator in Python:In many cases <expression 1> is also used as Boolean evaluated <condition>. Then you can use short-circuit evaluation.One big pro of short-circuit evaluation is the possibility of chaining more than two expressions:When working with functions it is more different in detail:PS: Of course, a short-circuit evaluation is not a ternary operator, but often the ternary is used in cases where the short circuit would be enough. It has a better readability and can be chained.', 'Simulating the Python ternary operator.For exampleOutput:', 'Just memorize this pyramid if you have trouble remembering:', 'The ternary conditional operator simply allows testing a condition in a single line replacing the multiline if-else making the code compact.[on_true] if [expression] else [on_false]Above approach can be written as:', \"Vinko Vrsalovic's answer is good enough. There is only one more thing:Note that conditionals are an expression, not a statement. This means you can't use assignment statements or pass or other statements within a conditional expressionAfter the walrus operator was introduced in Python 3.8, something changed.gives a = 3 and b is not defined,gives a is not defined and b = 5, andgives c = 5, a is not defined and b = 5.Even if this may be ugly, assignments can be done inside conditional expressions after Python 3.8. Anyway, it is still better to use normal if statement instead in this case.\", \"More a tip than an answer (I don't need to repeat the obvious for the hundredth time), but I sometimes use it as a one-liner shortcut in such constructs:, becomes:Some (many :) may frown upon it as unpythonic (even, Ruby-ish :), but I personally find it more natural - i.e., how you'd express it normally, plus a bit more visually appealing in large blocks of code.\", 'You can do this:Example:This would print \"odd\" if the number is odd or \"even\" if the number is even.The result: If condition is true, exp_1 is executed, else exp_2 is executed.Note: 0, None, False, emptylist, and emptyString evaluates as False.And any data other than 0 evaluates to True.If the condition [condition] becomes \"True\", then expression_1 will be evaluated, but not expression_2.If we \"and\" something with 0 (zero), the result will always to be false. So in the below statement,The expression exp won\\'t be evaluated at all since \"and\" with 0 will always evaluate to zero and there is no need to evaluate the expression. This is how the compiler itself works, in all languages.Inthe expression exp won\\'t be evaluated at all since \"or\" with 1 will always be 1. So it won\\'t bother to evaluate the expression exp since the result will be 1 anyway (compiler optimization methods).But in case ofThe second expression exp2 won\\'t be evaluated since True and exp1 would be True when exp1 isn\\'t false.Similarly inThe expression exp1 won\\'t be evaluated since False is equivalent to writing 0 and doing \"and\" with 0 would be 0 itself, but after exp1 since \"or\" is used, it will evaluate the expression exp2 after \"or\".Note:- This kind of branching using \"or\" and \"and\" can only be used when the expression_1 doesn\\'t have a Truth value of False (or 0 or None or emptylist [ ] or emptystring \\' \\'.) since if expression_1 becomes False, then the expression_2 will be evaluated because of the presence \"or\" between exp_1 and exp_2.In case you still want to make it work for all the cases regardless of what exp_1 and exp_2 truth values are, do this:', \"Many programming languages derived from C usually have the following syntax of the ternary conditional operator:At first, the Python's benevolent dictator for life (I mean Guido van Rossum, of course) rejected it (as non-Pythonic style), since it's quite hard to understand for people not used to C language. Also, the colon sign : already has many uses in Python. After PEP 308 was approved, Python finally received its own shortcut conditional expression (what we use now):So, firstly it evaluates the condition. If it returns True, expression1 will be evaluated to give the result, otherwise expression2 will be evaluated. Due to lazy evaluation mechanics \u2013 only one expression will be executed.Here are some examples (conditions will be evaluated from left to right):Ternary operators can be chained in series:The following one is the same as previous one:\", 'Yes, Python have a ternary operator, here is the syntax and an example code to demonstrate the same :)', 'Other answers correctly talk about the Python ternary operator. I would like to complement by mentioning a scenario for which the ternary operator is often used, but for which there is a better idiom. This is the scenario of using a default value.Suppose we want to use option_value with a default value if it is not set:or, if option_value is never set to a falsy value (0, \"\", etc.), simplyHowever, in this case an ever better solution is simply to write', \"The syntax for the ternary operator in Python is:[on_true] if [expression] else [on_false]Using that syntax, here is how we would rewrite the code above using Python\u2019s ternary operator:It's still pretty clear, but much shorter. Note that the expression could be any type of expression, including a function call, that returns a value that evaluates to True or False.\", \"Python has a ternary form for assignments; however there may be even a shorter form that people should be aware of.It's very common to need to assign to a variable one value or another depending on a condition.^ This is the long form for doing such assignments.Below is the ternary form. But this isn't the most succinct way - see the last example.With Python, you can simply use or for alternative assignments.The above works since li1 is None and the interpreter treats that as False in logic expressions. The interpreter then moves on and evaluates the second expression, which is not None and it's not an empty list - so it gets assigned to a.This also works with empty lists. For instance, if you want to assign a whichever list has items.Knowing this, you can simply such assignments whenever you encounter them. This also works with strings and other iterables. You could assign a whichever string isn't empty.I always liked the C ternary syntax, but Python takes it a step further!I understand that some may say this isn't a good stylistic choice, because it relies on mechanics that aren't immediately apparent to all developers. I personally disagree with that viewpoint. Python is a syntax-rich language with lots of idiomatic tricks that aren't immediately apparent to the dabbler. But the more you learn and understand the mechanics of the underlying system, the more you appreciate it.\", 'Pythonic way of doing the things:But there always exists a different way of doing a ternary condition too:', 'There are multiple ways. The simplest one is to use the condition inside the \"print\" method.You can useWhich is equivalent to:In this way, more than two statements are also possible to print. For example:can be written as:', 'The if else-if version can be written as:', 'Yes, it has, but it\\'s different from C-syntax-like programming languages (which is condition ? value_if_true : value_if_falseIn Python, it goes like this: value_if_true if condition else value_if_falseExample: even_or_odd = \"even\" if x % 2 == 0 else \"odd\"', 'A neat way to chain multiple operators:', 'I find the default Python syntax val = a if cond else b cumbersome, so sometimes I do this:Of course, it has the downside of always evaluating both sides (a and b), but the syntax is way clearer to me.']",
            "url": "https://stackoverflow.com/questions/394809"
        },
        {
            "tag": "python",
            "question": [
                "What are metaclasses in Python?"
            ],
            "votes": "7075",
            "answer": "['Before understanding metaclasses, you need to master classes in Python. And Python has a very peculiar idea of what classes are, borrowed from the Smalltalk language.In most languages, classes are just pieces of code that describe how to produce an object. That\\'s kinda true in Python too:But classes are more than that in Python. Classes are objects too.Yes, objects.As soon as you use the keyword class, Python executes it and creates\\nan object. The instructioncreates in memory an object with the name ObjectCreator.This object (the class) is itself capable of creating objects (the instances),\\nand this is why it\\'s a class.But still, it\\'s an object, and therefore:e.g.:Since classes are objects, you can create them on the fly, like any object.First, you can create a class in a function using class:But it\\'s not so dynamic, since you still have to write the whole class yourself.Since classes are objects, they must be generated by something.When you use the class keyword, Python creates this object automatically. But as\\nwith most things in Python, it gives you a way to do it manually.Remember the function type? The good old function that lets you know what\\ntype an object is:Well, type has also a completely different ability: it can create classes on the fly. type can take the description of a class as parameters,\\nand return a class.(I  know, it\\'s silly that the same function can have two completely different uses according to the parameters you pass to it. It\\'s an issue due to backward\\ncompatibility in Python)type works this way:Where:e.g.:can be created manually this way:You\\'ll notice that we use MyShinyClass as the name of the class\\nand as the variable to hold the class reference. They can be different,\\nbut there is no reason to complicate things.type accepts a dictionary to define the attributes of the class. So:Can be translated to:And used as a normal class:And of course, you can inherit from it, so:would be:Eventually, you\\'ll want to add methods to your class. Just define a function\\nwith the proper signature and assign it as an attribute.And you can add even more methods after you dynamically create the class, just like adding methods to a normally created class object.You see where we are going: in Python, classes are objects, and you can create a class on the fly, dynamically.This is what Python does when you use the keyword class, and it does so by using a metaclass.Metaclasses are the \\'stuff\\' that creates classes.You define classes in order to create objects, right?But we learned that Python classes are objects.Well, metaclasses are what create these objects. They are the classes\\' classes,\\nyou can picture them this way:You\\'ve seen that type lets you do something like this:It\\'s because the function type is in fact a metaclass. type is the\\nmetaclass Python uses to create all classes behind the scenes.Now you wonder \"why the heck is it written in lowercase, and not Type?\"Well, I guess it\\'s a matter of consistency with str, the class that creates\\nstrings objects, and int the class that creates integer objects. type is\\njust the class that creates class objects.You see that by checking the __class__ attribute.Everything, and I mean everything, is an object in Python. That includes integers,\\nstrings, functions and classes. All of them are objects. And all of them have\\nbeen created from a class:Now, what is the __class__ of any __class__ ?So, a metaclass is just the stuff that creates class objects.You can call it a \\'class factory\\' if you wish.type is the built-in metaclass Python uses, but of course, you can create your\\nown metaclass.In Python 2, you can add a __metaclass__ attribute when you write a class (see next section for the Python 3 syntax):If you do so, Python will use the metaclass to create the class Foo.Careful, it\\'s tricky.You write class Foo(object) first, but the class object Foo is not created\\nin memory yet.Python will look for __metaclass__ in the class definition. If it finds it,\\nit will use it to create the object class Foo. If it doesn\\'t, it will use\\ntype to create the class.Read that several times.When you do:Python does the following:Is there a __metaclass__ attribute in Foo?If yes, create in-memory a class object (I said a class object, stay with me here), with the name Foo by using what is in __metaclass__.If Python can\\'t find __metaclass__, it will look for a __metaclass__ at the MODULE level, and try to do the same (but only for classes that don\\'t inherit anything, basically old-style classes).Then if it can\\'t find any __metaclass__ at all, it will use the Bar\\'s (the first parent) own metaclass (which might be the default type) to create the class object.Be careful here that the __metaclass__ attribute will not be inherited, the metaclass of the parent (Bar.__class__) will be. If Bar used a __metaclass__ attribute that created Bar with type() (and not type.__new__()), the subclasses will not inherit that behavior.Now the big question is, what can you put in __metaclass__?The answer is something that can create a class.And what can create a class? type, or anything that subclasses or uses it.The syntax to set the metaclass has been changed in Python 3:i.e. the __metaclass__ attribute is no longer used, in favor of a keyword argument in the list of base classes.The behavior of metaclasses however stays largely the same.One thing added to metaclasses in Python 3 is that you can also pass attributes as keyword-arguments into a metaclass, like so:Read the section below for how Python handles this.The main purpose of a metaclass is to change the class automatically,\\nwhen it\\'s created.You usually do this for APIs, where you want to create classes matching the\\ncurrent context.Imagine a stupid example, where you decide that all classes in your module\\nshould have their attributes written in uppercase. There are several ways to\\ndo this, but one way is to set __metaclass__ at the module level.This way, all classes of this module will be created using this metaclass,\\nand we just have to tell the metaclass to turn all attributes to uppercase.Luckily, __metaclass__ can actually be any callable, it doesn\\'t need to be a\\nformal class (I know, something with \\'class\\' in its name doesn\\'t need to be\\na class, go figure... but it\\'s helpful).So we will start with a simple example, by using a function.Let\\'s check:Now, let\\'s do exactly the same, but using a real class for a metaclass:Let\\'s rewrite the above, but with shorter and more realistic variable names now that we know what they mean:You may have noticed the extra argument cls. There is\\nnothing special about it: __new__ always receives the class it\\'s defined in, as the first parameter. Just like you have self for ordinary methods which receive the instance as the first parameter, or the defining class for class methods.But this is not proper OOP. We are calling type directly and we aren\\'t overriding or calling the parent\\'s __new__. Let\\'s do that instead:We can make it even cleaner by using super, which will ease inheritance (because yes, you can have metaclasses, inheriting from metaclasses, inheriting from type):Oh, and in Python 3 if you do this call with keyword arguments, like this:It translates to this in the metaclass to use it:That\\'s it. There is really nothing more about metaclasses.The reason behind the complexity of the code using metaclasses is not because\\nof metaclasses, it\\'s because you usually use metaclasses to do twisted stuff\\nrelying on introspection, manipulating inheritance, vars such as __dict__, etc.Indeed, metaclasses are especially useful to do black magic, and therefore\\ncomplicated stuff. But by themselves, they are simple:Since __metaclass__ can accept any callable, why would you use a class\\nsince it\\'s obviously more complicated?There are several reasons to do so:Now the big question. Why would you use some obscure error-prone feature?Well, usually you don\\'t:Metaclasses are deeper magic that\\n99% of users should never worry about it.\\nIf you wonder whether you need them,\\nyou don\\'t (the people who actually\\nneed them know with certainty that\\nthey need them, and don\\'t need an\\nexplanation about why).Python Guru Tim PetersThe main use case for a metaclass is creating an API. A typical example of this is the Django ORM. It allows you to define something like this:But if you do this:It won\\'t return an IntegerField object. It will return an int, and can even take it directly from the database.This is possible because models.Model defines __metaclass__ and\\nit uses some magic that will turn the Person you just defined with simple statements\\ninto a complex hook to a database field.Django makes something complex look simple by exposing a simple API\\nand using metaclasses, recreating code from this API to do the real job\\nbehind the scenes.First, you know that classes are objects that can create instances.Well, in fact, classes are themselves instances. Of metaclasses.Everything is an object in Python, and they are all either instance of classes\\nor instances of metaclasses.Except for type.type is actually its own metaclass. This is not something you could\\nreproduce in pure Python, and is done by cheating a little bit at the implementation\\nlevel.Secondly, metaclasses are complicated. You may not want to use them for\\nvery simple class alterations. You can change classes by using two different techniques:99% of the time you need class alteration, you are better off using these.But 98% of the time, you don\\'t need class alteration at all.', \"A metaclass is the class of a class. A class defines how an instance of the class (i.e. an object) behaves while a metaclass defines how a class behaves. A class is an instance of a metaclass.While in Python you can use arbitrary callables for metaclasses (like Jerub shows), the better approach is to make it an actual class itself. type is the usual metaclass in Python. type is itself a class, and it is its own type. You won't be able to recreate something like type purely in Python, but Python cheats a little. To create your own metaclass in Python you really just want to subclass type.A metaclass is most commonly used as a class-factory. When you create an object by calling the class, Python creates a new class (when it executes the 'class' statement) by calling the metaclass. Combined with the normal __init__ and __new__ methods, metaclasses therefore allow you to do 'extra things' when creating a class, like registering the new class with some registry or replace the class with something else entirely.When the class statement is executed, Python first executes the body of the class statement as a normal block of code. The resulting namespace (a dict) holds the attributes of the class-to-be. The metaclass is determined by looking at the baseclasses of the class-to-be (metaclasses are inherited), at the __metaclass__ attribute of the class-to-be (if any) or the __metaclass__ global variable. The metaclass is then called with the name, bases and attributes of the class to instantiate it.However, metaclasses actually define the type of a class, not just a factory for it, so you can do much more with them. You can, for instance, define normal methods on the metaclass. These metaclass-methods are like classmethods in that they can be called on the class without an instance, but they are also not like classmethods in that they cannot be called on an instance of the class. type.__subclasses__() is an example of a method on the type metaclass. You can also define the normal 'magic' methods, like __add__, __iter__ and __getattr__, to implement or change how the class behaves.Here's an aggregated example of the bits and pieces:\", 'Note, this answer is for Python 2.x as it was written in 2008, metaclasses are slightly different in 3.x.Metaclasses are the secret sauce that make \\'class\\' work. The default metaclass for a new style object is called \\'type\\'.Metaclasses take 3 args. \\'name\\', \\'bases\\' and \\'dict\\'Here is where the secret starts. Look for where name, bases and the dict come from in this example class definition.Lets define a metaclass that will demonstrate how \\'class:\\' calls it.And now, an example that actually means something, this will automatically make the variables in the list \"attributes\" set on the class, and set to None.Note that the magic behaviour that Initialised gains by having the metaclass init_attributes is not passed onto a subclass of Initialised.Here is an even more concrete example, showing how you can subclass \\'type\\' to make a metaclass that performs an action when the class is created. This is quite tricky:', \"Others have explained how metaclasses work and how they fit into the Python type system. Here's an example of what they can be used for. In a testing framework I wrote, I wanted to keep track of the order in which classes were defined, so that I could later instantiate them in this order. I found it easiest to do this using a metaclass.Anything that's a subclass of MyType then gets a class attribute _order that records the order in which the classes were defined.\", 'One use for metaclasses is adding new properties and methods to an instance automatically.For example, if you look at Django models, their definition looks a bit confusing. It looks as if you are only defining class properties:However, at runtime the Person objects are filled with all sorts of useful methods. See the source for some amazing metaclassery.', \"I think the ONLamp introduction to metaclass programming is well written and gives a really good introduction to the topic despite being several years old already.http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html (archived at https://web.archive.org/web/20080206005253/http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html)In short: A class is a blueprint for the creation of an instance, a metaclass is a blueprint for the creation of a class. It can be easily seen that in Python classes need to be first-class objects too to enable this behavior.I've never written one myself, but I think one of the nicest uses of metaclasses can be seen in the Django framework. The model classes use a metaclass approach to enable a declarative style of writing new models or form classes. While the metaclass is creating the class, all members get the possibility to customize the class itself.The thing that's left to say is: If you don't know what metaclasses are, the probability that you will not need them is 99%.\", \"TLDR: A metaclass instantiates and defines behavior for a class just like a class instantiates and defines behavior for an instance.Pseudocode:The above should look familiar. Well, where does Class come from? It's an instance of a metaclass (also pseudocode):In real code, we can pass the default metaclass, type, everything we need to instantiate a class and we get a class:A class is to an instance as a metaclass is to a class.When we instantiate an object, we get an instance:Likewise, when we define a class explicitly with the default metaclass, type, we instantiate it:Put another way, a class is an instance of a metaclass:Put a third way, a metaclass is a class's class.When you write a class definition and Python executes it, it uses a metaclass to instantiate the class object (which will, in turn, be used to instantiate instances of that class).Just as we can use class definitions to change how custom object instances behave, we can use a metaclass class definition to change the way a class object behaves.What can they be used for? From the docs:The potential uses for metaclasses are boundless. Some ideas that have been explored include logging, interface checking, automatic delegation, automatic property creation, proxies, frameworks, and automatic resource locking/synchronization.Nevertheless, it is usually encouraged for users to avoid using metaclasses unless absolutely necessary.When you write a class definition, for example, like this,You instantiate a class object.It is the same as functionally calling type with the appropriate arguments and assigning the result to a variable of that name:Note, some things automatically get added to the __dict__, i.e., the namespace:The metaclass of the object we created, in both cases, is type.(A side-note on the contents of the class __dict__: __module__ is there because classes must know where they are defined, and  __dict__ and __weakref__ are there because we don't define __slots__ - if we define __slots__ we'll save a bit of space in the instances, as we can disallow __dict__ and __weakref__ by excluding them. For example:... but I digress.)Here's the default __repr__ of classes:One of the most valuable things we can do by default in writing a Python object is to provide it with a good __repr__. When we call help(repr) we learn that there's a good test for a __repr__ that also requires a test for equality - obj == eval(repr(obj)). The following simple implementation of __repr__ and __eq__ for class instances of our type class provides us with a demonstration that may improve on the default __repr__ of classes:So now when we create an object with this metaclass, the __repr__ echoed on the command line provides a much less ugly sight than the default:With a nice __repr__ defined for the class instance, we have a stronger ability to debug our code. However, much further checking with eval(repr(Class)) is unlikely (as functions would be rather impossible to eval from their default __repr__'s).If, for example, we want to know in what order a class's methods are created in, we could provide an ordered dict as the namespace of the class. We would do this with __prepare__ which returns the namespace dict for the class if it is implemented in Python 3:And usage:And now we have a record of the order in which these methods (and other class attributes) were created:Note, this example was adapted from the documentation - the new enum in the standard library does this.So what we did was instantiate a metaclass by creating a class. We can also treat the metaclass as we would any other class. It has a method resolution order:And it has approximately the correct repr (which we can no longer eval unless we can find a way to represent our functions.):\", \"Python 3 updateThere are (at this point) two key methods in a metaclass:__prepare__ lets you supply a custom mapping (such as an OrderedDict) to be used as the namespace while the class is being created.  You must return an instance of whatever namespace you choose.  If you don't implement __prepare__ a normal dict is used.__new__ is responsible for the actual creation/modification of the final class.A bare-bones, do-nothing-extra metaclass would like:A simple example:Say you want some simple validation code to run on your attributes -- like it must always be an int or a str.  Without a metaclass, your class would look something like:As you can see, you have to repeat the name of the attribute twice.  This makes typos possible along with irritating bugs.A simple metaclass can address that problem:This is what the metaclass would look like (not using __prepare__ since it is not needed):A sample run of:produces:Note:  This example is simple enough it could have also been accomplished with a class decorator, but presumably an actual metaclass would be doing much more.The 'ValidateType' class for reference:\", \"If you've done Python programming for more than a few months you'll eventually stumble upon code that looks like this:The latter is possible when you implement the __call__() magic method on the class.The __call__() method is invoked when an instance of a class is used as a callable. But as we've seen from previous answers a class itself is an instance of a metaclass, so when we use the class as a callable (i.e. when we create an instance of it) we're actually calling its metaclass' __call__() method. At this point most Python programmers are a bit confused because they've been told that when creating an instance like this instance = SomeClass() you're calling its __init__() method. Some who've dug a bit deeper know that before __init__() there's __new__(). Well, today another layer of truth is being revealed, before __new__() there's the metaclass' __call__().Let's study the method call chain from specifically the perspective of creating an instance of a class.This is a metaclass that logs exactly the moment before an instance is created and the moment it's about to return it.This is a class that uses that metaclassAnd now let's create an instance of Class_1Observe that the code above doesn't actually do anything more than logging the tasks. Each method delegates the actual work to its parent's implementation, thus keeping the default behavior. Since type is Meta_1's parent class (type being the default parent metaclass) and considering the ordering sequence of the output above, we now have a clue as to what would be the pseudo implementation of type.__call__():We can see that the metaclass' __call__() method is the one that's called first. It then delegates creation of the instance to the class's __new__() method and initialization to the instance's __init__(). It's also the one that ultimately returns the instance.From the above it stems that the metaclass' __call__() is also given the opportunity to decide whether or not a call to Class_1.__new__() or Class_1.__init__() will eventually be made. Over the course of its execution it could actually return an object that hasn't been touched by either of these methods. Take for example this approach to the singleton pattern:Let's observe what happens when repeatedly trying to create an object of type Class_2\", 'A metaclass is a class that tells how (some) other class should be created.This is a case where I saw metaclass as a solution to my problem:\\nI had a really complicated problem, that probably could have been solved differently, but I chose to solve it using a metaclass.  Because of the complexity, it is one of the few modules I have written where the comments in the module surpass the amount of code that has been written.  Here it is...', 'The type(obj) function gets you the type of an object.The type() of a class is its metaclass.To use a metaclass:type is its own metaclass. The class of a class is a metaclass-- the body of a class is the arguments passed to the metaclass that is used to construct the class.Here you can read about how to use metaclasses to customize class construction.', 'type is actually a metaclass -- a class that creates another classes.\\nMost metaclass are the subclasses of type. The metaclass receives the new class as its first argument and provide access to class object with details as mentioned below:Note:Notice that the class was not instantiated at any time; the simple act of creating the class triggered execution of the metaclass.', \"Python classes are themselves objects - as in instance - of their meta-class.The default metaclass, which is applied when when you determine classes as:meta class are used to apply some rule to an entire set of classes. For example, suppose you're building an ORM to access a database, and you want records from each table to be of a class mapped to that table (based on fields, business rules, etc..,), a possible use of metaclass is for instance, connection pool logic, which is share by all classes of record from all tables. Another use is logic to to support foreign keys, which involves multiple classes of records.when you define metaclass, you subclass type, and can overrided the following magic methods to insert your logic.anyhow, those two are the most commonly used hooks. metaclassing is powerful, and above is nowhere near and exhaustive list of uses for metaclassing.\", 'The type() function can return the type of an object or create a new type,for example, we can create a Hi class with the type() function and do not  need to use this way with class Hi(object):In addition to using type() to create classes dynamically, you can control creation behavior of class and use metaclass.According to the Python object model, the class is the object, so the class must be an instance of another certain class.\\nBy default, a Python class is instance of the type class. That is, type is metaclass of most of the built-in classes and metaclass of user-defined classes.Magic will take effect when we passed keyword arguments in metaclass, it indicates the Python interpreter to create the CustomList through ListMetaclass. new (), at this point, we can modify the class definition, for example, and add a new method and then return the revised definition.', \"In addition to the published answers I can say that a metaclass defines the behaviour for a class. So, you can explicitly set your metaclass. Whenever Python gets a keyword class then it starts searching for the metaclass. If it's not found \u2013 the default metaclass type is used to create the class's object. Using the __metaclass__ attribute, you can set metaclass of your class:It'll produce the output like this:And, of course, you can create your own metaclass to define the behaviour of any class that are created using your class.For doing that, your default metaclass type class must be inherited as this is the main metaclass:The output will be:\", 'Note that in python 3.6 a new dunder method __init_subclass__(cls, **kwargs) was introduced to replace a lot of common use cases for metaclasses. Is is called when a subclass of the defining class is created. See python docs.', \"Here's another example of what it can be used for:The metaclass is powerful, there are many things (such as monkey magic) you can do with it, but be careful this may only be known to you.\", \"The top answer is correct.But readers may be coming here searching answers about similarly named inner classes. They are present in popular libraries, such as Django and WTForms.As DavidW points out in the comments beneath this answer, these are library-specific features and are not to be confused with the advanced, unrelated Python language feature with a similar name.Rather, these are namespaces within classes' dicts. They are constructed using inner classes for sake of readability.In this example special field, abstract is visibly separate from fields of Author model.Another example is from the documentation for WTForms:This syntax does not get special treatment in the python programming language. Meta is not a keyword here, and does not trigger metaclass behavior. Rather, third-party library code in packages like Django and WTForms reads this property in the constructors of certain classes, and elsewhere.The presence of these declarations modifies the behavior of the classes that have these declarations. For example, WTForms reads self.Meta.csrf to determine if the form needs a csrf field.\", 'In object-oriented programming, a metaclass is a class whose instances are classes. Just as an ordinary class defines the behavior of certain objects, a metaclass defines the behavior of certain class and their instances\\nThe term metaclass simply means something used to create classes. In other words, it is the class of a class. The metaclass is used to create the class so like the object being an instance of a class, a class is an instance of a metaclass. In python classes are also considered objects.', 'A class, in Python, is an object, and just like any other object, it is an instance of \"something\". This \"something\" is what is termed as a Metaclass. This metaclass is a special type of class that creates other class\\'s objects. Hence, metaclass is responsible for making new classes. This allows the programmer to customize the way classes are generated.To create a metaclass, overriding of new() and init() methods is usually done. new() can be overridden to change the way objects are created, while init() can be overridden to change the way of initializing the object. Metaclass can be created by a number of ways. One of the ways is to use type() function. type() function, when called with 3 parameters, creates a metaclass. The parameters are :-Another way of creating a metaclass comprises of \\'metaclass\\' keyword. Define the metaclass as a simple class. In the parameters of inherited class, pass metaclass=metaclass_nameMetaclass can be specifically used in the following situations :-', 'I saw an interesting use case for metaclasses in a package called classutilities. It checks if all class variables are in upper case format (it is convenient to have unified logic for configuration classes), and checks if there are no instance level methods in class.\\nAnother interesting example for metaclases was deactivation of unittests based on complex conditions (checking values of multiple environmental variables).', \"In Python, a metaclass is a subclass of a subclass that determines how a subclass behaves. A class is an instance of another metaclass. In Python, a class specifies how the class's instance will behave.Since metaclasses are in charge of class generation, you can\\xa0write your own custom metaclasses to change how classes are created by performing additional actions or injecting code. Custom metaclasses aren't always important, but they can be.\", 'look this:In other words, when an object was not created (type of object), we looking MetaClass.']",
            "url": "https://stackoverflow.com/questions/100003"
        },
        {
            "tag": "python",
            "question": [
                "How do I check whether a file exists without exceptions?"
            ],
            "votes": "6861",
            "answer": "[\"If the reason you're checking is so you can do something like if file_exists: open_it(), it's safer to use a try around the attempt to open it. Checking and then opening risks the file being deleted or moved or something between when you check and when you try to open it.If you're not planning to open the file immediately, you can use os.path.isfileReturn True if path is an existing regular file. This follows symbolic links, so both islink() and isfile() can be true for the same path.if you need to be sure it's a file.Starting with Python 3.4, the pathlib module offers an object-oriented approach (backported to pathlib2 in Python 2.7):To check a directory, do:To check whether a Path object exists independently of whether is it a file or directory, use exists():You can also use resolve(strict=True) in a try block:\", 'Use os.path.exists to check both files and directories:Use os.path.isfile to check only files (note: follows symbolic links):', \"Unlike isfile(), exists() will return True for directories. So depending on if you want only plain files or also directories, you'll use isfile() or exists(). Here is some simple REPL output:\", '', 'Use os.path.isfile() with os.access():', '', 'Although almost every possible way has been listed in (at least one of) the existing answers (e.g. Python 3.4 specific stuff was added), I\\'ll try to group everything together.Note: every piece of Python standard library code that I\\'m going to post, belongs to version 3.5.3.Problem statement:Check file (arguable: also folder (\"special\" file) ?) existenceDon\\'t use try / except / else / finally blocksPossible solutions:Also check other function family members like os.path.isfile, os.path.isdir, os.path.lexists for slightly different behaviors:Return True if path refers to an existing path or an open file descriptor. Returns False for broken symbolic links. On some platforms, this function may return False if permission is not granted to execute os.stat() on the requested file, even if the path physically exists.All good, but if following the import tree:os.path - posixpath.py (ntpath.py)genericpath.py - line ~20+it\\'s just a try / except block around [Python.Docs]: os.stat(path, *, dir_fd=None, follow_symlinks=True). So, your code is try / except free, but lower in the framestack there\\'s (at least) one such block. This also applies to other functions (including os.path.isfile).It\\'s a fancier (and more [Wiktionary]: Pythonic) way of handling paths, butUnder the hood, it does exactly the same thing (pathlib.py - line ~1330):Either:Create one:And its usage - I\\'ll replicate the os.path.isfile behavior (note that this is just for demonstrating purposes, do not attempt to write such code for production):Use [Python.Docs]: contextlib.suppress(*exceptions) - which was specifically designed for selectively suppressing exceptionsBut, they seem to be wrappers over try / except / else / finally blocks, as [Python.Docs]: Compound statements - The with statement states:This allows common try...except...finally usage patterns to be encapsulated for convenient reuse.Search the results for matching item(s):[Python.Docs]: os.listdir(path=\\'.\\') (or [Python.Docs]: os.scandir(path=\\'.\\') on Python v3.5+, backport: [PyPI]: scandir)Under the hood, both use:Nix: [Man7]: OPENDIR(3) / [Man7]: READDIR(3) / [Man7]: CLOSEDIR(3)Win: [MS.Learn]: FindFirstFileW function (fileapi.h) / [MS.Learn]: FindNextFileW function (fileapi.h) / [MS.Learn]: FindClose function (fileapi.h)via [GitHub]: python/cpython - (main) cpython/Modules/posixmodule.cUsing scandir() instead of listdir() can significantly increase the performance of code that also needs file type or file attribute information, because os.DirEntry objects expose this information if the operating system provides it when scanning a directory. All os.DirEntry methods may perform a system call, but is_dir() and is_file() usually only require a system call for symbolic links; os.DirEntry.stat() always requires a system call on Unix, but only requires one for symbolic links on Windows.[Python.Docs]: os.walk(top, topdown=True, onerror=None, followlinks=False)[Python.Docs]: glob.iglob(pathname, *, root_dir=None, dir_fd=None, recursive=False, include_hidden=False) (or its predecessor: glob.glob)Since these iterate over folders, (in most of the cases) they are inefficient for our problem (there are exceptions, like non wildcarded globbing - as @ShadowRanger pointed out), so I\\'m not going to insist on them. Not to mention that in some cases, filename processing might be required.Its behavior is close to os.path.exists (actually it\\'s wider, mainly because of the 2nd argument).User permissions might restrict the file \"visibility\" as the doc states:... test if the invoking user has the specified access to path. mode should be F_OK to test the existence of path...Security considerations:Using access() to check if a user is authorized to e.g. open a file before actually doing so using open() creates a security hole, because the user might exploit the short time interval between checking and opening the file to manipulate it.Since I also work in C, I use this method as well because under the hood, it calls native APIs (again, via \"${PYTHON_SRC_DIR}/Modules/posixmodule.c\"), but it also opens a gate for possible user errors, and it\\'s not as Pythonic as other variants. So, don\\'t use it unless you know what you\\'re doing:Nix: [Man7]: ACCESS(2)Warning: Using these calls to check if a user is authorized to, for example, open a file before actually doing so using open(2) creates a security hole, because the user might exploit the short time interval between checking and opening the file to manipulate it. For this reason, the use of this system call should be avoided.Win: [MS.Learn]: GetFileAttributesW function (fileapi.h)As seen, this approach is highly discouraged (especially on Nix).Note: calling native APIs is also possible via [Python.Docs]: ctypes - A foreign function library for Python, but in most cases it\\'s more complicated. Before working with CTypes, check [SO]: C function called from Python via ctypes returns incorrect value (@CristiFati\\'s answer) out.(Win specific): since vcruntime###.dll (msvcr###.dll for older VStudio versions - I\\'m going to refer to it as UCRT) exports a [MS.Learn]: _access, _waccess function family as well, here\\'s an example (note that the recommended [Python.Docs]: msvcrt - Useful routines from the MS VC++ runtime doesn\\'t export them):Notes:Although it\\'s not a good practice, I\\'m using os.F_OK in the call, but that\\'s just for clarity (its value is 0)I\\'m using _waccess so that the same code works on Python 3 and Python 2 (in spite of [Wikipedia]: Unicode related differences between them - [SO]: Passing utf-16 string to a Windows function (@CristiFati\\'s answer))Although this targets a very specific area, it was not mentioned in any of the previous answersThe Linux (Ubuntu ([Wikipedia]: Ubuntu version history) 16 x86_64 (pc064)) counterpart as well:Notes:Instead hardcoding libc.so (LibC)\\'s path (\"/lib/x86_64-linux-gnu/libc.so.6\") which may (and most likely, will) vary across systems, None (or the empty string) can be passed to CDLL constructor (ctypes.CDLL(None).access(b\"/tmp\", os.F_OK)). According to [Man7]: DLOPEN(3):If filename is NULL, then the returned handle is for the main\\nprogram.  When given to dlsym(3), this handle causes a search for a\\nsymbol in the main program, followed by all shared objects loaded at\\nprogram startup, and then all shared objects loaded by dlopen() with\\nthe flag RTLD_GLOBAL.Main (current) program (python) is linked against LibC, so its symbols (including access) will be loadedThis has to be handled with care, since functions like main, Py_Main and (all the) others are available; calling them could have disastrous effects (on the current program)This doesn\\'t also apply to Windows (but that\\'s not such a big deal, since UCRT is located in \"%SystemRoot%\\\\System32\" which is in %PATH% by default). I wanted to take things further and replicate this behavior on Windows (and submit a patch), but as it turns out, [MS.Learn]: GetProcAddress function (libloaderapi.h) only \"sees\" exported symbols, so unless someone declares the functions in the main executable as __declspec(dllexport) (why on Earth the common person would do that?), the main program is loadable, but it is pretty much unusableMost likely, will rely on one of the ways above (maybe with slight customizations). One example would be (again, Win specific) [GitHub]: mhammond/pywin32 - Python for Windows (pywin32) Extensions, which is a Python wrapper over WinAPIs.But, since this is more like a workaround, I\\'m stopping here.I consider this a (lame) workaround (gainarie): use Python as a wrapper to execute shell commands:Win:Nix ([Wikipedia]: Unix-like) - Ubuntu:Do use try / except / else / finally blocks, because they can prevent you running into a series of nasty problemsA possible counterexample that I can think of, is performance: such blocks are costly, so try not to place them in code that it\\'s supposed to run hundreds of thousands times per second (but since (in most cases) it involves disk access, it won\\'t be the case)', \"Python 3.4+ has an object-oriented path module: pathlib.  Using this new module, you can check whether a file exists like this:You can (and usually should) still use a try/except block when opening files:The pathlib module has lots of cool stuff in it: convenient globbing, checking file's owner, easier path joining, etc.  It's worth checking out.  If you're on an older Python (version 2.6 or later), you can still install pathlib with pip:Then import it as follows:\", \"This is the simplest way to check if a file exists. Just because the file existed when you checked doesn't guarantee that it will be there when you need to open it.\", 'Now available since Python 3.4, import and instantiate a Path object with the file name, and check the is_file method (note that this returns True for symlinks pointing to regular files as well):If you\\'re on Python 2, you can backport the pathlib module from pypi, pathlib2, or otherwise check isfile from the os.path module:Now the above is probably the best pragmatic direct answer here, but there\\'s the possibility of a race condition (depending on what you\\'re trying to accomplish), and the fact that the underlying implementation uses a try, but Python uses try everywhere in its implementation.Because Python uses try everywhere, there\\'s really no reason to avoid an implementation that uses it.But the rest of this answer attempts to consider these caveats.Available since Python 3.4, use the new Path object in pathlib. Note that .exists is not quite right, because directories are not files (except in the unix sense that everything is a file).So we need to use is_file:Here\\'s the help on is_file:So let\\'s get a file that we know is a file:By default, NamedTemporaryFile deletes the file when closed (and will automatically close when no more references exist to it).If you dig into the implementation, though, you\\'ll see that is_file uses try:We like try because it avoids race conditions. With try, you simply attempt to read your file, expecting it to be there, and if not, you catch the exception and perform whatever fallback behavior makes sense.If you want to check that a file exists before you attempt to read it, and you might be deleting it and then you might be using multiple threads or processes, or another program knows about that file and could delete it - you risk the chance of a race condition if you check it exists, because you are then racing to open it before its condition (its existence) changes.Race conditions are very hard to debug because there\\'s a very small window in which they can cause your program to fail.But if this is your motivation, you can get the value of a try statement by using the suppress context manager.Python 3.4 gives us the suppress context manager (previously the ignore context manager), which does semantically exactly the same thing in fewer lines, while also (at least superficially) meeting the original ask to avoid a try statement:Usage:For earlier Pythons, you could roll your own suppress, but without a try will be more verbose than with. I do believe this actually is the only answer that doesn\\'t use try at any level in the Python that can be applied to prior to Python 3.4 because it uses a context manager instead:Perhaps easier with a try:isfilefrom the docs:os.path.isfile(path)Return True if path is an existing regular file. This follows symbolic\\n  links, so both islink() and isfile() can be true for the same path.But if you examine the source of this function, you\\'ll see it actually does use a try statement:All it\\'s doing is using the given path to see if it can get stats on it,  catching OSError and then checking if it\\'s a file if it didn\\'t raise the exception.If you intend to do something with the file, I would suggest directly attempting it with a try-except to avoid a race condition:os.accessAvailable for Unix and Windows is os.access, but to use you must pass flags, and it does not differentiate between files and directories. This is more used to test if the real invoking user has access in an elevated privilege environment:It also suffers from the same race condition problems as isfile. From the docs:Note:\\n  Using access() to check if a user is authorized to e.g. open a file\\n  before actually doing so using open() creates a security hole, because\\n  the user might exploit the short time interval between checking and\\n  opening the file to manipulate it. It\u2019s preferable to use EAFP\\n  techniques. For example:is better written as:Avoid using os.access. It is a low level function that has more opportunities for user error than the higher level objects and functions discussed above.Another answer says this about os.access:Personally, I prefer this one because under the hood, it calls native APIs (via \"${PYTHON_SRC_DIR}/Modules/posixmodule.c\"), but it also opens a gate for possible user errors, and it\\'s not as Pythonic as other variants:This answer says it prefers a non-Pythonic, error-prone method, with no justification. It seems to encourage users to use low-level APIs without understanding them.It also creates a context manager which, by unconditionally returning True, allows all Exceptions (including KeyboardInterrupt and SystemExit!) to pass silently, which is a good way to hide bugs.This seems to encourage users to adopt poor practices.', \"Prefer the try statement. It's considered better style and avoids race conditions.Don't take my word for it. There's plenty of support for this theory. Here's a couple:\", 'Use:Importing os makes it easier to navigate and perform standard actions with your operating system.For reference, also see How do I check whether a file exists without exceptions?.If you need high-level operations, use shutil.', 'Testing for files and folders with os.path.isfile(), os.path.isdir() and os.path.exists()Assuming that the \"path\" is a valid path, this table shows what is returned by each function for files and folders:You can also test if a file is a certain type of file using os.path.splitext() to get the extension (if you don\\'t already know it)', 'TL;DR \\nThe answer is: use the pathlib modulePathlib is probably the most modern and convenient way for almost all of the file operations. For the existence of a file or a folder a single line of code is enough. If file is not exists, it will not throw any exception.The pathlib module was introduced in Python 3.4, so you need to have Python 3.4+. This library makes your life much easier while working with files and folders, and it is pretty to use. Here is more documentation about it: pathlib \u2014 Object-oriented filesystem paths.BTW, if you are going to reuse the path, then it is better to assign it to a variable.So it will become:', 'In 2016 the best way is still using os.path.isfile:Or in Python 3 you can use pathlib:', \"It doesn't seem like there's a meaningful functional difference between try/except and isfile(), so you should use which one makes sense.If you want to read a file, if it exists, doBut if you just wanted to rename a file if it exists, and therefore don't need to open it, doIf you want to write to a file, if it doesn't exist, doIf you need file locking, that's a different matter.\", \"You could try this (safer):The ouput would be:([Errno 2] No such file or directory:\\n  'whatever.txt')Then, depending on the result, your program can just keep running from there or you can code to stop it if you want.\", 'Date: 2017-12-04Every possible solution has been listed in other answers.An intuitive and arguable way to check if a file exists is the following:I made an exhaustive cheat sheet for your reference:', \"Although I always recommend using try and except statements, here are a few possibilities for you (my personal favourite is using os.access):Try opening the file:Opening the file will always verify the existence of the file. You can make a function just like so:If it's False, it will stop execution with an unhanded IOError\\nor OSError in later versions of Python. To catch the exception,\\nyou have to use a try except clause. Of course, you can always\\nuse a try except` statement like so (thanks to hsandt\\nfor making me think):Use os.path.exists(path):This will check the existence of what you specify. However, it checks for files and directories so beware about how you use it.Use os.access(path, mode):This will check whether you have access to the file. It will check for permissions. Based on the os.py documentation, typing in os.F_OK, it will check the existence of the path. However, using this will create a security hole, as someone can attack your file using the time between checking the permissions and opening the file. You should instead go directly to opening the file instead of checking its permissions. (EAFP vs LBYP). If you're not going to open the file afterwards, and only checking its existence, then you can use this.Anyway, here:I should also mention that there are two ways that you will not be able to verify the existence of a file. Either the issue will be permission denied or no such file or directory. If you catch an IOError, set the IOError as e (like my first option), and then type in print(e.args) so that you can hopefully determine your issue. I hope it helps! :)\", 'If the file is for opening you could use one of the following techniques:Note: This finds either a file or a directory with the given name.', 'Additionally, os.access():Being R_OK, W_OK, and X_OK the flags to test for permissions (doc).', 'Raising exceptions is considered to be an acceptable, and Pythonic,\\napproach for flow control in your program. Consider handling missing\\nfiles with IOErrors. In this situation, an IOError exception will be\\nraised if the file exists but the user does not have read permissions.Source: Using Python: How To Check If A File Exists', 'If you imported NumPy already for other purposes then there is no need to import other libraries like pathlib, os, paths, etc.This will return true or false based on its existence.', 'You can follow these three ways:Note 1: The os.path.isfile used only for filesNote 2: The os.path.exists is used for both files and directories', \"You can write Brian's suggestion without the try:.suppress is part of Python 3.4. In older releases you can quickly write your own suppress:\", \"Adding one more slight variation which isn't exactly reflected in the other answers.This will handle the case of the file_path being None or empty string.Adding a variant based on suggestion from ShahbazAdding a variant based on suggestion from Peter Wood\", 'I\\'m the author of a package that\\'s been around for about 10 years, and it has a function that addresses this question directly. Basically, if you are on a non-Windows system, it uses Popen to access find.  However, if you are on Windows, it replicates find with an efficient filesystem walker.The code itself does not use a try block\u2026 except in determining the operating system and thus steering you to the \"Unix\"-style find or the hand-buillt find. Timing tests showed that the try was faster in determining the OS, so I did use one there (but nowhere else).And the doc\u2026The implementation, if you care to look, is here:\\nhttps://github.com/uqfoundation/pox/blob/89f90fb308f285ca7a62eabe2c38acb87e89dad9/pox/shutils.py#L190', \"Here's a one-line Python command for the Linux command line environment. I find this very handy since I'm not such a hot Bash guy.\", 'You can use the \"OS\" library of Python:', 'How do I check whether a file exists, without using the try statement?In 2016, this is still arguably the easiest way to check if both a file exists and if it is a file:isfile is actually just a helper method that internally uses os.stat and stat.S_ISREG(mode) underneath. This os.stat is a lower-level method that will provide you with detailed information about files, directories, sockets, buffers, and more. More about os.stat hereNote: However, this approach will not lock the file in any way and therefore your code can become vulnerable to \"time of check to time of use\" (TOCTTOU) bugs.So raising exceptions is considered to be an acceptable, and Pythonic, approach for flow control in your program. And one should consider handling missing files with IOErrors, rather than if statements (just an advice).']",
            "url": "https://stackoverflow.com/questions/82831"
        },
        {
            "tag": "python",
            "question": [
                "How do I merge two dictionaries in a single expression?"
            ],
            "votes": "6564",
            "answer": "['For dictionaries x and y, their shallowly-merged dictionary z takes values from y, replacing those from x.In Python 3.9.0 or greater (released 17 October 2020, PEP-584, discussed here):In Python 3.5 or greater:In Python 2, (or 3.4 or lower) write a function:and now:Say you have two dictionaries and you want to merge them into a new dictionary without altering the original dictionaries:The desired result is to get a new dictionary (z) with the values merged, and the second dictionary\\'s values overwriting those from the first.A new syntax for this, proposed in PEP 448 and available as of Python 3.5, isAnd it is indeed a single expression.Note that we can merge in with literal notation as well:and now:It is now showing as implemented in the release schedule for 3.5, PEP 478, and it has now made its way into the What\\'s New in Python 3.5 document.However, since many organizations are still on Python 2, you may wish to do this in a backward-compatible way. The classically Pythonic way, available in Python 2 and Python 3.0-3.4, is to do this as a two-step process:In both approaches, y will come second and its values will replace x\\'s values, thus b will point to 3 in our final result.If you are not yet on Python 3.5 or need to write backward-compatible code, and you want this in a single expression, the most performant while the correct approach is to put it in a function:and then you have a single expression:You can also make a function to merge an arbitrary number of dictionaries, from zero to a very large number:This function will work in Python 2 and 3 for all dictionaries. e.g. given dictionaries a to g:and key-value pairs in g will take precedence over dictionaries a to f, and so on.Don\\'t use what you see in the formerly accepted answer:In Python 2, you create two lists in memory for each dict, create a third list in memory with length equal to the length of the first two put together, and then discard all three lists to create the dict. In Python 3, this will fail because you\\'re adding two dict_items objects together, not two lists -and you would have to explicitly create them as lists, e.g. z = dict(list(x.items()) + list(y.items())). This is a waste of resources and computation power.Similarly, taking the union of items() in Python 3 (viewitems() in Python 2.7) will also fail when values are unhashable objects (like lists, for example). Even if your values are hashable, since sets are semantically unordered, the behavior is undefined in regards to precedence. So don\\'t do this:This example demonstrates what happens when values are unhashable:Here\\'s an example where y should have precedence, but instead the value from x is retained due to the arbitrary order of sets:Another hack you should not use:This uses the dict constructor and is very fast and memory-efficient (even slightly more so than our two-step process) but unless you know precisely what is happening here (that is, the second dict is being passed as keyword arguments to the dict constructor), it\\'s difficult to read, it\\'s not the intended usage, and so it is not Pythonic.Here\\'s an example of the usage being remediated in django.Dictionaries are intended to take hashable keys (e.g. frozensets or tuples), but this method fails in Python 3 when keys are not strings.From the mailing list, Guido van Rossum, the creator of the language, wrote:I am fine with\\ndeclaring dict({}, **{1:3}) illegal, since after all it is abuse of\\nthe ** mechanism.andApparently dict(x, **y) is going around as \"cool hack\" for \"call\\nx.update(y) and return x\". Personally, I find it more despicable than\\ncool.It is my understanding (as well as the understanding of the creator of the language) that the intended usage for dict(**y) is for creating dictionaries for readability purposes, e.g.:instead ofDespite what Guido says, dict(x, **y) is in line with the dict specification, which btw. works for both Python 2 and 3. The fact that this only works for string keys is a direct consequence of how keyword parameters work and not a short-coming of dict. Nor is using the ** operator in this place an abuse of the mechanism, in fact, ** was designed precisely to pass dictionaries as keywords.Again, it doesn\\'t work for 3 when keys are not strings. The implicit calling contract is that namespaces take ordinary dictionaries, while users must only pass keyword arguments that are strings. All other callables enforced it. dict broke this consistency in Python 2:This inconsistency was bad given other implementations of Python (PyPy, Jython, IronPython). Thus it was fixed in Python 3, as this usage could be a breaking change.I submit to you that it is malicious incompetence to intentionally write code that only works in one version of a language or that only works given certain arbitrary constraints.More comments:dict(x.items() + y.items()) is still the most readable solution for Python 2. Readability counts.My response: merge_two_dicts(x, y) actually seems much clearer to me, if we\\'re actually concerned about readability. And it is not forward compatible, as Python 2 is increasingly deprecated.{**x, **y} does not seem to handle nested dictionaries. the contents of nested keys are simply overwritten, not merged [...] I ended up being burnt by these answers that do not merge recursively and I was surprised no one mentioned it. In my interpretation of the word \"merging\" these answers describe \"updating one dict with another\", and not merging.Yes. I must refer you back to the question, which is asking for a shallow merge of two dictionaries, with the first\\'s values being overwritten by the second\\'s - in a single expression.Assuming two dictionaries of dictionaries, one might recursively merge them in a single function, but you should be careful not to modify the dictionaries from either source, and the surest way to avoid that is to make a copy when assigning values. As keys must be hashable and are usually therefore immutable, it is pointless to copy them:Usage:Coming up with contingencies for other value types is far beyond the scope of this question, so I will point you at my answer to the canonical question on a \"Dictionaries of dictionaries merge\".These approaches are less performant, but they will provide correct behavior.\\nThey will be much less performant than copy and update or the new unpacking because they iterate through each key-value pair at a higher level of abstraction, but they do respect the order of precedence (latter dictionaries have precedence)You can also chain the dictionaries manually inside a dict comprehension:or in Python 2.6 (and perhaps as early as 2.4 when generator expressions were introduced):itertools.chain will chain the iterators over the key-value pairs in the correct order:I\\'m only going to do the performance analysis of the usages known to behave correctly. (Self-contained so you can copy and paste yourself.)In Python 3.8.1, NixOS:', \"In your case, you can do:This will, as you want it, put the final dict in z, and make the value for key b be properly overridden by the second (y) dict's value:If you use Python 2, you can even remove the list() calls. To create z:If you use Python version 3.9.0a4 or greater, then you can directly use:\", 'An alternative:', \"Another, more concise, option:Note: this has become a popular answer, but it is important to point out that if y has any non-string keys, the fact that this works at all is an abuse of a CPython implementation detail, and it does not work in Python 3, or in PyPy, IronPython, or Jython. Also, Guido is not a fan. So I can't recommend this technique for forward-compatible or cross-implementation portable code, which really means it should be avoided entirely.\", \"This probably won't be a popular answer, but you almost certainly do not want to do this.  If you want a copy that's a merge, then use copy (or deepcopy, depending on what you want) and then update.  The two lines of code are much more readable - more Pythonic - than the single line creation with .items() + .items().  Explicit is better than implicit.In addition, when you use .items() (pre Python 3.0), you're creating a new list that contains the items from the dict.  If your dictionaries are large, then that is quite a lot of overhead (two large lists that will be thrown away as soon as the merged dict is created).  update() can work more efficiently, because it can run through the second dict item-by-item.In terms of time:IMO the tiny slowdown between the first two is worth it for the readability.  In addition, keyword arguments for dictionary creation was only added in Python 2.3, whereas copy() and update() will work in older versions.\", 'In a follow-up answer, you asked about the relative performance of these two alternatives:On my machine, at least (a fairly ordinary x86_64 running Python 2.5.2), alternative z2 is not only shorter and simpler but also significantly faster.  You can verify this for yourself using the timeit module that comes with Python.Example 1: identical dictionaries mapping 20 consecutive integers to themselves:z2 wins by a factor of 3.5 or so.  Different dictionaries seem to yield quite different results, but z2 always seems to come out ahead.  (If you get inconsistent results for the same test, try passing in -r with a number larger than the default 3.)Example 2: non-overlapping dictionaries mapping 252 short strings to integers and vice versa:z2 wins by about a factor of 10.  That\\'s a pretty big win in my book!After comparing those two, I wondered if z1\\'s poor performance could be attributed to the overhead of constructing the two item lists, which in turn led me to wonder if this variation might work better:A few quick tests, e.g.lead me to conclude that z3 is somewhat faster than z1, but not nearly as fast as z2.  Definitely not worth all the extra typing.This discussion is still missing something important, which is a performance comparison of these alternatives with the \"obvious\" way of merging two lists: using the update method.  To try to keep things on an equal footing with the expressions, none of which modify x or y, I\\'m going to make a copy of x instead of modifying it in-place, as follows:A typical result:In other words, z0 and z2 seem to have essentially identical performance.  Do you think this might be a coincidence?  I don\\'t....In fact, I\\'d go so far as to claim that it\\'s impossible for pure Python code to do any better than this.  And if you can do significantly better in a C extension module, I imagine the Python folks might well be interested in incorporating your code (or a variation on your approach) into the Python core.  Python uses dict in lots of places; optimizing its operations is a big deal.You could also write this asas Tony does, but (not surprisingly) the difference in notation turns out not to have any measurable effect on performance.  Use whichever looks right to you.  Of course, he\\'s absolutely correct to point out that the two-statement version is much easier to understand.', 'In Python 3.0 and later, you can use collections.ChainMap which groups multiple dicts or other mappings together to create a single, updateable view:Update for Python 3.5 and later: You can use PEP 448 extended dictionary packing and unpacking.  This is fast and easy:Update for Python 3.9 and later:  You can use the PEP 584 union operator:', 'I wanted something similar, but with the ability to specify how the values on duplicate keys were merged, so I hacked this out (but did not heavily test it).  Obviously this is not a single expression, but it is a single function call.', 'Demonstration:Outputs:Thanks rednaw for edits.', 'Python 3.5 (PEP 448) allows a nicer syntax option:Or evenIn Python 3.9 you also use | and |= with the below example from PEP 584', \"For items with keys in both dictionaries ('b'), you can control which one ends up in the output by putting that one last.\", \"The best version I could think while not using copy would be:It's faster than dict(x.items() + y.items()) but not as fast as n = copy(a); n.update(b), at least on CPython. This version also works in Python 3 if you change iteritems() to items(), which is automatically done by the 2to3 tool.Personally I like this version best because it describes fairly good what I want in a single  functional syntax. The only minor problem is that it doesn't make completely obvious that values from y takes precedence over values from x, but I don't believe it's difficult to figure that out.\", 'While the question has already been answered several times,\\nthis simple solution to the problem has not been listed yet.It is as fast as z0 and the evil z2 mentioned above, but easy to understand and change.', 'Among such shady and dubious answers, this shining example is the one and only good way to merge dicts in Python, endorsed by dictator for life Guido van Rossum himself!  Someone else suggested half of this, but did not put it in a function.gives:', 'I benchmarked the suggested with perfplot and found that the good oldis the fastest solution together with the good oldandCode to reproduce the plot:', 'Be Pythonic. Use a comprehension:', 'If you think lambdas are evil then read no further.\\nAs requested, you can write the fast and memory-efficient solution with one expression:As suggested above, using two lines or writing a function is probably a better way to go.', \"In python3, the items method no longer returns a list, but rather a view, which acts like a set. In this case you'll need to take the set union since concatenating with + won't work:For python3-like behavior in version 2.7, the viewitems method should work in place of items:I prefer this notation anyways since it seems more natural to think of it as a set union operation rather than concatenation (as the title shows).Edit:A couple more points for python 3. First, note that the dict(x, **y) trick won't work in python 3 unless the keys in y are strings.Also, Raymond Hettinger's Chainmap answer is pretty elegant, since it can take an arbitrary number of dicts as arguments, but from the docs it looks like it sequentially looks through a list of all the dicts for each lookup:Lookups search the underlying mappings successively until a key is found.This can slow you down if you have a lot of lookups in your application:So about an order of magnitude slower for lookups. I'm a fan of Chainmap, but looks less practical where there may be many lookups.\", 'Two dictionariesn dictionariessum has bad performance. See https://mathieularose.com/how-not-to-flatten-a-list-of-lists-in-python/', \"Simple solution using itertools that preserves order (latter dicts have precedence)And it's usage:\", \"Abuse leading to a one-expression solution for Matthew's answer:You said you wanted one expression, so I abused lambda to bind a name, and tuples to override lambda's one-expression limit. Feel free to cringe.You could also do this of course if you don't care about copying it:\", \"If you don't mind mutating x,Simple, readable, performant. You know update() always returns None, which is a false value. So the above expression will always evaluate to x, after updating it.Most mutating methods in the standard library (like .update()) return None by convention, so this kind of pattern will work on those too. However, if you're using a dict subclass or some other method that doesn't follow this convention, then or may return its left operand, which may not be what you want. Instead, you can use a tuple display and index, which works regardless of what the first element evaluates to (although it's not quite as pretty):If you don't have x in a variable yet, you can use lambda to make a local without using an assignment statement. This amounts to using lambda as a let expression, which is a common technique in functional languages, but is maybe unpythonic.Although it's not that different from the following use of the new walrus operator (Python 3.8+ only),especially if you use a default argument:If you do want a copy, PEP 584 style x | y is the most Pythonic on 3.9+. If you must support older versions, PEP 448 style {**x, **y} is easiest for 3.5+. But if that's not available in your (even older) Python version, the let expression pattern works here too.(That is, of course, nearly equivalent to (z := x.copy()).update(y) or z, but if your Python version is new enough for that, then the PEP 448 style will be available.)\", \"Drawing on ideas here and elsewhere I've comprehended a function:Usage (tested in python 3):You could use a lambda instead.\", 'New in Python 3.9: Use the union operator (|) to merge dicts similar to sets:For matching keys, the right dict takes precedence.This also works for |= to modify a dict in-place:', \"It's so silly that .update returns nothing.\\nI just use a simple helper function to solve the problem:Examples:\", \"(For Python\\xa02.7* only; there are simpler solutions for Python\\xa03*.)If you're not averse to importing a standard library module, you can do(The or a bit in the lambda is necessary because dict.update always returns None on success.)\", 'The problem I have with solutions listed to date is that, in the merged dictionary, the value for key \"b\" is 10 but, to my way of thinking, it should be 12.\\nIn that light, I present the following:', \"There will be a new option when Python 3.8 releases (scheduled for 20 October, 2019), thanks to PEP 572: Assignment Expressions. The new assignment expression operator := allows you to assign the result of the copy and still use it to call update, leaving the combined code a single expression, rather than two statements, changing:to:while behaving identically in every way. If you must also return the resulting dict (you asked for an expression returning the dict; the above creates and assigns to newdict, but doesn't return it, so you couldn't use it to pass an argument to a function as is, a la myfunc((newdict := dict1.copy()).update(dict2))), then just add or newdict to the end (since update returns None, which is falsy, it will then evaluate and return newdict as the result of the expression):Important caveat: In general, I'd discourage this approach in favor of:The unpacking approach is clearer (to anyone who knows about generalized unpacking in the first place, which you should), doesn't require a name for the result at all (so it's much more concise when constructing a temporary that is immediately passed to a function or included in a list/tuple literal or the like), and is almost certainly faster as well, being (on CPython) roughly equivalent to:but done at the C layer, using the concrete dict API, so no dynamic method lookup/binding or function call dispatch overhead is involved (where (newdict := dict1.copy()).update(dict2) is unavoidably identical to the original two-liner in behavior, performing the work in discrete steps, with dynamic lookup/binding/invocation of methods.It's also more extensible, as merging three dicts is obvious:where using assignment expressions won't scale like that; the closest you could get would be:or without the temporary tuple of Nones, but with truthiness testing of each None result:either of which is obviously much uglier, and includes further inefficiencies (either a wasted temporary tuple of Nones for comma separation, or pointless truthiness testing of each update's None return for or separation).The only real advantage to the assignment expression approach occurs if:\", 'This should solve your problem.', \"This can be done with a single dict comprehension:In my view the best answer for the 'single expression' part as no extra functions are needed, and it is short.\"]",
            "url": "https://stackoverflow.com/questions/38987"
        },
        {
            "tag": "python",
            "question": [
                "How do I execute a program or call a system command?"
            ],
            "votes": "5871",
            "answer": "['Use the subprocess module in the standard library:The advantage of subprocess.run over os.system is that it is more flexible (you can get the stdout, stderr, the \"real\" status code, better error handling, etc...).Even the documentation for os.system recommends using subprocess instead:The subprocess module provides more powerful facilities for spawning new processes and retrieving their results; using that module is preferable to using this function. See the Replacing Older Functions with the subprocess Module section in the subprocess documentation for some helpful recipes.On Python 3.4 and earlier, use subprocess.call instead of .run:', 'Here is a summary of ways to call external programs, including their advantages and disadvantages:os.system passes the command and arguments to your system\\'s shell. This is nice because you can actually run multiple commands at once in this manner and set up pipes and input/output redirection. For example:However, while this is convenient, you have to manually handle the escaping of shell characters such as spaces, et cetera. On the other hand, this also lets you run commands which are simply shell commands and not actually external programs.os.popen will do the same thing as os.system except that it gives you a file-like object that you can use to access standard input/output for that process. There are 3 other variants of popen that all handle the i/o slightly differently. If you pass everything as a string, then your command is passed to the shell; if you pass them as a list then you don\\'t need to worry about escaping anything. Example:subprocess.Popen. This is intended as a replacement for os.popen, but has the downside of being slightly more complicated by virtue of being so comprehensive. For example, you\\'d say:instead ofbut it is nice to have all of the options there in one unified class instead of 4 different popen functions. See the documentation.subprocess.call. This is basically just like the Popen class and takes all of the same arguments, but it simply waits until the command completes and gives you the return code. For example:subprocess.run. Python 3.5+ only. Similar to the above but even more flexible and returns a CompletedProcess object when the command finishes executing.os.fork, os.exec, os.spawn are similar to their C language counterparts, but I don\\'t recommend using them directly.The subprocess module should probably be what you use.Finally, please be aware that for all methods where you pass the final command to be executed by the shell as a string and you are responsible for escaping it. There are serious security implications if any part of the string that you pass can not be fully trusted. For example, if a user is entering some/any part of the string. If you are unsure, only use these methods with constants. To give you a hint of the implications consider this code:and imagine that the user enters something \"my mama didnt love me && rm -rf /\" which could erase the whole filesystem.', \"Typical implementation:You are free to do what you want with the stdout data in the pipe.  In fact, you can simply omit those parameters (stdout= and stderr=) and it'll behave like os.system().\", \"Some hints on detaching the child process from the calling one (starting the child process in background).Suppose you want to start a long task from a CGI script. That is, the child process should live longer than the CGI script execution process.The classical example from the subprocess module documentation is:The idea here is that you do not want to wait in the line 'call subprocess' until the longtask.py is finished. But it is not clear what happens after the line 'some more code here' from the example.My target platform was FreeBSD, but the development was on Windows, so I faced the problem on Windows first.On Windows (Windows\\xa0XP), the parent process will not finish until the longtask.py has finished its work. It is not what you want in a CGI script. The problem is not specific to Python; in the PHP community the problems are the same.The solution is to pass DETACHED_PROCESS Process Creation Flag to the underlying CreateProcess function in Windows API.\\nIf you happen to have installed pywin32, you can import the flag from the win32process module, otherwise you should define it yourself:/* UPD 2015.10.27 @eryksun in a comment below notes, that the semantically correct flag is CREATE_NEW_CONSOLE (0x00000010) */On FreeBSD we have another problem: when the parent process is finished, it finishes the child processes as well. And that is not what you want in a CGI script either. Some experiments showed that the problem seemed to be in sharing sys.stdout. And the working solution was the following:I have not checked the code on other platforms and do not know the reasons of the behaviour on FreeBSD. If anyone knows, please share your ideas. Googling on starting background processes in Python does not shed any light yet.\", \"Note that this is dangerous, since the command isn't cleaned. I leave it up to you to google for the relevant documentation on the 'os' and 'sys' modules. There are a bunch of functions (exec* and spawn*) that will do similar things.\", \"I'd recommend using the subprocess module instead of os.system because it does shell escaping for you and is therefore much safer.\", 'If you want to return the results of the command, you can use os.popen. However, this is deprecated since version 2.6 in favor of the subprocess module, which other answers have covered well.', 'There are lots of different libraries which allow you to call external commands with Python. For each library I\\'ve given a description and shown an example of calling an external command. The command I used as the example is ls -l (list all files). If you want to find out more about any of the libraries I\\'ve listed and linked the documentation for each of them.Hopefully this will help you make a decision on which library to use :)Subprocess allows you to call external commands and connect them to their input/output/error pipes (stdin, stdout, and stderr). Subprocess is the default choice for running commands, but sometimes other modules are better.os is used for \"operating system dependent functionality\". It can also be used to call external commands with os.system and os.popen (Note: There is also a subprocess.popen). os will always run the shell and is a simple alternative for people who don\\'t need to, or don\\'t know how to use subprocess.run.sh is a subprocess interface which lets you call programs as if they were functions. This is useful if you want to run a command multiple times.plumbum is a library for \"script-like\" Python programs. You can call programs like functions as in sh. Plumbum is useful if you want to run a pipeline without the shell.pexpect lets you spawn child applications, control them and find patterns in their output. This is a better alternative to subprocess for commands that expect a tty on Unix.fabric is a Python 2.5 and 2.7 library. It allows you to execute local and remote shell commands. Fabric is simple alternative for running commands in a secure shell (SSH)envoy is known as \"subprocess for humans\". It is used as a convenience wrapper around the subprocess module.commands contains wrapper functions for os.popen, but it has been removed from Python 3 since subprocess is a better alternative.', \"Use the subprocess module (Python 3):It is the recommended standard way. However, more complicated tasks (pipes, output, input, etc.) can be tedious to construct and write.Note on Python version: If you are still using Python 2, subprocess.call works in a similar way.ProTip: shlex.split can help you to parse the command for run, call, and other subprocess functions in case you don't want (or you can't!) provide them in form of lists:If you do not mind external dependencies, use plumbum:It is the best subprocess wrapper. It's cross-platform, i.e. it works on both Windows and Unix-like systems. Install by pip install plumbum.Another popular library is sh:However, sh dropped Windows support, so it's not as awesome as it used to be. Install by pip install sh.\", 'I always use fabric for this things like:But this seem to be a good tool: sh (Python subprocess interface).Look at an example:', 'Check the \"pexpect\" Python library, too.It allows for interactive controlling of external programs/commands, even ssh, ftp, telnet, etc. You can just type something like:', 'If you need the output from the command you are calling,\\nthen you can use subprocess.check_output (Python 2.7+).Also note the shell parameter.If shell is True, the specified command will be executed through the shell. This can be useful if you are using Python primarily for the enhanced control flow it offers over most system shells and still want convenient access to other shell features such as shell pipes, filename wildcards, environment variable expansion, and expansion of ~ to a user\u2019s home directory. However, note that Python itself offers implementations of many shell-like features (in particular, glob, fnmatch, os.walk(), os.path.expandvars(), os.path.expanduser(), and shutil).', \"subprocess.run is the recommended approach as of Python 3.5 if your code does not need to maintain compatibility with earlier Python versions. It's more consistent and offers similar ease-of-use as Envoy. (Piping isn't as straightforward though. See this question for how.)Here's some examples from the documentation.Run a process:Raise on failed run:Capture output:I recommend trying Envoy. It's a wrapper for subprocess, which in turn aims to replace the older modules and functions. Envoy is subprocess for humans.Example usage from the README:Pipe stuff around too:\", 'This is how I run my commands. This code has everything you need pretty much', \"Simple, use subprocess.run, which returns a CompletedProcess object:(run wants a list of lexically parsed shell arguments - this is what you'd type in a shell, separated by spaces, but not where the spaces are quoted, so use a specialized function, split, to split up what you would literally type into your shell)As of Python 3.5, the documentation recommends subprocess.run:The recommended approach to invoking subprocesses is to use the run() function for all use cases it can handle. For more advanced use cases, the underlying Popen interface can be used directly.Here's an example of the simplest possible usage - and it does exactly as asked:run waits for the command to successfully finish, then returns a CompletedProcess object. It may instead raise TimeoutExpired (if you give it a timeout= argument) or CalledProcessError (if it fails and you pass check=True).As you might infer from the above example, stdout and stderr both get piped to your own stdout and stderr by default.We can inspect the returned object and see the command that was given and the returncode:If you want to capture the output, you can pass subprocess.PIPE to the appropriate stderr or stdout:And those respective attributes return bytes.One might easily move from manually providing a command string (like the question suggests) to providing a string built programmatically. Don't build strings programmatically. This is a potential security issue. It's better to assume you don't trust the input.Note, only args should be passed positionally.Here's the actual signature in the source and as shown by help(run):The popenargs and kwargs are given to the Popen constructor. input can be a string of bytes (or unicode, if specify encoding or universal_newlines=True) that will be piped to the subprocess's stdin.The documentation describes timeout= and check=True better than I could:The timeout argument is passed to Popen.communicate(). If the timeout\\nexpires, the child process will be killed and waited for. The\\nTimeoutExpired exception will be re-raised after the child process has\\nterminated.If check is true, and the process exits with a non-zero exit code, a\\nCalledProcessError exception will be raised. Attributes of that\\nexception hold the arguments, the exit code, and stdout and stderr if\\nthey were captured.and this example for check=True is better than one I could come up with:Here's an expanded signature, as given in the documentation:Note that this indicates that only the args list should be passed positionally. So pass the remaining arguments as keyword arguments.When use Popen instead? I would struggle to find use-case based on the arguments alone. Direct usage of Popen would, however, give you access to its methods, including poll, 'send_signal', 'terminate', and 'wait'.Here's the Popen signature as given in the source. I think this is the most precise encapsulation of the information (as opposed to help(Popen)):But more informative is the Popen documentation:Execute a child program in a new process. On POSIX, the class uses\\nos.execvp()-like behavior to execute the child program. On Windows,\\nthe class uses the Windows CreateProcess() function. The arguments to\\nPopen are as follows.Understanding the remaining documentation on Popen will be left as an exercise for the reader.\", 'Use subprocess....or for a very simple command:', 'As of Python 3.7.0 released on June 27th 2018 (https://docs.python.org/3/whatsnew/3.7.html), you can achieve your desired result in the most powerful while equally simple way. This answer intends to show you the essential summary of various options in a short manner. For in-depth answers, please see the other ones.The big advantage of os.system(...) was its simplicity. subprocess is better and still easy to use, especially as of Python 3.5.Note: This is the exact answer to your question - running a commandlike in a shellIf possible, remove the shell overhead and run the command directly (requires a list).Pass program arguments in a list. Don\\'t include \\\\\"-escaping for arguments containing spaces.The following code speaks for itself:result.stdout is all normal program output excluding errors. Read result.stderr to get them.capture_output=True - turns capturing on. Otherwise result.stderr and result.stdout would be None. Available from Python 3.7.text=True - a convenience argument added in Python 3.7 which converts the received binary data to Python strings you can easily work with.DoIf you just want to check if the program succeeded (returncode == 0) and otherwise throw an Exception, there is a more convenient function:But it\\'s Python, so there\\'s an even more convenient argument check which does the same thing automatically for you:You might want to have all program output inside stdout, even errors. To accomplish this, runresult.stderr will then be None and result.stdout will contain everything.shell=False expects a list of arguments. You might however, split an argument string on your own using shlex.That\\'s it.Chances are high you just started using Python when you come across this question. Let\\'s look at some common problems.FileNotFoundError: [Errno 2] No such file or directory: \\'ls -a\\': \\'ls -a\\'You\\'re running a subprocess without shell=True . Either use a list ([\"ls\", \"-a\"]) or set shell=True.TypeError: [...] NoneType [...]Check that you\\'ve set capture_output=True.TypeError: a bytes-like object is required, not [...]You always receive byte results from your program. If you want to work with it like a normal string, set text=True.subprocess.CalledProcessError: Command \\'[...]\\' returned non-zero exit status 1.Your command didn\\'t run successfully. You could disable returncode checking or check your actual program\\'s validity.TypeError: init() got an unexpected keyword argument [...]You\\'re likely using a version of Python older than 3.7.0; update it to the most recent one available. Otherwise there are other answers in this Stack Overflow post showing you older alternative solutions.', \"os.system is OK, but kind of dated.  It's also not very secure.  Instead, try subprocess.  subprocess does not call sh directly and is therefore more secure than os.system.Get more information here.\", 'There is also Plumbum', 'Use:os - This module provides a portable way of using operating system-dependent functionality.For the more os functions, here is the documentation.', 'It can be this simple:', 'There is another difference here which is not mentioned previously.subprocess.Popen executes the <command> as a subprocess. In my case, I need to execute file <a> which needs to communicate with another program, <b>.I tried subprocess, and execution was successful. However <b> could not communicate with <a>.\\nEverything is normal when I run both from the terminal.One more: \\n(NOTE: kwrite behaves different from other applications. If you try the below with Firefox, the results will not be the same.)If you try os.system(\"kwrite\"), program flow freezes until the user closes kwrite. To overcome that I tried instead os.system(konsole -e kwrite). This time program continued to flow, but kwrite became the subprocess of the console.Anyone runs the kwrite not being a subprocess (i.e. in the system monitor it must appear at the leftmost edge of the tree).', 'os.system does not allow you to store results, so if you want to store results in some list or something, a subprocess.call works.', \"subprocess.check_call is convenient if you don't want to test return values. It throws an exception on any error.\", 'I tend to use subprocess together with shlex (to handle escaping of quoted strings):', \"I wrote a library for this, shell.py.It's basically a wrapper for popen and shlex for now. It also supports piping commands, so you can chain commands easier in Python. So you can do things like:\", 'In Windows you can just import the subprocess module and run external commands by calling subprocess.Popen(), subprocess.Popen().communicate() and subprocess.Popen().wait() as below:Output:', 'Under Linux, in case you would like to call an external command that will execute independently (will keep running after the Python script terminates), you can use a simple queue as task spooler or the at command.An example with task spooler:Notes about task spooler (ts):You could set the number of concurrent processes to be run (\"slots\") with:ts -S <number-of-slots>Installing ts doesn\\'t requires admin privileges. You can download and compile it from source with a simple make, add it to your path and you\\'re done.', 'Invoke is a Python (2.7 and 3.4+) task execution tool and library. It provides a clean, high-level API for running shell commands:', \"You can use Popen, and then you can check the procedure's status:Check out subprocess.Popen.\"]",
            "url": "https://stackoverflow.com/questions/89228"
        },
        {
            "tag": "python",
            "question": [
                "How can I safely create a nested directory?"
            ],
            "votes": "5406",
            "answer": "[\"On Python \u2265 3.5, use pathlib.Path.mkdir:For older versions of Python, I see two answers with good qualities, each with a small flaw, so I will give my take on it:Try os.path.exists, and consider os.makedirs for the creation.As noted in comments and elsewhere, there's a race condition \u2013 if the directory is created between the os.path.exists and the os.makedirs calls, the os.makedirs will fail with an OSError. Unfortunately, blanket-catching OSError and continuing is not foolproof, as it will ignore a failure to create the directory due to other factors, such as insufficient permissions, full disk, etc.One option would be to trap the OSError and examine the embedded error code (see Is there a cross-platform way of getting information from Python\u2019s OSError):Alternatively, there could be a second os.path.exists, but suppose another created the directory after the first check, then removed it before the second one \u2013 we could still be fooled.Depending on the application, the danger of concurrent operations may be more or less than the danger posed by other factors such as file permissions. The developer would have to know more about the particular application being developed and its expected environment before choosing an implementation.Modern versions of Python improve this code quite a bit, both by exposing FileExistsError (in 3.3+)......and by allowing a keyword argument to os.makedirs called exist_ok (in 3.2+).\", \"pathlib.Path.mkdir as used above recursively creates the directory and does not raise an exception if the directory already exists. If you don't need or want the parents to be created, skip the parents argument.Using pathlib:If you can, install the current pathlib backport named pathlib2. Do not install the older unmaintained backport named pathlib. Next, refer to the Python 3.5+ section above and use it the same.If using Python 3.4, even though it comes with pathlib, it is missing the useful exist_ok option. The backport is intended to offer a newer and superior implementation of mkdir which includes this missing option.Using os:os.makedirs as used above recursively creates the directory and does not raise an exception if the directory already exists. It has the optional exist_ok argument only if using Python 3.2+, with a default value of False. This argument does not exist in Python 2.x up to 2.7. As such, there is no need for manual exception handling as with Python 2.7.Using pathlib:If you can, install the current pathlib backport named pathlib2. Do not install the older unmaintained backport named pathlib. Next, refer to the Python 3.5+ section above and use it the same.Using os:While a naive solution may first use os.path.isdir followed by os.makedirs, the solution above reverses the order of the two operations. In doing so, it prevents a common race condition having to do with a duplicated attempt at creating the directory, and also disambiguates files from directories.Note that capturing the exception and using errno is of limited usefulness because OSError: [Errno 17] File exists, i.e. errno.EEXIST, is raised for both files and directories. It is more reliable simply to check if the directory exists.mkpath creates the nested directory, and does nothing if the directory already exists. This works in both Python 2 and 3.Per Bug 10948, a severe limitation of this alternative is that it works only once per python process for a given path. In other words, if you use it to create a directory, then delete the directory from inside or outside Python, then use mkpath again to recreate the same directory, mkpath will simply silently use its invalid cached info of having previously created the directory, and will not actually make the directory again. In contrast, os.makedirs doesn't rely on any such cache. This limitation may be okay for some applications.With regard to the directory's mode, please refer to the documentation if you care about it.\", \"Using try except and the right error code from errno module gets rid of the race condition and is cross-platform:In other words, we try to create the directories, but if they already exist we ignore the error. On the other hand, any other error gets reported. For example, if you create dir 'a' beforehand and remove all permissions from it, you will get an OSError raised with errno.EACCES (Permission denied, error 13).\", 'Starting from Python 3.5, pathlib.Path.mkdir has an exist_ok flag:This recursively creates the directory and does not raise an exception if the directory already exists.(just as os.makedirs got an exist_ok flag starting from python 3.2 e.g os.makedirs(path, exist_ok=True))Note: when i posted this answer none of the other answers mentioned exist_ok...', \"I would personally recommend that you use os.path.isdir() to test instead of os.path.exists().If you have:And a foolish user input:... You're going to end up with a directory named filename.etc when you pass that argument to os.makedirs() if you test with os.path.exists().\", 'Check os.makedirs:  (It makes sure the complete path exists.)\\n To handle the fact the directory might exist, catch OSError.\\n(If exist_ok is False (the default), an OSError is raised if the target directory already exists.)', 'Try the os.path.exists function', \"You give a particular file at a certain path and you pull the directory from the file path. Then after making sure you have the directory, you attempt to open a file for reading. To comment on this code:We want to avoid overwriting the builtin function, dir. Also, filepath or perhaps fullfilepath is probably a better semantic name than filename so this would be better written:Your end goal is to open this file, you initially state, for writing, but you're essentially approaching this goal (based on your code) like this, which opens the file for reading:Why would you make a directory for a file that you expect to be there and be able to read?Just attempt to open the file.If the directory or file isn't there, you'll get an IOError with an associated error number: errno.ENOENT will point to the correct error number regardless of your platform. You can catch it if you want, for example:This is probably what you're wanting.In this case, we probably aren't facing any race conditions. So just do as you were, but note that for writing, you need to open with the w mode (or a to append). It's also a Python best practice to use the context manager for opening files.However, say we have several Python processes that attempt to put all their data into the same directory. Then we may have contention over creation of the directory. In that case it's best to wrap the makedirs call in a try-except block.\", \"I have put the following down. It's not totally foolproof though.Now as I say, this is not really foolproof, because we have the possiblity of failing to create the directory, and another process creating it during that period.\", \"Check if a directory exists and create it if necessary?The direct answer to this is, assuming a simple situation where you don't expect other users or processes to be messing with your directory:or if making the directory is subject to race conditions (i.e. if after checking the path exists, something else may have already made it) do this:But perhaps an even better approach is to sidestep the resource contention issue, by using temporary directories via tempfile:Here's the essentials from the online doc:There's a new Path object (as of 3.4) with lots of methods one would want to use with paths - one of which is mkdir.(For context, I'm tracking my weekly rep with a script. Here's the relevant parts of code from the script that allow me to avoid hitting Stack Overflow more than once a day for the same data.)First the relevant imports:We don't have to deal with os.path.join now - just join path parts with a /:Then I idempotently ensure the directory exists - the exist_ok argument shows up in Python 3.5:Here's the relevant part of the documentation:If exist_ok is true, FileExistsError exceptions will be ignored (same behavior as the POSIX mkdir -p command), but only if the last path component is not an existing non-directory file.Here's a little more of the script - in my case, I'm not subject to a race condition, I only have one process that expects the directory (or contained files) to be there, and I don't have anything trying to remove the directory.Path objects have to be coerced to str before other APIs that expect str paths can use them.Perhaps Pandas should be updated to accept instances of the abstract base class, os.PathLike.\", 'Best way to do this in python', 'In Python 3.4 you can also use the brand new pathlib module:', 'fastest safest way to do it is:\\nit will create if not exists and skip if exists:', 'For a one-liner solution, you can use IPython.utils.path.ensure_dir_exists():From the documentation: Ensure that a directory exists. If it doesn\u2019t exist, try to create it and protect against a race condition if another process is doing the same.IPython is an extension package, not part of the standard library.', \"In Python3, os.makedirs supports setting exist_ok. The default setting is False, which means an OSError will be raised if the target directory already exists. By setting exist_ok to True, OSError (directory exists) will be ignored and the directory will not be created.In Python2, os.makedirs doesn't support setting exist_ok. You can use the approach in heikki-toivonen's answer:\", 'The relevant Python documentation suggests the use of the EAFP coding style (Easier to Ask for Forgiveness than Permission). This means that the codeis better than the alternativeThe documentation suggests this exactly because of the race condition discussed in this question. In addition, as others mention here, there is a performance advantage in querying once instead of twice the OS. Finally, the argument placed forward, potentially, in favour of the second code in some cases --when the developer knows the environment the application is running-- can only be advocated in the special case that the program has set up a private environment for itself (and other instances of the same program).Even in that case, this is a bad practice and can lead to long useless debugging. For example, the fact we set the permissions for a directory should not leave us with the impression permissions are set appropriately for our purposes. A parent directory could be mounted with other permissions. In general, a program should always work correctly and the programmer should not expect one specific environment.', 'I found this Q/A after I was puzzled by some of the failures and errors I was getting while working with directories in Python. I am working in Python 3 (v.3.5 in an Anaconda virtual environment on an Arch Linux x86_64 system).Consider this directory structure:Here are my experiments/notes, which provides clarification:Conclusion: in my opinion, \"Method 2\" is more robust.[1] How can I safely create a nested directory?[2] https://docs.python.org/3/library/os.html#os.makedirs', 'You can use mkpathNote that it will create the ancestor directories as well.It works for Python 2 and 3.', \"In case you're writing a file to a variable path, you can use this on the file's path to make sure that the parent directories are created.Works even if path_to_file is file.ext (zero directories deep).See pathlib.PurePath.parent and pathlib.Path.mkdir.\", \"Why not use subprocess module if running on a machine that supports command \\nmkdir with -p option ? \\nWorks on python 2.7 and python 3.6Should do the trick on most systems.In situations where portability doesn't matter (ex, using docker) the solution is a clean 2 lines. You also don't have to add logic to check if directories exist or not. Finally, it is safe to re-run without any side effectsIf you need error handling:\", 'You have to set the full path before creating the directory:This works for me and hopefully, it will works for you as well', \"I saw Heikki Toivonen and A-B-B's answers and thought of this variation.\", 'I use os.path.exists(), here is a Python 3 script that can be used to check if a directory exists, create one if it does not exist, and delete it if it does exist (if desired).It prompts users for input of the directory and can be easily modified.', 'Use this command check and create dir', 'Call the function create_dir() at the entry point of your program/project.', 'If you consider the following:means a directory (path) exists AND is a directory. So for me this way does what I need. So I can make sure it is folder (not a file) and exists.', 'You can use os.listdir for this:', 'This may not exactly answer the question. But I guess your real intention is to create a file and its parent directories, given its content all in 1 command.You can do that with fastcore extension to pathlib: path.mk_write(data)See more in fastcore documentation']",
            "url": "https://stackoverflow.com/questions/273192"
        },
        {
            "tag": "python",
            "question": [
                "Accessing the index in 'for' loops"
            ],
            "votes": "4979",
            "answer": "['Use the built-in function enumerate():It is non-pythonic to manually index via for i in range(len(xs)): x = xs[i] or manually manage an additional state variable.Check out PEP 279 for more.', 'Use enumerate to get the index with the element as you iterate:And note that Python\\'s indexes start at zero, so you would get 0 to 4 with the above. If you want the count, 1 to 5, do this:What you are asking for is the Pythonic equivalent of the following, which is the algorithm most programmers of lower-level languages would use:Or in languages that do not have a for-each loop:or sometimes more commonly (but unidiomatically) found in Python:Python\\'s enumerate function reduces the visual clutter by hiding the accounting for the indexes, and encapsulating the iterable into another iterable (an enumerate object) that yields a two-item tuple of the index and the item that the original iterable would provide. That looks like this:This code sample is fairly well the canonical example of the difference between code that is idiomatic of Python and code that is not. Idiomatic code is sophisticated (but not complicated) Python, written in the way that it was intended to be used. Idiomatic code is expected by the designers of the language, which means that usually this code is not just more readable, but also more efficient.Even if you don\\'t need indexes as you go, but you need a count of the iterations (sometimes desirable) you can start with 1 and the final number will be your count.The count seems to be more what you intend to ask for (as opposed to index) when you said you wanted from 1 to 5.To break these examples down, say we have a list of items that we want to iterate over with an index:Now we pass this iterable to enumerate, creating an enumerate object:We can pull the first item out of this iterable that we would get in a loop with the next function:And we see we get a tuple of 0, the first index, and \\'a\\', the first item:we can use what is referred to as \"sequence unpacking\" to extract the elements from this two-tuple:and when we inspect index, we find it refers to the first index, 0, and item refers to the first item, \\'a\\'.So do this:', \"It's pretty simple to start it from 1 other than 0:\", '', \"Here's how you can access the indices and array's elements using for-in loops.Result:Result:Result:Result:Result:Result:Result:Result:Result:\", 'As is the norm in Python, there are several ways to do this. In all examples assume: lst = [1, 2, 3, 4, 5]This is also the safest option in my opinion because the chance of going into infinite recursion has been eliminated. Both the item and its index are held in variables and there is no need to write any further code to access the item.As explained before, there are other ways to do this that have not been explained here and they may even apply more in other situations. For example, using itertools.chain with for. It handles nested loops better than the other examples.', 'Old fashioned way:List comprehension:', 'The fastest way to access indexes of list within loop in Python 3.7 is to use the enumerate method for small, medium and huge lists.Please see different approaches which can be used to iterate over list and access index value and their performance metrics (which I suppose would be useful for you) in code samples below:See performance metrics for each method below:As the result, using enumerate method is the fastest method for iteration when the index needed.Adding some useful links below:What is the difference between range and xrange functions in Python 2.X?What is faster for loop using enumerate or for loop using xrange in Python?range(len(list)) or enumerate(list)?', 'You can use enumerate and embed expressions inside string literals to obtain the solution.This is a simple way:', \"First of all, the indexes will be from 0 to 4. Programming languages start counting from 0; don't forget that or you will come across an index-out-of-bounds exception. All you need in the for loop is a variable counting from 0 to 4 like so:Keep in mind that I wrote 0 to 5 because the loop stops one number before the maximum. :)To get the value of an index, use\", 'You can do it with this code:Use this code if you need to reset the index value at the end of the loop:', \"According to this discussion: object's list indexLoop counter iterationThe current idiom for looping over the indices makes use of the built-in range function:Looping over both elements and indices can be achieved either by the old idiom or by using the new zip built-in function:orvia PEP 212 \u2013 Loop Counter Iteration.\", 'In your question, you write \"how do I access the loop index, from 1 to 5 in this case?\"However, the index for a list runs from zero.  So, then we need to know if what you actually want is the index and item for each item in a list, or whether you really want numbers starting from 1.  Fortunately, in Python, it is easy to do either or both.First, to clarify, the enumerate function iteratively returns the index and corresponding item for each item in a list.The output for the above is then,Notice that the index runs from 0. This kind of indexing is common among modern programming languages including Python and C.If you want your loop to span a part of the list, you can use the standard Python syntax for a part of the list. For example, to loop from the second item in a list up to but not including the last item, you could useNote that once again, the output index runs from 0,That brings us to the start=n switch for enumerate().  This simply offsets the index, you can equivalently simply add a number to the index inside the loop.for which the output is', 'If I were to iterate nums = [1, 2, 3, 4, 5] I would doOr get the length as l = len(nums)', 'If there is no duplicate value in the list:', 'You can also try this:The output is', 'You can use the index method:It is highlighted in a comment that this method doesn\u2019t work if there are duplicates in ints. The method below should work for any values in ints:Or alternativelyif you want to get both the index and the value in ints as a list of tuples.It uses the method of enumerate in the selected answer to this question, but with list comprehension, making it faster with less code.', 'A simple answer using a while loop:Output:', 'You can simply use a variable such as count to count the number of elements in the list:', 'To print a tuple of (index, value) in a list comprehension using a for loop:Output:', \"In addition to all the excellent answers above, here is a solution to this problem when working with pandas Series objects. In many cases, pandas Series have custom/unique indices (for example, unique identifier strings) that can't be accessed with the enumerate() function.Output:We can see below that enumerate() doesn't give us the desired result:Output:We can access the indices of a pandas Series in a for loop using .items():Output:\", 'One-liner lovers:Explaination:Points to take:Thanks. Keep me in your prayers.', 'You can use range(len(some_list)) and then lookup the index like thisOr use the Python\u2019s built-in enumerate function which allows you to loop over a list and retrieve the index and the value of each item in the list', 'It can be achieved with the following code:Here, range(1, len(xs)+1); If you expect the output to start from 1 instead of 0, you need to start the range from 1 and add 1 to the total length estimated since python starts indexing the number from 0 by default.', 'A loop with a \"counter\" variable set as an initialiser that will be a parameter, in formatting the string, as the item number.The for loop accesses the \"listos\" variable which is the list. As we access the list by \"i\", \"i\" is formatted as the item price (or whatever it is).Output:', 'This serves the purpose well enough:']",
            "url": "https://stackoverflow.com/questions/522563"
        },
        {
            "tag": "python",
            "question": [
                "How do I make a flat list out of a list of lists?"
            ],
            "votes": "4967",
            "answer": "['Given a list of lists l,which means:is faster than the shortcuts posted so far. (l is the list to flatten.)Here is the corresponding function:As evidence, you can use the timeit module in the standard library:Explanation: the shortcuts based on + (including the implied use in sum) are, of necessity, O(L**2) when there are L sublists -- as the intermediate result list keeps getting longer, at each step a new intermediate result list object gets allocated, and all the items in the previous intermediate result must be copied over (as well as a few new ones added at the end). So, for simplicity and without actual loss of generality, say you have L sublists of I items each: the first I items are copied back and forth L-1 times, the second I items L-2 times, and so on; total number of copies is I times the sum of x for x from 1 to L excluded, i.e., I * (L**2)/2.The list comprehension just generates one list, once, and copies each item over (from its original place of residence to the result list) also exactly once.', \"You can use itertools.chain():Or you can use itertools.chain.from_iterable() which doesn't require unpacking the list with the * operator:This approach is arguably more readable than [item for sublist in l for item in sublist] and appears to be faster too:\", \"Note from the author: This is very inefficient. But fun, because monoids are awesome.sum sums the elements of the iterable xss, and uses the second argument as the initial value [] for the sum. (The default initial value is 0, which is not a list.)Because you are summing nested lists, you actually get [1,3]+[2,4] as a result of sum([[1,3],[2,4]],[]), which is equal to [1,3,2,4].Note that only works on lists of lists. For lists of lists of lists, you'll need another solution.\", 'I tested most suggested solutions with perfplot (a pet project of mine, essentially a wrapper around timeit), and foundto be the fastest solution, both when many small lists and few long lists are concatenated. (operator.iadd is equally fast.)A simpler and also acceptable variant isIf the number of sublists is large, this performs a little worse than the above suggestion.Code to reproduce the plot:', 'Using functools.reduce, which adds an accumulated list xs to the next list ys:Output:A faster way using operator.concat:Output:', 'Here is a general approach that applies to numbers, strings, nested lists and mixed containers.  This can flatten both simple and complicated containers (see also Demo).CodeNotes:DemoReference', \"To flatten a data-structure that is deeply nested, use iteration_utilities.deepflatten1:It's a generator so you need to cast the result to a list or explicitly iterate over it.To flatten only one level and if each of the items is itself iterable you can also use iteration_utilities.flatten which itself is just a thin wrapper around itertools.chain.from_iterable:Just to add some timings (based on Nico Schl\u00f6mer's answer that didn't include the function presented in this answer):It's a log-log plot to accommodate for the huge range of values spanned. For qualitative reasoning: Lower is better.The results show that if the iterable contains only a few inner iterables then sum will be fastest, however for long iterables only the itertools.chain.from_iterable, iteration_utilities.deepflatten or the nested comprehension have reasonable performance with itertools.chain.from_iterable being the fastest (as already noticed by Nico Schl\u00f6mer).1 Disclaimer: I'm the author of that library\", 'The following seems simplest to me:', 'Consider installing the more_itertools package.It ships with an implementation for flatten (source, from the itertools recipes):Note: as mentioned in the docs, flatten requires a list of lists.  See below on flattening more irregular inputs.As of version 2.4, you can flatten more complicated, nested iterables with more_itertools.collapse (source, contributed by  abarnet).', \"The reason your function didn't work is because the extend extends an array in-place and doesn't return it. You can still return x from lambda, using something like this:Note: extend is more efficient than + on lists.\", 'matplotlib.cbook.flatten() will work for nested lists even if they nest more deeply than the example.Result:This is 18x faster than underscore._.flatten:', 'According your list [[1, 2, 3], [4, 5, 6], [7], [8, 9]] which is 1 list level, we can simply use sum(list,[]) without using any librariesTo extend the advantage of this method when there is a tuple or number existing inside. Simply adding a mapping function for each element by map to the listIn here, there is a clear explanation of the drawback in terms of memory for this approach. In short, it recursively creates list objects, which should be avoided :(', \"One can also use NumPy's flat:It only works when sublists have identical dimensions.\", 'You can use the list extend method. It shows to be the fastest:Performance:Output:', \"There are several answers with the same recursive appending scheme as below, but none makes use of try, which makes the solution more robust and Pythonic.Usage: this is a generator, and you typically want to enclose it in an iterable builder like list() or tuple() or use it in a for loop.Advantages of this solution are:N.B.: Since all iterables are flattened, strings are decomposed into sequences of single characters. If you don't like/want such behavior, you can use the following version which filters out from flattening iterables like strings and bytes:\", 'Note: Below applies to Python 3.3+ because it uses yield_from.  six is also a third-party package, though it is stable.  Alternately, you could use sys.version.In the case of obj = [[1, 2,], [3, 4], [5, 6]], all of the solutions here are good, including list comprehension and itertools.chain.from_iterable.However, consider this slightly more complex case:There are several problems here:You can remedy this as follows:Here, you check that the sub-element (1) is iterable with Iterable, an ABC from itertools, but also want to ensure that (2) the element is not \"string-like.\"', 'If you are willing to give up a tiny amount of speed for a cleaner look, then you could use numpy.concatenate().tolist() or numpy.concatenate().ravel().tolist():You can find out more here in the documentation, numpy.concatenate and numpy.ravel.', '', 'I wanted a solution which can deal with multiple nesting ([[1], [[[2]], [3]]], [1, 2, 3] for example), but would also not be recursive (I had a big level of recursion and I got a recursion error.This is what I came up with:and tests:', \"This may not be the most efficient way, but I thought to put a one-liner (actually a two-liner). Both versions will work on arbitrary hierarchy nested lists, and exploits language features (Python\\xa03.5) and recursion.The output isThis works in a depth first manner. The recursion goes down until it finds a non-list element, then extends the local variable flist and then rolls back it to the parent. Whenever flist is returned, it is extended to the parent's flist in the list comprehension. Therefore, at the root, a flat list is returned.The above one creates several local lists and returns them which are used to extend the parent's list. I think the way around for this may be creating a gloabl flist, like below.The output is againAlthough I am not sure at this time about the efficiency.\", \"Not a one-liner, but seeing all the answers here, I guess this long list missed some pattern matching, so here it is :)The two methods are probably not efficient, but anyway, it's easy to read (to me at least; perhaps I'm spoiled by functional programming):The second version considers lists of lists of lists... whatever the nesting:\", 'Another unusual approach that works for hetero- and homogeneous lists of integers:', 'A non-recursive function to flatten lists of lists of any depth:', 'If you want to unnest everything and keep a distinct list of elements, you could use this as well.', 'If you have a numpy array a:produces:np.flatten also accepts other parameters:More details about parameters are available here.', 'For a list containing multiple list here a recursive solution that work for me and that i hope is correct:Output:', \"I would suggest using generators with yield statement and yield from.\\nHere's an example:\", \"If I want to add something to the great previous answers, here is my recursive flatten function which can flatten not only nested lists, but also any given container or any generally any object which can throw out items. This does also work for any depth of nesting and it is a lazy iterator which yields the items as requested:This way, you can exclude types you don't want to be flattened, like str or what else.The idea is if an object can pass the iter() it's ready to yield items. So the iterable can have even generator expressions as an item.Someone could argue: Why did you write this that generic when the OP didn't ask for it? OK, you're right. I just felt like this might help someone (like it did for myself).Test cases:Output:\", 'Simplest Way to do in python without any libraryThis function will work for even multidimensional list alsousing recursion we can achieve any combination of list inside list, we can flatten it without using any library.', 'Considering the list has just integers:']",
            "url": "https://stackoverflow.com/questions/952914"
        },
        {
            "tag": "python",
            "question": [
                "Difference between @staticmethod and @classmethod"
            ],
            "votes": "4462",
            "answer": "['Maybe a bit of example code will help: Notice the difference in the call signatures of foo, class_foo and static_foo:Below is the usual way an object instance calls a method. The object instance, a, is implicitly passed as the first argument.With classmethods, the class of the object instance is implicitly passed as the first argument instead of self.You can also call class_foo using the class. In fact, if you define something to be\\na classmethod, it is probably because you intend to call it from the class rather than from a class instance. A.foo(1) would have raised a TypeError, but A.class_foo(1) works just fine:One use people have found for class methods is to create inheritable alternative constructors.With staticmethods, neither self (the object instance) nor  cls (the class) is implicitly passed as the first argument. They behave like plain functions except that you can call them from an instance or the class:Staticmethods are used to group functions which have some logical connection with a class to the class.foo is just a function, but when you call a.foo you don\\'t just get the function,\\nyou get a \"partially applied\" version of the function with the object instance a bound as the first argument to the function. foo expects 2 arguments, while a.foo only expects 1 argument.a is bound to foo. That is what is meant by the term \"bound\" below:With a.class_foo, a is not bound to class_foo, rather the class A is bound to class_foo.Here, with a staticmethod, even though it is a method, a.static_foo just returns\\na good \\'ole function with no arguments bound. static_foo expects 1 argument, and\\na.static_foo expects 1 argument too.And of course the same thing happens when you call static_foo with the class A instead.', 'A staticmethod is a method that knows nothing about the class or instance it was called on. It just gets the arguments that were passed, no implicit first argument. It is basically useless in Python -- you can just use a module function instead of a staticmethod.A classmethod, on the other hand, is a method that gets passed the class it was called on, or the class of the instance it was called on, as first argument. This is useful when you want the method to be a factory for the class: since it gets the actual class it was called on as first argument, you can always instantiate the right class, even when subclasses are involved. Observe for instance how dict.fromkeys(), a classmethod, returns an instance of the subclass when called on a subclass:', \"Basically @classmethod makes a method whose first argument is the class it's called from (rather than the class instance), @staticmethod does not have any implicit arguments.\", 'To decide whether to use @staticmethod or @classmethod you have to look inside your method. If your method accesses other variables/methods in your class then use @classmethod. On the other hand, if your method does not touches any other parts of the class then use @staticmethod.', 'Official python docs:@classmethodA class method receives the class as\\n  implicit first argument, just like an\\n  instance method receives the instance.\\n  To declare a class method, use this\\n  idiom:The @classmethod form is a function\\n  decorator \u2013 see the description of\\n  function definitions in Function\\n  definitions for details.It can be called either on the class\\n  (such as C.f()) or on an instance\\n  (such as C().f()). The instance is\\n  ignored except for its class. If a\\n  class method is called for a derived\\n  class, the derived class object is\\n  passed as the implied first argument.Class methods are different than C++\\n  or Java static methods. If you want\\n  those, see staticmethod() in this\\n  section.@staticmethodA static method does not receive an\\n  implicit first argument. To declare a\\n  static method, use this idiom:The @staticmethod form is a function\\n  decorator \u2013 see the description of\\n  function definitions in Function\\n  definitions for details.It can be called either on the class\\n  (such as C.f()) or on an instance\\n  (such as C().f()). The instance is\\n  ignored except for its class.Static methods in Python are similar\\n  to those found in Java or C++. For a\\n  more advanced concept, see\\n  classmethod() in this section.', 'Here is a short article on this question@staticmethod function is nothing more than a function defined inside a class. It is callable without instantiating the class first. It\u2019s definition is immutable via inheritance.@classmethod function also callable without instantiating the class, but its definition follows Sub class, not Parent class, via inheritance. That\u2019s because the first argument for @classmethod function must always be cls (class).', 'You may have seen Python code like this pseudocode, which demonstrates the signatures of the various method types and provides a docstring to explain each:First I\\'ll explain a_normal_instance_method. This is precisely called an \"instance method\". When an instance method is used, it is used as a partial function (as opposed to a total function, defined for all values when viewed in source code) that is, when used, the first of the arguments is predefined as the instance of the object, with all of its given attributes. It has the instance of the object bound to it, and it must be called from an instance of the object. Typically, it will access various attributes of the instance.For example, this is an instance of a string:if we use the instance method, join on this string, to join another iterable,\\nit quite obviously is a function of the instance, in addition to being a function of the iterable list, [\\'a\\', \\'b\\', \\'c\\']:Instance methods can be bound via a dotted lookup for use later.For example, this binds the str.join method to the \\':\\' instance:And later we can use this as a function that already has the first argument bound to it. In this way, it works like a partial function on the instance:The static method does not take the instance as an argument.It is very similar to a module level function.However, a module level function must live in the module and be specially imported to other places where it is used.If it is attached to the object, however, it will follow the object conveniently through importing and inheritance as well.An example of a static method is str.maketrans, moved from the string module in Python 3.  It makes a translation table suitable for consumption by str.translate. It does seem rather silly when used from an instance of a string, as demonstrated below, but importing the function from the string module is rather clumsy, and it\\'s nice to be able to call it from the class, as in str.maketransIn python 2, you have to import this function from the increasingly less useful string module:A class method is a similar to an instance method in that it takes an implicit first argument, but instead of taking the instance, it takes the class. Frequently these are used as alternative constructors for better semantic usage and it will support inheritance.The most canonical example of a builtin classmethod is dict.fromkeys. It is used as an alternative constructor of dict, (well suited for when you know what your keys are and want a default value for them.)When we subclass dict, we can use the same constructor, which creates an instance of the subclass.See the pandas source code for other similar examples of alternative constructors, and see also the official Python documentation on classmethod and staticmethod.', \"I started learning programming language with C++ and then Java and then Python and so this question bothered me a lot as well, until I understood the simple usage of each.Class Method: Python unlike Java and C++ doesn't have constructor overloading.  And so to achieve this you could use classmethod. Following example will explain thisLet's consider we have a Person class which takes two arguments first_name and last_name and creates the instance of Person.Now, if the requirement comes where you need to create a class using a single name only, just a first_name, you can't do something like this in Python.This will give you an error when you will try to create an object (instance).However, you could achieve the same thing using @classmethod as mentioned belowStatic Method: This is rather simple, it's not bound to instance or class and you can simply call that using class name.So let's say in above example you need a validation that first_name should not exceed 20 characters, you can simply do this.and you could simply call using class name\", 'Only the first argument differs:In more detail...The \"standard\" method, as in every object oriented language. When an object\\'s method is called, it is automatically given an extra argument self as its first argument. That is, methodmust be called with 2 arguments. self is automatically passed, and it is the object itself. Similar to the this that magically appears in eg. java/c++, only in python it is shown explicitly.actually, the first argument does not have to be called self, but it\\'s the standard convention, so keep itWhen the method is decoratedthe automatically provided argument is not self, but the class of self.When the method is decoratedthe method is not given any automatic argument at all. It is only given the parameters that it is called with.', 'I think a better question is \"When would you use @classmethod vs @staticmethod?\"@classmethod allows you easy access to private members that are associated to the class definition. this is a great way to do singletons, or factory classes that control the number of instances of the created objects exist.@staticmethod provides marginal performance gains, but I have yet to see a productive use of a static method within a class that couldn\\'t be achieved as a standalone function outside the class.', 'Static Methods:Benefits of Static Methods:More convenient to import versus module-level functions since each method does not have to be specially importedClass Methods:These are created with classmethod in-built function.', \"@decorators were added in python 2.4 If you're using python < 2.4 you can use the classmethod() and staticmethod() function.For example, if you want to create a factory method (A function returning an instance of a different implementation of a class depending on what argument it gets) you can do something like:Also observe that this is a good example for using a classmethod and a static method,\\nThe static method clearly belongs to the class, since it uses the class Cluster internally.\\nThe classmethod only needs information about the class, and no instance of the object.Another benefit of making the _is_cluster_for method a classmethod is so a subclass can decide to change it's implementation, maybe because it is pretty generic and can handle more than one type of cluster, so just checking the name of the class would not be enough.\", \"Let me tell the similarity between a method decorated with @classmethod vs @staticmethod first.Similarity: Both of them can be called on the Class itself, rather than just the instance of the class. So, both of them in a sense are Class's methods.Difference: A classmethod will receive the class itself as the first argument, while a staticmethod does not.So a static method is, in a sense, not bound to the Class itself and is just hanging in there just because it may have a related functionality.\", '@staticmethod just disables the default function as method descriptor.  classmethod wraps your function in a container callable that passes a reference to the owning class as first argument:As a matter of fact, classmethod has a runtime overhead but makes it possible to access the owning class.  Alternatively I recommend using a metaclass and putting the class methods on that metaclass:', 'The definitive guide on how to use static, class or abstract methods in Python is one good link for this topic, and summary it as following.@staticmethod function is nothing more than a function defined inside a class. It is callable without instantiating the class first. It\u2019s definition is immutable via inheritance.@classmethod function also callable without instantiating the class, but its definition follows Sub class, not Parent class, via inheritance, can be overridden by subclass. That\u2019s because the first argument for @classmethod function must always be cls (class).', 'Another consideration with respect to staticmethod vs classmethod comes up with inheritance.  Say you have the following class:And you then want to override bar() in a child class:This works, but note that now the bar() implementation in the child class (Foo2) can no longer take advantage of anything specific to that class.  For example, say Foo2 had a method called magic() that you want to use in the Foo2 implementation of bar():The workaround here would be to call Foo2.magic() in bar(), but then you\\'re repeating yourself (if the name of Foo2 changes, you\\'ll have to remember to update that bar() method).To me, this is a slight violation of the open/closed principle, since a decision made in Foo is impacting your ability to refactor common code in a derived class (ie it\\'s less open to extension).  If bar() were a classmethod we\\'d be fine:Gives: In Foo2 MAGICAlso: historical note: Guido Van Rossum (Python\\'s creator) once referred to staticmethod\\'s as \"an accident\": https://mail.python.org/pipermail/python-ideas/2012-May/014969.htmlwe all know how limited static methods are. (They\\'re basically an accident -- back in the Python 2.2 days when I was inventing new-style classes and descriptors, I meant to implement class methods but at first I didn\\'t understand them and accidentally implemented static methods first. Then it was too late to remove them and only provide class methods.Also: https://mail.python.org/pipermail/python-ideas/2016-July/041189.htmlHonestly, staticmethod was something of a mistake -- I was trying to do something like Java class methods but once it was released I found what was really needed was classmethod. But it was too late to get rid of staticmethod.', 'I will try to explain the basic difference using an example.1 - we can directly call static and classmethods without initializing2- Static method cannot call self method but can call other static and classmethod3- Static method belong to class and will not use object at all.4- Class method are not bound to an object but to a class.', \"Python comes with several built-in decorators. The big three are:First let's note that any function of a class can be called with instance of this class (after we initialized this class).@classmethod is the way to call function not only as an instance of a class but also directly by the class itself as its first argument.@staticmethod is a way of putting a function into a class (because it logically belongs there), while indicating that it does not require access to the class (so we don't need to use self in function definition).Let's consider the following class:Let's see how it works:Here you can see some use cases for those methods.Bonus: you can read about @property decorator here\", 'The difference occurs when there is inheritance.Suppose that there are two classes-- Parent and Child. If one wants to use @staticmethod, print_name method should be written twice because the name of the class should be written in the print line.However, for @classmethod, it is not required to write print_name method twice.', \"Instance Method:+ Can modify object instance state+ Can modify class stateClass Method:- Can't modify object instance state+ Can modify class stateStatic Method:- Can't modify object instance state- Can't modify class stateoutput:The instance method we actually had access to the object instance , right so this was an instance off a my class object whereas with the class method we have access to the class itself. But not to any of the objects,  because the class method doesn't really care about an object existing. However you can both call a class method and static method on an object instance. This is going to work it doesn't really make a difference, so again when you call static method here it's going to work and it's going to know which method you want to call.The Static methods are used to do some utility tasks, and class methods are used for factory methods. The factory methods can return class objects for different use cases.And finally, a short example for better understanding:\", '@classmethod : can be used to create a shared global access to all the instances created of that class..... like updating a record by multiple users....\\nI particulary found it use ful when creating singletons as well..:)@static method:  has nothing to do with the class or instance being associated with ...but for readability can use static method', 'My contribution demonstrates the difference amongst @classmethod, @staticmethod, and instance methods, including how an instance can indirectly call a @staticmethod. But instead of indirectly calling a @staticmethod from an instance, making it private may be more \"pythonic.\" Getting something from a private method isn\\'t demonstrated here but it\\'s basically the same concept.', 'A class method receives the class as implicit first argument, just like an instance method receives the instance. It is a method which is bound to the class and not the object of the class.It has access to the state of the class as it takes a class parameter that points to the class and not the object instance. It can modify a class state that would apply across all the instances of the class. For example it can modify a class variable that will be applicable to all the instances.On the other hand, a static method does not receive an implicit first argument, compared to class methods or instance methods. And can\u2019t access or modify class state. It only belongs to the class because from design point of view that is the correct way. But in terms of functionality is not bound, at runtime, to the class.as a guideline, use static methods as utilities, use class methods for example as factory . Or maybe to define a singleton. And use instance methods to model the state and behavior of instances.Hope I was clear !', 'You might want to consider the difference between:andThis has changed between python2 and python3:python2:python3:So using  @staticmethod for methods only called directly from the class has become optional in python3. If you want to call them from both class and instance, you still need to use the @staticmethod decorator.The other cases have been well covered by unutbus answer.', \"Class methods, as the name suggests, are used to make changes to classes and not the objects. To make changes to classes, they will modify the class attributes(not object attributes), since that is how you update classes.\\nThis is the reason that class methods take the class(conventionally denoted by 'cls') as the first argument.Static methods on the other hand, are used to perform functionalities that are not bound to the class i.e. they will not read or write class variables. Hence, static methods do not take classes as arguments. They are used so that classes can perform functionalities that are not directly related to the purpose of the class.\", 'I think giving a purely Python version of staticmethod and classmethod would help to understand the difference between them at language level (Refers to Descriptor Howto Guide).Both of them are non-data descriptors (It would be easier to understand them if you are familiar with descriptors first).', \"Analyze @staticmethod literally providing different insights.A normal method of a class is an implicit dynamic method which takes the instance as first argument.\\nIn contrast, a staticmethod does not take the instance as first argument, so is called 'static'.A staticmethod is indeed such a normal function the same as those outside a class definition.\\nIt is luckily grouped into the class just in order to stand closer where it is applied, or you might scroll around to find it.\", \"One pretty important practical difference occurs when subclassing. If you don't mind, I'll hijack @unutbu's example:In class_foo, the method knows which class it is called on:In static_foo, there is no way to determine whether it is called on A or B:Note that this doesn't mean you can't use other methods in a staticmethod, you just have to reference the class directly, which means subclasses' staticmethods will still reference the parent class:\", 'tldr;A staticmethod is essentially a function bound to a class (and consequently its instances)A classmethod is essentially an inheritable staticmethod.For details, see the excellent answers by others.', \"First let's start with an example code that we'll use to understand both concepts:Class methodA class method accepts the class itself as an implicit argument and -optionally- any other arguments specified in the definition. It\u2019s important to understand that a class method, does not have access to object instances (like instance methods do). Therefore, class methods cannot be used to alter the state of an instantiated object but instead, they are capable of changing the class state which is shared amongst all the instances of that class.\\nClass methods are typically useful when we need to access the class itself \u2014 for example, when we want to create a factory method, that is a method that creates instances of the class. In other words, class methods can serve as alternative constructors.In our example code, an instance of Employee can be constructed by providing three arguments; first_name , last_name and salary.Now let\u2019s assume that there\u2019s a chance that the name of an Employee can be provided in a single field in which the first and last names are separated by a whitespace. In this case, we could possibly use our class method called employee_from_full_name that accepts three arguments in total. The first one, is the class itself, which is an implicit argument which means that it won\u2019t be provided when calling the method \u2014 Python will automatically do this for us:Note that it is also possible to call employee_from_full_name from object instances although in this context it doesn\u2019t make a lot of sense:Another reason why we might want to create a class method, is when we need to change the state of the class. In our example, the class variable NO_OF_EMPLOYEES keeps track of the number of employees currently working for the company. This method is called every time a new instance of Employee is created and it updates the count accordingly:Static methodsOn the other hand, in static methods neither the instance (i.e. self) nor the class itself (i.e. cls) is passed as an implicit argument. This means that such methods, are not capable of accessing the class itself or its instances.\\nNow one could argue that static methods are not useful in the context of classes as they can also be placed in helper modules instead of adding them as members of the class. In object oriented programming, it is important to structure your classes into logical chunks and thus, static methods are quite useful when we need to add a method under a class simply because it logically belongs to the class.\\nIn our example, the static method named get_employee_legal_obligations_txt simply returns a string that contains the legal obligations of every single employee of a company. This function, does not interact with the class itself nor with any instance. It could have been placed into a different helper module however, it is only relevant to this class and therefore we have to place it under the Employee class.A static method can be access directly from the class itselfor from an instance of the class:References\"]",
            "url": "https://stackoverflow.com/questions/136097"
        },
        {
            "tag": "python",
            "question": [
                "Understanding slicing"
            ],
            "votes": "4409",
            "answer": "['The syntax is:There is also the step value, which can be used with any of the above:The key point to remember is that the :stop value represents the first value that is not in the selected slice. So, the difference between stop and start is the number of elements selected (if step is 1, the default).The other feature is that start or stop may be a negative number, which means it counts from the end of the array instead of the beginning. So:Similarly, step may be a negative number:Python is kind to the programmer if there are fewer items than you ask for. For example, if you ask for a[:-2] and a only contains one element, you get an empty list instead of an error. Sometimes you would prefer the error, so you have to be aware that this may happen.A slice object can represent a slicing operation, i.e.:is equivalent to:Slice objects also behave slightly differently depending on the number of arguments, similarly to range(), i.e. both slice(stop) and slice(start, stop[, step]) are supported.\\nTo skip specifying a given argument, one might use None, so that e.g. a[start:] is equivalent to a[slice(start, None)] or a[::-1] is equivalent to a[slice(None, None, -1)].While the :-based notation is very helpful for simple slicing, the explicit use of slice() objects simplifies the programmatic generation of slicing.', 'The Python tutorial talks about it (scroll down a bit until you get to the part about slicing).The ASCII art diagram is helpful too for remembering how slices work:One way to remember how slices work is to think of the indices as pointing between characters, with the left edge of the first character numbered 0. Then the right edge of the last character of a string of n characters has index n.', \"Enumerating the possibilities allowed by the grammar for the sequence x:Of course, if (high-low)%stride != 0, then the end point will be a little lower than high-1.If stride is negative, the ordering is changed a bit since we're counting down:Extended slicing (with commas and ellipses) are mostly used only by special data structures (like NumPy); the basic sequences don't support them.\", 'The answers above don\\'t discuss slice assignment. To understand slice assignment, it\\'s helpful to add another concept to the ASCII art:One heuristic is, for a slice from zero to n, think: \"zero is the beginning, start at the beginning and take n items in a list\".Another heuristic is, \"for any slice, replace the start by zero, apply the previous heuristic to get the end of the list, then count the first number back up to chop items off the beginning\"The first rule of slice assignment is that since slicing returns a list, slice assignment requires a list (or other iterable):The second rule of slice assignment, which you can also see above, is that whatever portion of the list is returned by slice indexing, that\\'s the same portion that is changed by slice assignment:The third rule of slice assignment is, the assigned list (iterable) doesn\\'t have to have the same length; the indexed slice is simply sliced out and replaced en masse by whatever is being assigned:The trickiest part to get used to is assignment to empty slices. Using heuristic 1 and 2 it\\'s easy to get your head around indexing an empty slice:And then once you\\'ve seen that, slice assignment to the empty slice makes sense too:Note that, since we are not changing the second number of the slice (4), the inserted items always stack right up against the \\'o\\', even when we\\'re assigning to the empty slice. So the position for the empty slice assignment is the logical extension of the positions for the non-empty slice assignments.Backing up a little bit, what happens when you keep going with our procession of counting up the slice beginning?With slicing, once you\\'re done, you\\'re done; it doesn\\'t start slicing backwards. In Python you don\\'t get negative strides unless you explicitly ask for them by using a negative number.There are some weird consequences to the \"once you\\'re done, you\\'re done\" rule:In fact, compared to indexing, Python slicing is bizarrely error-proof:This can come in handy sometimes, but it can also lead to somewhat strange behavior:Depending on your application, that might... or might not... be what you were hoping for there!Below is the text of my original answer. It has been useful to many people, so I didn\\'t want to delete it.This may also clarify the difference between slicing and indexing.', 'In short, the colons (:) in subscript notation (subscriptable[subscriptarg]) make slice notation, which has the optional arguments start, stop, and step:Python slicing is a computationally fast way to methodically access parts of your data. In my opinion, to be even an intermediate Python programmer, it\\'s one aspect of the language that it is necessary to be familiar with.To begin with, let\\'s define a few terms:start: the beginning index of the slice, it will include the element at this index unless it is the same as stop, defaults to 0, i.e. the first index. If it\\'s negative, it means to start n items from the end.stop: the ending index of the slice, it does not include the element at this index, defaults to length of the sequence being sliced, that is, up to and including the end.step: the amount by which the index increases, defaults to 1. If it\\'s negative, you\\'re slicing over the iterable in reverse.You can make any of these positive or negative numbers. The meaning of the positive numbers is straightforward, but for negative numbers, just like indexes in Python, you count backwards from the end for the start and stop, and for the step, you simply decrement your index. This example is from the documentation\\'s tutorial, but I\\'ve modified it slightly to indicate which item in a sequence each index references:To use slice notation with a sequence that supports it, you must include at least one colon in the square brackets that follow the sequence (which actually implement the __getitem__ method of the sequence, according to the Python data model.)Slice notation works like this:And recall that there are defaults for start, stop, and step, so to access the defaults, simply leave out the argument.Slice notation to get the last nine elements from a list (or any other sequence that supports it, like a string) would look like this:When I see this, I read the part in the brackets as \"9th from the end, to the end.\" (Actually, I abbreviate it mentally as \"-9, on\")The full notation isand to substitute the defaults (actually when step is negative, stop\\'s default is -len(my_list) - 1, so None for stop really just means it goes to whichever end step takes it to):The colon, :,  is what tells Python you\\'re giving it a slice and not a regular index. That\\'s why the idiomatic way of making a shallow copy of lists in Python 2 isAnd clearing them is with:(Python 3 gets a list.copy and list.clear method.)By default, when the step argument is empty (or None), it is assigned to +1.But you can pass in a negative integer, and the list (or most other standard sliceables) will be sliced from the end to the beginning.Thus a negative slice will change the defaults for start and stop!I like to encourage users to read the source as well as the documentation. The source code for slice objects and this logic is found here. First we determine if step is negative:If so, the lower bound is -1  meaning we slice all the way up to and including the beginning, and the upper bound is the length minus 1, meaning we start at the end. (Note that the semantics of this -1 is different from a -1 that users may pass indexes in Python indicating the last item.)Otherwise step is positive, and the lower bound will be zero and the upper bound (which we go up to but not including) the length of the sliced list.Then, we may need to apply the defaults for start and stop\u2014the default then for start is calculated as the upper bound when step is negative:and stop, the lower bound:You may find it useful to separate forming the slice from passing it to the list.__getitem__ method (that\\'s what the square brackets do). Even if you\\'re not new to it, it keeps your code more readable so that others that may have to read your code can more readily understand what you\\'re doing.However, you can\\'t just assign some integers separated by colons to a variable. You need to use the slice object:The second argument, None, is required, so that the first argument is interpreted as the start argument otherwise it would be the stop argument.You can then pass the slice object to your sequence:It\\'s interesting that ranges also take slices:Since slices of Python lists create new objects in memory, another important function to be aware of is itertools.islice. Typically you\\'ll want to iterate over a slice, not just have it created statically in memory. islice is perfect for this. A caveat, it doesn\\'t support negative arguments to start, stop, or step, so if that\\'s an issue you may need to calculate indices or reverse the iterable in advance.and now:The fact that list slices make a copy is a feature of lists themselves. If you\\'re slicing advanced objects like a Pandas DataFrame, it may return a view on the original, and not a copy.', \"And a couple of things that weren't immediately obvious to me when I first saw the slicing syntax:Easy way to reverse sequences!And if you wanted, for some reason, every second item in the reversed sequence:\", 'In Python 2.7Slicing in PythonUnderstanding index assignment is very important.When you say [a:b:c], you are saying depending on the sign of c (forward or backward), start at a and end at b (excluding element at bth index). Use the indexing rule above and remember you will only find elements in this range:But this range continues in both directions infinitely:For example:If your choice of a, b, and c allows overlap with the range above as you traverse using rules for a,b,c above you will either get a list with elements (touched during traversal) or you will get an empty list.One last thing: if a and b are equal, then also you get an empty list:', 'Found this great table at http://wiki.python.org/moin/MovingToPythonFromOtherLanguages', 'After using it a bit I realise that the simplest description is that it is exactly the same as the arguments in a for loop...Any of them are optional:Then the negative indexing just needs you to add the length of the string to the negative indices to understand it.This works for me anyway...', 'I find it easier to remember how it works, and then I can figure out any specific start/stop/step combination.It\\'s instructive to understand range() first:Begin from start, increment by step, do not reach stop.  Very simple.The thing to remember about negative step is that stop is always the excluded end, whether it\\'s higher or lower. If you want same slice in opposite order, it\\'s much cleaner to do the reversal separately: e.g. \\'abcde\\'[1:-2][::-1] slices off one char from left, two from right, then reverses. (See also reversed().)Sequence slicing is same, except it first normalizes negative indexes, and it can never go outside the sequence:TODO: The code below had a bug with \"never go outside the sequence\" when abs(step)>1; I think I patched it to be correct, but it\\'s hard to understand.Don\\'t worry about the is None details - just remember that omitting start and/or stop always does the right thing to give you the whole sequence.Normalizing negative indexes first allows start and/or stop to be counted from the end independently: \\'abcde\\'[1:-2] == \\'abcde\\'[1:3] == \\'bc\\' despite range(1,-2) == [].\\nThe normalization is sometimes thought of as \"modulo the length\", but note it adds the length just once: e.g. \\'abcde\\'[-53:42] is just the whole string.', 'I use the \"an index points between elements\" method of thinking about it myself, but one way of describing it which sometimes helps others get it is this:X is the index of the first element you want.\\nY is the index of the first element you don\\'t want.', 'I hope this will help you to model the list in Python.Reference: http://wiki.python.org/moin/MovingToPythonFromOtherLanguages', \"This is how I teach slices to newbies:Understanding the difference between indexing and slicing:Wiki Python has this amazing picture which clearly distinguishes indexing and slicing.It is a list with six elements in it. To understand slicing better, consider that list as a set of six boxes placed together. Each box has an alphabet in it.Indexing is like dealing with the contents of box. You can check contents of any box. But you can't check the contents of multiple boxes at once. You can even replace the contents of the box. But you can't place two balls in one box or replace two balls at a time.Slicing is like dealing with boxes themselves. You can pick up the first box and place it on another table. To pick up the box, all you need to know is the position of beginning and ending of the box.You can even pick up the first three boxes or the last two boxes or all boxes between 1 and 4. So, you can pick any set of boxes if you know the beginning and ending. These positions are called start and stop positions.The interesting thing is that you can replace multiple boxes at once. Also you can place multiple boxes wherever you like.Slicing With Step:Till now you have picked boxes continuously. But sometimes you need to pick up discretely. For example, you can pick up every second box. You can even pick up every third box from the end. This value is called step size. This represents the gap between your successive pickups. The step size should be positive if You are picking boxes from the beginning to end and vice versa.How Python Figures Out Missing Parameters:When slicing, if you leave out any parameter, Python tries to figure it out automatically.If you check the source code of CPython, you will find a function called PySlice_GetIndicesEx() which figures out indices to a slice for any given parameters. Here is the logical equivalent code in Python.This function takes a Python object and optional parameters for slicing and returns the start, stop, step, and slice length for the requested slice.This is the intelligence that is present behind slices. Since Python has an built-in function called slice, you can pass some parameters and check how smartly it calculates missing parameters.Note: This post was originally written in my blog, The Intelligence Behind Python Slices.\", 'Python slicing notation:The notation extends to (numpy) matrices and multidimensional arrays.  For example, to slice entire columns you can use:Slices hold references, not copies, of the array elements.  If you want to make a separate copy an array, you can use deepcopy().', 'You can also use slice assignment to remove one or more elements from a list:', 'This is just for some extra info...\\nConsider the list belowFew other tricks for reversing the list:', \"To make it simple, remember slice has only one form\uff1aand here is how it works:Another import thing: all start,end, step can be omitted! And if they are omitted, their default value will be used: 0,len(s),1 accordingly.So possible variations are:NOTE: If start >= end (considering only when step>0), Python will return a empty slice [].The above part explains the core features on how slice works, and it will work on most occasions. However, there can be pitfalls you should watch out, and this part explains them.The very first thing that confuses Python learners is that an index can be negative!\\nDon't panic: a negative index means count backwards.For example:Making things more confusing is that step can be negative too!A negative step means iterate the array backwards: from the end to start, with the end index included, and the start index excluded from the result.NOTE: when step is negative, the default value for start is len(s) (while end does not equal to 0, because s[::-1] contains s[0]). For example:Be surprised: slice does not raise an IndexError when the index is out of range!If the index is out of range, Python will try its best to set the index to 0 or len(s) according to the situation. For example:Let's finish this answer with examples, explaining everything we have discussed:\", 'As a general rule, writing code with a lot of hardcoded index values leads to a readability\\nand maintenance mess. For example, if you come back to the code a year later, you\u2019ll\\nlook at it and wonder what you were thinking when you wrote it. The solution shown\\nis simply a way of more clearly stating what your code is actually doing.\\nIn general, the built-in slice() creates a slice object that can be used anywhere a slice\\nis allowed. For example:If you have a slice instance s, you can get more information about it by looking at its\\ns.start, s.stop, and s.step attributes, respectively. For example:', 'The previous answers don\\'t discuss multi-dimensional array slicing which is possible using the famous NumPy package:Slicing can also be applied to multi-dimensional arrays.The \":2\" before the comma operates on the first dimension and the \"0:3:2\" after the comma operates on the second dimension.', 'I- Convert upper bound  and lower bound into common signs.II- Then check if the step size is a positive or a negative value.(i) If the step size is a positive value, upper bound should be greater than lower bound, otherwise empty string is printed. For example:The output:However if we run the following code:It will return an empty string.(ii) If the step size if a negative value, upper bound should be lesser than lower bound, otherwise empty string will be printed. For example:The output:But if we run the following code:The output will be an empty string.Thus in the code:In the first str2=str[l-1:0:-1], the upper bound is lesser than the lower bound, thus dcb is printed.However in str2=str[l-1:-1:-1], the upper bound is not less than the lower bound (upon converting lower bound into negative value which is -1: since index of last element is -1 as well as 3).', 'In my opinion, you will understand and memorize better the Python string slicing notation if you look at it the following way (read on).Let\\'s work with the following string ...For those who don\\'t know, you can create any substring from azString using the notation azString[x:y]Coming from other programming languages, that\\'s when the common sense gets compromised. What are x and y?I had to sit down and run several scenarios in my quest for a memorization technique that will help me remember what x and y are and help me slice strings properly at the first attempt.My conclusion is that x and y should be seen as the boundary indexes that are surrounding the strings that we want to extra. So we should see the expression as azString[index1, index2] or even more clearer as azString[index_of_first_character, index_after_the_last_character].Here is an example visualization of that ...So all you have to do is setting index1 and index2 to the values that will surround the desired substring. For instance, to get the substring \"cdefgh\", you can use azString[2:8], because the index on the left side of \"c\" is 2 and the one on the right size of \"h\" is 8.Remember that we are setting the boundaries. And those boundaries are the positions where you could place some brackets that will be wrapped around the substring like this ...a b [ c d e f g h ] i jThat trick works all the time and is easy to memorize.', 'I personally think about it like a for loop:Also, note that negative values for start and end are relative to the end of the list and computed in the example above by given_index + a.shape[0].', 'You can run this script and experiment with it, below is some samples that I got from the script.When using a negative step, notice that the answer is shifted to the right by 1.', \"My brain seems happy to accept that lst[start:end] contains the start-th item. I might even say that it is a 'natural assumption'.But occasionally a doubt creeps in and my brain asks for reassurance that it does not contain the end-th element.In these moments I rely on this simple theorem:This pretty property tells me that lst[start:end] does not contain the end-th item because it is in lst[end:].Note that this theorem is true for any n at all. For example, you can check thatreturns True.\", 'In Python, the most basic form for slicing is the following:where l is some collection, start is an inclusive index, and end is an exclusive index.When slicing from the start, you can omit the zero index, and when slicing to the end, you can omit the final index since it is redundant, so do not be verbose:Negative integers are useful when doing offsets relative to the end of a collection:It is possible to provide indices that are out of bounds when slicing such as:Keep in mind that the result of slicing a collection is a whole new collection. In addition, when using slice notation in assignments, the length of the slice assignments do not need to be the same. The values before and after the assigned slice will be kept, and the collection will shrink or grow to contain the new values:If you omit the start and end index, you will make a copy of the collection:If the start and end indexes are omitted when performing an assignment operation, the entire content of the collection will be replaced with a copy of what is referenced:Besides basic slicing, it is also possible to apply the following notation:where l is a collection, start is an inclusive index, end is an exclusive index, and step is a stride that can be used to take every nth item in l.Using step provides a useful trick to reverse a collection in Python:It is also possible to use negative integers for step as the following example:However, using a negative value for step could become very confusing. Moreover, in order to be Pythonic, you should avoid using start, end, and step in a single slice. In case this is required, consider doing this in two assignments (one to slice, and the other to stride).', \"I want to add one Hello, World! example that explains the basics of slices for the very beginners. It helped me a lot.Let's have a list with six values ['P', 'Y', 'T', 'H', 'O', 'N']:Now the simplest slices of that list are its sublists. The notation is [<index>:<index>] and the key is to read it like this:Now if you make a slice [2:5] of the list above, this will happen:You made a cut before the element with index 2 and another cut before the element with index 5. So the result will be a slice between those two cuts, a list ['T', 'H', 'O'].\", 'Most of the previous answers clears up questions about slice notation.The extended indexing syntax used for slicing is aList[start:stop:step], and basic examples are::More slicing examples: 15 Extended Slices', 'The below is the example of an index of a string:Slicing example: [start:end:step]Below is the example usage:', \"If you feel negative indices in slicing is confusing, here's a very easy way to think about it: just replace the negative index with len - index. So for example, replace -3 with len(list) - 3.The best way to illustrate what slicing does internally is just show it in code that implements this operation:\", \"I don't think that the Python tutorial diagram (cited in various other answers) is good as this suggestion works for positive stride, but does not for a negative stride.This is the diagram:From the diagram, I expect a[-4,-6,-1] to be yP but it is ty.What always work is to think in characters or slots and use indexing as a half-open interval -- right-open if positive stride, left-open if negative stride.This way, I can think of a[-4:-6:-1] as a(-6,-4] in interval terminology.\"]",
            "url": "https://stackoverflow.com/questions/509211"
        },
        {
            "tag": "python",
            "question": [
                "Finding the index of an item in a list"
            ],
            "votes": "4215",
            "answer": "['The simplest case is handled by the built-in .index method of the list:Return zero-based index in the list of the first item whose value is equal to x. Raises a ValueError if there is no such item.The optional arguments start and end are interpreted as in the slice notation and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the start argument.Thus, we can do:An index call checks every element of the list in order, until it finds a match. If the list is long, and if there is no guarantee that the value will be near the beginning, this can slow down the code.This problem can only be completely avoided by using a different data structure. However, if the element is known to be within a certain part of the list, the start and end parameters can be used to narrow the search.For example:The second call is orders of magnitude faster, because it only has to search through 10 elements, rather than all 1 million.A call to index searches through the list in order until it finds a match, and stops there. If there could be more than one occurrence of the value, and all indices are needed, index cannot solve the problem:Instead, use a list comprehension or generator expression to do the search, with enumerate to get indices:The list comprehension and generator expression techniques still work if there is only one match, and are more generalizable.As noted in the documentation above, using .index will raise an exception if the searched-for value is not in the list:If this is a concern, either explicitly check first using item in my_list, or handle the exception with try/except as appropriate.The explicit check is simple and readable, but it must iterate the list a second time. See What is the EAFP principle in Python? for more guidance on this choice.', \"The majority of answers explain how to find a single index, but their methods do not return multiple indexes if the item is in the list multiple times. Use enumerate():The index() function only returns the first occurrence, while enumerate() returns all occurrences.As a list comprehension:Here's also another small solution with itertools.count() (which is pretty much the same approach as enumerate):This is more efficient for larger lists than using enumerate():\", 'To get all indexes:', 'index() returns the first index of value!|  index(...)\\n   |      L.index(value, [start, [stop]]) -> integer -- return first index of value', 'A problem will arise if the element is not in the list. This function handles the issue:', '', \"You have to set a condition to check if the element you're searching is in the list\", 'If you want all indexes, then you can use NumPy:It is clear, readable solution.', \"All of the proposed functions here reproduce inherent language behavior but obscure what's going on.Why write a function with exception handling if the language provides the methods to do what you want itself?\", 'For a list [\"foo\", \"bar\", \"baz\"] and an item in the list \"bar\", what\\'s the cleanest way to get its index (1) in Python?Well, sure, there\\'s the index method, which returns the index of the first occurrence:There are a couple of issues with this method:If the value could be missing, you need to catch the ValueError.You can do so with a reusable definition like this:And use it like this:And the downside of this is that you will probably have a check for if the returned value is or is not None:If you could have more occurrences, you\\'ll not get complete information with list.index:You might enumerate into a list comprehension the indexes:If you have no occurrences, you can check for that with boolean check of the result, or just do nothing if you loop over the results:If you have pandas, you can easily get this information with a Series object:A comparison check will return a series of booleans:Pass that series of booleans to the series via subscript notation, and you get just the matching members:If you want just the indexes, the index attribute returns a series of integers:And if you want them in a list or tuple, just pass them to the constructor:Yes, you could use a list comprehension with enumerate too, but that\\'s just not as elegant, in my opinion - you\\'re doing tests for equality in Python, instead of letting builtin code written in C handle it:The XY problem is asking about your attempted solution rather than your actual problem.Why do you think you need the index given an element in a list?If you already know the value, why do you care where it is in a list?If the value isn\\'t there, catching the ValueError is rather verbose - and I prefer to avoid that.I\\'m usually iterating over the list anyways, so I\\'ll usually keep a pointer to any interesting information, getting the index with enumerate.If you\\'re munging data, you should probably be using pandas - which has far more elegant tools than the pure Python workarounds I\\'ve shown.I do not recall needing list.index, myself. However, I have looked through the Python standard library, and I see some excellent uses for it.There are many, many uses for it in idlelib, for GUI and text parsing.The keyword module uses it to find comment markers in the module to automatically regenerate the list of keywords in it via metaprogramming.In Lib/mailbox.py it seems to be using it like an ordered mapping:andIn Lib/http/cookiejar.py, seems to be used to get the next month:In Lib/tarfile.py similar to distutils to get a slice up to an item:In Lib/pickletools.py:What these usages seem to have in common is that they seem to operate on lists of constrained sizes (important because of O(n) lookup time for list.index), and they\\'re mostly used in parsing (and UI in the case of Idle).While there are use-cases for it, they are fairly uncommon. If you find yourself looking for this answer, ask yourself if what you\\'re doing is the most direct usage of the tools provided by the language for your use-case.', 'With enumerate(alist) you can store the first element (n) that is the index of the list when the element x is equal to what you look for.This function takes the item and the list as arguments and return the position of the item in the list, like we saw before.OutputOutput:', 'You can apply this for any member of the list to get their index', 'All indexes with the zip function:', 'Simply you can go with', 'Another option', \"... like confirming the existence of the item before getting the index.  The nice thing about this approach is the function always returns a list of indices -- even if it is an empty list.  It works with strings as well.When pasted into an interactive python window:After another year of heads-down python development, I'm a bit embarrassed by my original answer, so to set the record straight, one can certainly use the above code; however, the much more idiomatic way to get the same behavior would be to use list comprehension, along with the enumerate() function.Something like this:Which, when pasted into an interactive python window yields:And now, after reviewing this question and all the answers, I realize that this is exactly what FMc suggested in his earlier answer.  At the time I originally answered this question, I didn't even see that answer, because I didn't understand it.  I hope that my somewhat more verbose example will aid understanding.If the single line of code above still doesn't make sense to you, I highly recommend you Google 'python list comprehension' and take a few minutes to familiarize yourself.  It's just one of the many powerful features that make it a joy to use Python to develop code.\", 'My friend, I have made the easiest code to solve your question. While you were receiving gigantic lines of codes, I am here to cater you a two line code which is all due to the help of index() function in python.Output:I Hope I have given you the best and the simplest answer which might help you greatly.', 'A variant on the answer from FMc and user7177 will give a dict that can return all indices for any entry:You could also use this as a one liner to get all indices for a single entry. There are no guarantees for efficiency, though I did use set(a) to reduce the number of times the lambda is called.', 'Finding index of item x in list L:', \"This solution is not as powerful as others, but if you're a beginner and only know about forloops it's still possible to find the first index of an item while avoiding the ValueError:\", 'There is a chance that that value may not be present so to avoid this ValueError, we can check if that actually exists in the list .', 'List comprehension would be the best option to acquire a compact implementation in finding the index of an item in a list.', 'It just uses the python function array.index() and with a simple Try / Except it returns the position of the record if it is found in the list and return -1 if it is not found in the list (like on JavaScript with the function indexOf()).In this case \"mango\" is not present in the list fruits so the pos variable is -1, if I had searched for \"cherry\" the pos variable would be 2.', 'There is a more functional answer to this.More generic form:', '', 'Python index() method throws an error if the item was not found. So instead you can make it similar to the indexOf() function of JavaScript which returns -1 if the item was not found:', \"This accounts for if the string is not in the list too, if it isn't in the list then location = -1\", 'If you are going to find an index once then using \"index\" method is fine. However, if you are going to search your data more than once then I recommend using bisect module. Keep in mind that using bisect module data must be sorted. So you sort data once and then you can use bisect.\\nUsing bisect module on my machine is about 20 times faster than using index method.Here is an example of code using Python 3.8 and above syntax:Output:', 'Since Python lists are zero-based, we can use the zip built-in function as follows:where \"haystack\" is the list in question and \"needle\" is the item to look for.(Note: Here we are iterating using i to get the indexes, but if we need rather to focus on the items we can switch to j.)', 'It is mentioned in numerous answers that the built-in method of list.index(item) method is an O(n) algorithm. It is fine if you need to perform this once. But if you need to access the indices of elements a number of times, it makes more sense to first create a dictionary (O(n)) of item-index pairs, and then access the index at O(1) every time you need it.If you are sure that the items in your list are never repeated, you can easily:If you may have duplicate elements, and need to return all of their indices:']",
            "url": "https://stackoverflow.com/questions/176918"
        },
        {
            "tag": "python",
            "question": [
                "Iterating over dictionaries using 'for' loops"
            ],
            "votes": "4067",
            "answer": "[\"key is just a variable name.will simply loop over the keys in the dictionary, rather than the keys and values.  To loop over both key and value you can use the following:For Python 3.x:For Python 2.x:To test for yourself, change the word key to poop.In Python 3.x, iteritems() was replaced with simply items(), which returns a set-like view backed by the dict, like iteritems() but even better. \\nThis is also available in 2.7 as viewitems().The operation items() will work for both 2 and 3, but in 2 it will return a list of the dictionary's (key, value) pairs, which will not reflect changes to the dict that happen after the items() call. If you want the 2.x behavior in 3.x, you can call list(d.items()).\", 'It\\'s not that key is a special word, but that dictionaries implement the iterator protocol.  You could do this in your class, e.g. see this question for how to build class iterators.In the case of dictionaries, it\\'s implemented at the C level.  The details are available in PEP 234.  In particular, the section titled \"Dictionary Iterators\":Dictionaries implement a tp_iter slot that returns an efficient\\n  iterator that iterates over the keys of the dictionary. [...] This \\n  means that we can writewhich is equivalent to, but much faster thanas long as the restriction on modifications to the dictionary\\n  (either by the loop or by another thread) are not violated.Add methods to dictionaries that return different kinds of\\n  iterators explicitly:This means that for x in dict is shorthand for for x in\\n   dict.iterkeys().In Python 3, dict.iterkeys(), dict.itervalues() and dict.iteritems() are no longer supported. Use dict.keys(), dict.values() and dict.items() instead.', \"Iterating over a dict iterates through its keys in no particular order, as you can see here:(This is no longer the case in Python 3.6, but note that it's not guaranteed behaviour yet.)For your example, it is a better idea to use dict.items():This gives you a list of tuples. When you loop over them like this, each tuple is unpacked into k and v automatically:Using k and v as variable names when looping over a dict is quite common if the body of the loop is only a few lines. For more complicated loops it may be a good idea to use more descriptive names:It's a good idea to get into the habit of using format strings:\", 'key is simply a variable.For Python2.X:... or better,For Python3.X:', 'When you iterate through dictionaries using the for .. in ..-syntax, it always iterates over the keys (the values are accessible using dictionary[key]).To iterate over key-value pairs, use the following:', \"This is a very common looping idiom. in is an operator. For when to use for key in dict and when it must be for key in dict.keys() see David Goodger's Idiomatic Python article (archived copy).\", 'I have a use case where I have to iterate through the dict to get the key, value pair, also the index indicating where I am. This is how I do it:Note that the parentheses around the key, value are important, without them, you\\'d get an ValueError \"not enough values to unpack\".', 'How does Python recognize that it needs only to read the key from the\\n  dictionary? Is key a special word in Python? Or is it simply a\\n  variable?It\\'s not just for loops. The important word here is \"iterating\".A dictionary is a mapping of keys to values:Any time we iterate over it, we iterate over the keys. The variable name key is only intended to be descriptive - and it is quite apt for the purpose.This happens in a list comprehension:It happens when we pass the dictionary to list (or any other collection type object):The way Python iterates is, in a context where it needs to, it calls the __iter__ method of the object (in this case the dictionary) which returns an iterator (in this case, a keyiterator object):We shouldn\\'t use these special methods ourselves, instead, use the respective builtin function to call it, iter:Iterators have a __next__ method - but we call it with the builtin function, next:When an iterator is exhausted, it raises StopIteration. This is how Python knows to exit a for loop, or a list comprehension, or a generator expression, or any other iterative context. Once an iterator raises StopIteration it will always raise it - if you want to iterate again, you need a new one.We\\'ve seen dicts iterating in many contexts. What we\\'ve seen is that any time we iterate over a dict, we get the keys. Back to the original example:If we change the variable name, we still get the keys. Let\\'s try it:If we want to iterate over the values, we need to use the .values method of dicts, or for both together, .items:In the example given, it would be more efficient to iterate over the items like this:But for academic purposes, the question\\'s example is just fine.', 'For Iterating through dictionaries, The below code can be used.', \"You can check the implementation of CPython's dicttype on GitHub. This is the signature of method that implements the dict iterator:CPython dictobject.c\", 'To iterate over keys, it is slower but better to use my_dict.keys(). If you tried to do something like this:it would create a runtime error because you are changing the keys while the program is running. If you are absolutely set on reducing time, use the for key in my_dict way, but you have been warned.', 'If you are looking for a clear and visual example:Result:', 'This will print the output in sorted order by values in ascending order.Output:', \"Let's get straight to the point. If the word key is just a variable, as you have mentioned then the main thing to note is that when you run a 'FOR LOOP' over a dictionary it runs through only the 'keys' and ignores the 'values'.rather try this:but if you use a function like:in the above case 'keys' is just not a variable, its a function.\", 'A dictionary in Python is a collection of key-value pairs. Each key is connected to a value, and you can use a key to access the value associated with that key. A key\\'s value can be a number, a string, a list, or even another dictionary. In this case, threat each \"key-value pair\" as a separate row in the table: d is your table with two columns. the key is the first column, key[value] is your second column. Your for loop is a standard way to iterate over a table.']",
            "url": "https://stackoverflow.com/questions/3294889"
        },
        {
            "tag": "python",
            "question": [
                "Using global variables in a function"
            ],
            "votes": "3798",
            "answer": "[\"You can use a global variable within other functions by declaring it as global within each function that assigns a value to it:Since it's unclear whether globvar = 1 is creating a local variable or changing a global variable, Python defaults to creating a local variable, and makes you explicitly choose the other behavior with the global keyword.See other answers if you want to share a global variable across modules.\", \"If I'm understanding your situation correctly, what you're seeing is the result of how Python handles local (function) and global (module) namespaces.Say you've got a module like this:You might expecting this to print 42, but instead it prints 5.  As has already been mentioned, if you add a 'global' declaration to func1(), then func2() will print 42.What's going on here is that Python assumes that any name that is assigned to, anywhere within a function, is local to that function unless explicitly told otherwise.  If it is only reading from a name, and the name doesn't exist locally, it will try to look up the name in any containing scopes (e.g. the module's global scope).When you assign 42 to the name _my_global, therefore, Python creates a local variable that shadows the global variable of the same name.  That local goes out of scope and is garbage-collected when func1() returns; meanwhile, func2() can never see anything other than the (unmodified) global name.  Note that this namespace decision happens at compile time, not at runtime -- if you were to read the value of _my_global inside func1() before you assign to it, you'd get an UnboundLocalError, because Python has already decided that it must be a local variable but it has not had any value associated with it yet.  But by using the 'global' statement, you tell Python that it should look elsewhere for the name instead of assigning to it locally.(I believe that this behavior originated largely through an optimization of local namespaces -- without this behavior,  Python's VM would need to perform at least three name lookups each time a new name is assigned to inside a function (to ensure that the name didn't already exist at module/builtin level), which would significantly slow down a very common operation.)\", 'You may want to explore the notion of namespaces. In Python, the module is the natural place for global data:Each module has its own private symbol table, which is used as the global symbol table by all functions defined in the module. Thus, the author of a module can use global variables in the module without worrying about accidental clashes with a user\u2019s global variables. On the other hand, if you know what you are doing you can touch a module\u2019s global variables with the same notation used to refer to its functions, modname.itemname.A specific use of global-in-a-module is described here - How do I share global variables across modules?, and for completeness the contents are shared here:The canonical way to share information across modules within a single program is to create a special configuration module (often called config or cfg). Just import the configuration module in all modules of your application; the module then becomes available as a global name. Because there is only one instance of each module, any changes made to the module object get reflected everywhere. For example:File: config.pyFile: mod.pyFile: main.py', 'Python uses a simple heuristic to decide which scope it should load a variable from, between local and global.  If a variable name appears on the left hand side of an assignment, but is not declared global, it is assumed to be local.  If it does not appear on the left hand side of an assignment, it is assumed to be global.See how baz, which appears on the left side of an assignment in foo(), is the only LOAD_FAST variable.', \"If you want to refer to a global variable in a function, you can use the global keyword to declare which variables are global. You don't have to use it in all cases (as someone here incorrectly claims) - if the name referenced in an expression cannot be found in local scope or scopes in the functions in which this function is defined, it is looked up among global variables.However, if you assign to a new variable not declared as global in the function, it is implicitly declared as local, and it can overshadow any existing global variable with the same name.Also, global variables are useful, contrary to some OOP zealots who claim otherwise - especially for smaller scripts, where OOP is overkill.\", 'We can create a global with the following function:Writing a function does not actually run its code. So we call the create_global_variable function:You can just use it, so long as you don\\'t expect to change which object it points to:For example,and now we can use the global variable:To point the global variable at a different object, you are required to use the global keyword again:Note that after writing this function, the code actually changing it has still not run:So after calling the function:we can see that the global variable has been changed. The global_variable name now points to \\'Bar\\':Note that \"global\" in Python is not truly global - it\\'s only global to the module level. So it is only available to functions written in the modules in which it is global. Functions remember the module in which they are written, so when they are exported into other modules, they still look in the module in which they were created to find global variables.If you create a local variable with the same name, it will overshadow a global variable:But using that misnamed local variable does not change the global variable:Note that you should avoid using the local variables with the same names as globals unless you know precisely what you are doing and have a very good reason to do so. I have not yet encountered such a reason.A follow on comment asks:what to do if I want to create a global variable inside a function inside a class and want to use that variable inside another function inside another class?Here I demonstrate we get the same behavior in methods as we do in regular functions:And now:But I would suggest instead of using global variables you use class attributes, to avoid cluttering the module namespace. Also note we don\\'t use self arguments here - these could be class methods (handy if mutating the class attribute from the usual cls argument) or static methods (no self or cls).', 'In addition to already existing answers and to make this more confusing:In Python, variables that are only referenced inside a function are\\n  implicitly global. If a variable is assigned a new value anywhere\\n  within the function\u2019s body, it\u2019s assumed to be a local. If a variable\\n  is ever assigned a new value inside the function, the variable is\\n  implicitly local, and you need to explicitly declare it as \u2018global\u2019.Though a bit surprising at first, a moment\u2019s consideration explains\\n  this. On one hand, requiring global for assigned variables provides a\\n  bar against unintended side-effects. On the other hand, if global was\\n  required for all global references, you\u2019d be using global all the\\n  time. You\u2019d have to declare as global every reference to a built-in\\n  function or to a component of an imported module. This clutter would\\n  defeat the usefulness of the global declaration for identifying\\n  side-effects.Source: What are the rules for local and global variables in Python?.', \"With parallel execution, global variables can cause unexpected results if you don't understand what is happening. Here is an example of using a global variable within multiprocessing. We can clearly see that each process works with its own copy of the variable:Output:\", 'As it turns out the answer is always simple.Here is a small sample module with a simple way to show it in a main definition:Here is how to show it in a main definition:This simple code works just like that, and it will execute. I hope it helps.', 'What you are saying is to use the method like this:But the better way is to use the global variable like this:Both give the same output.', 'You need to reference the global variable in every function you want to use.As follows:', 'Try this:', \"You're not actually storing the global in a local variable, just creating a local reference to the same object that your original global reference refers to. Remember that pretty much everything in Python is a name referring to an object, and nothing gets copied in usual operation.If you didn't have to explicitly specify when an identifier was to refer to a predefined global, then you'd presumably have to explicitly specify when an identifier is a new local variable instead (for example, with something like the 'var' command seen in JavaScript). Since local variables are more common than global variables in any serious and non-trivial system, Python's system makes more sense in most cases.You could have a language which attempted to guess, using a global variable if it existed or creating a local variable if it didn't. However, that would be very error-prone. For example, importing another module could inadvertently introduce a global variable by that name, changing the behaviour of your program.\", 'In case you have a local variable with the same name, you might want to use the globals() function.', 'Following on and as an add on, use a file to contain all global variables all declared locally and then import as:File initval.py:File getstocks.py:', 'Writing to explicit elements of a global array does not apparently need the global declaration, though writing to it \"wholesale\" does have that requirement:', 'I\\'m adding this as I haven\\'t seen it in any of the other answers and it might be useful for someone struggling with something similar. The globals() function returns a mutable global symbol dictionary where you can \"magically\" make data available for the rest of your code. \\nFor example:andWill just let you dump/load variables out of and into the global namespace. Super convenient, no muss, no fuss. Pretty sure it\\'s Python 3 only.', 'Reference the class namespace where you want the change to show up.In this example, runner is using max from the file config. I want my test to change the value of max when runner is using it.main/config.pymain/runner.pytests/runner_test.py', \"Explanation:global_var is a global variable and all functions and classes can access that variable.The func_1() accessed that global variable using the keyword global which points to the variable which is written in the global scope. If I didn't write the global keyword the variable global_var inside func_1 is considered a local variable that is only usable inside the function. Then inside func_1, I have incremented that global variable by 1.The same happened in func_2().After calling func_1 and func_2, you'll see the global_var is changed\", \"Globals in connection with multiprocessing on different platforms/envrionments \\nas Windows/Mac OS on the one side and Linux on the other are troublesome.I will show you this with a simple example pointing out a problem which I run into some time ago.If you want to understand, why things are different on Windows/MacOs and Linux you \\nneed to know that, the default mechanism to start a new process on ...They are different in Memory allocation an initialisation ... (but I don't go into this\\nhere).Let's have a look at the problem/example ...If you run this on Windows (And I suppose on MacOS too), you get the following output ...If you run this on Linux, you get the following instead.\", 'There are 2 ways to declare a variable as global:1. assign variable inside functions and use global line2. assign variable outside functions:Now we can use these declared global variables in the other functions:Note 1:If you want to change a global variable inside another function like update_variables() you should use global line in that function before assigning the variable:Note 2:There is a exception for note 1 for list and dictionary variables while not using global line inside a function:', 'Though this has been answered, I am giving solution again as I prefer single line\\nThis is if you wish to create global variable within function', 'Like this code:Key:If you declare a variable outside the strings, it become global.If you declare a variable inside the strings, it become local.If you want to declare a global variable inside the strings, use the keyword global before the variable you want to declare:and then you have 100 in the document.', 'Here we are comparing global variable Initialized that 0, so while loop condition got trueFunction will get called.Loop will be infinite', \"if you want to access global var you just add global keyword inside your function\\nex:\\nglobal_var = 'yeah'\"]",
            "url": "https://stackoverflow.com/questions/423379"
        },
        {
            "tag": "python",
            "question": [
                "How to iterate over rows in a DataFrame in Pandas"
            ],
            "votes": "3713",
            "answer": "['DataFrame.iterrows is a generator which yields both the index and row (as a Series):', 'Iteration in Pandas is an anti-pattern and is something you should only do when you have exhausted every other option. You should not use any function with \"iter\" in its name for more than a few thousand rows or you will have to get used to a lot of waiting.Do you want to print a DataFrame? Use DataFrame.to_string().Do you want to compute something? In that case, search for methods in this order (list modified from here):iterrows and itertuples (both receiving many votes in answers to this question) should be used in very rare circumstances, such as generating row objects/nametuples for sequential processing, which is really the only thing these functions are useful for.Appeal to AuthorityThe documentation page on iteration has a huge red warning box that says:Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed [...].* It\\'s actually a little more complicated than \"don\\'t\". df.iterrows() is the correct answer to this question, but \"vectorize your ops\" is the better one. I will concede that there are circumstances where iteration cannot be avoided (for example, some operations where the result depends on the value computed for the previous row). However, it takes some familiarity with the library to know when. If you\\'re not sure whether you need an iterative solution, you probably don\\'t. PS: To know more about my rationale for writing this answer, skip to the very bottom.A good number of basic operations and computations are \"vectorised\" by pandas (either through NumPy, or through Cythonized functions). This includes arithmetic, comparisons, (most) reductions, reshaping (such as pivoting), joins, and groupby operations. Look through the documentation on Essential Basic Functionality to find a suitable vectorised method for your problem.If none exists, feel free to write your own using custom Cython extensions.List comprehensions should be your next port of call if 1) there is no vectorized solution available, 2) performance is important, but not important enough to go through the hassle of cythonizing your code, and 3) you\\'re trying to perform elementwise transformation on your code. There is a good amount of evidence to suggest that list comprehensions are sufficiently fast (and even sometimes faster) for many common Pandas tasks.The formula is simple,If you can encapsulate your business logic into a function, you can use a list comprehension that calls it. You can make arbitrarily complex things work through the simplicity and speed of raw Python code.CaveatsList comprehensions assume that your data is easy to work with - what that means is your data types are consistent and you don\\'t have NaNs, but this cannot always be guaranteed.*Your mileage may vary for the reasons outlined in the Caveats section above.Let\\'s demonstrate the difference with a simple example of adding two pandas columns A + B. This is a vectorizable operation, so it will be easy to contrast the performance of the methods discussed above.Benchmarking code, for your reference. The line at the bottom measures a function written in numpandas, a style of Pandas that mixes heavily with NumPy to squeeze out maximum performance. Writing numpandas code should be avoided unless you know what you\\'re doing. Stick to the API where you can (i.e., prefer vec over vec_numpy).I should mention, however, that it isn\\'t always this cut and dry. Sometimes the answer to \"what is the best method for an operation\" is \"it depends on your data\". My advice is to test out different approaches on your data before settling on one.Most of the analyses performed on the various alternatives to the iter family has been through the lens of performance. However, in most situations you will typically be working on a reasonably sized dataset (nothing beyond a few thousand or 100K rows) and performance will come second to simplicity/readability of the solution.Here is my personal preference when selecting a method to use for a problem.For the novice:Vectorization (when possible); apply(); List Comprehensions; itertuples()/iteritems(); iterrows(); CythonFor the more experienced:Vectorization (when possible); apply(); List Comprehensions; Cython; itertuples()/iteritems(); iterrows()Vectorization prevails as the most idiomatic method for any problem that can be vectorized. Always seek to vectorize! When in doubt, consult the docs, or look on Stack Overflow for an existing question on your particular task.I do tend to go on about how bad apply is in a lot of my posts, but I do concede it is easier for a beginner to wrap their head around what it\\'s doing. Additionally, there are quite a few use cases for apply has explained in this post of mine.Cython ranks lower down on the list because it takes more time and effort to pull off correctly. You will usually never need to write code with pandas that demands this level of performance that even a list comprehension cannot satisfy.* As with any personal opinion, please take with heaps of salt!10 Minutes to pandas, and Essential Basic Functionality - Useful links that introduce you to Pandas and its library of vectorized*/cythonized functions.Enhancing Performance - A primer from the documentation on enhancing standard Pandas operationsAre for-loops in pandas really bad? When should I care? - a detailed write-up by me on list comprehensions and their suitability for various operations (mainly ones involving non-numeric data)When should I (not) want to use pandas apply() in my code? - apply is slow (but not as slow as the iter* family. There are, however, situations where one can (or should) consider apply as a serious alternative, especially in some GroupBy operations).* Pandas string methods are \"vectorized\" in the sense that they are specified on the series but operate on each element. The underlying mechanisms are still iterative, because string operations are inherently hard to vectorize.A common trend I notice from new users is to ask questions of the form \"How can I iterate over my df to do X?\". Showing code that calls iterrows() while doing something inside a for loop. Here is why. A new user to the library who has not been introduced to the concept of vectorization will likely envision the code that solves their problem as iterating over their data to do something. Not knowing how to iterate over a DataFrame, the first thing they do is Google it and end up here, at this question. They then see the accepted answer telling them how to, and they close their eyes and run this code without ever first questioning if iteration is the right thing to do.The aim of this answer is to help new users understand that iteration is not necessarily the solution to every problem, and that better, faster and more idiomatic solutions could exist, and that it is worth investing time in exploring them. I\\'m not trying to start a war of iteration vs. vectorization, but I want new users to be informed when developing solutions to their problems with this library.', 'First consider if you really need to iterate over rows in a DataFrame. See this answer for alternatives.If you still need to iterate over rows, you can use methods below. Note some  important caveats which are not mentioned in any of the other answers.DataFrame.iterrows()DataFrame.itertuples()itertuples() is supposed to be faster than iterrows()But be aware, according to the docs (pandas 0.24.2 at the moment):Because iterrows returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). To preserve dtypes while iterating over the rows, it is better to use itertuples() which returns namedtuples of the values and which is generally much faster than iterrows()You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect.Use DataFrame.apply() instead:The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. With a large number of columns (>255), regular tuples are returned.See pandas docs on iteration for more details.', 'You should use df.iterrows(). Though iterating row-by-row is not especially efficient since Series objects have to be created.', 'While iterrows() is a good option, sometimes itertuples() can be much faster:', 'You can use the df.iloc function as follows:', 'You can also use df.apply() to iterate over rows and access multiple columns for a function.docs: DataFrame.apply()', \"If you really have to iterate a Pandas dataframe, you will probably want to avoid using iterrows(). There are different methods and the usual iterrows() is far from being the best. itertuples() can be 100 times faster.In short:Generate a random dataframe with a million rows and 4 columns:1) The usual iterrows() is convenient, but damn slow:2) The default itertuples() is already much faster, but it doesn't work with column names such as My Col-Name is very Strange (you should avoid this method if your columns are repeated or if a column name cannot be simply converted to a Python variable name).:3) The default itertuples() using name=None is even faster but not really convenient as you have to define a variable per column.4) Finally, the named itertuples() is slower than the previous point, but you do not have to define a variable per column and it works with column names such as My Col-Name is very Strange.Output:This article is a very interesting comparison between iterrows and itertuples\", 'I was looking for How to iterate on rows and columns and ended here so:', 'We have multiple options to do the same, and lots of folks have shared their answers.I found the below two methods easy and efficient to do:Example:Note: itertuples() is supposed to be faster than iterrows()', \"You can write your own iterator that implements namedtupleThis is directly comparable to pd.DataFrame.itertuples.  I'm aiming at performing the same task with more efficiency.For the given dataframe with my function:Or with pd.DataFrame.itertuples:A comprehensive test\\nWe test making all columns available and subsetting the columns.\", 'To loop all rows in a dataframe you can use:', '', \"Update: cs95 has updated his answer to include plain numpy vectorization. You can simply refer to his answer.cs95 shows that Pandas vectorization far outperforms other Pandas methods for computing stuff with dataframes.I wanted to add that if you first convert the dataframe to a NumPy array and then use vectorization, it's even faster than Pandas dataframe vectorization, (and that includes the time to turn it back into a dataframe series).If you add the following functions to cs95's benchmark code, this becomes pretty evident:\", 'Sometimes a useful pattern is:Which results in:', 'To loop all rows in a dataframe and use values of each row conveniently, namedtuples can be converted to ndarrays. For example:Iterating over the rows:results in:Please note that if index=True, the index is added as the first element of the tuple, which may be undesirable for some applications.', 'In short', \"There is a way to iterate throw rows while getting a DataFrame in return, and not a Series. I don't see anyone mentioning that you can pass index as a list for the row to be returned as a DataFrame:Note the usage of double brackets. This returns a DataFrame with a single row.\", \"For both viewing and modifying values, I would use iterrows(). In a for loop and by using tuple unpacking (see the example: i, row), I use the row for only viewing the value and use i with the loc method when I want to modify values. As stated in previous answers, here you should not modify something you are iterating over.Here the row in the loop is a copy of that row, and not a view of it. Therefore, you should NOT write something like row['A'] = 'New_Value', it will not modify the DataFrame. However, you can use i and loc and specify the DataFrame to do the work.\", 'There are so many ways to iterate over the rows in Pandas dataframe. One very simple and intuitive way is:', 'The easiest way, use the apply function', 'As many answers here correctly point out, your default plan in Pandas should be to write vectorized code (with its implicit loops) rather than attempting an explicit loop yourself.  But the question remains whether you should ever write loops in Pandas, and if so what\\'s the best way to loop in those situations.I believe there is at least one general situation where loops are appropriate: when you need to calculate some function that depends on values in other rows in a somewhat complex manner.  In this case, the looping code is often simpler, more readable, and less error prone than vectorized code.The looping code might even be faster too, as you\\'ll see below, so loops might make sense in cases where speed is of utmost importance. But really, those are just going to be subsets of cases where you probably should have been working in numpy/numba (rather than Pandas) to begin with, because optimized numpy/numba will almost always be faster than Pandas.Let\\'s show this with an example.  Suppose you want to take a cumulative sum of a column, but reset it whenever some other column equals zero:This is a good example where you could certainly write one line of Pandas to achieve this, although it\\'s not especially readable, especially if you aren\\'t fairly experienced with Pandas already:That\\'s going to be fast enough for most situations, although you could also write faster code by avoiding the groupby, but it will likely be even less readable.Alternatively, what if we write this as a loop?  You could do something like the following with NumPy:Admittedly, there\\'s a bit of overhead there required to convert DataFrame columns to NumPy arrays, but the core piece of code is just one line of code that you could read even if you didn\\'t know anything about Pandas or NumPy:And this code is actually faster than the vectorized code.  In some quick tests with 100,000 rows, the above is about 10x faster than the groupby approach.  Note that one key to the speed there is numba, which is optional.  Without the \"@nb.jit\" line, the looping code is actually about 10x slower than the groupby approach.Clearly this example is simple enough that you would likely prefer the one line of pandas to writing a loop with its associated overhead.  However, there are more complex versions of this problem for which the readability or speed of the NumPy/numba loop approach likely makes sense.', \"You can also do NumPy indexing for even greater speed ups. It's not really iterating but works much better than iteration for certain applications.You may also want to cast it to an array. These indexes/selections are supposed to act like NumPy arrays already, but I ran into issues and needed to cast\", 'df.iterrows() returns tuple(a, b) where a is the index and b is the row.', 'Probably the most elegant solution (but certainly not the most efficient):Note that:Still, I think this option should be included here, as a straightforward solution to a (one should think) trivial problem.', 'This example uses iloc to isolate each digit in the data frame.', 'Disclaimer: Although here are so many answers which recommend not using an iterative (loop) approach (and I mostly agree), I would still see it as a reasonable approach for the following situation:Let\\'s say you have a large dataframe which contains incomplete user data. Now you have to extend this data with additional columns, for example, the user\\'s age and gender.Both values have to be fetched from a backend API. I\\'m assuming the API doesn\\'t provide a \"batch\" endpoint (which would accept multiple user IDs at once). Otherwise, you should rather call the API only once.The costs (waiting time) for the network request surpass the iteration of the dataframe by far. We\\'re talking about network round trip times of hundreds of milliseconds compared to the negligibly small gains in using alternative approaches to iterations.So in this case, I would absolutely prefer using an iterative approach. Although the network request is expensive, it is guaranteed being triggered only once for each row in the dataframe. Here is an example using DataFrame.iterrows:', \"Some libraries (e.g. a Java interop library that I use) require values to be passed in a row at a time, for example, if streaming data. To replicate the streaming nature, I 'stream' my dataframe values one by one, I wrote the below, which comes in handy from time to time.Which can be used:And preserves the values/ name mapping for the rows being iterated. Obviously, is a lot slower than using apply and Cython as indicated above, but is necessary in some circumstances.\", \"As the accepted answer states, the fastest way to apply a function over rows is to use a vectorized function, the so-called NumPy ufuncs (universal functions).But what should you do when the function you want to apply isn't already implemented in NumPy?Well, using the vectorize decorator from numba, you can easily create ufuncs directly in Python like this:The documentation for this function is here: Creating NumPy universal functions\", 'Along with the great answers in this post I am going to propose Divide and Conquer approach, I am not writing this answer to abolish the other great answers but to fulfill them with another approach which was working efficiently for me. It has two steps of splitting and merging the pandas dataframe:PROS of Divide and Conquer:CONS of Divide and Conquer:===================    Divide and Conquer Approach    =================Step 1: Splitting/SlicingIn this step, we are going to divide the iteration over the entire dataframe. Think that you are going to read a CSV file into pandas df then iterate over it. In may case I have 5,000,000 records and I am going to split it into 100,000 records.NOTE: I need to reiterate as other runtime analysis explained in the other solutions in this page, \"number of records\" has exponential proportion of \"runtime\" on search on the df. Based on the benchmark on my data here are the results:Step 2: MergingThis is going to be an easy step, just merge all the written CSV files into one dataframe and write it into a bigger CSV file.Here is the sample code:Reference:Efficient way of iteration over datafreameConcatenate CSV files into one Pandas Dataframe']",
            "url": "https://stackoverflow.com/questions/16476924"
        },
        {
            "tag": "python",
            "question": [
                "How do I get the current time?"
            ],
            "votes": "3655",
            "answer": "['Use datetime:For just the clock time without the date:To save typing, you can import the datetime object from the datetime module:Then remove the prefix datetime. from all of the above.', 'Use time.strftime():', \"Example output: '2013-09-18 11:16:32'See list of strftime directives.\", \"Similar to Harley's answer, but use the str() function for a quick-n-dirty, slightly more human readable format:\", 'The time module provides functions that tell us the time in \"seconds since the epoch\" as well as other utilities.This is the format you should get timestamps in for saving in databases. It is a simple floating-point number that can be converted to an integer. It is also good for arithmetic in seconds, as it represents the number of seconds since Jan 1, 1970, 00:00:00, and it is memory light relative to the other representations of time we\\'ll be looking at next:This timestamp does not account for leap-seconds, so it\\'s not linear - leap seconds are ignored. So while it is not equivalent to the international UTC standard, it is close, and therefore quite good for most cases of record-keeping.This is not ideal for human scheduling, however. If you have a future event you wish to take place at a certain point in time, you\\'ll want to store that time with a string that can be parsed into a datetime object or a serialized datetime object (these will be described later).You can also represent the current time in the way preferred by your operating system (which means it can change when you change your system preferences, so don\\'t rely on this to be standard across all systems, as I\\'ve seen others expect). This is typically user friendly, but doesn\\'t typically result in strings one can sort chronologically:You can hydrate timestamps into human readable form with ctime as well:This conversion is also not good for record-keeping (except in text that will only be parsed by humans - and with improved Optical Character Recognition and Artificial Intelligence, I think the number of these cases will diminish).The datetime module is also quite useful here:The datetime.now is a class method that returns the current time. It uses the time.localtime without the timezone info (if not given, otherwise see timezone aware below). It has a representation (which would allow you to recreate an equivalent object) echoed on the shell, but when printed (or coerced to a str), it is in human readable (and nearly ISO) format, and the lexicographic sort is equivalent to the chronological sort:You can get a datetime object in UTC time, a global standard, by doing this:UTC is a time standard that is nearly equivalent to the GMT timezone. (While GMT and UTC do not change for Daylight Savings Time, their users may switch to other timezones, like British Summer Time, during the Summer.)However, none of the datetime objects we\\'ve created so far can be easily converted to various timezones. We can solve that problem with the pytz module:Equivalently, in Python 3 we have the timezone class with a utc timezone instance attached, which also makes the object timezone aware (but to convert to another timezone without the handy pytz module is left as an exercise to the reader):And we see we can easily convert to timezones from the original UTC object.You can also make a naive datetime object aware with the pytz timezone localize method, or by replacing the tzinfo attribute (with replace, this is done blindly), but these are more last resorts than best practices:The pytz module allows us to make our datetime objects timezone aware and convert the times to the hundreds of timezones available in the pytz module.One could ostensibly serialize this object for UTC time and store that in a database, but it would require far more memory and be more prone to error than simply storing the Unix Epoch time, which I demonstrated first.The other ways of viewing times are much more error-prone, especially when dealing with data that may come from different time zones. You want there to be no confusion as to which timezone a string or serialized datetime object was intended for.If you\\'re displaying the time with Python for the user, ctime works nicely, not in a table (it doesn\\'t typically sort well), but perhaps in a clock. However, I personally recommend, when dealing with time in Python, either using Unix time, or a timezone aware UTC datetime object.', 'DoThere is some difference for Unix and Windows platforms.', 'That outputs the current GMT in the specified format. There is also a localtime() method.This page has more details.', 'The previous answers are all good suggestions, but I find it easiest to use ctime():This gives a nicely formatted string representation of the current local time.', 'The quickest way is:', 'If you need current time as a time object:', 'You can use the time module:The use of the capital Y gives the full year, and using y would give 06/02/15.You could also use the following code to give a more lengthy time:', \".isoformat() is in the documentation, but not yet here\\n(this is mighty similar to @Ray Vega's answer):\", 'Why not ask the U.S. Naval Observatory, the official timekeeper of the United States Navy?If you live in the D.C. area (like me) the latency might not be too bad...', 'Using pandas to get the current time, kind of overkilling the problem at hand:Output:', 'if you are using numpy already then directly you can use numpy.datetime64() \\nfunction.for only date:or, if you are using pandas already then you can use pandas.to_datetime() functionor,', 'This is what I ended up going with:Also, this table is a necessary reference for choosing the appropriate format codes to get the date formatted just the way you want it (from Python \"datetime\" documentation here).', 'datetime.now() returns the current time as a naive datetime object that represents time in the local timezone. That value may be ambiguous e.g., during DST transitions (\"fall back\"). To avoid ambiguity either UTC timezone should be used:Or a timezone-aware object that has the corresponding timezone info attached (Python 3.2+):', 'Do dir(date) or any variables including the package. You can get all the attributes and methods associated with the variable.', '', \"This question doesn't need a new answer just for the sake of it ... a shiny new-ish toy/module, however, is enough justification.  That being the Pendulum library, which appears to do the sort of things which arrow attempted, except without the inherent flaws and bugs which beset arrow.For instance, the answer to the original question:There's a lot of standards which need addressing, including multiple RFCs and ISOs, to worry about.  Ever get them mixed up; not to worry, take a little look into dir(pendulum.constants) There's a bit more than RFC and ISO formats there, though.When we say local, though what do we mean?  Well I mean:Presumably most of the rest of you mean somewhere else.And on it goes.  Long story short: Pendulum attempts to do for date and time what requests did for HTTP.  It's worth consideration, particularly for both its ease of use and extensive documentation.\", 'By default, now() function returns output in the YYYY-MM-DD HH:MM:SS:MS format. Use the below sample script to get the current date and time in a Python script and print results on the screen. Create file getDateTime1.py with the below content.The output looks like below:', 'Try the arrow module from http://crsmithdev.com/arrow/:Or the UTC version:To change its output, add .format():For a specific timezone:An hour ago:Or if you want the gist.', 'Current time of a timezone', 'To get exactly 3 decimal points for milliseconds 11:34:23.751 run this:More context:I want to get the time with milliseconds. A simple way to get them:But I want only milliseconds, right? The shortest way to get them:Add or remove zeroes from the last multiplication to adjust number of decimal points, or just:', 'If you just want the current timestamp in ms (for example, to measure execution time), you can also use the \"timeit\" module:', \"You can use this function to get the time (unfortunately it doesn't say AM or PM):To get the hours, minutes, seconds and milliseconds to merge later, you can use these functions:Hour:Minute:Second:Millisecond:\", 'You can  try the followingor', \"Because no one has mentioned it yet, and this is something I ran into recently... a pytz timezone's fromutc() method combined with datetime's utcnow() is the best way I've found to get a useful current time (and date) in any timezone.If all you want is the time, you can then get that with local_time.time().\", '', \"Method1: Getting Current Date and Time from system datetimeThe datetime module supplies classes for manipulating dates and times.CodeOutput will be likeMethod2: Getting Current Date and Time if Network is availableurllib package helps us to handle the url's that means webpages. Here we collects data from the webpage http://just-the-time.appspot.com/ and parses dateime from the webpage using the package dateparser.CodeOutput will be likeMethod3: Getting Current Date and Time from Local Time of the MachinePython's time module provides a function for getting local time from the number of seconds elapsed since the epoch called localtime(). ctime() function takes seconds passed since epoch as an argument and returns a string representing local time.CodeOutput will be like\"]",
            "url": "https://stackoverflow.com/questions/415511"
        },
        {
            "tag": "python",
            "question": [
                "Catch multiple exceptions in one line (except block)"
            ],
            "votes": "3633",
            "answer": "['From Python Documentation:An except clause may name multiple exceptions as a parenthesized tuple, for exampleOr, for Python 2 only:Separating the exception from the variable with a comma will still work in Python 2.6 and 2.7, but is now deprecated and does not work in Python 3; now you should be using as.', \"Do this:The parentheses are required due to older syntax that used the commas to assign the error object to a name. The as keyword is used for the assignment. You can use any name for the error object, I prefer error personally.To do this in a manner currently and forward compatible with Python, you need to separate the Exceptions with commas and wrap them with parentheses to differentiate from earlier syntax that assigned the exception instance to a variable name by following the Exception type to be caught with a comma.Here's an example of simple usage:I'm specifying only these exceptions to avoid hiding bugs, which if I encounter I expect the full stack trace from.This is documented here: https://docs.python.org/tutorial/errors.htmlYou can assign the exception to a variable, (e is common, but you might prefer a more verbose variable if you have long exception handling or your IDE only highlights selections larger than that, as mine does.) The instance has an args attribute. Here is an example:Note that in Python 3, the err object falls out of scope when the except block is concluded.You may see code that assigns the error with a comma. This usage, the only form available in Python 2.5 and earlier, is deprecated, and if you wish your code to be forward compatible in Python 3, you should update the syntax to use the new form:If you see the comma name assignment in your codebase, and you're using Python 2.5 or higher, switch to the new way of doing it so your code remains compatible when you upgrade.The accepted answer is really 4 lines of code, minimum:The try, except, pass lines can be handled in a single line with the suppress context manager, available in Python 3.4:So when you want to pass on certain exceptions, use suppress.\", 'From Python documentation -> 8.3 Handling Exceptions:A try statement may have more than one except clause, to specify\\n  handlers for different exceptions. At most one handler will be\\n  executed. Handlers only handle exceptions that occur in the\\n  corresponding try clause, not in other handlers of the same try\\n  statement. An except clause may name multiple exceptions as a\\n  parenthesized tuple, for example:Note that the parentheses around this tuple are required, because\\n  except ValueError, e: was the syntax used for what is normally\\n  written as except ValueError as e: in modern Python (described\\n  below). The old syntax is still supported for backwards compatibility.\\n  This means except RuntimeError, TypeError is not equivalent to\\n  except (RuntimeError, TypeError): but to except RuntimeError as\\nTypeError: which is not what you want.', \"If you frequently use a large number of exceptions, you can pre-define a tuple, so you don't have to re-type them many times.NOTES:If you, also, need to catch other exceptions than those in the\\npre-defined tuple, you will need to define another except block.If you just cannot tolerate a global variable, define it in main()\\nand pass it around where needed...\", \"One of the way to do this is..and another way is to create method which performs task executed by except block and call it through all of the except block that you write..I know that second one is not the best way to do this, but i'm just showing number of ways to do this thing.\", 'As of Python 3.11 you can take advantage of the except* clause that is used to handle multiple exceptions.PEP-654 introduced a new standard exception type called ExceptionGroup that corresponds to a group of exceptions that are being propagated together. The ExceptionGroup can be handled using a new except* syntax. The * symbol indicates that multiple exceptions can be handled by each except* clause.For example, you can handle multiple exceptionsFor more details see PEP-654.']",
            "url": "https://stackoverflow.com/questions/6470428"
        },
        {
            "tag": "python",
            "question": [
                "Does Python have a string 'contains' substring method?"
            ],
            "votes": "3588",
            "answer": "['Use the in operator:', 'If it\\'s just a substring search you can use string.find(\"substring\").You do have to be a little careful with find, index, and in though, as they are substring searches. In other words, this:It would print Found \\'is\\' in the string. Similarly, if \"is\" in s: would evaluate to True. This may or may not be what you want.', '99% of use cases will be covered using the keyword, in, which returns True or False:For the use case of getting the index, use str.find (which returns -1 on failure, and has optional positional arguments):or str.index (like find but raises ValueError on failure):Use the in comparison operator becauseThe opposite (complement), which the original question asked for, is not in:This is semantically the same as not \\'foo\\' in \\'**foo**\\' but it\\'s much more readable and explicitly provided for in the language as a readability improvement.The \"contains\" method implements the behavior for in. This example,returns True. You could also call this function from the instance of the superstring:But don\\'t. Methods that start with underscores are considered semantically non-public. The only reason to use this is when implementing or extending the in and not in functionality (e.g. if subclassing str):and now:Don\\'t use the following string methods to test for \"contains\":Other languages may have no methods to directly test for substrings, and so you would have to use these types of methods, but with Python, it is much more efficient to use the in comparison operator.Also, these are not drop-in replacements for in. You may have to handle the exception or -1 cases, and if they return 0 (because they found the substring at the beginning) the boolean interpretation is False instead of True.If you really mean not any_string.startswith(substring) then say it.We can compare various ways of accomplishing the same goal.And now we see that using in is much faster than the others.\\nLess time to do an equivalent operation is better:This is a fine follow-on question.Let\\'s disassemble functions with the methods of interest:so we see that the .__contains__ method has to be separately looked up and then called from the Python virtual machine - this should adequately explain the difference.', \"if needle in haystack: is the normal use, as @Michael says -- it relies on the in operator, more readable and faster than a method call.If you truly need a method instead of an operator (e.g. to do some weird key= for a very peculiar sort...?), that would be 'haystack'.__contains__.  But since your example is for use in an if, I guess you don't really mean what you say;-).  It's not good form (nor readable, nor efficient) to use special methods directly -- they're meant to be used, instead, through the operators and builtins that delegate to them.\", 'Here are a few useful examples that speak for themselves concerning the in method:Caveat. Lists are iterables, and the in method acts on iterables, not just strings.If you want to compare strings in a more fuzzy way to measure how \"alike\" they are, consider using the Levenshtein packageHere\\'s an answer that shows how it works.', 'If you are happy with \"blah\" in somestring but want it to be a function/method call, you can probably do thisAll operators in Python can be more or less found in the operator module including in.', 'So apparently there is nothing similar for vector-wise comparison. An obvious Python way to do so would be:', 'You can use y.count().It will return the integer value of the number of times a sub string appears in a string.For example:', 'Here is your answer:For checking if it is false:OR:', 'You can use regular expressions to get the occurrences:']",
            "url": "https://stackoverflow.com/questions/3437059"
        },
        {
            "tag": "python",
            "question": [
                "What is the difference between __str__ and __repr__?"
            ],
            "votes": "3530",
            "answer": "['Alex summarized well but, surprisingly, was too succinct.First, let me reiterate the main points in Alex\u2019s post:Default implementation is uselessThis is mostly a surprise because Python\u2019s defaults tend to be fairly useful. However, in this case, having a default for __repr__ which would act like:would have been too dangerous (for example, too easy to get into infinite recursion if objects reference each other). So Python cops out. Note that there is one default which is true: if __repr__ is defined, and __str__ is not, the object will behave as though __str__=__repr__.This means, in simple terms: almost every object you implement should have a functional __repr__ that\u2019s usable for understanding the object. Implementing __str__ is optional: do that if you need a \u201cpretty print\u201d functionality (for example, used by a report generator).The goal of __repr__ is to be unambiguousLet me come right out and say it \u2014 I do not believe in debuggers. I don\u2019t really know how to use any debugger, and have never used one seriously. Furthermore, I believe that the big fault in debuggers is their basic nature \u2014 most failures I debug happened a long long time ago, in a galaxy far far away. This means that I do believe, with religious fervor, in logging. Logging is the lifeblood of any decent fire-and-forget server system. Python makes it easy to log: with maybe some project specific wrappers, all you need is aBut you have to do the last step \u2014 make sure every object you implement has a useful repr, so code like that can just work. This is why the \u201ceval\u201d thing comes up: if you have enough information so eval(repr(c))==c, that means you know everything there is to know about c. If that\u2019s easy enough, at least in a fuzzy way, do it. If not, make sure you have enough information about c anyway. I usually use an eval-like format: \"MyClass(this=%r,that=%r)\" % (self.this,self.that). It does not mean that you can actually construct MyClass, or that those are the right constructor arguments \u2014 but it is a useful form to express \u201cthis is everything you need to know about this instance\u201d.Note: I used %r above, not %s. You always want to use repr() [or %r formatting character, equivalently] inside __repr__ implementation, or you\u2019re defeating the goal of repr. You want to be able to differentiate MyClass(3) and MyClass(\"3\").The goal of __str__ is to be readableSpecifically, it is not intended to be unambiguous \u2014 notice that str(3)==str(\"3\"). Likewise, if you implement an IP abstraction, having the str of it look like 192.168.1.1 is just fine. When implementing a date/time abstraction, the str can be \"2010/4/12 15:35:22\", etc. The goal is to represent it in a way that a user, not a programmer, would want to read it. Chop off useless digits, pretend to be some other class \u2014 as long is it supports readability, it is an improvement.Container\u2019s __str__ uses contained objects\u2019 __repr__This seems surprising, doesn\u2019t it? It is a little, but how readable would it be if it used their __str__?Not very. Specifically, the strings in a container would find it way too easy to disturb its string representation. In the face of ambiguity, remember, Python resists the temptation to guess. If you want the above behavior when you\u2019re printing a list, just(you can probably also figure out what to do about dictionaries.SummaryImplement __repr__ for any class you implement. This should be second nature. Implement __str__ if you think it would be useful to have a string version which errs on the side of readability.', 'My rule of thumb:  __repr__ is for developers, __str__ is for customers.', \"Unless you specifically act to ensure otherwise, most classes don't have helpful results for either:As you see -- no difference, and no info beyond the class and object's id.  If you only override one of the two...:as you see, if you override __repr__, that's ALSO used for __str__, but not vice versa.Other crucial tidbits to know: __str__ on a built-on container uses the __repr__, NOT the __str__, for the items it contains. And, despite the words on the subject found in typical docs, hardly anybody bothers making the __repr__ of objects be a string that eval may use to build an equal object (it's just too hard, AND not knowing how the relevant module was actually imported makes it actually flat out impossible).So, my advice: focus on making __str__ reasonably human-readable, and __repr__ as unambiguous as you possibly can, even if that interferes with the fuzzy unattainable goal of making __repr__'s returned value acceptable as input to __eval__!\", '__repr__: representation of python object usually eval will convert it back to that object__str__: is whatever you think is that object in text forme.g.', \"In short, the goal of __repr__ is to be unambiguous and __str__ is to be\\n  readable.Here is a good example:Read this documentation for repr:repr(object)Return a string containing a printable representation of an object. This is the same value yielded by conversions (reverse\\n  quotes). It is sometimes useful to be able to access this operation as\\n  an ordinary function. For many types, this function makes an attempt\\n  to return a string that would yield an object with the same value when\\n  passed to eval(), otherwise the representation is a string enclosed in\\n  angle brackets that contains the name of the type of the object\\n  together with additional information often including the name and\\n  address of the object. A class can control what this function returns\\n  for its instances by defining a __repr__() method.Here is the documentation for str:str(object='')Return a string containing a nicely printable\\n  representation of an object. For strings, this returns the string\\n  itself. The difference with repr(object) is that str(object) does not\\n  always attempt to return a string that is acceptable to eval(); its\\n  goal is to return a printable string. If no argument is given, returns\\n  the empty string, ''.\", '__str__ (read as \"dunder (double-underscore) string\") and __repr__ (read as \"dunder-repper\" (for \"representation\")) are both special methods that return strings based on the state of the object.__repr__ provides backup behavior if __str__ is missing.So one should first write a __repr__ that allows you to reinstantiate an equivalent object from the string it returns e.g. using eval or by typing it in character-for-character in a Python shell.At any time later, one can write a __str__ for a user-readable string representation of the instance, when one believes it to be necessary.If you print an object, or pass it to format, str.format, or str, then if a __str__ method is defined, that method will be called, otherwise, __repr__ will be used.The __repr__ method is called by the builtin function repr and is what is echoed on your python shell when it evaluates an expression that returns an object.Since it provides a backup for __str__, if you can only write one, start with __repr__Here\\'s the builtin help on repr:That is, for most objects, if you type in what is printed by repr, you should be able to create an equivalent object. But this is not the default implementation.The default object __repr__ is (C Python source) something like:That means by default you\\'ll print the module the object is from, the class name, and the hexadecimal representation of its location in memory - for example:This information isn\\'t very useful, but there\\'s no way to derive how one might accurately create a canonical representation of any given instance, and it\\'s better than nothing, at least telling us how we might uniquely identify it in memory.Let\\'s look at how useful it can be, using the Python shell and datetime objects. First we need to import the datetime module:If we call datetime.now in the shell, we\\'ll see everything we need to recreate an equivalent datetime object. This is created by the datetime __repr__:If we print a datetime object, we see a nice human readable (in fact, ISO) format. This is implemented by datetime\\'s __str__:It is a simple matter to recreate the object we lost because we didn\\'t assign it to a variable by copying and pasting from the __repr__ output, and then printing it, and we get it in the same human readable output as the other object:#How do I implement them?As you\\'re developing, you\\'ll want to be able to reproduce objects in the same state, if possible. This, for example, is how the datetime object defines __repr__ (Python source). It is fairly complex, because of all of the attributes needed to reproduce such an object:If you want your object to have a more human readable representation, you can implement __str__ next. Here\\'s how the datetime object (Python source) implements __str__, which it easily does because it already has a function to display it in ISO format:This is a critique of another answer here that suggests setting __repr__ = __str__.Setting __repr__ = __str__ is silly - __repr__ is a fallback for __str__ and a __repr__, written for developers usage in debugging, should be written before you write a __str__.You need a __str__ only when you need a textual representation of the object.Define __repr__ for objects you write so you and other developers have a reproducible example when using it as you develop. Define __str__ when you need a human readable string representation of it.', \"On page 358 of the book Python scripting for computational science by Hans Petter Langtangen, it clearly states thatSo, I prefer to understand them asfrom the user's point of view\\nalthough this is a misunderstanding I made when learning python.A small but good example is also given on the same page as follows:\", \"Apart from all the answers given, I would like to add few points :-1) __repr__() is invoked when you simply write object's name on interactive python console and press enter.2) __str__() is invoked when you use object with print statement.3) In case, if __str__ is missing, then print and any function using str() invokes __repr__() of object.4) __str__() of containers, when invoked will execute __repr__() method of its contained elements.5) str() called within __str__() could potentially recurse without a base case, and error on maximum recursion depth.6) __repr__() can call repr() which will attempt to avoid infinite recursion automatically, replacing an already represented object with ....\", '(2020 entry)Q: What\\'s the difference between __str__() and __repr__()?TL;DR:LONGThis question has been around a long time, and there are a variety of answers of which most are correct (not to mention from several Python community legends[!]). However when it comes down to the nitty-gritty, this question is analogous to asking the difference between the str() and repr() built-in functions. I\\'m going to describe the differences in my own words (which means I may be \"borrowing\" liberally from Core Python Programming so pls forgive me).Both str() and repr() have the same basic job: their goal is to return a string representation of a Python object. What kind of string representation is what differentiates them.For example, let\\'s assign a string to x and an int to y, and simply showing human-readable string versions of each:Can we take what is inside the quotes in both cases and enter them verbatim into the Python interpreter? Let\\'s give it a try:Clearly you can for an int but not necessarily for a str. Similarly, while I can pass \\'123\\' to eval(), that doesn\\'t work for \\'foo\\':So this tells you the Python shell just eval()s what you give it. Got it? Now, let\\'s repr() both expressions and see what we get. More specifically, take its output and dump those out in the interpreter (there\\'s a point to this which we\\'ll address afterwards):Wow, they both work? That\\'s because \\'foo\\', while a printable string representation of that string, it\\'s not evaluatable, but \"\\'foo\\'\" is. 123 is a valid Python int called by either str() or repr(). What happens when we call eval() with these?It works because 123 and \\'foo\\' are valid Python objects. Another key takeaway is that while sometimes both return the same thing (the same string representation), that\\'s not always the case. (And yes, yes, I can go create a variable foo where the eval() works, but that\\'s not the point.)More factoids about both pairs', \"To put it simply:__str__ is used in to show a string representation of your object to be read easily by others.__repr__ is used to show a string representation of the object.Let's say I want to create a Fraction class where the string representation of a fraction is '(1/2)' and the object (Fraction class) is to be represented as 'Fraction (1,2)'So we can create a simple Fraction class:\", 'From an (An Unofficial) Python Reference Wiki (archive copy) by effbot:__str__ \"computes the \"informal\" string representation of an object. This differs from __repr__ in that it does not have to be a valid Python expression: a more convenient or concise representation may be used instead.\"', \"In all honesty, eval(repr(obj)) is never used. If you find yourself using it, you should stop, because eval is dangerous, and strings are a very inefficient way to serialize your objects (use pickle instead).Therefore, I would recommend setting __repr__ = __str__. The reason is that str(list) calls repr on the elements (I consider this to be one of the biggest design flaws of Python that was not addressed by Python 3). An actual repr will probably not be very helpful as the output of print([your, objects]).To qualify this, in my experience, the most useful use case of the repr function is to put a string inside another string (using string formatting). This way, you don't have to worry about escaping quotes or anything. But note that there is no eval happening here.\", 'str - Creates a new string object from the given object.repr - Returns the canonical string representation of the object.The differences:str():repr():', \"One aspect that is missing in other answers. It's true that in general the pattern is:Unfortunately, this differentiation is flawed, because the Python REPL and also IPython use __repr__ for printing objects in a REPL console (see related questions for Python and IPython). Thus, projects which are targeted for interactive console work (e.g., Numpy or Pandas) have started to ignore above rules and provide a human-readable __repr__ implementation instead.\", 'From the book Fluent Python:A basic requirement for a Python object is to provide usable \\n       string   representations of itself, one used for debugging and\\n       logging, another for presentation to end users. That is why the\\n       special methods __repr__ and __str__ exist in the data model.', '__str__ can be invoked on an object by calling str(obj) and should return a human readable string.__repr__ can be invoked on an object by calling repr(obj) and should return internal object (object fields/attributes)This example may help:', 'You can get some insight from this code:', 'Excellent answers already cover the difference between __str__ and __repr__, which for me boils down to the former being readable even by an end user, and the latter being as useful as possible to developers. Given that, I find that the default implementation of __repr__ often fails to achieve this goal because it omits information useful to developers.For this reason, if I have a simple enough __str__, I generally just try to get the best of both worlds with something like:', 'When print() is called on the result of decimal.Decimal(23) / decimal.Decimal(\"1.05\") the raw number is printed; this output is in string form which can be achieved with __str__(). If we simply enter the expression we get a decimal.Decimal output \u2014 this output is in representational form which can be achieved with __repr__(). All Python objects have two output forms. String form is designed to be human-readable. The representational form is designed to produce output that if fed to a Python interpreter would (when possible) reproduce the represented object.', 'One important thing to keep in mind is that container\\'s __str__ uses contained objects\\' __repr__.Python favors unambiguity over readability, the __str__ call of a tuple calls the contained objects\\' __repr__, the \"formal\" representation of an object. Although the formal representation is harder to read than an informal one, it is unambiguous and more robust against bugs.', 'In a nutshell:', 'Understand __str__ and __repr__ intuitively and permanently distinguish them at all.__str__ return the string disguised body of a given object for readable of eyes\\n__repr__ return the real flesh body of a given object (return itself) for unambiguity to identify.See it in an exampleAs to __repr__We can do arithmetic operation on __repr__ results conveniently.if apply the operation on __str__Returns nothing but error.Another example.Hope this help you build concrete grounds to explore more answers.', 'Source: https://www.journaldev.com/22460/python-str-repr-functions', '__repr__ is used everywhere, except by print and str methods (when a __str__is defined !)', 'Every object inherits __repr__  from the base class that all objects created.if you call repr(p) you will get this as default:But if you call str(p) you will get the same output. it is because when __str__ does not exist, Python calls __repr__Let\\'s implement our own __str__print(p) and str(p)will returnlet\\'s add __str__()if we call print(p) and str(p), it will call __str__() so it will returnrepr(p) will returnrepr called\\n\"Person(name=\\'ali, age=self.age\\')\"Let\\'s omit __repr__ and just implement __str__.print(p) will look for the __str__ and will return:NOTE= if we had __repr__ and __str__ defined, f\\'name is {p}\\' would call __str__', 'Programmers with prior experience in languages with a toString method tend to implement __str__ and not __repr__.\\nIf you only implement one of these special methods in Python, choose __repr__.From Fluent Python book, by Ramalho, Luciano.', 'Basically __str__ or str() is used for creating output that is human-readable are must be for end-users.\\nOn the other hand, repr() or __repr__ mainly returns canonical string representation of objects which serve the purpose of debugging and development helps the programmers.', 'repr() used when we debug or log.It is used for developers to understand code.\\none the other hand str() user for non developer like(QA) or user.']",
            "url": "https://stackoverflow.com/questions/1436703"
        },
        {
            "tag": "python",
            "question": [
                "Convert bytes to a string"
            ],
            "votes": "3527",
            "answer": "['Decode the bytes object to produce a string:The above example assumes that the bytes object is in UTF-8, because it is a common encoding. However, you should use the encoding your data is actually in!', 'Decode the byte string and turn it in to a character (Unicode) string.Python 3:orPython 2:or', 'This joins together a list of bytes into a string:', \"If you don't know the encoding, then to read binary input into string in Python 3 and Python 2 compatible way, use the ancient MS-DOS CP437 encoding:Because encoding is unknown, expect non-English symbols to translate to characters of cp437 (English characters are not translated, because they match in most single byte encodings and UTF-8).Decoding arbitrary binary input to UTF-8 is unsafe, because you may get this:The same applies to latin-1, which was popular (the default?) for Python 2. See the missing points in Codepage Layout - it is where Python chokes with infamous ordinal not in range.UPDATE 20150604: There are rumors that Python 3 has the surrogateescape error strategy for encoding stuff into binary data without data loss and crashes, but it needs conversion tests, [binary] -> [str] -> [binary], to validate both performance and reliability.UPDATE 20170116: Thanks to comment by Nearoo - there is also a possibility to slash escape all unknown bytes with backslashreplace error handler. That works only for Python 3, so even with this workaround you will still get inconsistent output from different Python versions:See Python\u2019s Unicode Support for details.UPDATE 20170119: I decided to implement slash escaping decode that works for both Python\\xa02 and Python\\xa03. It should be slower than the cp437 solution, but it should produce identical results on every Python version.\", 'In Python 3, the default encoding is \"utf-8\", so you can directly use:which is equivalent toOn the other hand, in Python 2, encoding defaults to the default string encoding. Thus, you should use:where encoding is the encoding you want.Note: support for keyword arguments was added in Python\\xa02.7.', \"I think you actually want this:Aaron's answer was correct, except that you need to know which encoding to use. And I believe that Windows uses 'windows-1252'. It will only matter if you have some unusual (non-ASCII) characters in your content, but then it will make a difference.By the way, the fact that it does matter is the reason that Python moved to using two different types for binary and text data: it can't convert magically between them, because it doesn't know the encoding unless you tell it! The only way YOU would know is to read the Windows documentation (or read it here).\", \"Since this question is actually asking about subprocess output, you have more direct approaches available. The most modern would be using subprocess.check_output and passing text=True (Python 3.7+) to automatically decode stdout using the system default coding:For Python 3.6, Popen accepts an encoding keyword:The general answer to the question in the title, if you're not dealing with subprocess output, is to decode bytes to text:With no argument, sys.getdefaultencoding() will be used.  If your data is not sys.getdefaultencoding(), then you must specify the encoding explicitly in the decode call:\", 'Set universal_newlines to True, i.e.', \"To interpret a byte sequence as a text, you have to know the\\ncorresponding character encoding:Example:ls command may produce output that can't be interpreted as text. File names\\non Unix may be any sequence of bytes except slash b'/' and zero\\nb'\\\\0':Trying to decode such byte soup using utf-8 encoding raises UnicodeDecodeError.It can be worse. The decoding may fail silently and produce mojibake\\nif you use a wrong incompatible encoding:The data is corrupted but your program remains unaware that a failure\\nhas occurred.In general, what character encoding to use is not embedded in the byte sequence itself. You have to communicate this info out-of-band. Some outcomes are more likely than others and therefore chardet module exists that can guess the character encoding. A single Python script may use multiple character encodings in different places.ls output can be converted to a Python string using os.fsdecode()\\nfunction that succeeds even for undecodable\\nfilenames (it uses\\nsys.getfilesystemencoding() and surrogateescape error handler on\\nUnix):To get the original bytes, you could use os.fsencode().If you pass universal_newlines=True parameter then subprocess uses\\nlocale.getpreferredencoding(False) to decode bytes e.g., it can be\\ncp1252 on Windows.To decode the byte stream on-the-fly,\\nio.TextIOWrapper()\\ncould be used: example.Different commands may use different character encodings for their\\noutput e.g., dir internal command (cmd) may use cp437. To decode its\\noutput, you could pass the encoding explicitly (Python 3.6+):The filenames may differ from os.listdir() (which uses Windows\\nUnicode API) e.g., '\\\\xb6' can be substituted with '\\\\x14'\u2014Python's\\ncp437 codec maps b'\\\\x14' to control character U+0014 instead of\\nU+00B6 (\u00b6). To support filenames with arbitrary Unicode characters, see  Decode PowerShell output possibly containing non-ASCII Unicode characters into a Python string\", 'While @Aaron Maenpaa\\'s answer just works, a user recently asked:Is there any more simply way? \\'fhand.read().decode(\"ASCII\")\\' [...] It\\'s so long!You can use:decode() has a standard argument:codecs.decode(obj, encoding=\\'utf-8\\', errors=\\'strict\\')', \"If you should get the following by trying decode():AttributeError: 'str' object has no attribute 'decode'You can also specify the encoding type straight in a cast:\", 'ororor', \"If you have had this error:utf-8 codec can't decode byte 0x8a,then it is better to use the following code to convert bytes to a string:\", 'For Python 3, this is a much safer and Pythonic approach to convert from byte to string:Output:', \"When working with data from Windows systems (with \\\\r\\\\n line endings), my answer isWhy? Try this with a multiline Input.txt:All your line endings will be doubled (to \\\\r\\\\r\\\\n), leading to extra empty lines. Python's text-read functions usually normalize line endings so that strings use only \\\\n. If you receive binary data from a Windows system, Python does not have a chance to do that. Thus,will replicate your original file.\", 'We can decode the bytes object to produce a string using bytes.decode(encoding=\\'utf-8\\', errors=\\'strict\\').\\nFor documentation see bytes.decode.Python 3 example:Output:Note: In Python 3, by default the encoding type is UTF-8. So, <byte_string>.decode(\"utf-8\") can be also written as <byte_string>.decode()', 'For your specific case of \"run a shell command and get its output as text instead of bytes\", on Python 3.7, you should use subprocess.run and pass in text=True (as well as capture_output=True to capture the output)text used to be called universal_newlines, and was changed (well, aliased) in Python 3.7. If you want to support Python versions before 3.7, pass in universal_newlines=True instead of text=True', \"From sys \u2014 System-specific parameters and functions:To write or read binary data from/to the standard streams, use the underlying binary buffer. For example, to write bytes to stdout, use sys.stdout.buffer.write(b'abc').\", 'Try this:', \"Decode with .decode(). This will decode the string. Pass in 'utf-8') as the value in the inside.\", '', 'If you want to convert any bytes, not just string converted to bytes:This is not very efficient, however. It will turn a 2 MB picture into 9 MB.', 'Try using this one; this function will ignore all the non-character sets (like UTF-8) binaries and return a clean string. It is tested for Python\\xa03.6 and above.Here, the function will take the binary and decode it (converts binary data to characters using the Python predefined character set and the ignore argument ignores all non-character set data from your binary and finally returns your desired string value.If you are not sure about the encoding, use sys.getdefaultencoding() to get the default encoding of your device.']",
            "url": "https://stackoverflow.com/questions/606191"
        },
        {
            "tag": "python",
            "question": [
                "How to copy files"
            ],
            "votes": "3494",
            "answer": "[\"shutil has many methods you can use. One of which is:Another shutil method to look at is shutil.copy2(). It's similar but preserves more metadata (e.g. time stamps).If you use os.path operations, use copy rather than copyfile. copyfile will only accept strings.\", '', 'copy2(src,dst) is often more useful than copyfile(src,dst) because:Here is a short example:', 'In Python, you can copy the files usingshutil.copyfile  signatureshutil.copy  signatureshutil.copy2  signatureshutil.copyfileobj  signatureos.popen  signatureos.system  signaturesubprocess.call  signaturesubprocess.check_output  signature', 'You can use one of the copy functions from the shutil package:Example:', 'Copying a file is a relatively straightforward operation as shown by the examples below, but you should instead use the shutil stdlib module for that.If you want to copy by filename you could do something like this:', 'Use the shutil module.Copy the contents of the file named src to a file named dst. The destination location must be writable; otherwise, an IOError exception will be raised. If dst already exists, it will be replaced. Special files such as character or block devices and pipes cannot be copied with this function. src and dst are path names given as strings.Take a look at filesys for all the file and directory handling functions available in standard Python modules.', \"Directory and File copy example, from Tim Golden's Python Stuff:\", \"For small files and using only Python built-ins, you can use the following one-liner:This is not optimal way for applications where the file is too large or when memory is critical, thus Swati's answer should be preferred.\", 'Firstly, I made an exhaustive cheat sheet of the shutil methods for your reference.Secondly, explaining methods of copy in examples:shutil.copyfileobj(fsrc, fdst[, length]) manipulate opened objectsshutil.copyfile(src, dst, *, follow_symlinks=True)  Copy and renameshutil.copy()  Copy without preseving the metadatashutil.copy2()  Copy with preserving the metadatashutil.copytree()Recursively copy an entire directory tree rooted at src, returning the destination directory.', 'shutil module offers some high-level operations on files. It supports file copying and removal.Refer to the table below for your use case.', \"As of Python 3.5 you can do the following for small files (ie: text files, small jpegs):write_bytes will overwrite whatever was at the destination's location\", \"You could use os.system('cp nameoffilegeneratedbyprogram /otherdirectory/').Or as I did it,where rawfile is the name that I had generated inside the program.This is a Linux-only solution.\", 'Use subprocess.call to copy the file', 'For large files, I read the file line by line and read each line into an array. Then, once the array reached a certain size, append it to a new file.', 'UseOpen the source file in read mode, and write to the destination file in write mode.', \"In case you've come this far down. The answer is that you need the entire path and file name\", \"Here is a simple way to do it, without any module. It's similar to this answer, but has the benefit to also work if it's a big file that doesn't fit in RAM:Since we're writing a new file, it does not preserve the modification time, etc.\\nWe can then use os.utime for this if needed.\", 'Similar to the accepted answer, the following code block might come in handy if you also want to make sure to create any (non-existent) folders in the path to the destination.As the accepted answers notes, these lines will overwrite any file which exists at the destination path, so sometimes it might be useful to also add: if not path.exists(destination_path): before this code block.', 'There are two best ways to copy file in Python.Code Example:There are other methods available also other than copyfile, like copy, copy2, etc, but copyfile is best in terms of performance,Code Example:Another method is by the use of a subprocess, but it is not preferable as it\u2019s one of the call methods and is not secure.', 'You can use system.For Unix-like systems:', 'You can use os.link to create a hard link to a file:This is not an independent clone, but if you plan to only read (not modify) the new file and its content must remain the same as the original, this will work well. It also has a benefit that if you want to check whether the copy already exists, you can compare the hard links (with os.stat) instead of their content.In Linux, the command cp with keyscreates a hard link. Therefore a hard link may be considered a copy. Sometimes a person would need exactly this behaviour (access to file content from a different place), and not need a separate copy.', 'Here is an answer utilizing \"shutil.copyfileobj\" and it is highly efficient. I used it in a tool I created some time ago. I didn\\'t write this originally, but I tweaked it a little bit.', 'Python provides in-built functions for easily copying files using the operating system shell utilities.The Following command is used to copy a file:The following command is used to copy a file with metadata information:', 'shutil.copy(src, dst, *, follow_symlinks=True)']",
            "url": "https://stackoverflow.com/questions/123198"
        },
        {
            "tag": "python",
            "question": [
                "How do I list all files of a directory?"
            ],
            "votes": "3467",
            "answer": "[\"os.listdir() returns everything inside a directory -- including both files and directories.os.path's isfile() can be used to only list files:Alternatively, os.walk() yields two lists for each directory it visits -- one for files and one for dirs. If you only want the top directory you can break the first time it yields:or, shorter:\", 'I prefer using the glob module, as it does pattern matching and expansion.It does pattern matching intuitivelyIt will return a list with the queried files and directories:Note that glob ignores files and directories that begin with a dot ., as those are considered hidden files and directories, unless the pattern is something like .*.Use glob.escape to escape strings that are not meant to be patterns:', \"list in the current directoryWith listdir in os module you get the files and the folders in the current dirLooking in a directorywith glob you can specify a type of file to list like thisorget the full path of only files in the current directoryGetting the full path name with os.path.abspathYou get the full path in returnWalk: going through sub directoriesos.walk returns the root, the directories list and the files list, that is why I unpacked them in r, d, f in the for loop; it, then, looks for other files and directories in the subfolders of the root and so on until there are no subfolders.To go up in the directory treeGet files of a particular subdirectory with os.listdir()os.walk('.') - current directorynext(os.walk('.')) and os.path.join('dir', 'file')next... walkos.walkos.listdir() - get only txt filesUsing glob to get the full path of the filesUsing os.path.isfile to avoid directories in the listUsing pathlib from Python 3.4With list comprehension:Use glob method in pathlib.Path()Get all and only files with os.walk: checks only in the third element returned, i.e. the list of the filesGet only files with next in a directory: returns only the file in the root folderGet only directories with next and walk in a directory, because in the [1] element there are the folders onlyGet all the subdir names with walkos.scandir() from Python 3.5 and greater\", 'will return a list of all files and directories in \"somedirectory\".', 'A one-line solution to get only list of files (no subdirectories):or absolute pathnames:', 'Getting Full File Paths From a Directory and All Its Subdirectoriesprint full_file_paths which will print the list:If you\\'d like, you can open and read the contents, or focus only on files with the extension \".dat\" like in the code below:/Users/johnny/Desktop/TEST/SUBFOLDER/file3.dat', \"Since version 3.4 there are builtin iterators for this which are a lot more efficient than os.listdir():pathlib: New in version 3.4.According to PEP 428, the aim of the pathlib library is to provide a simple hierarchy of classes to handle filesystem paths and the common operations users do over them.os.scandir(): New in version 3.5.Note that os.walk() uses os.scandir() instead of os.listdir() from version 3.5, and its speed got increased by 2-20 times according to PEP 471.Let me also recommend reading ShadowRanger's comment below.\", 'Although there\\'s a clear differentiation between file and directory terms in the question text, some may argue that directories are actually special filesThe statement: \"all files of a directory\" can be interpreted in two ways:All direct (or level 1) descendants onlyAll descendants in the whole directory tree (including the ones in sub-directories)When the question was asked, I imagine that Python 2, was the LTS version, however the code samples will be run by Python 3(.5) (I\\'ll keep them as Python 2 compliant as possible; also, any code belonging to Python that I\\'m going to post, is from v3.5.4 - unless otherwise specified).\\nThat has consequences related to another keyword in the question: \"add them into a list\":In pre Python 2.2 versions, sequences (iterables) were mostly represented by lists (tuples, sets, ...)In Python 2.2, the concept of generator ([Python.Wiki]: Generators) - courtesy of [Python.Docs]: Simple statements - The yield statement) - was introduced. As time passed, generator counterparts started to appear for functions that returned / worked with listsIn Python 3, generator is the default behaviorNot sure if returning a list is still mandatory (or a generator would do as well), but passing a generator to the list constructor, will create a list out of it (and also consume it). The example below illustrates the differences on [Python.Docs]: Built-in functions - map(function, iterable, *iterables)The examples will be based on a directory called root_dir with the following structure (this example is for Win, but I\\'m using the same tree on Nix as well). Note that I\\'ll be reusing the console:Return a list containing the names of the entries in the directory given by path. The list is in arbitrary order, and does not include the special entries \\'.\\' and \\'..\\' ...A more elaborate example (code_os_listdir.py):Notes:There are two implementations:One that uses generators (of course here it seems useless, since I immediately convert the result to a list)The classic one (function names ending in _old)Recursion is used (to get into subdirectories)For each implementation there are two functions:One that starts with an underscore (_): \"private\" (should not be called directly) - that does all the workThe public one (wrapper over previous): it just strips off the initial path (if required) from the returned entries. It\\'s an ugly implementation, but it\\'s the only idea that I could come with at this pointIn terms of performance, generators are generally a little bit faster (considering both creation and  iteration times), but I didn\\'t test them in recursive functions, and also I am iterating inside the function over inner generators - don\\'t know how performance friendly is thatPlay with the arguments to get different resultsOutput:In Python 3.5+ only, backport: [PyPI]: scandir:Return an iterator of os.DirEntry objects corresponding to the entries in the directory given by path. The entries are yielded in arbitrary order, and the special entries \\'.\\' and \\'..\\' are not included.Using scandir() instead of listdir() can significantly increase the performance of code that also needs file type or file attribute information, because os.DirEntry objects expose this information if the operating system provides it when scanning a directory. All os.DirEntry methods may perform a system call, but is_dir() and is_file() usually only require a system call for symbolic links; os.DirEntry.stat() always requires a system call on Unix but only requires one for symbolic links on Windows.Notes:Similar to os.listdirBut it\\'s also more flexible (and offers more functionality), more Pythonic (and in some cases, faster)Generate the file names in a directory tree by walking the tree either top-down or bottom-up. For each directory in the tree rooted at directory top (including top itself), it yields a 3-tuple (dirpath, dirnames, filenames).Notes:Under the scenes, it uses os.scandir (os.listdir on older (Python) versions)It does the heavy lifting by recurring in subfoldersOr glob.iglob:Return a possibly-empty list of path names that match pathname, which must be a string containing a path specification. pathname can be either absolute (like /usr/src/Python-1.5/Makefile) or relative (like ../../Tools/*/*.gif), and can contain shell-style wildcards. Broken symlinks are included in the results (as in the shell)....Changed in version 3.5: Support for recursive globs using \u201c**\u201d.Notes:Uses os.listdirFor large trees (especially if recursive is on), iglob is preferredAllows advanced filtering based on name (due to the wildcard)Python 3.4+, backport: [PyPI]: pathlib2.Notes:This is one way of achieving our goalIt\\'s the OOP style of handling pathsOffers lots of functionalitiesPython 2 onlyBut, according to [GitHub]: python/cpython - (2.7) cpython/Lib/dircache.py, it\\'s just a (thin) wrapper over os.listdir with cachingPOSIX specific:[Man7]: OPENDIR(3)[Man7]: READDIR(3)[Man7]: CLOSEDIR(3)Available via [Python.Docs]: ctypes - A foreign function library for Python:ctypes is a foreign function library for Python. It provides C compatible data types, and allows calling functions in DLLs or shared libraries. It can be used to wrap these libraries in pure Python.Not directly related, but check [SO]: C function called from Python via ctypes returns incorrect value (@CristiFati\\'s answer) out before working with CTypes.code_ctypes.py:Notes:It loads the three functions from LibC (libc.so - loaded in the current process) and calls them (for more details check [SO]: How do I check whether a file exists without exceptions? (@CristiFati\\'s answer) - last notes from item #4.). That would place this approach very close to the Python / C edgeNixDirent64 is the CTypes representation of struct dirent64 from [Man7]: dirent.h(0P) (so are the DT_ constants) from my Ubuntu OS. On other flavors / versions, the structure definition might differ, and if so, the CTypes alias should be updated, otherwise it will yield Undefined BehaviorIt returns data in the os.walk\\'s format. I didn\\'t bother to make it recursive, but starting from the existing code, that would be a fairly trivial taskEverything is doable on Win as well, the data (libraries, functions, structs, constants, ...) differOutput:Win specific:Retrieves a list of matching filenames, using the Windows Unicode API. An interface to the API FindFirstFileW/FindNextFileW/Find close functions.Notes:Most likely, will rely on one (or more) of the above (maybe with slight customizations).Code is meant to be portable (except places that target a specific area - which are marked) or cross:OS (Nix, Win, )Python version (2, 3, )Multiple path styles (absolute, relatives) were used across the above variants, to illustrate the fact that the \"tools\" used are flexible in this directionos.listdir and os.scandir use opendir / readdir / closedir ([MS.Learn]: FindFirstFileW function (fileapi.h) / [MS.Learn]: FindNextFileW function (fileapi.h) / [MS.Learn]: FindClose function (fileapi.h)) (via [GitHub]: python/cpython - (main) cpython/Modules/posixmodule.c)win32file.FindFilesW uses those (Win specific) functions as well (via [GitHub]: mhammond/pywin32 - (main) pywin32/win32/src/win32file.i)_get_dir_content (from point #1.) can be implemented using any of these approaches (some will require more work and some less)Nota Bene! Since recursion is used, I must mention that I did some tests on my laptop (Win 10 pc064), totally unrelated to this problem, and when the recursion level was reaching values somewhere in the (990 .. 1000) range (recursionlimit - 1000 (default)), I got StackOverflow :). If the directory tree exceeds that limit (I am not an FS expert, so I don\\'t know if that is even possible), that could be a problem.\\nI must also mention that I didn\\'t try to increase recursionlimit, but in theory there will always be the possibility for failure, if the dir depth is larger than the highest possible recursionlimit (on that machine).\\nCheck [SO]: _csv.Error: field larger than field limit (131072) (@CristiFati\\'s answer) for more details on the topicCode samples are for demonstrative purposes only. That means that I didn\\'t take into account error handling (I don\\'t think there\\'s any try / except / else / finally block), so the code is not robust (the reason is: to keep it as simple and short as possible). For production, error handling should be added as wellEverything is done using another technologyThat technology is invoked from PythonThe most famous flavor that I know is what I call the SysAdmin approach:Use Python (or any programming language for that matter) in order to execute Shell commands (and parse their outputs)Some consider this a neat hackI consider it more like a lame workaround (gainarie), as the action per se is performed from Shell (Cmd in this case), and thus doesn\\'t have anything to do with PythonFiltering (grep / findstr) or output formatting could be done on both sides, but I\\'m not going to insist on it. Also, I deliberately used os.system instead of [Python.Docs]: subprocess - Subprocess management routines (run, check_output, ...)In general, this approach is to be avoided, since if some command output format slightly differs between OS versions / flavors, the parsing code should be adapted as well - not to mention differences between locales.', \"I really liked adamk's answer, suggesting that you use glob(), from the module of the same name. This allows you to have pattern matching with *s.But as other people pointed out in the comments, glob() can get tripped up over inconsistent slash directions. To help with that, I suggest you use the join() and expanduser() functions in the os.path module, and perhaps the getcwd() function in the os module, as well.As examples:The above is terrible - the path has been hardcoded and will only ever work on Windows between the drive name and the \\\\s being hardcoded into the path.The above works better, but it relies on the folder name Users which is often found on Windows and not so often found on other OSs. It also relies on the user having a specific name, admin.This works perfectly across all platforms.Another great example that works perfectly across platforms and does something a bit different:Hope these examples help you see the power of a few of the functions you can find in the standard Python library modules.\", '', 'If you are looking for a Python implementation of find, this is a recipe I use rather frequently:So I made a PyPI package out of it and there is also a GitHub repository. I hope that someone finds it potentially useful for this code.', \"For greater results, you can use listdir() method of the os module along with a generator (a generator is a powerful iterator that keeps its state, remember?). The following code works fine with both versions: Python 2 and Python 3.Here's a code:The listdir() method returns the list of entries for the given directory. The method os.path.isfile() returns True if the given entry is a file. And the yield operator quits the func but keeps its current state, and it returns only the name of the entry detected as a file. All the above allows us to loop over the generator function.\", 'Returning a list of absolute filepaths, does not recurse into subdirectories', 'A wise teacher told me once that:When there are several established ways to do something, none of them is good for all cases.I will thus add a solution for a subset of the problem: quite often, we only want to check whether a file matches a start string and an end string, without going into subdirectories. We would thus like a function that returns a list of filenames, like:If you care to first declare two functions, this can be done:This solution could be easily generalized with regular expressions (and you might want to add a pattern argument, if you do not want your patterns to always stick to the start or end of the filename).', 'Here I use a recursive structure.', 'Using generators', 'Another very readable variant for Python 3.4+ is using pathlib.Path.glob:It is simple to make more specific, e.g. only look for Python source files which are not symbolic links, also in all subdirectories:', 'For Python 2:Then do', \"Here's my general-purpose function for this.  It returns a list of file paths rather than filenames since I found that to be more useful.  It has a few optional arguments that make it versatile.  For instance, I often use it with arguments like pattern='*.txt' or subfolders=True.\", \"I will provide a sample one liner where sourcepath and file type can be provided as input. The code returns a list of filenames with csv extension. Use . in case all files needs to be returned. This will also recursively scans the subdirectories.[y for x in os.walk(sourcePath) for y in glob(os.path.join(x[0], '*.csv'))]Modify file extensions and source path as needed.\", 'dircache is  \"Deprecated since version 2.6: The dircache module has been removed in Python 3.0.\"']",
            "url": "https://stackoverflow.com/questions/3207219"
        },
        {
            "tag": "python",
            "question": [
                "What is __init__.py for?"
            ],
            "votes": "3424",
            "answer": "['It used to be a required part of a package (old, pre-3.3 \"regular package\", not newer 3.3+ \"namespace package\").Here\\'s the documentation.Python defines two types of packages, regular packages and namespace packages. Regular packages are traditional packages as they existed in Python 3.2 and earlier. A regular package is typically implemented as a directory containing an __init__.py file. When a regular package is imported, this __init__.py file is implicitly executed, and the objects it defines are bound to names in the package\u2019s namespace. The __init__.py file can contain the same Python code that any other module can contain, and Python will add some additional attributes to the module when it is imported.But just click the link, it contains an example, more information, and an explanation of namespace packages, the kind of packages without __init__.py.', 'Files named __init__.py are used to mark directories on disk as Python package directories.\\nIf you have the filesand mydir is on your path, you can import the code in module.py asorIf you remove the __init__.py file, Python will no longer look for submodules inside that directory, so attempts to import the module will fail.The __init__.py file is usually empty, but can be used to export selected portions of the package under more convenient name, hold convenience functions, etc.\\nGiven the example above, the contents of the init module can be accessed asbased on this', 'In addition to labeling a directory as a Python package and defining __all__, __init__.py allows you to define any variable at the package level. Doing so is often convenient if a package defines something that will be imported frequently, in an API-like fashion. This pattern promotes adherence to the Pythonic \"flat is better than nested\" philosophy.Here is an example from one of my projects, in which I frequently import a sessionmaker called Session to interact with my database. I wrote a \"database\" package with a few modules:My __init__.py contains the following code:Since I define Session here, I can start a new session using the syntax below. This code would be the same executed from inside or outside of the \"database\" package directory.Of course, this is a small convenience -- the alternative would be to define Session in a new file like \"create_session.py\" in my database package, and start new sessions using:There is a pretty interesting reddit thread covering appropriate uses of __init__.py here:http://www.reddit.com/r/Python/comments/1bbbwk/whats_your_opinion_on_what_to_include_in_init_py/The majority opinion seems to be that __init__.py files should be very thin to avoid violating the \"explicit is better than implicit\" philosophy.', \"There are 2 main reasons for __init__.pyFor convenience: the other users will not need to know your functions' exact location in your package hierarchy (documentation).then others can call add() bywithout knowing file1's inside functions, likeIf you want something to be initialized; for example, logging (which should be put in the top level):\", 'The __init__.py file makes Python treat directories containing it as modules.Furthermore, this is the first file to be loaded in a module, so you can use it to execute code that you want to run each time a module is loaded, or specify the submodules to be exported.', \"Since Python 3.3, __init__.py is no longer required to define directories as importable Python packages.Check PEP 420: Implicit Namespace Packages:Native support for package directories that don\u2019t require __init__.py marker files and can automatically span multiple path segments (inspired by various third party approaches to namespace packages, as described in PEP 420)Here's the test:references:\\nhttps://docs.python.org/3/whatsnew/3.3.html#pep-420-implicit-namespace-packages\\nhttps://www.python.org/dev/peps/pep-0420/\\nIs __init__.py not required for packages in Python 3?\", 'Although Python works without an __init__.py file you should still include one.It specifies that the directory should be treated as a package, so therefore include it (even if it is empty).There is also a case where you may actually use an __init__.py file:Imagine you had the following file structure:And methods.py contained this:To use foo() you would need one of the following:Maybe there you need (or want) to keep methods.py inside main_methods (runtimes/dependencies for example) but you only want to import main_methods.If you changed the name of methods.py to __init__.py then you could use foo() by just importing main_methods:This works because __init__.py is treated as part of the package.Some Python packages actually do this.  An example is with JSON, where running import json is actually importing __init__.py from the json package (see the package file structure here):Source code: Lib/json/__init__.py', 'In Python the definition of package is very simple. Like Java the hierarchical structure and the directory structure are the same. But you have to have __init__.py in a package. I will explain the __init__.py file with the example below:__init__.py can be empty, as long as it exists. It indicates that the directory should be regarded as a package. Of course, __init__.py can also set the appropriate content.If we add a function in module_n1:After running:Then we followed the hierarchy package and called module_n1 the function. We can use __init__.py in subPackage_b like this:After running:Hence using * importing, module package is subject to __init__.py content.', \"__init__.py will treat the directory it is in as a loadable module.For people who prefer reading code, I put Two-Bit Alchemist's comment here.\", \"It facilitates importing other python files. When you placed this file in a directory (say stuff)containing other py files, then you can do something like import stuff.other.Without this __init__.py inside the directory stuff, you couldn't import other.py, because Python doesn't know where the source code for stuff is and unable to recognize it as a package.\", \"An __init__.py file makes imports easy. When an __init__.py is present within a package, function a() can be imported from file b.py like so:Without it, however, you can't import directly. You have to amend the system path:\", \"One thing __init__.py allows is converting a module to a package without breaking the API or creating extraneous nested namespaces or private modules*. This helps when I want to extend a namespace.If I have a file util.py containingthen users will access foo withIf I then want to add utility functions for database interaction, and I want them to have their own namespace under util, I'll need a new directory**, and to keep API compatibility (so that from util import foo still works), I'll call it util/. I could move util.py into util/ like so,and in util/__init__.py dobut this is redundant. Instead of having a util/util.py file, we can just put the util.py contents in __init__.py and the user can nowI think this nicely highlights how a util package's __init__.py acts in a similar way to a util module* this is hinted at in the other answers, but I want to highlight it here\\n** short of employing import gymnastics. Note it won't work to create a new package with the same name as the file, see this\", \"If you're using Python 2 and want to load siblings of your file you can simply add the parent folder of your file to your system paths of the session. It will behave about the same as if your current file was an init file.After that regular imports relative to the file's directory will work just fine. E.g.Generally you want to use a proper init.py file instead though, but when dealing with legacy code you might be stuck with f.ex. a library hard-coded to load a particular file and nothing but. For those cases this is an alternative.\", 'init.py : It is a python file found in a package directory, it is invoked when the package or a module in the package is imported. You can use this to execute package initialization code, i.e. whenever the package is imported the python statements are executed first before the other modules in this folder gets executed. It is similar to main function of c or java program but this exists in the python package module(folder) rather than in the core python file.\\nalso it has access to global variables defined in this init.py file as when the module is imported into python file.for eg.\\nI have a init.py file in a folder called pymodlib, this file contains the following statements:print(f\\'Invoking init.py for {name}\\')\\npystructures = [\\'for_loop\\', \\'while__loop\\', \\'ifCondition\\']when I import this package \"pymodlib\" in the my solution module or notebook or python console:\\nthis two statements gets executed while importing.\\nSo in the log or console you would see the following output:import pymodlib\\nInvoking init.py for pymodlibin the next statement of python console: I can access the global variable:pymodlib.pystructures\\nit gives the following output:[\\'for_loop\\', \\'while__loop\\', \\'ifCondition\\']Now from python3.3 onwards the use of this file has been optional to make folder a python module. So you skip from including it in the python module folder.']",
            "url": "https://stackoverflow.com/questions/448271"
        },
        {
            "tag": "python",
            "question": [
                "How do I sort a dictionary by value?"
            ],
            "votes": "3416",
            "answer": "[\"Dicts preserve insertion order in Python 3.7+. Same in CPython 3.6, but it's an implementation detail.orIt is not possible to sort a dictionary, only to get a representation of a dictionary that is sorted. Dictionaries are inherently orderless, but other types, such as lists and tuples, are not. So you need an ordered data type to represent sorted values, which will be a list\u2014probably a list of tuples.For instance,sorted_x will be a list of tuples sorted by the second element in each tuple. dict(sorted_x) == x.And for those wishing to sort on keys instead of values:In Python3 since unpacking is not allowed we can useIf you want the output as a dict, you can use collections.OrderedDict:\", 'Well, it is actually possible to do a \"sort by dictionary values\". Recently I had to do that in a Code Golf (Stack Overflow question Code golf: Word frequency chart). Abridged, the problem was of the kind: given a text, count how often each word is encountered and display a list of the top words, sorted by decreasing frequency.If you construct a dictionary with the words as keys and the number of occurrences of each word as value, simplified here as:then you can get a list of the words, ordered by frequency of use with sorted(d, key=d.get) - the sort iterates over the dictionary keys, using the number of word occurrences as a sort key .I am writing this detailed explanation to illustrate what people often mean by \"I can easily sort a dictionary by key, but how do I sort by value\" - and I think the original post was trying to address such an issue. And the solution is to do sort of list of the keys, based on the values, as shown above.', 'You could use:This will sort the dictionary by the values of each entry within the dictionary from smallest to largest.To sort it in descending order just add reverse=True:Input:Output:', \"Dicts can't be sorted, but you can build a sorted list from them.A sorted list of dict values:A list of (key, value) pairs, sorted by value:\", 'In recent Python 2.7, we have the new OrderedDict type, which remembers the order in which the items were added.To make a new ordered dictionary from the original, sorting by the values:The OrderedDict behaves like a normal dict:', \"Whilst I found the accepted answer useful, I was also surprised that it hasn't been updated to reference OrderedDict from the standard library collections module as a viable, modern alternative - designed to solve exactly this type of problem.The official OrderedDict documentation offers a very similar example too, but using a lambda for the sort function:\", \"Pretty much the same as Hank Gay's answer:Or optimized slightly as suggested by John Fouhy:\", 'Good news, so the OP\\'s original use case of mapping pairs retrieved from a database with unique string ids as keys and numeric values as values into a built-in Python v3.6+ dict, should now respect the insert order.If say the resulting two column table expressions from a database query like:would be stored in two Python tuples, k_seq and v_seq (aligned by numerical index and with the same length of course), then:Allow to output later as:yielding in this case (for the new Python 3.6+ built-in dict!):in the same ordering per value of v.Where in the Python 3.5 install on my machine it currently yields:As proposed in 2012 by Raymond Hettinger (cf. mail on python-dev with subject \"More compact dictionaries with faster iteration\") and now (in 2016) announced in a mail by Victor Stinner to python-dev with subject \"Python 3.6 dict becomes compact and gets a private version; and keywords become ordered\" due to the fix/implementation of issue 27350 \"Compact and ordered dict\" in Python 3.6 we will now be able, to use a built-in dict to maintain insert order!!Hopefully this will lead to a thin layer OrderedDict implementation as a first step. As @JimFasarakis-Hilliard indicated, some see use cases for the OrderedDict type also in the future. I think the Python community at large will carefully inspect, if this will stand the test of time, and what the next steps will be.Time to rethink our coding habits to not miss the possibilities opened by stable ordering of:The first because it eases dispatch in the implementation of functions and methods in some cases.The second as it encourages to more easily use dicts as intermediate storage in processing pipelines.Raymond Hettinger kindly provided documentation explaining \"The Tech Behind Python 3.6 Dictionaries\" - from his San Francisco Python Meetup Group presentation 2016-DEC-08.And maybe quite some Stack Overflow high decorated question and answer pages will receive variants of this information and many high quality answers will require a per version update too.As @ajcr rightfully notes: \"The order-preserving aspect of this new implementation is considered an implementation detail and should not be relied upon.\" (from the whatsnew36) not nit picking, but the citation was cut a bit pessimistic ;-). It continues as \" (this may change in the future, but it is desired to have this new dict implementation in the language for a few releases before changing the language spec to mandate order-preserving semantics for all current and future Python implementations; this also helps preserve backwards-compatibility with older versions of the language where random iteration order is still in effect, e.g. Python 3.5).\"So as in some human languages (e.g. German), usage shapes the language, and the will now has been declared ... in whatsnew36.In a mail to the python-dev list, Guido van Rossum declared:Make it so. \"Dict keeps insertion order\" is the ruling. Thanks!So, the version 3.6 CPython side-effect of dict insertion ordering is now becoming part of the language spec (and not anymore only an implementation detail). That mail thread also surfaced some distinguishing design goals for collections.OrderedDict as reminded by Raymond Hettinger during discussion.', \"It can often be very handy to use namedtuple. For example, you have a dictionary of 'name' as keys and 'score' as values and you want to sort on 'score':sorting with lowest score first:sorting with highest score first:Now you can get the name and score of, let's say the second-best player (index=1) very Pythonically like this:\", 'I had the same problem, and I solved it like this:(People who answer \"It is not possible to sort a dict\" did not read the question! In fact, \"I can sort on the keys, but how can I sort based on the values?\" clearly means that he wants a list of the keys sorted according to the value of their values.)Please notice that the order is not well defined (keys with the same value will be in an arbitrary order in the output list).', 'If values are numeric you may also use Counter from collections.', 'In Python 2.7, simply do:copy-paste from : http://docs.python.org/dev/library/collections.html#ordereddict-examples-and-recipesEnjoy ;-)', \"Starting from Python 3.6, dict objects are now ordered by insertion order. It's officially in the specifications of Python 3.7.Before that, you had to use OrderedDict.Python 3.7 documentation says:Changed in version 3.7: Dictionary order is guaranteed to be insertion\\norder. This behavior was implementation detail of CPython from 3.6.\", 'This is the code:Here are the results:OriginalRoflRank', 'Try the following approach. Let us define a dictionary called mydict with the following data:If one wanted to sort the dictionary by keys, one could do something like:This should return the following output:On the other hand, if one wanted to sort a dictionary by value (as is asked in the question), one could do the following:The result of this command (sorting the dictionary by value) should return the following:', 'You can create an \"inverted index\", alsoNow your inverse has the values; each value has a list of applicable keys.', 'You can use the collections.Counter. Note, this will work for both numeric and non-numeric values.', \"The collections solution mentioned in another answer is absolutely superb, because you retain a connection between the key and value which in the case of dictionaries is extremely important.I don't agree with the number one choice presented in another answer, because it throws away the keys.I used the solution mentioned above (code shown below) and retained access to both keys and values and in my case the ordering was on the values, but the importance was the ordering of the keys after ordering the values.\", \"You can use a skip dict which is a dictionary that's permanently sorted by value.If you use keys(), values() or items() then you'll iterate in sorted order by value.It's implemented using the skip list datastructure.\", 'You can also use a custom function that can be passed to parameter key.', \"Of course, remember, you need to use OrderedDict because regular Python dictionaries don't keep the original order.If you do not have Python 2.7 or higher, the best you can do is iterate over the values in a generator function. (There is an OrderedDict for 2.4 and 2.6  here, buta) I don't know about how well it worksandb) You have to download and install it of course. If you do not have administrative access, then I'm afraid the option's out.)You can also print out every valuePlease remember to remove the parentheses after print if not using Python 3.0 or above\", '', 'Here is a solution using zip on d.values() and d.keys().  A few lines down this link (on Dictionary view objects) is:This allows the creation of (value, key) pairs using zip(): pairs = zip(d.values(), d.keys()).So we can do the following:', \"As pointed out by Dilettant, Python 3.6 will now keep the order! I thought I'd share a function I wrote that eases the sorting of an iterable (tuple, list, dict). In the latter case, you can sort either on keys or values, and it can take numeric comparison into account. Only for >= 3.6!When you try using sorted on an iterable that holds e.g. strings as well as ints, sorted() will fail. Of course you can force string comparison with str(). However, in some cases you want to do actual numeric comparison where 12 is smaller than 20 (which is not the case in string comparison). So I came up with the following. When you want explicit numeric comparison you can use the flag num_as_num which will try to do explicit numeric sorting by trying to convert all values to floats. If that succeeds, it will do numeric sorting, otherwise it'll resort to string comparison.Comments for improvement welcome.\", 'I just learned a relevant skill from Python for Everybody.You may use a temporary list to help you to sort the dictionary:If you want to sort the list in descending order, simply change the original sorting line to:Using list comprehension, the one-liner would be:Sample Output:', 'Use ValueSortedDict from dicts:', 'Iterate through a dict and sort it by its values in descending order:', 'If your values are integers, and you use Python 2.7 or newer, you can use collections.Counter instead of dict. The most_common method will give you all items, sorted by the value.', 'This works in 3.1.x:', 'For the sake of completeness, I am posting a solution using heapq. Note, this method will work for both numeric and non-numeric values']",
            "url": "https://stackoverflow.com/questions/613183"
        },
        {
            "tag": "python",
            "question": [
                "How can I add new keys to a dictionary?"
            ],
            "votes": "3411",
            "answer": "[\"You create a new key/value pair on a dictionary by assigning a value to that keyIf the key doesn't exist, it's added and points to that value. If it exists, the current value it points to is overwritten.\", 'I feel like consolidating info about Python dictionaries:The update operator |= now works for dictionaries:This uses a new feature called dictionary unpacking.The merge operator | now works for dictionaries:', 'To add multiple keys simultaneously, use dict.update():For adding a single key, the accepted answer has less computational overhead.', \"Yes it is possible, and it does have a method that implements this, but you don't want to use it directly.To demonstrate how and how not to use it, let's create an empty dict with the dict literal, {}:To update this dict with a single new key and value, you can use the subscript notation (see Mappings here) that provides for item assignment:my_dict is now:We can also update the dict with multiple values efficiently as well using the update method.  We may be unnecessarily creating an extra dict here, so we hope our dict has already been created and came from or was used for another purpose:my_dict is now:Another efficient way of doing this with the update method is with keyword arguments, but since they have to be legitimate python words, you can't have spaces or special symbols or start the name with a number, but many consider this a more readable way to create keys for a dict, and here we certainly avoid creating an extra unnecessary dict:and my_dict is now:So now we have covered three Pythonic ways of updating a dict.There's another way of updating a dict that you shouldn't use, which uses the __setitem__ method. Here's an example of how one might use the __setitem__ method to add a key-value pair to a dict, and a demonstration of the poor performance of using it:So we see that using the subscript notation is actually much faster than using __setitem__. Doing the Pythonic thing, that is, using the language in the way it was intended to be used, usually is both more readable and computationally efficient.\", '', 'If you want to add a dictionary within a dictionary you can do it this way.Example: Add a new entry to your dictionary & sub dictionaryOutput:NOTE: Python requires that you first add a subbefore adding entries.', 'The conventional syntax is d[key] = value, but if your keyboard is missing the square bracket keys you could also do:In fact, defining __getitem__ and __setitem__ methods is how you can make your own class support the  square bracket syntax. See Dive Into Python, Classes That Act Like Dictionaries.', 'You can create one:Gives:', 'This popular question addresses functional methods of merging dictionaries a and b.Here are some of the more straightforward methods (tested in Python 3)...Note: The first method above only works if the keys in b are strings.To add or modify a single element, the b dictionary would contain only that one element...This is equivalent to...', \"Let's pretend you want to live in the immutable world and do not want to modify the original but want to create a new dict that is the result of adding a new key to the original.In Python 3.5+ you can do:The Python 2 equivalent is:After either of these:params is still equal to {'a': 1, 'b': 2}andnew_params is equal to {'a': 1, 'b': 2, 'c': 3}There will be times when you don't want to modify the original (you only want the result of adding to the original). I find this a refreshing alternative to the following:orReference: What does `**` mean in the expression `dict(d1, **d2)`?\", 'There is also the strangely named, oddly behaved, and yet still handy dict.setdefault().Thisbasically just does this:E.g.,', 'This question has already been answered ad nauseam, but since my\\ncomment\\ngained a lot of traction, here it is as an answer:If you are here trying to figure out how to add a key and return a new dictionary (without modifying the existing one), you can do this using the techniques belowNote that with this approach, your key will need to follow the rules of valid identifier names in Python.', \"If you're not joining two dictionaries, but adding new key-value pairs to a dictionary, then using the subscript notation seems like the best way.However, if you'd like to add, for example, thousands of new key-value pairs, you should consider using the update() method.\", \"Here's another way that I didn't see here:You can use the dictionary constructor and implicit expansion to reconstruct a dictionary. Moreover, interestingly, this method can be used to control the positional order during dictionary construction (post Python 3.6). In fact, insertion order is guaranteed for Python 3.7 and above!The above is using dictionary comprehension.\", 'First to check whether the key already exists:Then you can add the new key and value.', 'Add a dictionary (key,value) class.', \"I think it would also be useful to point out Python's collections module that consists of many useful dictionary subclasses and wrappers that simplify the addition and modification of data types in a dictionary, specifically defaultdict:dict subclass that calls a factory function to supply missing valuesThis is particularly useful if you are working with dictionaries that always consist of the same data types or structures, for example a dictionary of lists.If the key does not yet exist, defaultdict assigns the value given (in our case 10) as the initial value to the dictionary (often used inside loops). This operation therefore does two things: it adds a new key to a dictionary (as per question), and assigns the value if the key doesn't yet exist. With the standard dictionary, this would have raised an error as the += operation is trying to access a value that doesn't yet exist:Without the use of defaultdict, the amount of code to add a new element would be much greater and perhaps looks something like:defaultdict can also be used with complex data types such as list and set:Adding an element automatically initialises the list.\", '', 'dico[\"new key\"] = \"value\"']",
            "url": "https://stackoverflow.com/questions/1024847"
        },
        {
            "tag": "python",
            "question": [
                "\"Least Astonishment\" and the Mutable Default Argument"
            ],
            "votes": "3258",
            "answer": "['Actually, this is not a design flaw, and it is not because of internals or performance. It comes simply from the fact that functions in Python are first-class objects, and not only a piece of code.As soon as you think of it this way, then it completely makes sense: a function is an object being evaluated on its definition; default parameters are kind of \"member data\" and therefore their state may change from one call to the other - exactly as in any other object.In any case, the effbot (Fredrik Lundh) has a very nice explanation of the reasons for this behavior in Default Parameter Values in Python.\\nI found it very clear, and I really suggest reading it for a better knowledge of how function objects work.', 'Suppose you have the following codeWhen I see the declaration of eat, the least astonishing thing is to think that if the first parameter is not given, that it will be equal to the tuple (\"apples\", \"bananas\", \"loganberries\")However, suppose later on in the code, I do something likethen if default parameters were bound at function execution rather than function declaration, I would be astonished (in a very bad way) to discover that fruits had been changed. This would be more astonishing IMO than discovering that your foo function above was mutating the list.The real problem lies with mutable variables, and all languages have this problem to some extent. Here\\'s a question: suppose in Java I have the following code:Now, does my map use the value of the StringBuffer key when it was placed into the map, or does it store the key by reference? Either way, someone is astonished; either the person who tried to get the object out of the Map using a value identical to the one they put it in with, or the person who can\\'t seem to retrieve their object even though the key they\\'re using is literally the same object that was used to put it into the map (this is actually why Python doesn\\'t allow its mutable built-in data types to be used as dictionary keys).Your example is a good one of a case where Python newcomers will be surprised and bitten. But I\\'d argue that if we \"fixed\" this, then that would only create a different situation where they\\'d be bitten instead, and that one would be even less intuitive. Moreover, this is always the case when dealing with mutable variables; you always run into cases where someone could intuitively expect one or the opposite behavior depending on what code they\\'re writing.I personally like Python\\'s current approach: default function arguments are evaluated when the function is defined and that object is always the default. I suppose they could special-case using an empty list, but that kind of special casing would cause even more astonishment, not to mention be backwards incompatible.', 'The relevant part of the documentation:Default parameter values are evaluated from left to right when the function definition is executed. This means that the expression is evaluated once, when the function is defined, and that the same \u201cpre-computed\u201d value is used for each call. This is especially important to understand when a default parameter is a mutable object, such as a list or a dictionary: if the function modifies the object (e.g. by appending an item to a list), the default value is in effect modified. This is generally not what was intended. A way around this is to use None as the default, and explicitly test for it in the body of the function, e.g.:', 'I know nothing about the Python interpreter inner workings (and I\\'m not an expert in compilers and interpreters either) so don\\'t blame me if I propose anything unsensible or impossible.Provided that python objects are mutable I think that this should be taken into account when designing the default arguments stuff.\\nWhen you instantiate a list:you expect to get a new list referenced by a.Why should the a=[] ininstantiate a new list on function definition and not on invocation?\\nIt\\'s just like you\\'re asking \"if the user doesn\\'t provide the argument then instantiate a new list and use it as if it was produced by the caller\".\\nI think this is ambiguous instead:user, do you want a to default to the datetime corresponding to when you\\'re defining or executing x?\\nIn this case, as in the previous one, I\\'ll keep the same behaviour as if the default argument \"assignment\" was the first instruction of the function (datetime.now() called on function invocation).\\nOn the other hand, if the user wanted the definition-time mapping he could write:I know, I know: that\\'s a closure. Alternatively Python might provide a keyword to force definition-time binding:', 'Well, the reason is quite simply that bindings are done when code is executed, and the function definition is executed, well... when the functions is defined.Compare this:This code suffers from the exact same unexpected happenstance. bananas is a class attribute, and hence, when you add things to it, it\\'s added to all instances of that class. The reason is exactly the same.It\\'s just \"How It Works\", and making it work differently in the function case would probably be complicated, and in the class case likely impossible, or at least slow down object instantiation a lot, as you would have to keep the class code around and execute it when objects are created.Yes, it is unexpected. But once the penny drops, it fits in perfectly with how Python works in general. In fact, it\\'s a good teaching aid, and once you understand why this happens, you\\'ll grok python much better.That said it should feature prominently in any good Python tutorial. Because as you mention, everyone runs into this problem sooner or later.', \"I'm really surprised no one has performed the insightful introspection offered by Python (2 and 3 apply) on callables.Given a simple little function func defined as:When Python encounters it, the first thing it will do is compile it in order to create a code object for this function. While this compilation step is done, Python evaluates* and then stores the default arguments (an empty list [] here) in the function object itself. As the top answer mentioned: the list a can now be considered a member of the function func.So, let's do some introspection, a before and after to examine how the list gets expanded inside the function object. I'm using Python 3.x for this, for Python 2 the same applies (use __defaults__ or func_defaults in Python 2; yes, two names for the same thing).After Python executes this definition it will take any default parameters specified (a = [] here) and cram them in the __defaults__ attribute for the function object (relevant section: Callables):O.k, so an empty list as the single entry in __defaults__, just as expected.Let's now execute this function:Now, let's see those __defaults__ again:Astonished? The value inside the object changes! Consecutive calls to the function will now simply append to that embedded list object:So, there you have it, the reason why this 'flaw' happens, is because default arguments are part of the function object. There's nothing weird going on here, it's all just a bit surprising.The common solution to combat this is to use None as the default and then initialize in the function body:Since the function body is executed anew each time, you always get a fresh new empty list if no argument was passed for a.To further verify that the list in __defaults__ is the same as that used in the function func you can just change your function to return the id of the list a used inside the function body. Then, compare it to the list in __defaults__ (position [0] in __defaults__) and you'll see how these are indeed refering to the same list instance:All with the power of introspection!* To verify that Python evaluates the default arguments during compilation of the function, try executing the following:as you'll notice, input() is called before the process of building the function and binding it to the name bar is made.\", \"I used to think that creating the objects at runtime would be the better approach.  I'm less certain now, since you do lose some useful features, though it may be worth it regardless simply to prevent newbie confusion.  The disadvantages of doing so are:1. PerformanceIf call-time evaluation is used, then the expensive function is called every time your function is used without an argument.  You'd either pay an expensive price on each call, or need to manually cache the value externally, polluting your namespace and adding verbosity.2. Forcing bound parametersA useful trick is to bind parameters of a lambda to the current binding of a variable when the lambda is created.  For example:This returns a list of functions that return 0,1,2,3... respectively.  If the behaviour is changed, they will instead bind i to the call-time value of i, so you would get a list of functions that all returned 9.The only way to implement this otherwise would be to create a further closure with the i bound, ie:3. IntrospectionConsider the code:We can get information about the arguments and defaults using the inspect module, whichThis information is very useful for things like document generation, metaprogramming, decorators etc.Now, suppose the behaviour of defaults could be changed so that this is the equivalent of:However, we've lost the ability to introspect, and see what the default arguments are.  Because the objects haven't been constructed, we can't ever get hold of them without actually calling the function.  The best we could do is to store off the source code and return that as a string.\", 'Simplicity: The behavior is simple in the following sense:\\nMost people fall into this trap only once, not several times.Consistency: Python always passes objects, not names.\\nThe default parameter is, obviously, part of the function\\nheading (not the function body). It therefore ought to be evaluated\\nat module load time (and only at module load time, unless nested), not\\nat function call time.Usefulness: As Frederik Lundh points out in his explanation\\nof \"Default Parameter Values in Python\", the\\ncurrent behavior can be quite useful for advanced programming.\\n(Use sparingly.)Sufficient documentation: In the most basic Python documentation,\\nthe tutorial, the issue is loudly announced as\\nan \"Important warning\" in the first subsection of Section\\n\"More on Defining Functions\".\\nThe warning even uses boldface,\\nwhich is rarely applied outside of headings.\\nRTFM: Read the fine manual.Meta-learning: Falling into the trap is actually a very\\nhelpful moment (at least if you are a reflective learner),\\nbecause you will subsequently better understand the point\\n\"Consistency\" above and that will\\nteach you a great deal about Python.', 'This behavior is easy explained by:So:', '1)  The so-called problem of \"Mutable Default Argument\" is in general a special example demonstrating that:\\n\"All functions with this problem suffer also from similar side effect problem on the actual parameter,\"\\nThat is against the rules of functional programming, usually undesiderable and should be fixed both together.Example:Solution:  a copy\\nAn absolutely safe solution is to copy or deepcopy the input object first and then to do whatever with the copy.Many builtin mutable types have a copy method like some_dict.copy() or some_set.copy() or can be copied easy like somelist[:] or list(some_list). Every object can be also copied by copy.copy(any_object) or more thorough by copy.deepcopy() (the latter useful if the mutable object is composed from mutable objects). Some objects are fundamentally based on side effects like \"file\" object and can not be meaningfully reproduced by copy. copyingExample problem for a similar SO questionIt shouldn\\'t be neither saved in any public attribute of an instance returned by this function. (Assuming that private attributes of instance should not be modified from outside of this class or subclasses by convention. i.e. _var1 is a private attribute )Conclusion:\\nInput parameters objects shouldn\\'t be modified in place (mutated) nor they should not be binded into an object returned by the function. (If we prefere programming without side effects which is strongly recommended. see Wiki about \"side effect\" (The first two paragraphs are relevent in this context.)\\n.)2)\\nOnly if the side effect on the actual parameter is required but unwanted on the default parameter then the useful solution is def ...(var1=None): if var1 is None: var1 = [] More..3) In some cases is the mutable behavior of default parameters useful.', \"What you're asking is why this:isn't internally equivalent to this:except for the case of explicitly calling func(None, None), which we'll ignore.In other words, instead of evaluating default parameters, why not store each of them, and evaluate them when the function is called?One answer is probably right there--it would effectively turn every function with default parameters into a closure.  Even if it's all hidden away in the interpreter and not a full-blown closure, the data's got to be stored somewhere.  It'd be slower and use more memory.\", \"This actually has nothing to do with default values, other than that it often comes up as an unexpected behaviour when you write functions with mutable default values.No default values in sight in this code, but you get exactly the same problem.The problem is that foo is modifying a mutable variable passed in from the caller, when the caller doesn't expect this. Code like this would be fine if the function was called something like append_5; then the caller would be calling the function in order to modify the value they pass in, and the behaviour would be expected. But such a function would be very unlikely to take a default argument, and probably wouldn't return the list (since the caller already has a reference to that list; the one it just passed in).Your original foo, with a default argument, shouldn't be modifying a whether it was explicitly passed in or got the default value. Your code should leave mutable arguments alone unless it is clear from the context/name/documentation that the arguments are supposed to be modified. Using mutable values passed in as arguments as local temporaries is an extremely bad idea, whether we're in Python or not and whether there are default arguments involved or not.If you need to destructively manipulate a local temporary in the course of computing something, and you need to start your manipulation from an argument value, you need to make a copy.\", \"Default arguments get evaluated at the time the function is compiled into a function object. When used by the function, multiple times by that function, they are and remain the same object.When they are mutable, when mutated (for example, by adding an element to it) they remain mutated on consecutive calls.They stay mutated because they are the same object each time.Since the list is bound to the function when the function object is compiled and instantiated, this:is almost exactly equivalent to this:Here's a demonstration - you can verify that they are the same object each time they are referenced byexample.pyand running it with python example.py:This order of execution is frequently confusing to new users of Python. If you understand the Python execution model, then it becomes quite expected.But this is why the usual instruction to new users is to create their default arguments like this instead:This uses the None singleton as a sentinel object to tell the function whether or not we've gotten an argument other than the default. If we get no argument, then we actually want to use a new empty list, [], as the default.As the tutorial section on control flow says:If you don\u2019t want the default to be shared between subsequent calls,\\n  you can write the function like this instead:\", 'The shortest answer would probably be \"definition is execution\", therefore the whole argument makes no strict sense. As a more contrived example, you may cite this:Hopefully it\\'s enough to show that not executing the default argument expressions at the execution time of the def statement isn\\'t easy or doesn\\'t make sense, or both.I agree it\\'s a gotcha when you try to use default constructors, though.', \"Already busy topic, but from what I read here, the following helped me realizing how it's working internally:\", \"It's a performance optimization.  As a result of this functionality, which of these two function calls do you think is faster?I'll give you a hint.  Here's the disassembly (see http://docs.python.org/library/dis.html):I doubt the experienced behavior has a practical use (who really used static variables in C, without breeding bugs ?)As you can see, there is a performance benefit when using immutable default arguments.  This can make a difference if it's a frequently called function or the default argument takes a long time to construct.  Also, bear in mind that Python isn't C.  In C you have constants that are pretty much free.  In Python you don't have this benefit.\", 'This behavior is not surprising if you take the following into consideration:The role of (2) has been covered extensively in this thread. (1) is likely the astonishment causing factor, as this behavior is not \"intuitive\" when coming from other languages.(1) is described in the Python tutorial on classes. In an attempt to assign a value to a read-only class attribute:...all variables found outside of the innermost scope are\\nread-only (an attempt to write to such a variable will simply create a\\nnew local variable in the innermost scope, leaving the identically\\nnamed outer variable unchanged).Look back to the original example and consider the above points:Here foo is an object and a is an attribute of foo (available at foo.func_defs[0]). Since a is a list, a is mutable and is thus a read-write attribute of foo. It is initialized to the empty list as specified by the signature when the function is instantiated, and is available for reading and writing as long as the function object exists.Calling foo without overriding a default uses that default\\'s value from foo.func_defs. In this case, foo.func_defs[0] is used for a within function object\\'s code scope. Changes to a change foo.func_defs[0], which is part of the foo object and persists between execution of the code in foo.Now, compare this to the example from the documentation on emulating the default argument behavior of other languages, such that the function signature defaults are used every time the function is executed:Taking (1) and (2) into account, one can see why this accomplishes the desired behavior:', 'A simple workaround using None', \"It may be true that:it is entirely consistent to hold to both of the features above and still make another point:The other answers, or at least some of them either make points 1 and 2 but not 3, or make point 3 and downplay points 1 and 2. But all three are true.It may be true that switching horses in midstream here would be asking for significant breakage, and that there could be more problems created by changing Python to intuitively handle Stefano's opening snippet. And it may be true that someone who knew Python internals well could explain a minefield of consequences. However,The existing behavior is not Pythonic, and Python is successful because very little about the language violates the principle of least astonishment anywhere near this badly. It is a real problem, whether or not it would be wise to uproot it. It is a design flaw. If you understand the language much better by trying to trace out the behavior, I can say that C++ does all of this and more; you learn a lot by navigating, for instance, subtle pointer errors. But this is not Pythonic: people who care about Python enough to persevere in the face of this behavior are people who are drawn to the language because Python has far fewer surprises than other language. Dabblers and the curious become Pythonistas when they are astonished at how little time it takes to get something working--not because of a design fl--I mean, hidden logic puzzle--that cuts against the intuitions of programmers who are drawn to Python because it Just Works.\", 'I am going to demonstrate an alternative structure to pass a default list value to a function (it works equally well with dictionaries).As others have extensively commented, the list parameter is bound to the function when it is defined as opposed to when it is executed.  Because lists and dictionaries are mutable, any alteration to this parameter will affect other calls to this function.  As a result, subsequent calls to the function will receive this shared list which may have been altered by any other calls to the function.  Worse yet, two parameters are using this function\\'s shared parameter at the same time oblivious to the changes made by the other.Wrong Method (probably...):You can verify that they are one and the same object by using id:Per Brett Slatkin\\'s \"Effective Python: 59 Specific Ways to Write Better Python\", Item 20: Use None and Docstrings to specify dynamic default arguments (p. 48)The convention for achieving the desired result in Python is to\\n  provide a default value of None and to document the actual behaviour\\n  in the docstring.This implementation ensures that each call to the function either receives the default list or else the list passed to the function.Preferred Method:There may be legitimate use cases for the \\'Wrong Method\\' whereby the programmer intended the default list parameter to be shared, but this is more likely the exception than the rule.', 'The solutions here are:The second option is nice because users of the function can pass in a callable, which may be already existing (such as a type)', 'You can get round this by replacing the object (and therefore the tie with the scope):Ugly, but it works.', \"When we do this:... we assign the argument a to an unnamed list, if the caller does not pass the value of a.To make things simpler for this discussion, let's temporarily give the unnamed list a name. How about pavlo ?At any time, if the caller doesn't tell us what a is, we reuse pavlo.If pavlo is mutable (modifiable), and foo ends up modifying it, an effect we notice the next time foo is called without specifying a.So this is what you see (Remember, pavlo is initialized to []):Now, pavlo is [5].Calling foo() again modifies pavlo again:Specifying a when calling foo() ensures pavlo is not touched.So, pavlo is still [5, 5].\", \"I sometimes exploit this behavior as an alternative to the following pattern:If singleton is only used by use_singleton, I like the following pattern as a replacement:I've used this for instantiating client classes that access external resources, and also for creating dicts or lists for memoization.Since I don't think this pattern is well known, I do put a short comment in to guard against future misunderstandings.\", 'Every other answer explains why this is actually a nice and desired behavior, or why you shouldn\\'t be needing this anyway. Mine is for those stubborn ones who want to exercise their right to bend the language to their will, not the other way around.We will \"fix\" this behavior with a decorator that will copy the default value instead of reusing the same instance for each positional argument left at its default value.Now let\\'s redefine our function using this decorator:This is particularly neat for functions that take multiple arguments. Compare:withIt\\'s important to note that the above solution breaks if you try to use keyword args, like so:The decorator could be adjusted to allow for that, but we leave this as an exercise for the reader ;)', 'This \"bug\" gave me a lot of overtime work hours! But I\\'m beginning to see a potential use of it (but I would have liked it to be at the execution time, still)I\\'m gonna give you what I see as a useful example.prints the following', \"This is not a design flaw. Anyone who trips over this is doing something wrong.There are 3 cases I see where you might run into this problem:The example in the question could fall into category 1 or 3. It's odd that it both modifies the passed list and returns it; you should pick one or the other.\", 'Just change the function to be:', 'TLDR: Define-time defaults are consistent and strictly more expressive.Defining a function affects two scopes: the defining scope containing the function, and the execution  scope contained by the function. While it is pretty clear how blocks map to scopes, the question is where def <name>(<args=defaults>): belongs to:The def name part must evaluate in the defining scope - we want name to be available there, after all. Evaluating the function only inside itself would make it inaccessible.Since parameter is a constant name, we can \"evaluate\" it at the same time as def name. This also has the advantage it produces the function with a known signature as name(parameter=...):, instead of a bare name(...):.Now, when to evaluate default?Consistency already says \"at definition\": everything else of def <name>(<args=defaults>): is best evaluated at definition as well. Delaying parts of it would be the astonishing choice.The two choices are not equivalent, either: If default is evaluated at definition time, it can still affect execution time. If default is evaluated at execution time, it cannot affect definition time. Choosing \"at definition\" allows expressing both cases, while choosing \"at execution\" can express only one:', 'I think the answer to this question lies in how python pass data to parameter (pass by value or by reference), not mutability or how python handle the \"def\" statement.A brief introduction. First, there are two type of data types in python, one is simple elementary data type, like numbers, and another data type is objects. Second, when passing data to parameters, python pass elementary data type by value, i.e., make a local copy of the value to a local variable, but pass object by reference, i.e., pointers to the object.Admitting the above two points, let\\'s explain what happened to the python code. It\\'s only because of passing by reference for objects, but has nothing to do with mutable/immutable, or arguably the fact that \"def\" statement is executed only once when it is defined.[] is an object, so python pass the reference of [] to a, i.e., a is only a pointer to [] which lies in memory as an object. There is only one copy of [] with, however, many references to it. For the first foo(), the list [] is changed to 1 by append method. But Note that there is only one copy of the list object and this object now becomes 1. When running the second foo(), what effbot webpage says (items is not evaluated any more) is wrong. a is evaluated to be the list object, although now the content of the object is 1. This is the effect of passing by reference! The result of foo(3) can be easily derived in the same way.To further validate my answer, let\\'s take a look at two additional codes.====== No. 2 ========[] is an object, so is None (the former is mutable while the latter is immutable. But the mutability has nothing to do with the question). None is somewhere in the space but we know it\\'s there and there is only one copy of None there. So every time foo is invoked, items is evaluated (as opposed to some answer that it is only evaluated once) to be None, to be clear, the reference (or the address) of None. Then in the foo, item is changed to [], i.e., points to another object which has a different address.====== No. 3 =======The invocation of foo(1) make items point to a list object [] with an address, say, 11111111. the content of the list is changed to 1 in the foo function in the sequel, but the address is not changed, still 11111111. Then foo(2,[]) is coming. Although the [] in foo(2,[]) has the same content as the default parameter [] when calling foo(1), their address are different! Since we provide the parameter explicitly, items has to take the address of this new [], say 2222222, and return it after making some change. Now foo(3) is executed. since only x is provided, items has to take its default value again. What\\'s the default value? It is set when defining the foo function: the list object located in 11111111. So the items is evaluated to be the address 11111111 having an element 1. The list located at 2222222 also contains one element 2, but it is not pointed by items any more. Consequently, An append of 3 will make items [1,3].From the above explanations, we can see that the effbot webpage recommended in the accepted answer failed to give a relevant answer to this question. What is more, I think a point in the effbot webpage is wrong. I think the code regarding the UI.Button is correct:Each button can hold a distinct callback function which will display different value of i. I can provide an example to show this:If we execute x[7]() we\\'ll get 7 as expected, and x[9]() will gives 9, another value of i.']",
            "url": "https://stackoverflow.com/questions/1132941"
        },
        {
            "tag": "python",
            "question": [
                "How do I concatenate two lists in Python?"
            ],
            "votes": "3245",
            "answer": "['Use the + operator to combine the lists:Output:', \"Python >= 3.5 alternative: [*l1, *l2]Another alternative has been introduced via the acceptance of PEP 448 which deserves mentioning.The PEP, titled Additional Unpacking Generalizations, generally reduced some syntactic restrictions when using the starred * expression in Python; with it, joining two lists (applies to any iterable) can now also be done with:This functionality was defined for Python 3.5, but it hasn't been backported to previous versions in the 3.x family. In unsupported versions a SyntaxError is going to be raised.As with the other approaches, this too creates as shallow copy of the elements in the corresponding lists.The upside to this approach is that you really don't need lists in order to perform it; anything that is iterable will do. As stated in the PEP:This is also useful as a more readable way of summing iterables into a\\nlist, such as my_list + list(my_tuple) + list(my_range) which is now\\nequivalent to just [*my_list, *my_tuple, *my_range].So while addition with + would raise a TypeError due to type mismatch:The following won't:because it will first unpack the contents of the iterables and then simply create a list from the contents.\", \"It's also possible to create a generator that simply iterates over the items in both lists using itertools.chain(). This allows you to chain lists (or any iterable) together for processing without copying the items to a new list:\", 'You could also use the list.extend() method in order to add a list to the end of another one:If you want to keep the original list intact, you can create a new list object, and extend both lists to it:', \"As of 3.9, these are the most popular stdlib methods for concatenating two (or more) lists in Python.FootnotesThis is a slick solution because of its succinctness. But sum performs concatenation in a pairwise fashion, which means this is a\\nquadratic operation as memory has to be allocated for each step. DO\\nNOT USE if your lists are large.See chain\\nand\\nchain.from_iterable\\nfrom the docs. You will need to from itertools import chain first.\\nConcatenation is linear in memory, so this is the best in terms of\\nperformance and version compatibility. chain.from_iterable was introduced in 2.6.This method uses Additional Unpacking Generalizations (PEP 448), but cannot\\ngeneralize to N lists unless you manually unpack each one yourself.a += b and a.extend(b) are more or less equivalent for all practical purposes. += when called on a list will internally call\\nlist.__iadd__, which extends the first list by the second.2-List Concatenation1There's not much difference between these methods but that makes sense given they all have the same order of complexity (linear). There's no particular reason to prefer one over the other except as a matter of style.N-List ConcatenationPlots have been generated using the perfplot module. Code, for your reference.1. The iadd (+=) and extend methods operate in-place, so a copy has to be generated each time before testing. To keep things fair, all methods have a pre-copy step for the left-hand list which can be ignored.DO NOT USE THE DUNDER METHOD list.__add__ directly in any way, shape or form. In fact, stay clear of dunder methods, and use the operators and operator functions like they were designed for. Python has careful semantics baked into these which are more complicated than just calling the dunder directly. Here is an example. So, to summarise, a.__add__(b) => BAD; a + b => GOOD.Some answers here offer reduce(operator.add, [a, b]) for pairwise concatenation -- this is the same as sum([a, b], []) only more wordy.Any method that uses set will drop duplicates and lose ordering. Use with caution.for i in b: a.append(i) is more wordy, and slower than a.extend(b), which is single function call and more idiomatic. append is slower because of the semantics with which memory is allocated and grown for lists. See here for a similar discussion.heapq.merge will work, but its use case is for merging sorted lists in linear time. Using it in any other situation is an anti-pattern.yielding list elements from a function is an acceptable method, but chain does this faster and better (it has a code path in C, so it is fast).operator.add(a, b) is an acceptable functional equivalent to a + b. It's use cases are mainly for dynamic method dispatch. Otherwise, prefer a + b which is shorter and more readable, in my opinion. YMMV.\", 'You can use sets to obtain merged list of unique values', 'This is quite simple, and I think it was even shown in the tutorial:', \"This question directly asks about joining two lists. However it's pretty high in search even when you are looking for a way of joining many lists (including the case when you joining zero lists).I think the best option is to use list comprehensions:You can create generators as well:Old AnswerConsider this more generic approach:Will output:Note, this also works correctly when a is [] or [[1,2,3]].However, this can be done more efficiently with itertools:If you don't need a list, but just an iterable, omit list().UpdateAlternative suggested by Patrick Collins in the comments could also work for you:\", 'You could simply use the + or += operator as follows:Or:Also, if you want the values in the merged list to be unique you can do:', \"It's worth noting that the itertools.chain function accepts variable number of arguments:If an iterable (tuple, list, generator, etc.) is the input, the from_iterable class method may be used:\", 'For cases with a low number of lists you can simply add the lists together or use in-place unpacking (available in Python-3.5+):As a more general way for cases with more number of lists you can use chain.from_iterable()1 function from itertools module. Also, based on this answer this function is the best; or at least a very good way for flatting a nested list as well.', 'With Python 3.3+ you can use yield from:Or, if you want to support an arbitrary number of iterators:', 'If you want to merge the two lists in sorted form, you can use the merge function from the heapq library.', \"If you can't use the plus operator (+),  you can use the operator import:Alternatively, you could also use the __add__ dunder function:\", 'If you need to merge two ordered lists with complicated sorting rules, you might have to roll it yourself like in the following code (using a simple sorting rule for readability :-) ).', 'If you are using NumPy, you can concatenate two arrays of compatible dimensions with this command:', \"Use a simple list comprehension:It has all the advantages of the newest approach of using Additional Unpacking Generalizations - i.e. you can concatenate an arbitrary number of different iterables (for example, lists, tuples, ranges, and generators) that way - and it's not limited to Python 3.5 or later.\", 'Another way:', 'The above code does not preserve order and removes duplicates from each list (but not from the concatenated list).', 'As already pointed out by many, itertools.chain() is the way to go if one needs to apply exactly the same treatment to both lists. In my case, I had a label and a flag which were different from one list to the other, so I needed something slightly more complex. As it turns out, behind the scenes itertools.chain() simply does the following:(see https://docs.python.org/2/library/itertools.html), so I took inspiration from here and wrote something along these lines:The main points to understand here are that lists are just a special case of iterable, which are objects like any other; and that for ... in loops in python can work with tuple variables, so it is simple to loop on multiple variables at the same time.', 'You could use the append() method defined on list objects:', 'In the above code, the \"+\" operator is used to concatenate the two lists into a single list.', \"I recommend three methods to concatenate the list, but the first method is most recommended,In the second method, I assign newlist to a copy of the listone, because I don't want to change listone.This is not a good way to concatenate lists because we are using a for loop to concatenate the lists. So time complexity is much higher than with the other two methods.\", 'The most common method used to concatenate lists are the plus operator and the built-in method append, for example:For most of the cases, this will work, but the append function will not extend a list if one was added. Because that is not expected, you can use another method called extend. It should work with structures:', 'A really concise way to combine a list of lists iswhich gives us', 'So there are two easy ways.Example:Example:Thus we see that out of two of most popular methods, extend is efficient.', 'You could also just use sum.This works for any length and any element type of list:The reason I add [], is because the start argument is set to 0 by default, so it loops through the list and adds to start, but 0 + [1, 2, 3] would give an error, so if we set the start to []. It would add to [], and [] + [1, 2, 3] would work as expected.', 'I assume you want one of the two methods:Keep duplicate elementsIt is very easy. Just concatenate like a string:Next, if you want to eliminate duplicate elements', 'The solutions provided are for a single list. In case there are lists within a list and the merging of corresponding lists is required, the \"+\" operation through a for loop does the work.Output: [[1, 2, 3, 0, 1, 2], [4, 5, 6, 7, 8, 9]]', 'All the possible ways to join lists that I could findOutput']",
            "url": "https://stackoverflow.com/questions/1720421"
        },
        {
            "tag": "python",
            "question": [
                "How do I select rows from a DataFrame based on column values?"
            ],
            "votes": "3234",
            "answer": "[\"To select rows whose column value equals a scalar, some_value, use ==:To select rows whose column value is in an iterable, some_values, use isin:Combine multiple conditions with &:Note the parentheses. Due to Python's operator precedence rules, & binds more tightly than <= and >=. Thus, the parentheses in the last example are necessary. Without the parenthesesis parsed aswhich results in a Truth value of a Series is ambiguous error.To select rows whose column value does not equal some_value, use !=:isin returns a boolean Series, so to select rows whose value is not in some_values, negate the boolean Series using ~:For example,yieldsIf you have multiple values you want to include, put them in a\\nlist (or more generally, any iterable) and use isin:yieldsNote, however, that if you wish to do this many times, it is more efficient to\\nmake an index first, and then use df.loc:yieldsor, to include multiple values from the index use df.index.isin:yields\", \"There are several ways to select rows from a Pandas dataframe:Below I show you examples of each, with advice when to use certain techniques. Assume our criterion is column 'A' == 'foo'(Note on performance: For each base type, we can keep things simple by using the Pandas API or we can venture outside the API, usually into NumPy, and speed things up.)SetupThe first thing we'll need is to identify a condition that will act as our criterion for selecting rows. We'll start with the OP's case column_name == some_value, and include some other common use cases.Borrowing from @unutbu:... Boolean indexing requires finding the true value of each row's 'A' column being equal to 'foo', then using those truth values to identify which rows to keep.  Typically, we'd name this series, an array of truth values, mask.  We'll do so here as well.We can then use this mask to slice or index the data frameThis is one of the simplest ways to accomplish this task and if performance or intuitiveness isn't an issue, this should be your chosen method.  However, if performance is a concern, then you might want to consider an alternative way of creating the mask.Positional indexing (df.iloc[...]) has its use cases, but this isn't one of them.  In order to identify where to slice, we first need to perform the same boolean analysis we did above.  This leaves us performing one extra step to accomplish the same task.Label indexing can be very handy, but in this case, we are again doing more work for no benefitpd.DataFrame.query is a very elegant/intuitive way to perform this task, but is often slower. However, if you pay attention to the timings below, for large data, the query is very efficient. More so than the standard approach and of similar magnitude as my best suggestion.My preference is to use the Boolean maskActual improvements can be made by modifying how we create our Boolean mask.mask alternative 1\\nUse the underlying NumPy array and forgo the overhead of creating another pd.SeriesI'll show more complete time tests at the end, but just take a look at the performance gains we get using the sample data frame.  First, we look at the difference in creating the maskEvaluating the mask with the NumPy array is ~ 30 times faster.  This is partly due to NumPy evaluation often being faster. It is also partly due to the lack of overhead necessary to build an index and a corresponding pd.Series object.Next, we'll look at the timing for slicing with one mask versus the other.The performance gains aren't as pronounced.  We'll see if this holds up over more robust testing.mask alternative 2\\nWe could have reconstructed the data frame as well.  There is a big caveat when reconstructing a dataframe\u2014you must take care of the dtypes when doing so!Instead of df[mask] we will do thisIf the data frame is of mixed type, which our example is, then when we get df.values the resulting array is of dtype object and consequently, all columns of the new data frame will be of dtype object.  Thus requiring the astype(df.dtypes) and killing any potential performance gains.However, if the data frame is not of mixed type, this is a very useful way to do it.GivenVersusWe cut the time in half.mask alternative 3@unutbu also shows us how to use pd.Series.isin to account for each element of df['A'] being in a set of values.  This evaluates to the same thing if our set of values is a set of one value, namely 'foo'.  But it also generalizes to include larger sets of values if needed.  Turns out, this is still pretty fast even though it is a more general solution.  The only real loss is in intuitiveness for those not familiar with the concept.However, as before, we can utilize NumPy to improve performance while sacrificing virtually nothing. We'll use np.in1dTimingI'll include other concepts mentioned in other posts as well for reference.Code BelowEach column in this table represents a different length data frame over which we test each function. Each column shows relative time taken, with the fastest function given a base index of 1.0.You'll notice that the fastest times seem to be shared between mask_with_values and mask_with_in1d.FunctionsTestingSpecial TimingLooking at the special case when we have a single non-object dtype for the entire data frame.Code BelowTurns out, reconstruction isn't worth it past a few hundred rows.FunctionsTesting\", \"The Pandas equivalent toisMultiple conditions:orIn the above code it is the line df[df.foo == 222] that gives the rows based on the column value, 222 in this case.Multiple conditions are also possible:But at that point I would recommend using the query function, since it's less verbose and yields the same result:\", \"I find the syntax of the previous answers to be redundant and difficult to remember. Pandas introduced the query() method in v0.13 and I much prefer it. For your question, you could do df.query('col == val').Reproduced from The query() Method (Experimental):You can also access variables in the environment by prepending an @.\", 'Since pandas >= 0.25.0 we can use the query method to filter dataframes with pandas methods and even column names which have spaces. Normally the spaces in column names would give an error, but now we can solve that using a backtick (`) - see GitHub:Using .query with method str.endswith:OutputAlso we can use local variables by prefixing it with an @ in our query:Output', 'For selecting only specific columns out of multiple columns for a given value in Pandas:Options loc:or query:', 'In newer versions of Pandas, inspired by the documentation (Viewing data):Combine multiple conditions by putting the clause in parentheses, (), and combining them with & and | (and/or). Like this:Other filters', \"Faster results can be achieved using numpy.where.For example, with unubtu's setup -Timing comparisons:\", 'Here is a simple example', \"To add: You can also do df.groupby('column_name').get_group('column_desired_value').reset_index() to make a new data frame with specified column having a particular value. E.g.,Running this gives:\", 'You can also use .apply:It actually works row-wise (i.e., applies the function to each row).The output isThe results is the same as using as mentioned by @unutbu', \"If you want to make query to your dataframe repeatedly and speed is important to you, the best thing is to convert your dataframe to dictionary and then by doing this you can make query thousands of times faster.After make my_dict dictionary you can go through:If you have duplicated values in column_name you can't make a dictionary. but you can use:\", 'With DuckDB we can query pandas DataFrames with SQL statements, in a highly performant way.Since the question is How do I select rows from a DataFrame based on column values?, and the example in the question is a SQL query, this answer looks logical in this topic.Example:', \"If the column name used to filter your dataframe comes from a local variable, f-strings may be useful. For example,In fact, f-strings can be used for the query variable as well (except for datetime):The pandas documentation recommends installing numexpr to speed up numeric calculation when using query(). Use pip install numexpr (or conda, sudo etc. depending on your environment) to install it.For larger dataframes (where performance actually matters), df.query() with numexpr engine performs much faster than df[mask]. In particular, it performs better for the following cases.Logical and/or comparison operators on columns of stringsIf a column of strings are compared to some other string(s) and matching rows are to be selected, even for a single comparison operation, query() performs faster than df[mask]. For example, for a dataframe with 80k rows, it's 30% faster1 and for a dataframe with 800k rows, it's 60% faster.2This gap increases as the number of operations increases (if 4 comparisons are chained df.query() is 2-2.3 times faster than df[mask])1,2 and/or the dataframe length increases.2Multiple operations on numeric columnsIf multiple arithmetic, logical or comparison operations need to be computed to create a boolean mask to filter df, query() performs faster. For example, for a frame with 80k rows, it's 20% faster1 and for a frame with 800k rows, it's 2 times faster.2This gap in performance increases as the number of operations increases and/or the dataframe length increases.2The following plot shows how the methods perform as the dataframe length increases.3Numexpr currently supports only logical (&, |, ~), comparison (==, >, <, >=, <=, !=) and basic arithmetic operators (+, -, *, /, **, %).For example, it doesn't support integer division (//). However, calling the equivalent pandas method (floordiv()) works.1 Benchmark code using a frame with 80k rows2 Benchmark code using a frame with 800k rows3: Code used to produce the performance graphs of the two methods for strings and numbers.\", 'You can use loc (square brackets) with a function:Output:orOutput:The advantage of this method is that you can chain selection with previous operations. For example:vsOutput:', 'Great answers. Only, when the size of the dataframe approaches million rows, many of the methods tend to take ages when using df[df[\\'col\\']==val]. I wanted to have all possible values of \"another_column\" that correspond to specific values in \"some_column\" (in this case in a dictionary). This worked and fast.']",
            "url": "https://stackoverflow.com/questions/17071871"
        },
        {
            "tag": "python",
            "question": [
                "How do I check if a list is empty?"
            ],
            "votes": "3229",
            "answer": "['Using the implicit booleanness of the empty list is quite Pythonic.', 'The Pythonic way to do it is from the PEP 8 style guide.For sequences, (strings, lists, tuples), use the fact that empty sequences are false:', \"I prefer it explicitly:This way it's 100% clear that li is a sequence (list) and we want to test its size. My problem with if not li: ... is that it gives the false impression that li is a boolean variable.\", 'This is the first google hit for \"python test empty array\" and similar queries, and other people are generalizing the question beyond just lists, so here\\'s a caveat for a different type of sequence that a lot of people use.You need to be careful with NumPy arrays, because other methods that work fine for lists or other standard containers fail for NumPy arrays.  I explain why below, but in short, the preferred method is to use size.The \"pythonic\" way fails with NumPy arrays because NumPy tries to cast the array to an array of bools, and if x tries to evaluate all of those bools at once for some kind of aggregate truth value.  But this doesn\\'t make any sense, so you get a ValueError:But at least the case above tells you that it failed.  If you happen to have a NumPy array with exactly one element, the if statement will \"work\", in the sense that you don\\'t get an error.  However, if that one element happens to be 0 (or 0.0, or False, ...), the if statement will incorrectly result in False:But clearly x exists and is not empty!  This result is not what you wanted.For example,returns 1, even though the array has zero elements.As explained in the SciPy FAQ, the correct method in all cases where you know you have a NumPy array is to use if x.size:If you\\'re not sure whether it might be a list, a NumPy array, or something else, you could combine this approach with the answer @dubiousjim gives to make sure the right test is used for each type.  Not very \"pythonic\", but it turns out that NumPy intentionally broke pythonicity in at least this sense.If you need to do more than just check if the input is empty, and you\\'re using other NumPy features like indexing or math operations, it\\'s probably more efficient (and certainly more common) to force the input to be a NumPy array.  There are a few nice functions for doing this quickly \u2014\\xa0most importantly numpy.asarray.  This takes your input, does nothing if it\\'s already an array, or wraps your input into an array if it\\'s a list, tuple, etc., and optionally converts it to your chosen dtype.  So it\\'s very quick whenever it can be, and it ensures that you just get to assume the input is a NumPy array.  We usually even just use the same name, as the conversion to an array won\\'t make it back outside of the current scope:This will make the x.size check work in all cases I see on this page.', 'For example, if passed the following:How do I check to see if a is empty?Place the list in a boolean context (for example, with an if or while statement). It will test False if it is empty, and True otherwise. For example:PEP 8, the official Python style guide for Python code in Python\\'s standard library, asserts:For sequences, (strings, lists, tuples), use the fact that empty sequences are false.We should expect that standard library code should be as performant and correct as possible. But why is that the case, and why do we need this guidance?I frequently see code like this from experienced programmers new to Python:And users of lazy languages may be tempted to do this:These are correct in their respective other languages. And this is even semantically correct in Python.But we consider it un-Pythonic because Python supports these semantics directly in the list object\\'s interface via boolean coercion.From the docs (and note specifically the inclusion of the empty list, []):By default, an object is considered true unless its class defines\\n  either a __bool__() method that returns False or a __len__() method\\n  that returns zero, when called with the object. Here are most of the built-in objects considered false:And the datamodel documentation:object.__bool__(self)Called to implement truth value testing and the built-in operation bool(); should return False or True. When this method is not defined,\\n  __len__() is called, if it is defined, and the object is considered true if its result is nonzero. If a class defines neither __len__()\\n  nor __bool__(), all its instances are considered true.andobject.__len__(self)Called to implement the built-in function len(). Should return the length of the object, an integer >= 0. Also, an object that doesn\u2019t define a __bool__() method and whose __len__() method returns zero is considered to be false in a Boolean context.So instead of this:or this:Do this:Does it pay off? (Note that less time to perform an equivalent operation is better:)For scale, here\\'s the cost of calling the function and constructing and returning an empty list, which you might subtract from the costs of the emptiness checks used above:We see that either checking for length with the builtin function len compared to 0 or checking against an empty list is much less performant than using the builtin syntax of the language as documented.Why?For the len(a) == 0 check:First Python has to check the globals to see if len is shadowed.Then it must call the function, load 0, and do the equality comparison in Python (instead of with C):And for the [] == [] it has to build an unnecessary list and then, again, do the comparison operation in Python\\'s virtual machine (as opposed to C)The \"Pythonic\" way is a much simpler and faster check since the length of the list is cached in the object instance header:PyVarObjectThis is an extension of PyObject that adds the ob_size field. This is only used for objects that have some notion of length. This type does not often appear in the Python/C API. It corresponds to the fields defined by the expansion of the PyObject_VAR_HEAD macro.From the c source in Include/listobject.h:I would point out that this is also true for the non-empty case though its pretty ugly as with l=[] then %timeit len(l) != 0 90.6 ns \u00b1 8.3 ns, %timeit l != [] 55.6 ns \u00b1 3.09, %timeit not not l 38.5 ns \u00b1 0.372. But there is no way anyone is going to enjoy not not l despite triple the speed. It looks ridiculous. But the speed wins out\\n  I suppose the problem is testing with timeit since just if l: is sufficient but surprisingly %timeit bool(l) yields 101 ns \u00b1 2.64 ns. Interesting there is no way to coerce to bool without this penalty. %timeit l is useless since no conversion would occur.IPython magic, %timeit, is not entirely useless here:We can see there\\'s a bit of linear cost for each additional not here. We want to see the costs, ceteris paribus, that is, all else equal - where all else is minimized as far as possible:Now let\\'s look at the case for an unempty list:What we can see here is that it makes little difference whether you pass in an actual bool to the condition check or the list itself, and if anything, giving the list, as is, is faster.Python is written in C; it uses its logic at the C level. Anything you write in Python will be slower. And it will likely be orders of magnitude slower unless you\\'re using the mechanisms built into Python directly.', \"An empty list is itself considered false in true value testing (see python documentation):To Daren Thomas's answer:EDIT: Another point against testing\\nthe empty list as False: What about\\npolymorphism? You shouldn't depend on\\na list being a list. It should just\\nquack like a duck - how are you going\\nto get your duckCollection to quack\\n''False'' when it has no elements?Your duckCollection should implement __nonzero__ or __len__ so the if a: will work without problems.\", 'Patrick\\'s (accepted) answer is right: if not a: is the right way to do it. Harley Holcombe\\'s answer is right that this is in the PEP 8 style guide. But what none of the answers explain is why it\\'s a good idea to follow the idiom\u2014even if you personally find it\\'s not explicit enough or confusing to Ruby users or whatever.Python code, and the Python community, has very strong idioms. Following those idioms makes your code easier to read for anyone experienced in Python. And when you violate those idioms, that\\'s a strong signal.It\\'s true that if not a: doesn\\'t distinguish empty lists from None, or numeric 0, or empty tuples, or empty user-created collection types, or empty user-created not-quite-collection types, or single-element NumPy array acting as scalars with falsey values, etc. And sometimes it\\'s important to be explicit about that. And in that case, you know what you want to be explicit about, so you can test for exactly that. For example, if not a and a is not None: means \"anything falsey except None\", while if len(a) != 0: means \"only empty sequences\u2014and anything besides a sequence is an error here\", and so on. Besides testing for exactly what you want to test, this also signals to the reader that this test is important.But when you don\\'t have anything to be explicit about, anything other than if not a: is misleading the reader. You\\'re signaling something as important when it isn\\'t. (You may also be making the code less flexible, or slower, or whatever, but that\\'s all less important.) And if you habitually mislead the reader like this, then when you do need to make a distinction, it\\'s going to pass unnoticed because you\\'ve been \"crying wolf\" all over your code.', 'No one seems to have addressed questioning your need to test the list in the first place.  Because you provided no additional context, I can imagine that you may not need to do this check in the first place, but are unfamiliar with list processing in Python.I would argue that the most Pythonic way is to not check at all, but rather to just process the list.  That way it will do the right thing whether empty or full.This has the benefit of handling any contents of a, while not requiring a specific check for emptiness.  If a is empty, the dependent block will not execute and the interpreter will fall through to the next line.If you do actually need to check the array for emptiness:is sufficient.', 'len() is an O(1) operation for Python lists, strings, dicts, and sets. Python internally keeps track of the number of elements in these containers.JavaScript has a similar notion of truthy/falsy.', 'I had written:which was voted -1. I\\'m not sure if that\\'s because readers objected to the strategy or thought the answer wasn\\'t helpful as presented. I\\'ll pretend it was the latter, since---whatever counts as \"pythonic\"---this is the correct strategy. Unless you\\'ve already ruled out, or are prepared to handle cases where a is, for example, False, you need a test more restrictive than just if not a:. You could use something like this:the first test is in response to @Mike\\'s answer, above. The third line could also be replaced with:if you only want to accept instances of particular types (and their subtypes), or with:You can get away without the explicit type check, but only if the surrounding context already assures you that a is a value of the types you\\'re prepared to handle, or if you\\'re sure that types you\\'re not prepared to handle are going to raise errors (e.g., a TypeError if you call len on a value for which it\\'s undefined) that you\\'re prepared to handle. In general, the \"pythonic\" conventions seem to go this last way. Squeeze it like a duck and let it raise a DuckError if it doesn\\'t know how to quack. You still have to think about what type assumptions you\\'re making, though, and whether the cases you\\'re not prepared to handle properly really are going to error out in the right places. The Numpy arrays are a good example where just blindly relying on len or the boolean typecast may not do precisely what you\\'re expecting.', 'From documentation on truth value testing:All values other than what is listed here are considered TrueAs can be seen, empty list [] is falsy, so doing what would be done to a boolean value sounds most efficient:', 'I prefer the following:', \"Here are a few ways you can check if a list is empty:1) The pretty simple pythonic way:In Python, empty containers such as lists,tuples,sets,dicts,variables etc are seen as False. One could simply treat the list as a predicate (returning a Boolean value). And  a True value would indicate that it's non-empty.2) A much explicit way: using the len() to find the length and check if it equals to 0:3) Or comparing it to an anonymous empty list:4) Another yet silly way to do is using exception and iter():\", 'Method 1 (preferred):Method 2:Method 3:', \"You can even try using bool() like this. Although it is less readable surely it's a concise way to perform this.I love this way for the checking list is empty or not.Very handy and useful.\", \"It is sometimes good to test for None and for emptiness separately as those are two different states. The code above produces the following output:Although it's worth nothing that None is falsy. So if you don't want to separate test for None-ness, you don't have to do that.produces expected\", \"To check whether a list is empty or not you can use two following ways. But remember, we should avoid the way of explicitly checking for a type of sequence (it's a less Pythonic way):The second way is a more Pythonic one. This method is an implicit way of checking and much more preferable than the previous one.\", 'Many answers have been given, and a lot of them are pretty good. I just wanted to add that the checkwill also pass for None and other types of empty structures. If you truly want to check for an empty list, you can do this:', 'If you want to check if a list is empty:If you want to check whether all the values in list is empty. However it will be True for an empty list:If you want to use both cases together:Now you can use:', 'a little more practical:and the shortest version:', 'We could use a simple if else:', \"Being inspired by dubiousjim's solution, I propose to use an additional general check of whether is it something iterable:Note: a string is considered to be iterable\u2014add and not isinstance(a,(str,unicode)) if you want the empty string to be excludedTest:\", 'Simply use is_empty() or make function like:-It can be used for any data_structure like a list,tuples, dictionary and many more. By these, you can call it many times using just is_empty(any_structure).', 'Simple way is checking the length is equal zero.', \"From python3 onwards you can useto check if the list is emptyEDIT : This works with python2.7 too..I am not sure why there are so many complicated answers.\\nIt's pretty clear and straightforward\", 'The truth value of an empty list is False whereas for a non-empty list it is True.', 'What brought me here is a special use-case: I actually wanted a function to tell me if a list is empty or not. I wanted to avoid writing my own function or using a lambda-expression here (because it seemed like it should be simple enough):And, of course, there is a very natural way to do it:Of course, do not use bool in if (i.e., if bool(L):) because it\\'s implied. But, for the cases when \"is not empty\" is explicitly needed as a function, bool is the best choice.']",
            "url": "https://stackoverflow.com/questions/53513"
        },
        {
            "tag": "python",
            "question": [
                "What does ** (double star/asterisk) and * (star/asterisk) do for parameters?"
            ],
            "votes": "3199",
            "answer": "['The *args and **kwargs is a common idiom to allow arbitrary number of arguments to functions as described in the section more on defining functions in the Python documentation.The *args will give you all function parameters as a tuple:The **kwargs will give you all\\nkeyword arguments except for those corresponding to a formal parameter as a dictionary.Both idioms can be mixed with normal arguments to allow a set of fixed and some variable arguments:It is also possible to use this the other way around:Another usage of the *l idiom is to unpack argument lists when calling a function.In Python 3 it is possible to use *l on the left side of an assignment (Extended Iterable Unpacking), though it gives a list instead of a tuple in this context:Also Python 3 adds new semantic (refer PEP 3102):For example the following works in python 3 but not python 2:Such function accepts only 3 positional arguments, and everything after * can only be passed as keyword arguments.', \"It's also worth noting that you can use * and ** when calling functions as well. This is a shortcut that allows you to pass multiple arguments to a function directly using either a list/tuple or a dictionary. For example, if you have the following function:You can do things like:Note: The keys in mydict have to be named exactly like the parameters of function foo. Otherwise it will throw a TypeError:\", \"The single * means that there can be any number of extra positional arguments. foo() can be invoked like foo(1,2,3,4,5). In the body of foo() param2 is a sequence containing 2-5.The double ** means there can be any number of extra named parameters. bar() can be invoked like bar(1, a=2, b=3). In the body of bar() param2 is a dictionary containing {'a':2, 'b':3 }With the following code:the output is\", 'They allow for functions to be defined to accept and for users to pass any number of arguments, positional (*) and keyword (**).*args allows for any number of optional positional arguments (parameters), which will be assigned to a tuple named args.**kwargs allows for any number of optional keyword arguments (parameters), which will be in a dict named kwargs.You can (and should) choose any appropriate name, but if the intention is for the arguments to be of non-specific semantics, args and kwargs are standard names.You can also use *args and **kwargs to pass in parameters from lists (or any iterable) and dicts (or any mapping), respectively.The function recieving the parameters does not have to know that they are being expanded.For example, Python 2\\'s xrange does not explicitly expect *args, but since it takes 3 integers as arguments:As another example, we can use dict expansion in str.format:You can have keyword only arguments after the *args - for example, here, kwarg2 must be given as a keyword argument - not positionally:Usage:Also, * can be used by itself  to indicate that keyword only arguments follow, without allowing for unlimited positional arguments.Here, kwarg2 again must be an explicitly named, keyword argument:And we can no longer accept unlimited positional arguments because we don\\'t have *args*:Again, more simply, here we require kwarg to be given by name, not positionally:In this example, we see that if we try to pass kwarg positionally, we get an error:We must explicitly pass the kwarg parameter as a keyword argument.*args (typically said \"star-args\") and **kwargs (stars can be implied by saying \"kwargs\", but be explicit with \"double-star kwargs\") are common idioms of Python for using the * and ** notation. These specific variable names aren\\'t required (e.g. you could use *foos and **bars), but a departure from convention is likely to enrage your fellow Python coders.We typically use these when we don\\'t know what our function is going to receive or how many arguments we may be passing, and sometimes even when naming every variable separately would get very messy and redundant (but this is a case where usually explicit is better than implicit).Example 1The following function describes how they can be used, and demonstrates behavior. Note the named b argument will be consumed by the second positional argument before :We can check the online help for the function\\'s signature, with help(foo), which tells usLet\\'s call this function with foo(1, 2, 3, 4, e=5, f=6, g=7)which prints:Example 2We can also call it using another function, into which we just provide a:bar(100) prints:Example 3: practical usage in decoratorsOK, so maybe we\\'re not seeing the utility yet. So imagine you have several functions with redundant code before and/or after the differentiating code. The following named functions are just pseudo-code for illustrative purposes.We might be able to handle this differently, but we can certainly extract the redundancy with a decorator, and so our below example demonstrates how *args and **kwargs can be very useful:And now every wrapped function can be written much more succinctly, as we\\'ve factored out the redundancy:And by factoring out our code, which *args and **kwargs allows us to do, we reduce lines of code, improve readability and maintainability, and have sole canonical locations for the logic in our program. If we need to change any part of this structure, we have one place in which to make each change.', \"Let us first understand what are positional arguments and keyword arguments.\\nBelow is an example of function definition with Positional arguments.So this is a function definition with positional arguments.\\nYou can call it with keyword/named arguments as well:Now let us study an example of function definition with keyword arguments:You can call this function with positional arguments as well:So we now know function definitions with positional as well as keyword arguments.Now let us study the '*' operator and '**' operator.Please note these operators can be used in 2 areas:a) function callb) function definitionThe use of '*' operator and '**' operator in function call.Let us get straight to an example and then discuss it.So rememberwhen the '*' or '**' operator is used in a function call -'*' operator unpacks data structure such as a list or tuple  into arguments needed by function definition.'**' operator unpacks a dictionary into arguments needed by function definition.Now let us study the '*' operator use in function definition.\\nExample:In function definition the '*' operator packs the received arguments into a tuple.Now let us see an example of '**' used in function definition:In function definition The '**' operator packs the received arguments into a dictionary.So remember:In a function call the '*' unpacks data structure of tuple or list into positional or keyword arguments to be received by function definition.In a function call the '**' unpacks data structure of dictionary into positional or keyword arguments to be received by function definition.In a function definition the '*' packs positional arguments into a tuple.In a function definition the '**' packs keyword arguments into a dictionary.\", \"This table is handy for using * and ** in function construction and function call:This really just serves to summarize Lorin Hochstein's answer but I find it helpful.Relatedly: uses for the star/splat operators have been expanded in Python 3\", '* and ** have special usage in the function argument list. *\\nimplies that the argument is a list and ** implies that the argument\\nis a dictionary. This allows functions to take arbitrary number of\\narguments', 'TL;DRBelow are 6 different use cases for * and ** in python programming:BONUS: From python 3.8 onward, one can use / in function definition to enforce  positional only parameters. In the following example, parameters a and b are positional-only, while c or d can be positional or keyword, and e or f are required to be keywords:BONUS 2: THIS ANSWER to the same question also brings a new perspective, where it shares what does * and ** means in a function call, functions signature, for loops, etc.', 'Let us show this by defining a function that takes two normal variables x, y, and can accept more arguments as myArgs, and can accept even more arguments as myKW. Later, we will show how to feed y using myArgDict.', 'From the Python documentation:If there are more positional arguments than there are formal parameter slots, a TypeError exception is raised, unless a formal parameter using the syntax \"*identifier\" is present; in this case, that formal parameter receives a tuple containing the excess positional arguments (or an empty tuple if there were no excess positional arguments).If any keyword argument does not correspond to a formal parameter name, a TypeError exception is raised, unless a formal parameter using the syntax \"**identifier\" is present; in this case, that formal parameter receives a dictionary containing the excess keyword arguments (using the keywords as keys and the argument values as corresponding values), or a (new) empty dictionary if there were no excess keyword arguments.', '* means receive variable arguments as tuple** means receive variable arguments as dictionaryUsed like the following:1) single *Output:2) Now **Output:', 'In Python 3.5, you can also use this syntax in list, dict, tuple, and set displays (also sometimes called literals). See PEP 488: Additional Unpacking Generalizations.It also allows multiple iterables to be unpacked in a single function call.(Thanks to mgilson for the PEP link.)', 'It packs arguments passed to the function into list and dict respectively inside the function body. When you define a function signature like this:it can be called with any number of arguments and keyword arguments. The non-keyword arguments get packed into a list called args inside the function body and the keyword arguments get packed into a dict called kwds inside the function body.now inside the function body, when the function is called, there are two local variables, args which is a list having value [\"this\", \"is a list of\", \"non-keyword\", \"arguments\"] and kwds which is a dict having value {\"keyword\" : \"ligma\", \"options\" : [1,2,3]}This also works in reverse, i.e. from the caller side. for example if you have a function defined as:you can call it with by unpacking iterables or mappings you have in the calling scope:', \"I want to give an example which others haven't  mentioned* can also unpack a generatorAn example from Python3 Documentunzip_x will be (1, 2, 3), unzip_y will be (4, 5, 6)The zip() receives multiple iretable args, and return a generator.\", \"Building on nickd's answer...Output:Basically, any number of positional arguments can use *args and any named arguments (or kwargs aka keyword arguments) can use **kwargs.\", 'In addition to function calls, *args and **kwargs are useful in class hierarchies and also avoid having to write __init__ method in Python. Similar usage can seen in frameworks like Django code.For example,A subclass can then beThe subclass then be instantiated asAlso, a subclass with a new attribute which makes sense only to that subclass instance can call the Base class __init__ to offload the attributes setting.\\nThis is done through *args and **kwargs. kwargs mainly used so that code is readable using named arguments. For example,which can be instatiated asThe complete code is here', 'Given a function that has 3 items as argumentImagine this toy with a bag of a triangle, a circle and a rectangle item. That bag does not directly fit. You need to unpack the bag to take those 3 items and now they fit. The Python * operator does this unpack process.', '*args and **kwargs: allow you to pass a variable number of arguments to a function.*args: is used to send a non-keyworded variable length argument list to the function:Will produce:**kwargs***kwargs allows you to pass keyworded variable length of arguments to a function. You should use **kwargs if you want to handle named arguments in a function.Will produce:', 'A good example of using both in a function is:', 'This example would help you remember *args, **kwargs and even super and inheritance in Python at once.', 'In addition to the answers in this thread, here is another detail that was not mentioned elsewhere. This expands on the answer by Brad SolomonUnpacking with ** is also useful when using python str.format.This is somewhat similar to what you can do with python f-strings f-string but with the added overhead of declaring a dict to hold the variables (f-string does not require a dict).', \"*args ( or *any ) means every parametersNOTICE : you can don't pass parameters to *argsThe *args is in type tuplefor access to elements don't use of *The **kwd**kwd or **any\\nThis is a dict type\", 'For example, the syntax for implementing varargs in Java as follows:', '*args and **kwargs are just some way to input unlimited characters to functions, like:', \"*args is the special parameter which can take 0 or more (positional) arguments as a tuple.**kwargs is the special parameter which can take 0 or more (keyword) arguments as a dictionary.*In Python, there are 2 kinds of arguments positional argument and keyword argument:For example, *args can take 0 or more arguments as a tuple as shown below:Output:And, when printing *args, 4 numbers are printed without parentheses and commas:Output:And, args has tuple type:Output:But, *args has no type:Output(Error):TypeError: type() takes 1 or 3 argumentsAnd, normal parameters can be put before *args as shown below:Output:But, **kwargs cannot be put before *args as shown below:Output(Error):SyntaxError: invalid syntaxAnd, normal parameters cannot be put after *args as shown below:Output(Error):TypeError: test() missing 2 required keyword-only arguments: 'num1' and 'num2'But, if normal parameters have default values, they can be put after *args as shown below:Output:And also, **kwargs can be put after *args as shown below:Output:For example, **kwargs can take 0 or more arguments as a dictionary as shown below:Output:And, when printing *kwargs, 2 keys are printed:Output:And, kwargs has dict type:Output:But, *kwargs and **kwargs have no type:Output(Error):TypeError: type() takes 1 or 3 argumentsAnd, normal parameters can be put before **kwargs as shown below:Output:And also, *args can be put before **kwargs as shown below:Output:And, normal parameters and *args cannot be put after **kwargs as shown below:Output(Error):SyntaxError: invalid syntaxActually, you can use other names for *args and **kwargs as shown below. *args and **kwargs are used conventionally:Output:\"]",
            "url": "https://stackoverflow.com/questions/36901"
        },
        {
            "tag": "python",
            "question": [
                "How do I pass a variable by reference?"
            ],
            "votes": "3180",
            "answer": "['Arguments are passed by assignment. The rationale behind this is twofold:So:If you pass a mutable object into a method, the method gets a reference to that same object and you can mutate it to your heart\\'s delight, but if you rebind the reference in the method, the outer scope will know nothing about it, and after you\\'re done, the outer reference will still point at the original object.If you pass an immutable object to a method, you still can\\'t rebind the outer reference, and you can\\'t even mutate the object.To make it even more clear, let\\'s have some examples.Let\\'s try to modify the list that was passed to a method:Output:Since the parameter passed in is a reference to outer_list, not a copy of it, we can use the mutating list methods to change it and have the changes reflected in the outer scope.Now let\\'s see what happens when we try to change the reference that was passed in as a parameter:Output:Since the the_list parameter was passed by value, assigning a new list to it had no effect that the code outside the method could see. The the_list was a copy of the outer_list reference, and we had the_list point to a new list, but there was no way to change where outer_list pointed.It\\'s immutable, so there\\'s nothing we can do to change the contents of the stringNow, let\\'s try to change the referenceOutput:Again, since the the_string parameter was passed by value, assigning a new string to it had no effect that the code outside the method could see. The the_string was a copy of the outer_string reference, and we had the_string point to a new string, but there was no way to change where outer_string pointed.I hope this clears things up a little.EDIT: It\\'s been noted that this doesn\\'t answer the question that @David originally asked, \"Is there something I can do to pass the variable by actual reference?\". Let\\'s work on that.As @Andrea\\'s answer shows, you could return the new value. This doesn\\'t change the way things are passed in, but does let you get the information you want back out:If you really wanted to avoid using a return value, you could create a class to hold your value and pass it into the function or use an existing class, like a list:Although this seems a little cumbersome.', \"The problem comes from a misunderstanding of what variables are in Python. If you're used to most traditional languages, you have a mental model of what happens in the following sequence:You believe that a is a memory location that stores the value 1, then is updated to store the value 2. That's not how things work in Python. Rather, a starts as a reference to an object with the value 1, then gets reassigned as a reference to an object with the value 2. Those two objects may continue to coexist even though a doesn't refer to the first one anymore; in fact they may be shared by any number of other references within the program.When you call a function with a parameter, a new reference is created that refers to the object passed in. This is separate from the reference that was used in the function call, so there's no way to update that reference and make it refer to a new object. In your example:self.variable is a reference to the string object 'Original'. When you call Change you create a second reference var to the object. Inside the function you reassign the reference var to a different string object 'Changed', but the reference self.variable is separate and does not change.The only way around this is to pass a mutable object. Because both references refer to the same object, any changes to the object are reflected in both places.\", 'I found the other answers rather long and complicated, so I created this simple diagram to explain the way Python treats variables and parameters.', 'It is neither pass-by-value or pass-by-reference - it is call-by-object. See this, by Fredrik Lundh:http://effbot.org/zone/call-by-object.htmHere is a significant quote:\"...variables [names] are not objects; they cannot be denoted by other variables or referred to by objects.\"In your example, when the Change method is called--a namespace is created for it; and var becomes a name, within that namespace, for the string object \\'Original\\'. That object then has a name in two namespaces. Next, var = \\'Changed\\' binds var to a new string object, and thus the method\\'s namespace forgets about \\'Original\\'. Finally, that namespace is forgotten, and the string \\'Changed\\' along with it.', \"Think of stuff being passed by assignment instead of by reference/by value. That way, it is always clear, what is happening as long as you understand what happens during the normal assignment.So, when passing a list to a function/method, the list is assigned to the parameter name. Appending to the list will result in the list being modified. Reassigning the list inside the function will not change the original list, since:Since immutable types cannot be modified, they seem like being passed by value - passing an int into a function means assigning the int to the function's parameter. You can only ever reassign that, but it won't change the original variables value.\", 'The key to understanding parameter passing is to stop thinking about \"variables\". There are names and objects in Python and together they\\nappear like variables, but it is useful to always distinguish the three.That is all there is to it. Mutability is irrelevant to this question.Example:This binds the name a to an object of type integer that holds the value 1.This binds the name b to the same object that the name x is currently bound to.\\nAfterward, the name b has nothing to do with the name x anymore.See sections 3.1 and 4.2 in the Python 3 language reference.In the code shown in the question, the statement self.Change(self.variable) binds the name var (in the scope of function Change) to the object that holds the value \\'Original\\' and the assignment var = \\'Changed\\' (in the body of function Change) assigns that same name again: to some other object (that happens to hold a string as well but could have been something else entirely).So if the thing you want to change is a mutable object, there is no problem, as everything is effectively passed by reference.If it is an immutable object (e.g. a bool, number, string), the way to go is to wrap it in a mutable object.\\nThe quick-and-dirty solution for this is a one-element list (instead of self.variable, pass [self.variable] and in the function modify var[0]).\\nThe more pythonic approach would be to introduce a trivial, one-attribute class. The function receives an instance of the class and manipulates the attribute.', 'Effbot (aka Fredrik Lundh) has described Python\\'s variable passing style as call-by-object:  http://effbot.org/zone/call-by-object.htmObjects are allocated on the heap and pointers to them can be passed around anywhere.When you make an assignment such as x = 1000, a dictionary entry is created that maps the string \"x\" in the current namespace to a pointer to the integer object containing one thousand.When you update \"x\" with x = 2000, a new integer object is created and the dictionary is updated to point at the new object.  The old one thousand object is unchanged (and may or may not be alive depending on whether anything else refers to the object).When you do a new assignment such as y = x, a new dictionary entry \"y\" is created that points to the same object as the entry for \"x\".Objects like strings and integers are immutable.  This simply means that there are no methods that can change the object after it has been created.  For example, once the integer object one-thousand is created, it will never change.  Math is done by creating new integer objects.Objects like lists are mutable.  This means that the contents of the object can be changed by anything pointing to the object.  For example, x = []; y = x; x.append(10); print y will print [10].  The empty list was created.  Both \"x\" and \"y\" point to the same list.  The append method mutates (updates) the list object (like adding a record to a database) and the result is visible to both \"x\" and \"y\" (just as a database update would be visible to every connection to that database).Hope that clarifies the issue for you.', \"Technically, Python always uses pass by reference values. I am going to repeat my other answer to support my statement.Python always uses pass-by-reference values. There isn't any exception. Any variable assignment means copying the reference value. No exception. Any variable is the name bound to the reference value. Always.You can think about a reference value as the address of the target object. The address is automatically dereferenced when used. This way, working with the reference value, it seems you work directly with the target object. But there always is a reference in between, one step more to jump to the target.Here is the example that proves that Python uses passing by reference:If the argument was passed by value, the outer lst could not be modified. The green are the target objects (the black is the value stored inside, the red is the object type), the yellow is the memory with the reference value inside -- drawn as the arrow. The blue solid arrow is the reference value that was passed to the function (via the dashed blue arrow path). The ugly dark yellow is the internal dictionary. (It actually could be drawn also as a green ellipse. The colour and the shape only says it is internal.)You can use the id() built-in function to learn what the reference value is (that is, the address of the target object).In compiled languages, a variable is a memory space that is able to capture the value of the type. In Python, a variable is a name (captured internally as a string) bound to the reference variable that holds the reference value to the target object. The name of the variable is the key in the internal dictionary, the value part of that dictionary item stores the reference value to the target.Reference values are hidden in Python. There isn't any explicit user type for storing the reference value. However, you can use a list element (or element in any other suitable container type) as the reference variable, because all containers do store the elements also as references to the target objects. In other words, elements are actually not contained inside the container -- only the references to elements are.\", 'A simple trick I normally use is to just wrap it in a list:(Yeah I know this can be inconvenient, but sometimes it is simple enough to do this.)', '(edit - Blair has updated his enormously popular answer so that it is now accurate)I think it is important to note that the current post with the most votes (by Blair Conrad), while being correct with respect to its result, is misleading and is borderline incorrect based on its definitions.  While there are many languages (like C) that allow the user to either pass by reference or pass by value, Python is not one of them.David Cournapeau\\'s answer points to the real answer and explains why the behavior in Blair Conrad\\'s post seems to be correct while the definitions are not.To the extent that Python is pass by value, all languages are pass by value since some piece of data (be it a \"value\" or a \"reference\") must be sent. However, that does not mean that Python is pass by value in the sense that a C programmer would think of it.If you want the behavior, Blair Conrad\\'s answer is fine.  But if you want to know the nuts and bolts of why Python is neither pass by value or pass by reference, read David Cournapeau\\'s answer.', 'You got some really good answers here.', 'Python\u2019s pass-by-assignment scheme isn\u2019t quite the same as C++\u2019s reference parameters option, but it turns out to be very similar to the argument-passing model of the C language (and others) in practice:', \"In this case the variable titled var in the method Change is assigned a reference to self.variable, and you immediately assign a string to var. It's no longer pointing to self.variable. The following code snippet shows what would happen if you modify the data structure pointed to by var and self.variable, in this case a list:I'm sure someone else could clarify this further.\", 'A lot of insights in answers here, but i think an additional point is not clearly mentioned here explicitly.   Quoting from python documentation https://docs.python.org/2/faq/programming.html#what-are-the-rules-for-local-and-global-variables-in-python\"In Python, variables that are only referenced inside a function are implicitly global. If a variable is assigned a new value anywhere within the function\u2019s body, it\u2019s assumed to be a local. If a variable is ever assigned a new value inside the function, the variable is implicitly local, and you need to explicitly declare it as \u2018global\u2019.\\nThough a bit surprising at first, a moment\u2019s consideration explains this. On one hand, requiring global for assigned variables provides a bar against unintended side-effects. On the other hand, if global was required for all global references, you\u2019d be using global all the time. You\u2019d have to declare as global every reference to a built-in function or to a component of an imported module. This clutter would defeat the usefulness of the global declaration for identifying side-effects.\"Even when passing a mutable object to a function this still applies. And to me clearly explains the reason for the difference in behavior between assigning to the object and operating on the object in the function.gives:The assignment to an global variable that is not declared global therefore creates a new local object and breaks the link to the original object.', 'As you can state you need to have a mutable object, but let me suggest you to check over the global variables as they can help you or even solve this kind of issue!http://docs.python.org/3/faq/programming.html#what-are-the-rules-for-local-and-global-variables-in-pythonexample:', \"Here is the simple (I hope) explanation of the concept pass by object used in Python.\\nWhenever you pass an object to the function, the object itself is passed (object in Python is actually what you'd call a value in other programming languages) not the reference to this object. In other words, when you call:The actual object - [0, 1] (which would be called a value in other programming languages) is being passed. So in fact the function change_me will try to do something like:which obviously will not change the object passed to the function. If the function looked like this:Then the call would result in:which obviously will change the object. This answer explains it well.\", \"Aside from all the great explanations on how this stuff works in Python, I don't see a simple suggestion for the problem. As you seem to do create objects and instances, the pythonic way of handling instance variables and changing them is the following:In instance methods, you normally refer to self to access instance attributes. It is normal to set instance attributes in __init__ and read or change them in instance methods. That is also why you pass self als the first argument to def Change.Another solution would be to create a static method like this:\", \"I used the following method to quickly convert a couple of Fortran codes to Python.  True, it's not pass by reference as the original question was posed, but is a simple work around in some cases.\", \"There is a little trick to pass an object by reference, even though the language doesn't make it possible. It works in Java too, it's the list with one item. ;-)It's an ugly hack, but it works. ;-P\", 'given the way python handles values and references to them, the only way you can reference an arbitrary instance attribute is by name:in real code you would, of course, add error checking on the dict lookup.', 'Since it seems to be nowhere mentioned an approach to simulate references as known from e.g. C++ is to use an \"update\" function and pass that instead of the actual variable (or rather, \"name\"):This is mostly useful for \"out-only references\" or in a situation with multiple threads / processes (by making the update function thread / multiprocessing safe).Obviously the above does not allow reading the value, only updating it.', 'Since your example happens to be object-oriented, you could make the following change to achieve a similar result:', 'Since dictionaries are passed by reference, you can use a dict variable to store any referenced values inside it.', 'While pass by reference is nothing that fits well into python and should be rarely used there are some workarounds that actually can work to get the object currently assigned to a local variable or even reassign a local variable from inside of a called function.The basic idea is to have a function that can do that access and can be passed as object into other functions or stored in a class.One way is to use global (for global variables) or nonlocal (for local variables in a function) in a wrapper function.The same idea works for reading and deleting a variable.For just reading there is even a shorter way of just using lambda: x which returns a callable that when called returns the current value of x. This is somewhat like \"call by name\" used in languages in the distant past.Passing 3 wrappers to access a variable is a bit unwieldy so those can be wrapped into a class that has a proxy attribute:Pythons \"reflection\" support makes it possible to get a object that is capable of reassigning a name/variable in a given scope without defining functions explicitly in that scope:Here the ByRef class wraps a dictionary access. So attribute access to wrapped is translated to a item access in the passed dictionary. By passing the result of the builtin locals and the name of a local variable this ends up accessing a local variable. The python documentation as of 3.5 advises that changing the dictionary might not work but it seems to work for me.', 'You can merely use an empty class as an instance to store reference objects because internally object attributes are stored in an instance dictionary. See the example.', 'Pass-By-Reference in Python is quite different from the concept of pass by reference in C++/Java.', \"I am new to Python, started yesterday (though I have been programming for 45 years).I came here because I was writing a function where I wanted to have two so called out-parameters. If it would have been only one out-parameter, I wouldn't get hung up right now on checking how reference/value works in Python. I would just have used the return value of the function instead. But since I needed two such out-parameters I felt I needed to sort it out.In this post I am going to show how I solved my situation. Perhaps others coming here can find it valuable, even though it is not exactly an answer to the topic question. Experienced Python programmers of course already know about the solution I used, but it was new to me.From the answers here I could quickly see that Python works a bit like Javascript in this regard, and that you need to use workarounds if you want the reference functionality.But then I found something neat in Python that I don't think I have seen in other languages before, namely that you can return more than one value from a function, in a simple comma separated way, like this:and that you can handle that on the calling side similarly, like thisThat was good enough for me and I was satisfied. No need to use some workaround.In other languages you can of course also return many values, but then usually in the from of an object, and you need to adjust the calling side accordingly.The Python way of doing it was nice and simple.If you want to mimic by reference even more, you could do as follows:which gives this result\", 'alternatively you could use ctypes witch would look something like thisas a is a c int and not a python integer and apperently passed by reference. however you have to be carefull as strange things could happen and is therefor not advised', \"Most likely not the most reliable method but this works, keep in mind that you are overloading the built-in str function which is typically something you don't want to do:\", 'What about dataclasses? Also, it allows you to apply type restriction (aka \"type hint\").I agree with folks that in most cases you\\'d better consider not to use it.And yet, when we\\'re talking about contexts it\\'s worth to know that way.You can design explicit context class though. When prototyping I prefer dataclasses, just because it\\'s easy to serialize them back and forth.Cheers!']",
            "url": "https://stackoverflow.com/questions/986006"
        },
        {
            "tag": "python",
            "question": [
                "How do I clone a list so that it doesn't change unexpectedly after assignment?"
            ],
            "votes": "3173",
            "answer": "[\"new_list = my_list doesn't actually create a second list. The assignment just copies the reference to the list, not the actual list, so both new_list and my_list refer to the same list after the assignment.To actually copy the list, you have several options:You can use the builtin list.copy() method (available since Python 3.3):You can slice it:Alex Martelli's opinion (at least back in 2007) about this is, that it is a weird syntax and it does not make sense to use it ever. ;) (In his opinion, the next one is more readable).You can use the built in list() constructor:You can use generic copy.copy():This is a little slower than list() because it has to find out the datatype of old_list first.If you need to copy the elements of the list as well, use generic copy.deepcopy():Obviously the slowest and most memory-needing method, but sometimes unavoidable. This operates recursively; it will handle any number of levels of nested lists (or other containers).Example:Result:\", \"Felix already provided an excellent answer, but I thought I'd do a speed comparison of the various methods:So the fastest is list slicing. But be aware that copy.copy(), list[:] and list(list), unlike copy.deepcopy() and the python version don't copy any lists, dictionaries and class instances in the list, so if the originals change, they will change in the copied list too and vice versa.(Here's the script if anyone's interested or wants to raise any issues:)\", \"I've been told that Python 3.3+ adds the list.copy() method, which should be as fast as slicing:\", \"In Python 3, a shallow copy can be made with:In Python 2 and 3, you can get a shallow copy with a full slice of the original:There are two semantic ways to copy a list. A shallow copy creates a new list of the same objects, a deep copy creates a new list containing new equivalent objects.A shallow copy only copies the list itself, which is a container of references to the objects in the list. If the objects contained themselves are mutable and one is changed, the change will be reflected in both lists.There are different ways to do this in Python 2 and 3. The Python 2 ways will also work in Python 3.In Python 2, the idiomatic way of making a shallow copy of a list is with a complete slice of the original:You can also accomplish the same thing by passing the list through the list constructor,but using the constructor is less efficient:In Python 3, lists get the list.copy method:In Python 3.5:Using new_list = my_list then modifies new_list every time my_list changes. Why is this?my_list is just a name that points to the actual list in memory. When you say new_list = my_list you're not making a copy, you're just adding another name that points at that original list in memory. We can have similar issues when we make copies of lists.The list is just an array of pointers to the contents, so a shallow copy just copies the pointers, and so you have two different lists, but they have the same contents. To make copies of the contents, you need a deep copy.To make a deep copy of a list, in Python 2 or 3, use deepcopy in the copy module:To demonstrate how this allows us to make new sub-lists:And so we see that the deep copied list is an entirely different list from the original. You could roll your own function - but don't. You're likely to create bugs you otherwise wouldn't have by using the standard library's deepcopy function.You may see this used as a way to deepcopy, but don't do it:In 64 bit Python 2.7:on 64 bit Python 3.5:\", \"Let's start from the beginning and explore this question.So let's suppose you have two lists:And we have to copy both lists, now starting from the first list:So first let's try by setting the variable copy to our original list, list_1:Now if you are thinking copy copied the list_1, then you are wrong. The id function can show us if two variables can point to the same object. Let's try this:The output is:Both variables are the exact same argument. Are you surprised?So as we know, Python doesn't store anything in a variable, Variables are just referencing to the object and object store the value. Here object is a list but we created two references to that same object by two different variable names. This means that both variables are pointing to the same object, just with different names.When you do copy = list_1, it is actually doing:Here in the image list_1 and copy are two variable names, but the object is same for both variable which is list.So if you try to modify copied list then it will modify the original list too because the list is only one there, you will modify that list no matter you do from the copied list or from the original list:Output:So it modified the original list:Now let's move onto a Pythonic method for copying lists.This method fixes the first issue we had:So as we can see our both list having different id and it means that both variables are pointing to different objects. So what actually going on here is:Now let's try to modify the list and let's see if we still face the previous problem:The output is:As you can see, it only modified the copied list. That means it worked.Do you think we're done? No. Let's try to copy our nested list.list_2 should reference to another object which is copy of list_2. Let's check:We get the output:Now we can assume both lists are pointing different object, so now let's try to modify it and let's see it is giving what we want:This gives us the output:This may seem a little bit confusing, because the same method we previously used worked. Let's try to understand this.When you do:You're only copying the outer list, not the inside list. We can use the id function once again to check this.The output is:When we do copy_2 = list_2[:], this happens:It creates the copy of list, but only outer list copy, not the nested list copy. The nested list is same for both variable, so if you try to modify the nested list then it will modify the original list too as the nested list object is same for both lists.What is the solution? The solution is the deepcopy function.Let's check this:Both outer lists have different IDs. Let's try this on the inner nested lists.The output is:As you can see both IDs are different, meaning we can assume that both nested lists are pointing different object now.This means when you do deep = deepcopy(list_2) what actually happens:Both nested lists are pointing different object and they have separate copy of nested list now.Now let's try to modify the nested list and see if it solved the previous issue or not:It outputs:As you can see, it didn't modify the original nested list, it only modified the copied list.\", \"There are many answers already that tell you how to make a proper copy, but none of them say why your original 'copy' failed.Python doesn't store values in variables; it binds names to objects. Your original assignment took the object referred to by my_list and bound it to new_list as well. No matter which name you use there is still only one list, so changes made when referring to it as my_list will persist when referring to it as new_list. Each of the other answers to this question give you different ways of creating a new object to bind to new_list.Each element of a list acts like a name, in that each element binds non-exclusively to an object. A shallow copy creates a new list whose elements bind to the same objects as before.To take your list copy one step further, copy each object that your list refers to, and bind those element copies to a new list.This is not yet a deep copy, because each element of a list may refer to other objects, just like the list is bound to its elements. To recursively copy every element in the list, and then each other object referred to by each element, and so on: perform a deep copy.See the documentation for more information about corner cases in copying.\", 'Use thing[:]', \"Here are the timing results using Python 3.6.8. Keep in mind these times are relative to one another, not absolute.I stuck to only doing shallow copies, and also added some new methods that weren't possible in Python\\xa02, such as list.copy() (the Python\\xa03 slice equivalent) and two forms of list unpacking (*new_list, = list and new_list = [*list]):We can see the Python 2 winner still does well, but doesn't edge out Python 3 list.copy() by much, especially considering the superior readability of the latter.The dark horse is the unpacking and repacking method (b = [*a]), which is ~25% faster than raw slicing, and more than twice as fast as the other unpacking method (*b, = a).b = a * 1 also does surprisingly well.Note that these methods do not output equivalent results for any input other than lists. They all work for sliceable objects, a few work for any iterable, but only copy.copy() works for more general Python objects.Here is the testing code for interested parties (Template from here):\", \"Python's idiom for doing this is newList = oldList[:]\", \"All of the other contributors gave great answers, which work when you have a single dimension (leveled) list, however of the methods mentioned so far, only copy.deepcopy() works to clone/copy a list and not have it point to the nested list objects when you are working with multidimensional, nested lists (list of lists). While Felix Kling refers to it in his answer, there is a little bit more to the issue and possibly a workaround using built-ins that might prove a faster alternative to deepcopy.While new_list = old_list[:], copy.copy(old_list)' and for Py3k old_list.copy() work for single-leveled lists, they revert to pointing at the list objects nested within the old_list and the new_list, and changes to one of the list objects are perpetuated in the other.As was pointed out by both Aaron Hall and PM 2Ring using eval() is not only a bad idea, it is also much slower than copy.deepcopy().This means that for multidimensional lists, the only option is copy.deepcopy(). With that being said, it really isn't an option as the performance goes way south when you try to use it on a moderately sized multidimensional array.  I tried to timeit using a 42x42 array, not unheard of or even that large for bioinformatics applications, and I gave up on waiting for a response and just started typing my edit to this post.It would seem that the only real option then is to initialize multiple lists and work on them independently. If anyone has any other suggestions, for how to handle multidimensional list copying, it would be appreciated.As others have stated, there  are significant performance issues using the copy module and copy.deepcopy for multidimensional lists.\", 'It surprises me that this hasn\\'t been mentioned yet, so for the sake of completeness...You can perform list unpacking with the \"splat operator\": *, which will also copy elements of your list.The obvious downside to this method is that it is only available in Python 3.5+.Timing wise though, this appears to perform better than other common methods.', \"new_list = my_listTry to understand this. Let's say that my_list is in the heap memory at location X, i.e., my_list is pointing to the X. Now by assigning new_list = my_list you're letting new_list point to the X. This is known as a shallow copy.Now if you assign new_list = my_list[:], you're simply copying each object of my_list to new_list. This is known as a deep copy.The other ways you can do this are:\", \"A very simple approach independent of python version was missing in already-given answers which you can use most of the time (at least I do):However, if my_list contains other containers (for example, nested lists) you must use deepcopy as others suggested in the answers above from the copy library. For example:.Bonus: If you don't want to copy elements use (AKA shallow copy):Let's understand difference between solution #1 and solution #2As you can see, solution #1 worked perfectly when we were not using the nested lists. Let's check what will happen when we apply solution #1 to nested lists.\", \"I wanted to post something a bit different than some of the other answers. Even though this is most likely not the most understandable, or fastest option, it provides a bit of an inside view of how deep copy works, as well as being another alternative option for deep copying. It doesn't really matter if my function has bugs, since the point of this is to show a way to copy objects like the question answers, but also to use this as a point to explain how deepcopy works at its core.At the core of any deep copy function is way to make a shallow copy. How? Simple. Any deep copy function only duplicates the containers of immutable objects. When you deepcopy a nested list, you are only duplicating the outer lists, not the mutable objects inside of the lists. You are only duplicating the containers. The same works for classes, too. When you deepcopy a class, you deepcopy all of its mutable attributes. So, how? How come you only have to copy the containers, like lists, dicts, tuples, iters, classes, and class instances?It's simple. A mutable object can't really be duplicated. It can never be changed, so it is only a single value. That means you never have to duplicate strings, numbers, bools, or any of those. But how would you duplicate the containers? Simple. You make just initialize a new container with all of the values. Deepcopy relies on recursion. It duplicates all the containers, even ones with containers inside of them, until no containers are left. A container is an immutable object.Once you know that, completely duplicating an object without any references is pretty easy. Here's a function for deepcopying basic data-types (wouldn't work for custom classes but you could always add that)Python's own built-in deepcopy is based around that example. The only difference is it supports other types, and also supports user-classes by duplicating the attributes into a new duplicate class, and also blocks infinite-recursion with a reference to an object it's already seen using a memo list or dictionary. And that's really it for making deep copies. At its core, making a deep copy is just making shallow copies. I hope this answer adds something to the question.EXAMPLESSay you have this list: [1, 2, 3]. The immutable numbers cannot be duplicated, but the other layer can. You can duplicate it using a list comprehension: [x for x in [1, 2, 3]]Now, imagine you have this list: [[1, 2], [3, 4], [5, 6]]. This time, you want to make a function, which uses recursion to deep copy all layers of the list. Instead of the previous list comprehension:It uses a new one for lists:And deepcopy_list looks like this:Then now you have a function which can deepcopy any list of strs, bools, floast, ints and even lists to infinitely many layers using recursion. And there you have it, deepcopying.TLDR: Deepcopy uses recursion to duplicate objects, and merely returns the same immutable objects as before, as immutable objects cannot be duplicated. However, it deepcopies the most inner layers of mutable objects until it reaches the outermost mutable layer of an object.\", 'Note that there are some cases where if you have defined your own custom class and you want to keep the attributes then you should use copy.copy() or copy.deepcopy() rather than the alternatives, for example in Python 3:Outputs:', \"Remember that in Python when you do:List2 isn't storing the actual list, but a reference to list1. So when you do anything to list1, list2 changes as well. use the copy module (not default, download on pip) to make an original copy of the list(copy.copy() for simple lists, copy.deepcopy() for nested ones). This makes a copy that doesn't change with the first list.\", 'A slight practical perspective to look into memory through id and gc.', 'There is another way of copying a list that was not listed until now: adding an empty list: l2 = l + [].I tested it with Python 3.8:It is not the best answer, but it works.', 'The deepcopy option is the only method that works for me:leads to output of:', 'This is because, the line new_list = my_list assigns a new reference to the variable my_list which is new_list\\nThis is similar to the C code given below,You should use the copy module to create a new list by', 'The method to use depends on the contents of the list being copied. If the list contains nested dicts than deepcopy is the only method that works, otherwise most of the methods listed in the answers (slice, loop [for], copy, extend, combine, or unpack) will work and execute in similar time (except for loop and deepcopy, which preformed the worst).', 'I often see code that tries to modify a copy of the list in some iterative fashion. To construct a trivial example, suppose we had non-working (because x should not be modified) code like:Naturally people will ask how to make y be a copy of x, rather than a name for the same list, so that the for loop will do the right thing.But this is the wrong approach. Functionally, what we really want to do is make a new list that is based on the original.We don\\'t need to make a copy first to do that, and we typically shouldn\\'t.The natural tool for this is a list comprehension. This way, we write the logic that tells us how the elements in the desired result, relate to the original elements. It\\'s simple, elegant and expressive; and we avoid the need for workarounds to modify the y copy in a for loop (since assigning to the iteration variable doesn\\'t affect the list - for the same reason that we wanted the copy in the first place!).For the above example, it looks like:List comprehensions are quite powerful; we can also use them to filter out elements by a rule with an if clause, and we can chain for and if clauses (it works like the corresponding imperative code, with the same clauses in the same order; only the value that will ultimately end up in the result list, is moved to the front instead of being in the \"innermost\" part). If the plan was to iterate over the original while modifying the copy to avoid problems, there is generally a much more pleasant way to do that with a filtering list comprehension.Suppose instead that we had something likeRather than making y a separate copy first in order to delete the part we don\\'t want, we can build a list by putting together the parts that we do want. Thus:Handling insertion, replacement etc. by slicing is left as an exercise. Just reason out which subsequences you want the result to contain. A special case of this is making a reversed copy - assuming we need a new list at all (rather than just to iterate in reverse), we can directly create it by slicing, rather than cloning and then using .reverse.These approaches - like the list comprehension - also have the advantage that they create the desired result as an expression, rather than by procedurally modifying an existing object in-place (and returning None). This is more convenient for writing code in a \"fluent\" style.', 'Short and simple explanations of each copy mode:A shallow copy constructs a new compound object and then (to the extent possible) inserts references into it to the objects found in the original - creating a shallow copy:A deep copy constructs a new compound object and then, recursively, inserts copies into it of the objects found in the original - creating a deep copy:list() works fine for deep copy of simple lists, like:But, for complex lists like......use deepcopy():']",
            "url": "https://stackoverflow.com/questions/2612802"
        },
        {
            "tag": "python",
            "question": [
                "How do I make a time delay? [duplicate]"
            ],
            "votes": "3134",
            "answer": "['This delays for 2.5 seconds:Here is another example where something is run approximately once a minute:', 'Use sleep() from the time module. It can take a float argument for sub-second resolution.', 'In a single thread I suggest the sleep function:This function actually suspends the processing of the thread in which it is called by the operating system, allowing other threads and processes to execute while it sleeps.Use it for that purpose, or simply to delay a function from executing. For example:\"hooray!\" is printed 3 seconds after I hit Enter.Again, sleep suspends your thread - it uses next to zero processing power.To demonstrate, create a script like this (I first attempted this in an interactive Python 3.5 shell, but sub-processes can\\'t find the party_later function for some reason):Example output from this script:You can trigger a function to be called at a later time in a separate thread with the Timer threading object:The blank line illustrates that the function printed to my standard output, and I had to hit Enter to ensure I was on a prompt.The upside of this method is that while the Timer thread was waiting, I was able to do other things, in this case, hitting Enter one time - before the function executed (see the first empty prompt).There isn\\'t a respective object in the multiprocessing library. You can create one, but it probably doesn\\'t exist for a reason. A sub-thread makes a lot more sense for a simple timer than a whole new subprocess.', 'Delays can be also implemented by using the following methods.The first method:The second method to delay would be using the implicit wait method:The third method is more useful when you have to wait until a particular action is completed or until an element is found:', \"There are five methods which I know: time.sleep(), pygame.time.wait(), matplotlib's pyplot.pause(), .after(), and asyncio.sleep().time.sleep() example (do not use if using tkinter):pygame.time.wait() example (not recommended if you are not using the pygame window, but you could exit the window instantly):matplotlib's function pyplot.pause() example (not recommended if you are not using the graph, but you could exit the graph instantly):The .after() method (best with Tkinter):Finally, the asyncio.sleep() method (has to be in an async loop):\", 'A bit of fun with a sleepy generator.The question is about time delay. It can be fixed time, but in some cases we might need a delay measured since last time. Here is one possible solution:The situation can be, we want to do something as regularly as possible and we do not want to bother with all the last_time, next_time stuff all around our code.The following code (sleepy.py) defines a buzzergen generator:And running it we see:We can also use it directly in a loop:And running it we might see:As we see, this buzzer is not too rigid and allow us to catch up with regular sleepy intervals even if we oversleep and get out of regular schedule.', 'The Tkinter library in the Python standard library is an interactive tool which you can import. Basically, you can create buttons and boxes and popups and stuff that appear as windows which you manipulate with code.If you use Tkinter, do not use time.sleep(), because it will muck up your program. This happened to me. Instead, use root.after() and replace the values for however many seconds, with a milliseconds. For example, time.sleep(1) is equivalent to root.after(1000) in Tkinter.Otherwise, time.sleep(), which many answers have pointed out, which is the way to go.', \"Delays are done with the time library, specifically the time.sleep() function.To just make it wait for a second:This works because by doing:You extract the sleep function only from the time library, which means you can just call it with:Rather than having to type outWhich is awkwardly long to type.With this method, you wouldn't get access to the other features of the time library and you can't have a variable called sleep. But you could create a variable called time.Doing from [library] import [function] (, [function2]) is great if you just want certain parts of a module.You could equally do it as:and you would have access to the other features of the time library like time.clock() as long as you type time.[function](), but you couldn't create the variable time because it would overwrite the import. A solution to this to dowhich would allow you to reference the time library as t, allowing you to do:This works on any library.\", 'Use time.sleep or Event().wait like this:Use threading.Timer like this:Outputs:', 'Notice in recent Python versions (Python\\xa03.4 or higher) you can use asyncio.sleep. It\\'s related to asynchronous programming and asyncio. Check out next example:We may think it will \"sleep\" for 2 seconds for first method and then 3 seconds in the second method, a total of 5 seconds running time of this code. But it will print:It is recommended to read asyncio official documentation for more details.', \"While everyone else has suggested the de facto time module, I thought I'd share a different method using matplotlib's pyplot function, pause.Typically this is used to prevent the plot from disappearing as soon as it is plotted or to make crude animations.This would save you an import if you already have matplotlib imported.\", 'This is an easy example of a time delay:Another, in Tkinter:', 'You also can try this:Now the shell will not crash or not react.']",
            "url": "https://stackoverflow.com/questions/510348"
        },
        {
            "tag": "python",
            "question": [
                "How can I delete a file or folder in Python?"
            ],
            "votes": "3117",
            "answer": "['os.remove() removes a file.os.rmdir() removes an empty directory.shutil.rmtree() deletes a directory and all its contents.Path objects from the Python 3.4+ pathlib module also expose these instance methods:pathlib.Path.unlink() removes a file or symbolic link.pathlib.Path.rmdir() removes an empty directory.', 'ororpathlib Library for Python version >= 3.4Unlink method used to remove the file or the symbolik link.First, check if the file or folder exists and then delete it. You can achieve this in two ways:EXAMPLE for os.path.isfileExample for shutil.rmtree()', 'Use(See complete documentation on shutil) and/orand(Complete documentation on os.)', 'Here is a robust function that uses both os.remove and shutil.rmtree:', 'You can use the built-in pathlib module (requires Python 3.4+, but there are backports for older versions on PyPI: pathlib, pathlib2).To remove a file there is the unlink method:Or the rmdir method to remove an empty folder:', 'There are multiple ways to Delete a File in Python but the best ways are the following:To delete all files inside a particular directory, you simply have to use the * symbol as the pattern string.\\n#Importing os and glob modules\\nimport os, glob\\n#Loop Through the folder projects all files and deleting them one by one\\nfor file in glob.glob(\"pythonpool/*\"):\\nos.remove(file)\\nprint(\"Deleted \" + str(file))os.unlink() is an alias or another name of os.remove() . As in the Unix OS remove is also known as unlink.\\nNote: All the functionalities and syntax is the same of os.unlink() and os.remove(). Both of them are used to delete the Python file path.\\nBoth are methods in the os module in Python\u2019s standard libraries which performs the deletion function.Pathlib module provides different ways to interact with your files. Rmdir is one of the path functions which allows you to delete an empty folder. Firstly, you need to select the Path() for the directory, and then calling rmdir() method will check the folder size. If it\u2019s empty, it\u2019ll delete it.This is a good way to deleting empty folders without any fear of losing actual data.', 'For Python 3, to remove the file and directory individually, use the unlink and rmdir Path object methods respectively:Note that you can also use relative paths with Path objects, and you can check your current working directory with Path.cwd.For removing individual files and directories in Python 2, see the section so labeled below.To remove a directory with contents, use shutil.rmtree, and note that this is available in Python 2 and 3:New in Python 3.4 is the Path object.Let\\'s use one to create a directory and file to demonstrate usage. Note that we use the / to join the parts of the path, this works around issues between operating systems and issues from using backslashes on Windows (where you\\'d need to either double up your backslashes like \\\\\\\\ or use raw strings, like r\"foo\\\\bar\"):and now:Now let\\'s delete them. First the file:We can use globbing to remove multiple files - first let\\'s create a few files for this:Then just iterate over the glob pattern:Now, demonstrating removing the directory:What if we want to remove a directory  and everything in it? \\nFor this use-case, use shutil.rmtreeLet\\'s recreate our directory and file:and note that rmdir fails unless it\\'s empty, which is why rmtree is so convenient:Now, import rmtree and pass the directory to the funtion:and we can see the whole thing has been removed:If you\\'re on Python 2, there\\'s a backport of the pathlib module called pathlib2, which can be installed with pip:And then you can alias the library to pathlibOr just directly import the Path object (as demonstrated here):If that\\'s too much, you can remove files with os.remove or os.unlinkorand you can remove directories with os.rmdir:Note that there is also a os.removedirs - it only removes empty directories recursively, but it may suit your use-case.', 'shutil.rmtree is the asynchronous function, \\nso if you want to check when it complete, you can use while...loop', 'This is my function for deleting dirs. The \"path\" requires the full pathname.', '', \"orBoth functions are semantically same. This functions removes (deletes) the file path. If path is not a file and it is directory, then exception is raised.orIn order to remove whole directory trees, shutil.rmtree() can be used. os.rmdir only works when the directory is empty and exists.It remove every empty parent directory with self until parent which has some contentex. os.removedirs('abc/xyz/pqr') will remove the directories by order 'abc/xyz/pqr', 'abc/xyz' and 'abc' if they are empty.For more info check official doc: os.unlink , os.remove, os.rmdir , shutil.rmtree, os.removedirs\", 'To remove all files in folderTo remove all folders in a directory', \"To avoid the TOCTOU issue highlighted by \u00c9ric Araujo's comment, you can catch an exception to call the correct method:Since shutil.rmtree() will only remove directories and os.remove() or os.unlink() will only remove files.\", 'My personal preference is to work with pathlib objects - it offers a more pythonic and less error-prone way to interact with the filesystem, especially if You develop cross-platform code.In that case, You might use pathlib3x - it offers a backport of the latest (at the date of writing this answer Python 3.10.a0) Python pathlib for Python 3.6 or newer, and a few additional functions like \"copy\", \"copy2\", \"copytree\", \"rmtree\" etc ...It also wraps shutil.rmtree:you can find it on github or PyPiDisclaimer: I\\'m the author of the pathlib3x library.', 'I recommend using subprocess if writing a beautiful and readable code is your cup of tea:And if you are not a software engineer, then maybe consider using Jupyter; you can simply type bash commands:Traditionally, you use shutil:']",
            "url": "https://stackoverflow.com/questions/6996603"
        },
        {
            "tag": "python",
            "question": [
                "What is the difference between Python's list methods append and extend?"
            ],
            "votes": "3113",
            "answer": "['append appends a specified object at the end of the list:extend extends the list by appending elements from the specified iterable:', 'append adds an element to a list. extend concatenates the first list with another list/iterable.', 'The list.append method appends an object to the end of the list.Whatever the object is, whether a number, a string, another list, or something else, it gets added onto the end of my_list as a single entry on the list.So keep in mind that a list is an object. If you append another list onto a list, the first list will be a single object at the end of the list (which may not be what you want):The list.extend method extends a list by appending elements from an iterable:So with extend, each element of the iterable gets appended onto the list. For example:Keep in mind that a string is an iterable, so if you extend a list with a string, you\\'ll append each character as you iterate over the string (which may not be what you want):Both + and += operators are defined for list. They are semantically similar to extend.my_list + another_list creates a third list in memory, so you can return the result of it, but it requires that the second iterable be a list.my_list += another_list modifies the list in-place (it is the in-place operator, and lists are mutable objects, as we\\'ve seen) so it does not create a new list. It also works like extend, in that the second iterable can be any kind of iterable.Don\\'t get confused - my_list = my_list + another_list is not equivalent to += - it gives you a brand new list assigned to my_list.Append has (amortized) constant time complexity, O(1).Extend has time complexity, O(k).Iterating through the multiple calls to append adds to the complexity, making it equivalent to that of extend, and since extend\\'s iteration is implemented in C, it will always be faster if you intend to append successive items from an iterable onto a list.Regarding \"amortized\" - from the list object implementation source:This means that we get the benefits of a larger than needed memory reallocation up front, but we may pay for it on the next marginal reallocation with an even larger one. Total time for all appends is linear at O(n), and that time allocated per append, becomes O(1).You may wonder what is more performant, since append can be used to achieve the same outcome as extend. The following functions do the same thing:So let\\'s time them:A commenter said:Perfect answer, I just miss the timing of comparing adding only one elementDo the semantically correct thing. If you want to append all elements in an iterable, use extend. If you\\'re just adding one element, use append.Ok, so let\\'s create an experiment to see how this works out in time:And we see that going out of our way to create an iterable just to use extend is a (minor) waste of time:We learn from this that there\\'s nothing gained from using extend when we have only one element to append.Also, these timings are not that important. I am just showing them to make the point that, in Python, doing the semantically correct thing is doing things the Right Way\u2122.It\\'s conceivable that you might test timings on two comparable operations and get an ambiguous or inverse result. Just focus on doing the semantically correct thing.We see that extend is semantically clearer, and that it can run much faster than append, when you intend to append each element in an iterable to a list.If you only have a single element (not in an iterable) to add to the list, use append.', 'append appends a single element. extend appends a list of elements.Note that if you pass a list to append, it still adds one element:', \"With append you can append a single element that will extend the list:If you want to extend more than one element you should use extend, because you can only append one elment or one list of element:So that you get a nested listInstead with extend, you can extend a single element like thisOr, differently, from append, extend more elements in one time without nesting the list into the original one (that's the reason of the name extend)Both append and extend can add one element to the end of the list, though append is simpler.If you use append for more than one element, you have to pass a list of elements as arguments and you will obtain a NESTED list!With extend, instead, you pass a list as an argument, but you will obtain a list with the new element that is not nested in the old one.So, with more elements, you will use extend to get a list with more items.\\nHowever, appending a list will not add more elements to the list, but one element that is a nested list as you can clearly see in the output of the code.\", 'The following two snippets are semantically equivalent:andThe latter may be faster as the loop is implemented in C.', 'The append() method adds a single item to the end of the list.The extend() method takes one argument, a list, and appends each of the items of the argument to the original list. (Lists are implemented as classes. \u201cCreating\u201d a list is really instantiating a class. As such, a list has methods that operate on it.)From Dive Into Python.', 'You can use \"+\" for returning extend, instead of extending in place.Similarly += for in place behavior, but with slight differences from append & extend. One of the biggest differences of += from append and extend is when it is used in function scopes, see this blog post.', 'append(object) updates the list by adding the object to the list.extend(list) concatenates the two lists essentially.', 'This is the equivalent of append and extend using the + operator:', \"extend() can be used with an iterator argument. Here is an example. You wish to make a list out of a list of lists this way:Fromyou wantYou may use itertools.chain.from_iterable() to do so. This method's output is an iterator. Its implementation is equivalent toBack to our example, we can doand get the wanted list.Here is how equivalently extend() can be used with an iterator argument:\", 'append(): It is basically used in Python to add one element.Example 1:Example 2:extend(): Where extend(), is used to merge two lists or insert multiple elements in one list.Example 1:Example 2:', 'An interesting point that has been hinted, but not explained, is that extend is faster than append. For any loop that has append inside should be considered to be replaced by list.extend(processed_elements).Bear in mind that apprending new elements might result in the realloaction of the whole list to a better location in memory. If this is done several times because we are appending 1 element at a time, overall performance suffers. In this sense, list.extend is analogous to \"\".join(stringlist).', 'Append adds the entire data at once. The whole data will be added to the newly created index. On the other hand, extend, as it name suggests, extends the current array.For exampleWith append we get:While on extend we get:', \"An English dictionary defines the words append and extend as:append: add (something) to the end of a written document. \\nextend: make larger. Enlarge or expandWith that knowledge, now let's understand1) The difference between append and extendappend:extend:2) Similarity between append and extendExample\", \"I hope I can make a useful supplement to this question. If your list stores a specific type object, for example Info, here is a situation that extend method is not suitable: In a for loop and and generating an Info object every time and using extend to store it into your list, it will fail. The exception is like below:TypeError: 'Info' object is not iterableBut if you use the append method, the result is OK. Because every time using the extend method, it will always treat it as a list or any other collection type, iterate it, and place it after the previous list. A specific object can not be iterated, obviously.\", \"To distinguish them intuitivelyIt's like l1 reproduce a body inside her body(nested).It's like that two separated individuals get married and construct an united family.Besides I make an exhaustive cheatsheet of all list's methods for your reference.\", 'extend(L) extends the list by appending all the items in the given list L.', 'append \"extends\" the list (in place) by only one item, the single object passed (as argument).extend \"extends\" the list (in place) by as many items as the object passed (as argument) contains.This may be slightly confusing for str objects.produces:', \"Append and extend are one of the extensibility mechanisms in python.Append: Adds an element to the end of the list.To add a new element to the list, we can use append method in the following way.The default location that the new element will be added is always in the (length+1) position.Insert: The insert method was used to overcome the limitations of append. With insert, we can explicitly define the exact position we want our new element to be inserted at.Method descriptor of insert(index, object). It takes two arguments, first being the index we want to insert our element and second the element itself.Extend: This is very useful when we want to join two or more lists into a single list. Without extend, if we want to join two lists, the resulting object will contain a list of lists.If we try to access the element at pos 2, we get a list ([3]), instead of the element. To join two lists, we'll have to use append.To join multiple lists\"]",
            "url": "https://stackoverflow.com/questions/252703"
        },
        {
            "tag": "python",
            "question": [
                "Understanding Python super() with __init__() methods [duplicate]"
            ],
            "votes": "3108",
            "answer": "[\"super() lets you avoid referring to the base class explicitly, which can be nice. But the main advantage comes with multiple inheritance, where all sorts of fun stuff can happen. See the standard docs on super if you haven't already.Note that the syntax changed in Python 3.0: you can just say super().__init__() instead of super(ChildB, self).__init__() which IMO is quite a bit nicer. The standard docs also refer to a guide to using super() which is quite explanatory.\", \"The reason we use super is so that child classes that may be using cooperative multiple inheritance will call the correct next parent class function in the Method Resolution Order (MRO).In Python 3, we can call it like this:In Python 2, we were required to call super like this with the defining class's name and self, but we'll avoid this from now on because it's redundant, slower (due to the name lookups), and more verbose (so update your Python if you haven't already!):Without super, you are limited in your ability to use multiple inheritance because you hard-wire the next parent's call:I further explain below.The primary difference in this code is that in ChildB you get a layer of indirection in the __init__ with super, which uses the class in which it is defined to determine the next class's __init__ to look up in the MRO.I illustrate this difference in an answer at the canonical question, How to use 'super' in Python?, which demonstrates dependency injection and cooperative multiple inheritance.Here's code that's actually closely equivalent to super (how it's implemented in C, minus some checking and fallback behavior, and translated to Python):Written a little more like native Python:If we didn't have the super object, we'd have to write this manual code everywhere (or recreate it!) to ensure that we call the proper next method in the Method Resolution Order!How does super do this in Python 3 without being told explicitly which class and instance from the method it was called from?It gets the calling stack frame, and finds the class (implicitly stored as a local free variable, __class__, making the calling function a closure over the class) and the first argument to that function, which should be the instance or class that informs it which Method Resolution Order (MRO) to use.Since it requires that first argument for the MRO, using super with static methods is impossible as they do not have access to the MRO of the class from which they are called.super() lets you avoid referring to the base class explicitly, which can be nice. . But the main advantage comes with multiple inheritance, where all sorts of fun stuff can happen. See the standard docs on super if you haven't already.It's rather hand-wavey and doesn't tell us much, but the point of super is not to avoid writing the parent class. The point is to ensure that the next method in line in the method resolution order (MRO) is called. This becomes important in multiple inheritance.I'll explain here.And let's create a dependency that we want to be called after the Child:Now remember, ChildB uses super, ChildA does not:And UserA does not call the UserDependency method:But UserB does in-fact call UserDependency because ChildB invokes super:In no circumstance should you do the following, which another answer suggests, as you'll definitely get errors when you subclass ChildB:(That answer is not clever or particularly interesting, but in spite of direct criticism in the comments and over 17 downvotes, the answerer persisted in suggesting it until a kind editor fixed his problem.)Explanation: Using self.__class__ as a substitute for the class name in super() will lead to recursion. super lets us look up the next parent in the MRO (see the first section of this answer) for child classes. If you tell super we're in the child instance's method, it will then lookup the next method in line (probably this one) resulting in recursion, probably causing a logical failure (in the answerer's example, it does) or a RuntimeError when the recursion depth is exceeded.Python 3's new super() calling method with no arguments fortunately allows us to sidestep this issue.\", \"It's been noted that in Python 3.0+ you can useto make your call, which is concise and does not require you to reference the parent OR class names explicitly, which can be handy. I just want to add that for Python 2.7 or under, some people implement a name-insensitive behaviour by writing self.__class__ instead of the class name, i.e.HOWEVER, this breaks calls to super for any classes that inherit from your class, where self.__class__ could return a child class. For example:Here I have a class Square, which is a sub-class of Rectangle. Say I don't want to write a separate constructor for Square because the constructor for Rectangle is good enough, but for whatever reason I want to implement a Square so I can reimplement some other method.When I create a Square using mSquare = Square('a', 10,10), Python calls the constructor for Rectangle because I haven't given Square its own constructor. However, in the constructor for Rectangle, the call super(self.__class__,self) is going to return the superclass of mSquare, so it calls the constructor for Rectangle again. This is how the infinite loop happens, as was mentioned by @S_C. In this case, when I run super(...).__init__() I am calling the constructor for Rectangle but since I give it no arguments, I will get an error.\", 'Super has no side effectsworks as expectedgets into infinite recursion.', \"Just a heads up... with Python 2.7, and I believe ever since super() was introduced in version 2.2, you can only call super() if one of the parents inherit from a class that eventually inherits object (new-style classes).Personally, as for python 2.7 code, I'm going to continue using BaseClassName.__init__(self, args) until I actually get the advantage of using super().\", \"There isn't, really. super() looks at the next class in the MRO (method resolution order, accessed with cls.__mro__) to call the methods. Just calling the base __init__ calls the base __init__. As it happens, the MRO has exactly one item-- the base. So you're really doing the exact same thing, but in a nicer way with super() (particularly if you get into multiple inheritance later).\", \"The main difference is that ChildA.__init__ will unconditionally call Base.__init__ whereas ChildB.__init__ will call __init__ in whatever class happens to be ChildB ancestor in self's line of ancestors\\n(which may differ from what you expect).If you add a ClassC that uses multiple inheritance:then Base is no longer the parent of ChildB for ChildC instances. Now super(ChildB, self) will point to Mixin if self is a ChildC instance.You have inserted Mixin in between ChildB and Base. And you can take advantage of it with super()So if you are designed your classes so that they can be used in a Cooperative Multiple Inheritance scenario, you use super because you don't really know who is going to be the ancestor at runtime.The super considered super post and pycon 2015 accompanying video explain this pretty well.\"]",
            "url": "https://stackoverflow.com/questions/576169"
        },
        {
            "tag": "python",
            "question": [
                "How to make function decorators and chain them together?"
            ],
            "votes": "3086",
            "answer": "['If you are not into long explanations, see Paolo Bergantino\u2019s answer.To understand decorators, you must first understand that functions are objects in Python. This has important consequences. Let\u2019s see why with a simple example :Keep this in mind. We\u2019ll circle back to it shortly.Another interesting property of Python functions is they can be defined inside another function!Okay, still here? Now the fun part...You\u2019ve seen that functions are objects. Therefore, functions:That means that a function can return another function.There\u2019s more!If you can return a function, you can pass one as a parameter:Well, you just have everything needed to understand decorators. You see, decorators are \u201cwrappers\u201d, which means that they let you execute code before and after the function they decorate without modifying the function itself.How you\u2019d do it manually:Now, you probably want that every time you call a_stand_alone_function, a_stand_alone_function_decorated is called instead. That\u2019s easy, just overwrite a_stand_alone_function with the function returned by my_shiny_new_decorator:The previous example, using the decorator syntax:Yes, that\u2019s all, it\u2019s that simple. @decorator is just a shortcut to:Decorators are just a pythonic variant of the decorator design pattern. There are several classic design patterns embedded in Python to ease development (like iterators).Of course, you can accumulate decorators:Using the Python decorator syntax:The order you set the decorators MATTERS:As a conclusion, you can easily see how to answer the question:You can now just leave happy, or burn your brain a little bit more and see advanced uses of decorators.One nifty thing about Python is that methods and functions are really the same.  The only difference is that methods expect that their first argument is a reference to the current object (self).That means you can build a decorator for methods the same way! Just remember to take self into consideration:If you\u2019re making general-purpose decorator--one you\u2019ll apply to any function or method, no matter its arguments--then just use *args, **kwargs:Great, now what would you say about passing arguments to the decorator itself?This can get somewhat twisted, since a decorator must accept a function as an argument. Therefore, you cannot pass the decorated function\u2019s arguments directly to the decorator.Before rushing to the solution, let\u2019s write a little reminder:It\u2019s exactly the same. \"my_decorator\" is called. So when you @my_decorator, you are telling Python to call the function \\'labelled by the variable \"my_decorator\"\\'.This is important! The label you give can point directly to the decorator\u2014or not.Let\u2019s get evil. \u263aNo surprise here.Let\u2019s do EXACTLY the same thing, but skip all the pesky intermediate variables:Let\u2019s make it even shorter:Hey, did you see that? We used a function call with the \"@\" syntax! :-)So, back to decorators with arguments. If we can use functions to generate the decorator on the fly, we can pass arguments to that function, right?Here it is: a decorator with arguments. Arguments can be set as variable:As you can see, you can pass arguments to the decorator like any function using this trick. You can even use *args, **kwargs if you wish. But remember decorators are called only once. Just when Python imports the script. You can\\'t dynamically set the arguments afterwards. When you do \"import x\", the function is already decorated, so you can\\'t\\nchange anything.Okay, as a bonus, I\\'ll give you a snippet to make any decorator accept generically any argument. After all, in order to accept arguments, we created our decorator using another function.We wrapped the decorator.Anything else we saw recently that wrapped function?Oh yes, decorators!Let\u2019s have some fun and write a decorator for the decorators:It can be used as follows:I know, the last time you had this feeling, it was after listening a guy saying: \"before understanding recursion, you must first understand recursion\". But now, don\\'t you feel good about mastering this?The functools module was introduced in Python 2.5. It includes the function functools.wraps(), which copies the name, module, and docstring of the decorated function to its wrapper.(Fun fact: functools.wraps() is a decorator! \u263a)Now the big question: What can I use decorators for?Seem cool and powerful, but a practical example would be great. Well, there are 1000 possibilities. Classic uses are extending a function behavior from an external lib (you can\\'t modify it), or for debugging (you don\\'t want to modify it because it\u2019s temporary).You can use them to extend several functions in a DRY\u2019s way, like so:Of course the good thing with decorators is that you can use them right away on almost anything without rewriting. DRY, I said:Python itself provides several decorators: property, staticmethod, etc.This really is a large playground.', 'Check out the documentation to see how decorators work. Here is what you asked for:', \"Alternatively, you could write a factory function which return a decorator which wraps the return value of the decorated function in a tag passed to the factory function. For example:This enables you to write:orPersonally I would have written the decorator somewhat differently:which would yield:Don't forget the construction for which decorator syntax is a shorthand:\", 'Decorators are just syntactical sugar.Thisexpands to', 'And of course you can return lambdas as well from a decorator function:', 'Python decorators add extra functionality to another functionAn italics decorator could be likeNote that a function is defined inside a function.\\nWhat it basically does is replace a function with the newly defined one. For example, I have this classNow say, I want both functions to print \"---\" after and before they are done.\\nI could add a print \"---\" before and after each print statement.\\nBut because I don\\'t like repeating myself, I will make a decoratorSo now I can change my class toFor more on decorators, check\\nhttp://www.ibm.com/developerworks/linux/library/l-cpdecor.html', \"You could make two separate decorators that do what you want as illustrated directly below. Note the use of *args, **kwargs in the declaration of the wrapped() function which supports the decorated function having multiple arguments (which isn't really necessary for the example say() function, but is included for generality).For similar reasons, the functools.wraps decorator is used to change the meta attributes of the wrapped function to be those of the one being decorated. This makes error messages and embedded function documentation (func.__doc__) be those of the decorated function instead of wrapped()'s.As you can see there's a lot of duplicate code in these two decorators. Given this similarity it would be better for you to instead make a generic one that was actually a decorator factory\u2014in other words, a decorator function that makes other decorators. That way there would be less code repetition\u2014and allow the DRY principle to be followed.To make the code more readable, you can assign a more descriptive name to the factory-generated decorators:or even combine them like this:While the above examples do all work, the code generated involves a fair amount of overhead in the form of extraneous function calls when multiple decorators are applied at once. This may not matter, depending the exact usage (which might be I/O-bound, for instance).If speed of the decorated function is important, the overhead can be kept to a single extra function call by writing a slightly different decorator factory-function which implements adding all the tags at once, so it can generate code that avoids the addtional function calls incurred by using separate decorators for each tag.This requires more code in the decorator itself, but this only runs when it's being applied to function definitions, not later when they themselves are called. This also applies when creating more readable names by using lambda functions as previously illustrated. Sample:\", 'Another way of doing the same thing:Or, more flexibly:', \"You want the following function, when called:To return:To most simply do this, make decorators that return lambdas (anonymous functions) that close over the function (closures) and call it:Now use them as desired:and now:But we seem to have nearly lost the original function.To find it, we'd need to dig into the closure of each lambda, one of which is buried in the other:So if we put documentation on this function, or wanted to be able to decorate functions that take more than one argument, or we just wanted to know what function we were looking at in a debugging session, we need to do a bit more with our wrapper.We have the decorator wraps from the functools module in the standard library!It is unfortunate that there's still some boilerplate, but this is about as simple as we can make it.In Python 3, you also get __qualname__ and __annotations__ assigned by default.So now:And now:So we see that wraps makes the wrapping function do almost everything except tell us exactly what the function takes as arguments.There are other modules that may attempt to tackle the problem, but the solution is not yet in the standard library.\", \"A decorator takes the function definition and creates a new function that executes this function and transforms the result.is equivalent to:Thisis equivalent to this65 <=> 'a'To understand the decorator, it is important to notice, that decorator created a new function do which is inner that executes function and transforms the result.\", \"This answer has long been answered, but I thought I would share my Decorator class which makes writing new decorators easy and compact.For one I think this makes the behavior of decorators very clear, but it also makes it easy to define new decorators very concisely. For the example listed above, you could then solve it as:You could also use it to do more complex tasks, like for instance a decorator which automatically makes the function get applied recursively to all arguments in an iterator:Which prints:Notice that this example didn't include the list type in the instantiation of the decorator, so in the final print statement the method gets applied to the list itself, not the elements of the list.\", 'You can also write decorator in Class', 'Here is a simple example of chaining decorators.  Note the last line - it shows what is going on under the covers.The output looks like:', 'Speaking of the counter example - as given above, the counter will be shared between all functions that use the decorator:That way, your decorator can be reused for different functions (or used to decorate the same function multiple times: func_counter1 = counter(func); func_counter2 = counter(func)), and the counter variable will remain private to each.', 'Result:', \"Paolo Bergantino's answer has the great advantage of only using the stdlib, and works for this simple example where there are no decorator arguments nor decorated function arguments.However it has 3 major limitations if you want to tackle more general cases:I wrote decopatch to solve the first issue, and wrote makefun.wraps to solve the other two. Note that makefun leverages the same trick than the famous decorator lib.This is how you would create a decorator with arguments, returning truly signature-preserving wrappers:decopatch provides you with two other development styles that hide or show the various python concepts, depending on your preferences. The most compact style is the following:In both cases you can check that the decorator works as expected:Please refer to the documentation for details.\", 'I add a case when you need to add custom parameters in decorator, pass it to final function and then work it with.the very decorators:and the final function:', \"Yet another example of nested decorators for plotting an image:Now, let's show a color image first without axis labels using the nested decorators:Next, let's show a gray scale image without axis labels using the nested decorators remove_axis and plot_gray (we need to cmap='gray', otherwise the default colormap is viridis, so a grayscale image is by default not displayed in black and white shades, unless explicitly specified)The above function call reduces down to the following nested call\", 'With make_bold() and make_italic() below:You can use them as decorators with say() as shown below:Output:And of course, you can directly use make_bold() and make_italic() without decorators as shown below:In short:Output:', 'Consider the following decorator, note that we are returning the wrapper() function as an objectSo Thisevaluates to thisNote that x is not the say() but the wrapper object that calls say() internally. That is how decorator works. It always returns the wrapper object which calls the actual function.\\nIn case of chaining thisgets converted to thisBelow is the complete codeThe above code will returnHope this helps']",
            "url": "https://stackoverflow.com/questions/739654"
        },
        {
            "tag": "python",
            "question": [
                "How do I change the size of figures drawn with Matplotlib?"
            ],
            "votes": "3063",
            "answer": "['figure tells you the call signature:figure(figsize=(1,1)) would create an inch-by-inch image, which would be 80-by-80 pixels unless you also give a different dpi argument.', 'If you\\'ve already got the figure created, you can use figure.set_size_inches to adjust the figure size:To propagate the size change to an existing GUI window, add forward=True:Additionally as Erik Shilts mentioned in the comments you can also use figure.set_dpi to \"[s]et the resolution of the figure in dots-per-inch\"', 'There is also this workaround in case you want to change the size without using the figure environment. So in case you are using plt.plot() for example, you can set a tuple with width and height.This is very useful when you plot inline (e.g., with IPython Notebook). As asmaier noticed, it is preferable to not put this statement in the same cell of the imports statements.To reset the global figure size back to default for subsequent plots:The figsize tuple accepts inches, so if you want to set it in centimetres you have to divide them by 2.54. Have a look at this question.', \"Deprecation note:\\nAs per the official Matplotlib guide, usage of the pylab module is no longer recommended. Please consider using the matplotlib.pyplot module instead, as described by this other answer.The following seems to work:This makes the figure's width 5 inches, and its height 10 inches.The Figure class then uses this as the default value for one of its arguments.\", \"In case you're looking for a way to change the figure size in Pandas, you could do:where df is a Pandas dataframe. Or, to use an existing figure or axes:If you want to change the default settings, you could do the following:For more details, check out the docs: pd.DataFrame.plot.\", \"The first link in Google for 'matplotlib figure size' is AdjustingImageSize (Google cache of the page).Here's a test script from the above page. It creates test[1-3].png files of different sizes of the same image:Output:Two notes:The module comments and the actual output differ.This answer allows easily to combine all three images in one image file to see the difference in sizes.\", 'You can simply use (from matplotlib.figure.Figure):As of Matplotlib 2.0.0, changes to your canvas will be visible immediately, as the forward keyword defaults to True.If you want to just change the width or height instead of both, you can usefig.set_figwidth(val) or fig.set_figheight(val)These will also immediately update your canvas, but only in Matplotlib 2.2.0 and newer.You need to specify forward=True explicitly in order to live-update your canvas in versions older than what is specified above. Note that the set_figwidth and set_figheight functions don\u2019t support the forward parameter in versions older than Matplotlib 1.5.0.', 'Try commenting out the fig = ... line', 'This works well for me:This forum post might also help: Resizing figure windows', 'Comparison of different approaches to set exact image sizes in pixelsThis answer will focus on:Here is a quick comparison of some of the approaches I\\'ve tried with images showing what the give.Summary of current status: things are messy, and I am not sure if it is a fundamental limitation, or if the use case just didn\\'t get enough attention from developers. I couldn\\'t easily find an upstream discussion about this.Baseline example without trying to set the image dimensionsJust to have a comparison point:Run:Outputs:My best approach so far: plt.savefig(dpi=h/fig.get_size_inches()[1] height-only controlI think this is what I\\'ll go with most of the time, as it is simple and scales:Run:Outputs:andOutputs:I tend to set just the height because I\\'m usually most concerned about how much vertical space the image is going to take up in the middle of my text.plt.savefig(bbox_inches=\\'tight\\' changes image sizeI always feel that there is too much white space around images, and tended to add bbox_inches=\\'tight\\' from:\\nRemoving white space around a saved imageHowever, that works by cropping the image, and you won\\'t get the desired sizes with it.Instead, this other approach proposed in the same question seems to work well:which gives the exact desired height for height equals 431:Fixed height, set_aspect, automatically sized width and small marginsErmmm, set_aspect messes things up again and prevents plt.tight_layout from actually removing the margins... this is an important use case that I don\\'t have a great solution for yet.Asked at: How to obtain a fixed height in pixels, fixed data x/y aspect ratio and automatically remove remove horizontal whitespace margin in Matplotlib?plt.savefig(dpi=h/fig.get_size_inches()[1] + width controlIf you really need a specific width in addition to height, this seems to work OK:Run:Output:and for a small width:Output:So it does seem that fonts are scaling correctly, we just get some trouble for very small widths with labels getting cut off, e.g. the 100 on the top left.I managed to work around those with Removing white space around a saved imagewhich gives:From this, we also see that tight_layout removes a lot of the empty space at the top of the image, so I just generally always use it.Fixed magic base height, dpi on fig.set_size_inches and plt.savefig(dpi= scalingI believe that this is equivalent to the approach mentioned at: https://stackoverflow.com/a/13714720/895245Run:Outputs:And to see if it scales nicely:Outputs:So we see that this approach also does work well. The only problem I have with it is that you have to set that magic_height parameter or equivalent.Fixed DPI + set_size_inchesThis approach gave a slightly wrong pixel size, and it makes it is hard to scale everything seamlessly.Run:Outputs:So the height is slightly off, and the image:The pixel sizes are also correct if I make it 3 times larger:Outputs:We understand from this however that for this approach to scale nicely, you need to make every DPI-dependant setting proportional to the size in inches.In the previous example, we only made the \"Hello\" text proportional, and it did retain its height between 60 and 80 as we\\'d expect. But everything for which we didn\\'t do that, looks tiny, including:SVGI could not find how to set it for SVG images, my approaches only worked for PNG, e.g.:Run:And the generated output contains:And identify says:And if I open it in Chromium 86 the browser debug tools mouse image hover confirm that height as 460.79.But of course, since SVG is a vector format, everything should in theory scale, so you can just convert to any fixed sized format without loss of resolution, e.g.:gives the exact height:I use Inkscape instead of ImageMagick\\'s convert here because you need to mess with -density as well to get sharp SVG resizes with ImageMagick:And setting <img height=\"\" on the HTML should also just work for the browser.It was tested on matplotlib 3.2.2.', \"Generalizing and simplifying psihodelia's answer:If you want to change the current size of the figure by a factor sizefactor:After changing the current size, it might occur that you have to fine tune the subplot layout. You can do that in the figure window GUI, or by means of the command subplots_adjustFor example,\", 'This resizes the figure immediately even after the figure has been drawn (at least using Qt4Agg/TkAgg - but not Mac\\xa0OS\\xa0X - with Matplotlib 1.4.0):', 'I always use the following pattern:With this example you are able to set figure dimensions in inches or in millimetres. When setting constrained_layout to True, plots fill your figure without borders.']",
            "url": "https://stackoverflow.com/questions/332289"
        },
        {
            "tag": "python",
            "question": [
                "Manually raising (throwing) an exception in Python"
            ],
            "votes": "3025",
            "answer": "['Use the most specific Exception constructor that semantically fits your issue.Be specific in your message, e.g.:Avoid raising a generic Exception. To catch it, you\\'ll have to catch all other more specific exceptions that subclass it.For example:And more specific catches won\\'t catch the general exception:Instead, use the most specific Exception constructor that semantically fits your issue.which also handily allows an arbitrary number of arguments to be passed to the constructor:These arguments are accessed by the args attribute on the Exception object. For example:printsIn Python 2.5, an actual message attribute was added to BaseException in favor of encouraging users to subclass Exceptions and stop using args, but the introduction of message and the original deprecation of args has been retracted.When inside an except clause, you might want to, for example, log that a specific type of error happened, and then re-raise. The best way to do this while preserving the stack trace is to use a bare raise statement. For example:You can preserve the stacktrace (and error value) with sys.exc_info(), but this is way more error prone and has compatibility problems between Python 2 and 3, prefer to use a bare raise to re-raise.To explain - the sys.exc_info() returns the type, value, and traceback.This is the syntax in Python 2 - note this is not compatible with Python 3:If you want to, you can modify what happens with your new raise - e.g. setting new args for the instance:And we have preserved the whole traceback while modifying the args. Note that this is not a best practice and it is invalid syntax in Python 3 (making keeping compatibility much harder to work around).In Python 3:Again: avoid manually manipulating tracebacks. It\\'s less efficient and more error prone. And if you\\'re using threading and sys.exc_info you may even get the wrong traceback (especially if you\\'re using exception handling for control flow - which I\\'d personally tend to avoid.)In Python 3, you can chain Exceptions, which preserve tracebacks:Be aware:These can easily hide and even get into production code. You want to raise an exception, and doing them will raise an exception, but not the one intended!Valid in Python 2, but not in Python 3 is the following:Only valid in much older versions of Python (2.4 and lower), you may still see people raising strings:In all modern versions, this will actually raise a TypeError, because you\\'re not raising a BaseException type. If you\\'re not checking for the right exception and don\\'t have a reviewer that\\'s aware of the issue, it could get into production.I raise Exceptions to warn consumers of my API if they\\'re using it incorrectly:\"I want to make an error on purpose, so that it would go into the except\"You can create your own error types, if you want to indicate something specific is wrong with your application, just subclass the appropriate point in the exception hierarchy:and usage:', \"Don't do this. Raising a bare Exception is absolutely not the right thing to do; see Aaron Hall's excellent answer instead.It can't get much more Pythonic than this:Replace Exception with the specific type of exception you want to throw.See the raise statement documentation for Python if you'd like more information.\", \"In Python 3 there are four different syntaxes for raising exceptions:If you use raise exception (args) to raise an exception then the args will be printed when you print the exception object - as shown in the example below.The raise statement without any arguments re-raises the last exception.This is useful if you need to perform some actions after catching the exception and then want to re-raise it. But if there wasn't any exception before, the raise statement raises  a TypeError Exception.This statement is used to create exception chaining in which an exception that is raised in response to another exception can contain the details of the original exception - as shown in the example below.Output:\", 'For the common case where you need to throw an exception in response to some unexpected conditions, and that you never intend to catch, but simply to fail fast to enable you to debug from there if it ever happens \u2014 the most logical one seems to be AssertionError:', 'Read the existing answers first, this is just an addendum.Notice that you can raise exceptions with or without arguments.Example:exits the program, but you might want to know what happened. So you can use this.This will print \"program exited\" to standard error before closing the program.', \"Just to note: there are times when you do want to handle generic exceptions. If you're processing a bunch of files and logging your errors, you might want to catch any error that occurs for a file, log it, and continue processing the rest of the files. In that case, ablock is a good way to do it. You'll still want to raise specific exceptions so you know what they mean, though.\", 'Another way to throw an exception is using assert. You can use assert to verify a condition is being fulfilled. If not, then it will raise AssertionError. For more details have a look here.', \"You might also want to raise custom exceptions. For example, if you're writing a library, it's a very good practice to make a base exception class for your module, and then have custom sub-exceptions to be more specific.You can achieve that like this:If you're not interested in having a custom base class, you can just inherit your custom exception classes from an ordinary exception class like Exception, TypeError, ValueError, etc.\", \"If you don't care about which error to raise, you could use assert to raise an AssertionError:The assert keyword raises an AssertionError if the condition is False. In this case, we specified False directly, so it raises the error, but to have it have a text we want it to raise to, we add a comma and specify the error text we want. In this case, I wrote Manually raised error and this raises it with that text.\", 'You should learn the raise statement of Python for that.It should be kept inside the try block.Example -', 'If you don\u2019t care about the raised exception, do:The good old division by 0.']",
            "url": "https://stackoverflow.com/questions/2052390"
        },
        {
            "tag": "python",
            "question": [
                "How do I print colored text to the terminal?"
            ],
            "votes": "2970",
            "answer": "['This somewhat depends on what platform you are on. The most common way to do this is by printing ANSI escape sequences. For a simple example, here\\'s some Python code from the Blender build scripts:To use code like this, you can do something like:Or, with Python 3.6+:This will work on unixes including OS X, Linux and Windows (provided you use ANSICON, or in Windows 10 provided you enable VT100 emulation). There are ANSI codes for setting the color, moving the cursor, and more.If you are going to get complicated with this (and it sounds like you are if you are writing a game), you should look into the \"curses\" module, which handles a lot of the complicated parts of this for you. The Python Curses HowTO is a good introduction.If you are not using extended ASCII (i.e., not on a PC), you are stuck with the ASCII characters below 127, and \\'#\\' or \\'@\\' is probably your best bet for a block. If you can ensure your terminal is using a IBM extended ASCII character set, you have many more options. Characters 176, 177, 178 and 219 are the \"block characters\".Some modern text-based programs, such as \"Dwarf Fortress\", emulate text mode in a graphical mode, and use images of the classic PC font. You can find some of these bitmaps that you can use on the Dwarf Fortress Wiki see (user-made tilesets).The Text Mode Demo Contest has more resources for doing graphics in text mode.', 'There is also the Python termcolor module. Usage is pretty simple:Or in Python 3:It may not be sophisticated enough, however, for game programming and the \"colored blocks\" that you want to do...To get the ANSI codes working on windows, first run', 'The answer is Colorama for all cross-platform coloring in Python.It supports Python 3.5+ as well as Python 2.7.And as of January 2021 it is maintained.Example Code:Example Screenshot:', \"Print a string that starts a color/style, then the string, and then end the color/style change with '\\\\x1b[0m':Get a table of format options for shell text with the following code:Reference: https://en.wikipedia.org/wiki/ANSI_escape_code#Colors\", 'Define a string that starts a color and a string that ends the color. Then print your text with the start string at the front and the end string at the end.This produces the following in Bash, in urxvt with a Zenburn-style color scheme:Through experimentation, we can get more colors:Note: \\\\33[5m and \\\\33[6m are blinking.This way we can create a full color collection:Here is the code to generate the test:', 'Here\\'s a solution that works on Windows 10 natively.Using a system call, such as os.system(\"\"), allows colours to be printed in Command Prompt and Powershell natively:Note: Windows does not fully support ANSI codes, whether through system calls or modules. Not all text decoration is supported, and although the bright colours display, they are identical to the regular colours.Thanks to @j-l for finding an even shorter method.tl;dr: Add os.system(\"\")', \"You want to learn about ANSI escape sequences. Here's a brief example:For more information, see ANSI escape code.For a block character, try a Unicode character like \\\\u2588:Putting it all together:\", \"sty is similar to colorama, but it's less verbose, supports 8-bit and 24-bit (RGB) colors, supports all effects (bold, underline, etc.), allows you to register your own styles, is fully typed and high performant, supports muting, is not messing with globals such as sys.stdout, is really flexible, well documented and more...Examples:prints:Demo:\", 'Rich is a relatively new Python library for working with color in the terminal.There are a few ways of working with color in Rich. The quickest way to get started would be the rich print method which renders a BBCode-like syntax in to ANSI control codes:There are other ways of applying color with Rich (regex, syntax) and related formatting features.', \"My favorite way is with the Blessings library (full disclosure: I wrote it). For example:To print colored bricks, the most reliable way is to print spaces with background colors. I use this technique to draw the progress bar in nose-progressive:You can print in specific locations as well:If you have to muck with other terminal capabilities in the course of your game, you can do that as well. You can use Python's standard string formatting to keep it readable:The nice thing about Blessings is that it does its best to work on all sorts of terminals, not just the (overwhelmingly common) ANSI-color ones. It also keeps unreadable escape sequences out of your code while remaining concise to use. Have fun!\", 'I generated a class with all the colors using a for loop to iterate every combination of color up to 100, and then wrote a class with Python colors. Copy and paste as you will, GPLv2 by me:', 'This is, in my opinion, the easiest method. As long as you have the RGB values of the color you want, this should work:An example of printing red text:Multi-colored text', 'Try this simple codePython 3 Example', 'Try it online', \"I have a library called colorit. It is super simple.Here are some examples:This gives you:It's also worth noting that this is cross platform and has been tested on Mac, Linux, and Windows.You might want to try it out: https://github.com/SuperMaZingCoder/coloritcolorit is now available to be installed with PyPi! You can install it with pip install color-it on Windows and pip3 install color-it on macOS and Linux.\", \"On Windows you can use module 'win32console' (available in some Python distributions) or module 'ctypes' (Python 2.5 and up) to access the Win32 API.To see complete code that supports both ways, see the color console reporting code from Testoob.ctypes example:\", \"I have wrapped joeld's answer into a module with global functions that I can use anywhere in my code.File: log.pyUse as follows:\", 'Try online', 'Here is my modern (2021) solution: yachalkIt is one of the few libraries that properly supports nested styles:Apart from that yachalk is auto-complete-friendly, has 256/truecolor support, comes with terminal-capability detection, and is fully typed.Here are some design decision you may consider for choosing your solution.Many answers to this question demonstrate how to ANSI escape codes directly, or suggest low-level libraries that require manual style enabling/disabling.These approaches have subtle issues: Inserting on/off styles manually isTherefore if compatibility with many terminals is a goal, it\\'s best to use a high-level library that offers automatic handling of style resets. This allows the library to take care of all edge cases by inserting the \"spurious\" ANSI escape codes where needed.In JavaScript the de-facto standard library for the task is chalk, and after using it for a while in JS projects, the solutions available in the Python world were lacking in comparison. Not only is the chalk API more convenient to use (fully auto-complete compatible), it also gets all the edge cases right.The idea of yachalk is to bring the same convenience to the Python ecosystem. If you\\'re interested in a comparison to other libraries I\\'ve started feature comparison on the projects page. In addition, here is a long (but still incomplete) list of alternatives that came up during my research -- a lot to choose from :)', 'I ended up doing this, and I felt it was cleanest:', \"For Windows you cannot print to console with colors unless you're using the Win32 API.For Linux it's as simple as using print, with the escape sequences outlined here:ColorsFor the character to print like a box, it really depends on what font you are using for the console window. The pound symbol works well, but it depends on the font:\", \"Stupidly simple, based on joeld's answer:Then just\", \"Building on joeld's answer, using https://pypi.python.org/pypi/lazyme \\npip install -U lazyme:Screenshot:Some updates to the color_print with new formatters, e.g.:Note: italic, fast blinking, and strikethrough may not work on all terminals, and they don't work on Mac and Ubuntu.E.g.,Screenshot:\", 'Note how well the with keyword mixes with modifiers like these that need to be reset (using Python 3 and Colorama):', 'You could use Clint:', \"You can use the Python implementation of the curses library:\\ncurses \u2014 Terminal handling for character-cell displaysAlso, run this and you'll find your box:\", 'You can use colors for text as others mentioned in their answers to have colorful text with a background or foreground color.But you can use emojis instead! for example, you can use\u26a0\ufe0f for warning messages and \ud83d\uded1 for error messages.Or simply use these notebooks as a color:This method also helps you to quickly scan and find logs directly in the source code.But some operating systems (including some Linux distributions in some version with some window managers) default emoji font is not colorful by default and you may want to make them colorful, first.mac os: control + command + spacewindows: win + .linux: control + . or  control + ;', 'If you are programming a game perhaps you would like to change the background color and use only spaces? For example:', 'An easier option would be to use the cprint function from the termcolor package.It also supports %s, %d format of printing:Results can be terminal dependant, so review the Terminal Properties section of the package documentation.', 'While I find this answer useful, I modified it a bit. This GitHub Gist is the resultIn addition, you can wrap common usages:']",
            "url": "https://stackoverflow.com/questions/287871"
        },
        {
            "tag": "python",
            "question": [
                "How do I split a list into equally-sized chunks?"
            ],
            "votes": "2961",
            "answer": "[\"Here's a generator that yields evenly-sized chunks:For Python 2, using xrange instead of range:Below is a list comprehension one-liner. The method above is preferable, though, since using named functions makes code easier to understand. For Python 3:For Python 2:\", 'Something super simple:For Python 2, use xrange() instead of range().', 'I know this is kind of old but nobody yet mentioned numpy.array_split:Result:', 'Directly from the (old) Python documentation (recipes for itertools):The current version, as suggested by J.F.Sebastian:I guess Guido\\'s time machine works\u2014worked\u2014will work\u2014will have worked\u2014was working again.These solutions work because [iter(iterable)]*n (or the equivalent in the earlier version) creates one iterator, repeated n times in the list. izip_longest then effectively performs a round-robin of \"each\" iterator; because this is the same iterator, it is advanced by each such call, resulting in each such zip-roundrobin generating one tuple of n items.', \"I'm surprised nobody has thought of using iter's two-argument form:Demo:This works with any iterable and produces output lazily. It returns tuples rather than iterators, but I think it has a certain elegance nonetheless. It also doesn't pad; if you want padding, a simple variation on the above will suffice:Demo:Like the izip_longest-based solutions, the above always pads. As far as I know, there's no one- or two-line itertools recipe for a function that optionally pads. By combining the above two approaches, this one comes pretty close:Demo:I believe this is the shortest chunker proposed that offers optional padding.As Tomasz Gandor observed, the two padding chunkers will stop unexpectedly if they encounter a long sequence of pad values. Here's a final variation that works around that problem in a reasonable way:Demo:\", 'Here is a generator that work on arbitrary iterables:Example:', 'Simple yet elegantor if you prefer:', \"Don't reinvent the wheel.UPDATE: The upcoming Python 3.12 introduces itertools.batched, which solves this problem at last.  See below.GivenCodeitertools.batched++more_itertools+(or DIY, if you want)The Standard LibraryReferences+ A third-party library that implements itertools recipes and more. > pip install more_itertools++Included in Python Standard Library 3.12+.  batched is similar to more_itertools.chunked.\", '', '\"Evenly sized chunks\", to me, implies that they are all the same length, or barring that option, at minimal variance in length. E.g. 5 baskets for 21 items could have the following results:A practical reason to prefer the latter result: if you were using these functions to distribute work, you\\'ve built-in the prospect of one likely finishing well before the others, so it would sit around doing nothing while the others continued working hard.When I originally wrote this answer, none of the other answers were evenly sized chunks - they all leave a runt chunk at the end, so they\\'re not well balanced, and have a higher than necessary variance of lengths.For example, the current top answer ends with:Others, like list(grouper(3, range(7))), and chunk(range(7), 3) both return: [(0, 1, 2), (3, 4, 5), (6, None, None)]. The None\\'s are just padding, and rather inelegant in my opinion. They are NOT evenly chunking the iterables.Why can\\'t we divide these better?A high-level balanced solution using itertools.cycle, which is the way I might do it today. Here\\'s the setup:Now we need our lists into which to populate the elements:Finally, we zip the elements we\\'re going to allocate together with a cycle of the baskets until we run out of elements, which, semantically, it exactly what we want:Here\\'s the result:To productionize this solution, we write a function, and provide the type annotations:In the above, we take our list of items, and the max number of baskets. We create a list of empty lists, in which to append each element, in a round-robin style.Another elegant solution is to use slices - specifically the less-commonly used step argument to slices. i.e.:This is especially elegant in that slices don\\'t care how long the data are - the result, our first basket, is only as long as it needs to be. We\\'ll only need to increment the starting point for each basket.In fact this could be a one-liner, but we\\'ll go multiline for readability and to avoid an overlong line of code:And islice from the itertools module will provide a lazily iterating approach, like that which was originally asked for in the question.I don\\'t expect most use-cases to benefit very much, as the original data is already fully materialized in a list, but for large datasets, it could save nearly half the memory usage.View results with:Here\\'s another balanced solution, adapted from a function I\\'ve used in production in the past, that uses the modulo operator:And I created a generator that does the same if you put it into a list:And finally, since I see that all of the above functions return elements in a contiguous order (as they were given):To test them out:Which prints out:Notice that the contiguous generator provide chunks in the same length patterns as the other two, but the items are all in order, and they are as evenly divided as one may divide a list of discrete elements.', \"If you know list size:If you don't (an iterator):In the latter case, it can be rephrased in a more beautiful way if you can be sure that the sequence always contains a whole number of chunks of given size (i.e. there is no incomplete last chunk).\", 'I saw the most awesome Python-ish answer in a duplicate of this question:You can create n-tuple for any n. If a = range(1, 15), then the result will be:If the list is divided evenly, then you can replace zip_longest with zip, otherwise the triplet (13, 14, None) would be lost. Python 3 is used above. For Python 2, use izip_longest.', 'Where AA is array, SS is chunk size. For example:To expand the ranges in py3 do', 'With Assignment Expressions in Python 3.8 it becomes quite nice:This works on an arbitrary iterable, not just a list.UPDATEStarting with Python 3.12, this exact implementation is available as itertools.batched', \"If you had a chunk size of 3 for example, you could do:source:\\nhttp://code.activestate.com/recipes/303060-group-a-list-into-sequential-n-tuples/I would use this when my chunk size is fixed number I can type, e.g. '3', and would never change.\", 'The toolz library has the partition function for this:', 'I was curious about the performance of different approaches and here it is:Tested on Python 3.5.1Results:', 'You may also use get_chunks function of utilspie library as:You can install utilspie via pip:Disclaimer: I am the creator of utilspie library.', \"I like the Python doc's version proposed by tzot and J.F.Sebastian a lot,\\n but it has two shortcomings:I'm using this one a lot in my code:UPDATE: A lazy chunks version:\", 'code:result:', 'heh, one line version', 'Another more explicit version.', 'At this point, I think we need a recursive generator, just in case...In python 2:In python 3:Also, in case of massive Alien invasion, a decorated recursive generator might become handy:', 'Without calling len() which is good for large lists:And this is for iterables:The functional flavour of the above:OR:OR:', 'usage:', 'See this referencePython3', '', \"Since everybody here talking about iterators. boltons has perfect method for that, called iterutils.chunked_iter.Output:But if you don't want to be mercy on memory, you can use old-way and store the full list in the first place with iterutils.chunked.\", 'Consider using matplotlib.cbook piecesfor example:', '']",
            "url": "https://stackoverflow.com/questions/312443"
        },
        {
            "tag": "python",
            "question": [
                "How can I access environment variables in Python?"
            ],
            "votes": "2951",
            "answer": "['Environment variables are accessed through os.environ:To see a list of all environment variables:If a key is not present, attempting to access it will raise a KeyError. To avoid this:', 'To check if the key exists (returns True or False)You can also use get() when printing the key; useful if you want to use a default.where /home/username/ is the default', 'Actually it can be done this way:Or simply:For viewing the value in the parameter:Or:To set the value:', \"Here's how to check if $FOO is set:\", 'You can access the environment variables usingTry to see the content of the PYTHONPATH or PYTHONHOME environment variables. Maybe this will be helpful for your second question.', 'As for the environment variables:', 'That will print all of the environment variables along with their values.', 'Import the os module:To get an environment variable:To set an environment variable:', \"If you are planning to use the code in a production web application code, using any web framework like Django and Flask, use projects like envparse. Using it, you can read the value as your defined type.NOTE: kennethreitz's autoenv is a recommended tool for making project-specific environment variables. For those who are using autoenv, please note to keep the .env file private (inaccessible to public).\", 'There are also a number of great libraries. Envs, for example, will allow you to parse objects out of your environment variables, which is rad. For example:', 'You can also try this:First, install python-decoupleImport it in your fileThen get the environment variableRead more about the Python library here.', \"Edited - October 2021Following @Peter's comment, here's how you can test it:main.pyIf this is true ... It's 1500x faster to use a dict() instead of accessing environ directly.A performance-driven approach - calling environ is expensive, so it's better to call it once and save it to a dictionary. Full example:P.S- if you worry about exposing private environment variables, then sanitize env_dict after the assignment.\", 'For Django, see Django-environ.', 'You should first import os usingand then actually print the environment variable valueof course, replace yourvariable as the variable you want to access.', 'The tricky part of using nested for-loops in one-liners is that you have to use list comprehension. So in order to print all your environment variables, without having to import a foreign library, you can use:']",
            "url": "https://stackoverflow.com/questions/4906977"
        },
        {
            "tag": "python",
            "question": [
                "Convert string \"Jun 1 2005 1:33PM\" into datetime"
            ],
            "votes": "2878",
            "answer": "['datetime.strptime parses an input string in the user-specified format into a timezone-naive datetime object:To obtain a date object using an existing datetime object, convert it using .date():Links:strptime docs: Python 2, Python 3strptime/strftime format string docs: Python 2, Python 3strftime.org format string cheatsheetNotes:', 'Use the third-party dateutil library:It can handle most date formats and is more convenient than strptime since it usually guesses the correct format. It is also very useful for writing tests, where readability is more important than performance.Install it with:', 'Check out strptime in the time module.  It is the inverse of strftime.', 'To convert a YYYY-MM-DD string to a datetime object, datetime.fromisoformat could be used.Caution from the documentation:This does not support parsing arbitrary ISO 8601 strings - it is only intended as the inverse operation of datetime.isoformat(). A more full-featured ISO 8601 parser, dateutil.parser.isoparse is available in the third-party package dateutil.', 'I have put together a project that can convert some really neat expressions. Check out timestring.', \"Remember this and you didn't need to get confused in datetime conversion again.String to datetime object = strptimedatetime object to other formats = strftimeJun 1 2005  1:33PMis equals to%b %d %Y %I:%M%p%b    Month as locale\u2019s abbreviated name(Jun)%d    Day of the month as a zero-padded decimal number(1)%Y    Year with century as a decimal number(2015)%I    Hour (12-hour clock) as a zero-padded decimal number(01)%M    Minute as a zero-padded decimal number(33)%p    Locale\u2019s equivalent of either AM or PM(PM)so you need strptime i-e converting string toOutputWhat if you have different format of dates you can use panda or dateutil.parseOutPut\", 'Many timestamps have an implied timezone. To ensure that your code will work in every timezone, you should use UTC internally and attach a timezone each time a foreign object enters the system.Python 3.2+:This assumes you know the offset. If you don\\'t, but you know e.g. the location, you can use the pytz package to query the IANA time zone database for the offset. I\\'ll use Tehran here as an example because it has a half-hour offset:As you can see, pytz has determined that the offset was +3:30 at that particular date. You can now convert this to UTC time, and it will apply the offset:Note that dates before the adoption of timezones will give you weird offsets. This is because the IANA has decided to use Local Mean Time:The weird \"7 hours and 34 minutes\" are derived from the longitude of Chicago. I used this timestamp because it is right before standardized time was adopted in Chicago.', 'If your string is in ISO 8601 format and you have Python 3.7+, you can use the following simple code:for dates andfor strings containing date and time. If timestamps are included, the function datetime.datetime.isoformat() supports the following format:Where * matches any single character. See also here and here.', \"Here are two solutions using Pandas to convert dates formatted as strings into datetime.date objects.TimingsAnd here is how to convert the OP's original date-time examples:There are many options for converting from the strings to Pandas Timestamps using to_datetime, so check the docs if you need anything special.Likewise, Timestamps have many properties and methods that can be accessed in addition to .date\", \"I personally like the solution using the parser module, which is the second answer to this question and is beautiful, as you don't have to construct any string literals to get it working. But, one downside is that it is 90% slower than the accepted answer with strptime.Output:10.70296801342902 \\n1.3627995655316933As long as you are not doing this a million times over and over again, I still think the parser method is more convenient and will handle most of the time formats automatically.\", \"Something that isn't mentioned here and is useful: adding a suffix to the day. I decoupled the suffix logic so you can use it for any number you like, not just dates.\", '', 'Django Timezone aware datetime object example.This conversion is very important for Django and Python when you have USE_TZ = True:', 'Create a small utility function like:This is versatile enough:', 'This would be helpful for converting a string to datetime and also with a time zone:', 'arrow offers many useful functions for dates and times. This bit of code provides an answer to the question and shows that arrow is also capable of formatting dates easily and displaying information for other locales.See http://arrow.readthedocs.io/en/latest/ for more.', 'You can also check out dateparser:dateparser provides modules to easily parse localized dates in almost\\nany string formats commonly found on web pages.Install:This is, I think, the easiest way you can parse dates.The most straightforward way is to use the dateparser.parse function,\\nthat wraps around most of the functionality in the module.Sample code:Output:', 'You can use easy_date to make it easy:', 'If you want only date format then you can manually convert it by passing your individual fields like:You can pass your split string values to convert it into date type like:You will get the resulting value in date format.', \"Similar to Javed's answer, I just wanted date from string - so combining Simon's and Javed's logic, we get:Outputdatetime.date(2021, 3, 4)\", 'It seems using pandas Timestamp is the fastest:If the string is an ISO\\xa08601 string, please use csio8601:', \"If you don't want to explicitly specify which format your string is in with respect to the date time format, you can use this hack to by pass that step:If you want to convert it into some other datetime format, just modify the last line with the format you like for example something like date.strftime('%Y/%m/%d %H:%M:%S.%f'):Try running the above snippet to have a better clarity.\", \"See my answer.In real-world data this is a real problem: multiple, mismatched, incomplete, inconsistent and multilanguage/region date formats, often mixed freely in one dataset. It's not ok for production code to fail, let alone go exception-happy like a fox.We need to try...catch multiple datetime formats fmt1,fmt2,...,fmtn and suppress/handle the exceptions (from strptime()) for all those that mismatch (and in particular, avoid needing a yukky n-deep indented ladder of try..catch clauses). From my solution\", 'A short sample mapping a yyyy-mm-dd date string to a datetime.date object:', 'Use:It shows \"Start Date Time\" Column and \"Last Login Time\" both are \"object = strings\" in data-frame:By using the parse_dates option in read_csv mention, you can convert your string datetime into the pandas datetime format.Output:', '']",
            "url": "https://stackoverflow.com/questions/466345"
        },
        {
            "tag": "python",
            "question": [
                "Why is \"1000000000000000 in range(1000000000000001)\" so fast in Python 3?"
            ],
            "votes": "2854",
            "answer": "[\"The Python 3 range() object doesn't produce numbers immediately; it is a smart sequence object that produces numbers on demand. All it contains is your start, stop and step values, then as you iterate over the object the next integer is calculated each iteration.The object also implements the object.__contains__ hook, and calculates if your number is part of its range. Calculating is a (near) constant time operation *. There is never a need to scan through all possible integers in the range.From the range() object documentation:The advantage of the range type over a regular list or tuple is that a range object will always take the same (small) amount of memory, no matter the size of the range it represents (as it only stores the start, stop and step values, calculating individual items and subranges as needed).So at a minimum, your range() object would do:This is still missing several things that a real range() supports (such as the .index() or .count() methods, hashing, equality testing, or slicing), but should give you an idea.I also simplified the __contains__ implementation to only focus on integer tests; if you give a real range() object a non-integer value (including subclasses of int), a slow scan is initiated to see if there is a match, just as if you use a containment test against a list of all the contained values. This was done to continue to support other numeric types that just happen to support equality testing with integers but are not expected to support integer arithmetic as well. See the original Python issue that implemented the containment test.* Near constant time because Python integers are unbounded and so math operations also grow in time as N grows, making this a O(log N) operation. Since it\u2019s all executed in optimised C code and Python stores integer values in 30-bit chunks, you\u2019d run out of memory before you saw any performance impact due to the size of the integers involved here.\", \"The fundamental misunderstanding here is in thinking that range is a generator. It's not. In fact, it's not any kind of iterator.You can tell this pretty easily:If it were a generator, iterating it once would exhaust it:What range actually is, is a sequence, just like a list. You can even test this:This means it has to follow all the rules of being a sequence:The difference between a range and a list is that a range is a lazy or dynamic sequence; it doesn't remember all of its values, it just remembers its start, stop, and step, and creates the values on demand on __getitem__.(As a side note, if you print(iter(a)), you'll notice that range uses the same listiterator type as list. How does that work? A listiterator doesn't use anything special about list except for the fact that it provides a C implementation of __getitem__, so it works fine for range too.)Now, there's nothing that says that Sequence.__contains__ has to be constant time\u2014in fact, for obvious examples of sequences like list, it isn't. But there's nothing that says it can't be. And it's easier to implement range.__contains__ to just check it mathematically ((val - start) % step, but with some extra complexity to deal with negative steps) than to actually generate and test all the values, so why shouldn't it do it the better way?But there doesn't seem to be anything in the language that guarantees this will happen. As Ashwini Chaudhari points out, if you give it a non-integral value, instead of converting to integer and doing the mathematical test, it will fall back to iterating all the values and comparing them one by one. And just because CPython 3.2+ and PyPy 3.x versions happen to contain this optimization, and it's an obvious good idea and easy to do, there's no reason that IronPython or NewKickAssPython 3.x couldn't leave it out. (And in fact, CPython 3.0-3.1 didn't include it.)If range actually were a generator, like my_crappy_range, then it wouldn't make sense to test __contains__ this way, or at least the way it makes sense wouldn't be obvious. If you'd already iterated the first 3 values, is 1 still in the generator? Should testing for 1 cause it to iterate and consume all the values up to 1 (or up to the first value >= 1)?\", 'Use the source, Luke!In CPython, range(...).__contains__ (a method wrapper) will eventually delegate to a simple calculation which checks if the value can possibly be in the range.  The reason for the speed here is we\\'re using mathematical reasoning about the bounds, rather than a direct iteration of the range object.  To explain the logic used:For example, 994 is in range(4, 1000, 2) because:The full C code is included below, which is a bit more verbose because of memory management and reference counting details, but the basic idea is there:The \"meat\" of the idea is mentioned in the comment lines:As a final note - look at the range_contains function at the bottom of the code snippet.  If the exact type check fails then we don\\'t use the clever algorithm described, instead falling back to a dumb iteration search of the range using _PySequence_IterSearch!  You can check this behaviour in the interpreter (I\\'m using v3.5.0 here):', 'To add to Martijn\u2019s answer, this is the relevant part of the source (in C, as the range object is written in native code):So for PyLong objects (which is int in Python 3), it will use the range_contains_long function to determine the result. And that function essentially checks if ob is in the specified range (although it looks a bit more complex in C).If it\u2019s not an int object, it falls back to iterating until it finds the value (or not).The whole logic could be translated to pseudo-Python like this:', 'If you\\'re wondering why this optimization was added to range.__contains__, and why it wasn\\'t added to xrange.__contains__ in 2.7:First, as Ashwini Chaudhary discovered, issue 1766304 was opened explicitly to optimize [x]range.__contains__. A patch for this was accepted and checked in for 3.2, but not backported to 2.7 because \"xrange has behaved like this for such a long time that I don\\'t see what it buys us to commit the patch this late.\" (2.7 was nearly out at that point.)Meanwhile:Originally, xrange was a not-quite-sequence object. As the 3.1 docs say:Range objects have very little behavior: they only support indexing, iteration, and the len function.This wasn\\'t quite true; an xrange object actually supported a few other things that come automatically with indexing and len,* including __contains__ (via linear search). But nobody thought it was worth making them full sequences at the time.Then, as part of implementing the Abstract Base Classes PEP, it was important to figure out which builtin types should be marked as implementing which ABCs, and xrange/range claimed to implement collections.Sequence, even though it still only handled the same \"very little behavior\". Nobody noticed that problem until issue 9213. The patch for that issue not only added index and count to 3.2\\'s range, it also re-worked the optimized __contains__ (which shares the same math with index, and is directly used by count).** This change went in for 3.2 as well, and was not backported to 2.x, because \"it\\'s a bugfix that adds new methods\". (At this point, 2.7 was already past rc status.)So, there were two chances to get this optimization backported to 2.7, but they were both rejected.* In fact, you even get iteration for free with indexing alone, but in 2.3 xrange objects got a custom iterator.** The first version actually reimplemented it, and got the details wrong\u2014e.g., it would give you MyIntSubclass(2) in range(5) == False. But Daniel Stutzbach\\'s updated version of the patch restored most of the previous code, including the fallback to the generic, slow _PySequence_IterSearch that pre-3.2 range.__contains__ was implicitly using when the optimization doesn\\'t apply.', \"The other answers explained it well already, but I'd like to offer another experiment illustrating the nature of range objects:As you can see, a range object is an object that remembers its range and can be used many times (even while iterating over it), not just a one-time generator.\", \"It's all about a lazy approach to the evaluation and some extra optimization of range.\\nValues in ranges don't need to be computed until real use, or even further due to extra optimization.By the way, your integer is not such big, consider sys.maxsizesys.maxsize in range(sys.maxsize) is pretty fastdue to optimization - it's easy to compare given integers just with min and max of range.but:Decimal(sys.maxsize) in range(sys.maxsize) is pretty slow.(in this case, there is no optimization in range, so if python receives unexpected Decimal, python will compare all numbers)You should be aware of an implementation detail but should not be relied upon, because this may change in the future.\", 'The object returned by range() is actually a range object. This object implements the iterator interface so you can iterate over its values sequentially, just like a generator, list, or tuple.But it also implements the __contains__ interface which is actually what gets called when an object appears on the right-hand side of the in operator. The __contains__() method returns a bool of whether or not the item on the left-hand side of the in is in the object. Since range objects know their bounds and stride, this is very easy to implement in O(1).', 'Take an example, 997 is in range(4, 1000, 3) because:4 <= 997 < 1000, and (997 - 4) % 3 == 0.', 'Try x-1 in (i for i in range(x)) for large x values, which uses a generator comprehension to avoid invoking the range.__contains__ optimisation.', 'TLDR;\\nthe range is an arithmetic series so it can very easily calculate whether the object is there. It could even get the index of it if it were list like really quickly.', '__contains__ method compares directly with the start and end of the range']",
            "url": "https://stackoverflow.com/questions/30081275"
        },
        {
            "tag": "python",
            "question": [
                "Find the current directory and file's directory [duplicate]"
            ],
            "votes": "2849",
            "answer": "[\"To get the full path to the directory a Python file is contained in, write this in that file:(Note that the incantation above won't work if you've already used os.chdir() to change your current working directory, since the value of the __file__ constant is relative to the current working directory and is not changed by an os.chdir() call.)To get the current working directory useDocumentation references for the modules, constants and functions used above:\", 'Current working directory:  os.getcwd()And the __file__ attribute can help you find out where the file you are executing is located. This Stack\\xa0Overflow post explains everything:  How do I get the path of the current executed file in Python?', 'You may find this useful as a reference:', 'The pathlib module, introduced in Python 3.4 (PEP 428 \u2014 The pathlib module \u2014 object-oriented filesystem paths), makes the path-related experience much much better.In order to get the current working directory, use Path.cwd():To get an absolute path to your script file, use the Path.resolve() method:And to get the path of a directory where your script is located, access .parent (it is recommended to call .resolve() before .parent):Remember that __file__ is not reliable in some situations: How do I get the path of the current executed file in Python?.Please note, that Path.cwd(), Path.resolve() and other Path methods return path objects (PosixPath in my case), not strings. In Python 3.4 and 3.5 that caused some pain, because open built-in function could only work with string or bytes objects, and did not support Path objects, so you had to convert Path objects to strings or use the Path.open() method, but the latter option required you to change old code:As you can see, open(p) does not work with Python 3.5.PEP 519 \u2014 Adding a file system path protocol, implemented in Python 3.6, adds support of PathLike objects to the open function, so now you can pass Path objects to the open function directly:', 'To get the current directory full pathOutput: \"C :\\\\Users\\\\admin\\\\myfolder\"To get the current directory folder name aloneOutput: \"myfolder\"', 'Pathlib can be used this way to get the directory containing the current script:', 'If you are trying to find the current directory of the file you are currently in:OS agnostic way:', \"If you're using Python 3.4, there is the brand new higher-level pathlib module which allows you to conveniently call pathlib.Path.cwd() to get a Path object representing your current working directory, along with many other new features.More info on this new API can be found here.\", 'To get the current directory full path:', 'Answer to #1:If you want the current directory, do this:If you want just any folder name and you have the path to that folder, do this:Answer to #2:', 'I think the most succinct way to find just the name of your current execution context would be:', \"If you're searching for the location of the currently executed script, you can use sys.argv[0] to get the full path.\", \"For question 1, use os.getcwd() # Get working directory and os.chdir(r'D:\\\\Steam\\\\steamapps\\\\common') # Set working directoryI recommend using sys.argv[0] for question 2 because sys.argv is immutable and therefore always returns the current file (module object path) and not affected by os.chdir(). Also you can do like this:But that snippet and sys.argv[0] will not work or will work weird when compiled by PyInstaller, because magic properties are not set in __main__ level and sys.argv[0] is the way your executable was called (it means that it becomes affected by the working directory).\"]",
            "url": "https://stackoverflow.com/questions/5137497"
        },
        {
            "tag": "python",
            "question": [
                "Renaming column names in Pandas"
            ],
            "votes": "2747",
            "answer": "[\"Use the df.rename() function and refer the columns to be renamed. Not all the columns have to be renamed:Minimal Code ExampleThe following methods all work and produce the same output:Remember to assign the result back, as the modification is not-inplace. Alternatively, specify inplace=True:From v0.25, you can also specify errors='raise' to raise errors if an invalid column-to-rename is specified. See v0.25 rename() docs.Use df.set_axis() with axis=1 and inplace=False (to return a copy).This returns a copy, but you can modify the DataFrame in-place by setting inplace=True (this is the default behaviour for versions <=0.24 but is likely to change in the future).You can also assign headers directly:\", 'Just assign it to the .columns attribute:', 'The rename method can take a function, for example:', 'As documented in Working with text data:', 'There have been some significant updates to column renaming in version 0.21.Construct sample DataFrame:orBoth result in the following:It is still possible to use the old method signature:The rename function also accepts functions that will be applied to each column name.orYou can supply a list to the set_axis method that is equal in length to the number of columns (or index). Currently, inplace defaults to True, but inplace will be defaulted to False in future releases.orThere is nothing wrong with assigning columns directly like this. It is a perfectly good solution.The advantage of using set_axis is that it can be used as part of a method chain and that it returns a new copy of the DataFrame. Without it, you would have to store your intermediate steps of the chain to another variable before reassigning the columns.', 'Since you only want to remove the $ sign in all column names, you could just do:OR', 'Renaming columns in Pandas is an easy task.', 'It will replace the existing names with the names you provide, in the order you provide.', 'Use:This way you can manually edit the new_names as you wish. It works great when you need to rename only a few columns to correct misspellings, accents, remove special characters, etc.', \"I'll focus on two things:OP clearly statesI have the edited column names stored it in a list, but I don't know how to replace the column names.I do not want to solve the problem of how to replace '$' or strip the first character off of each column header.  OP has already done this step.  Instead I want to focus on replacing the existing columns object with a new one given a list of replacement column names.df.columns = new where new is the list of new columns names is as simple as it gets.  The drawback of this approach is that it requires editing the existing dataframe's columns attribute and it isn't done inline.  I'll show a few ways to perform this via pipelining without editing the existing dataframe.Setup 1\\nTo focus on the need to rename of replace column names with a pre-existing list, I'll create a new sample dataframe df with initial column names and unrelated new column names.Solution 1\\npd.DataFrame.renameIt has been said already that if you had a dictionary mapping the old column names to new column names, you could use pd.DataFrame.rename.However, you can easily create that dictionary and include it in the call to rename.  The following takes advantage of the fact that when iterating over df, we iterate over each column name.This works great if your original column names are unique.  But if they are not, then this breaks down.Setup 2\\nNon-unique columnsSolution 2\\npd.concat using the keys argumentFirst, notice what happens when we attempt to use solution 1:We didn't map the new list as the column names.  We ended up repeating y765.  Instead, we can use the keys argument of the pd.concat function while iterating through the columns of df.Solution 3\\nReconstruct.  This should only be used if you have a single dtype for all columns.  Otherwise, you'll end up with dtype object for all columns and converting them back requires more dictionary work.Single dtypeMixed dtypeSolution 4\\nThis is a gimmicky trick with transpose and set_index.  pd.DataFrame.set_index allows us to set an index inline, but there is no corresponding set_columns.  So we can transpose, then set_index, and transpose back.  However, the same single dtype versus mixed dtype caveat from solution 3 applies here.Single dtypeMixed dtypeSolution 5\\nUse a lambda in pd.DataFrame.rename that cycles through each element of new.\\nIn this solution, we pass a lambda that takes x but then ignores it.  It also takes a y but doesn't expect it.  Instead, an iterator is given as a default value and I can then use that to cycle through one at a time without regard to what the value of x is.And as pointed out to me by the folks in sopython chat, if I add a * in between x and y, I can protect my y variable.  Though, in this context I don't believe it needs protecting.  It is still worth mentioning.\", \"I would like to explain a bit what happens behind the scenes.Dataframes are a set of Series.Series in turn are an extension of a numpy.array.numpy.arrays have a property .name.This is the name of the series. It is seldom that Pandas respects this attribute, but it lingers in places and can be used to hack some Pandas behaviors.A lot of answers here talks about the df.columns attribute being a list when in fact it is a Series. This means it has a .name attribute.This is what happens if you decide to fill in the name of the columns Series:Note that the name of the index always comes one column lower.The .name attribute lingers on sometimes. If you set df.columns = ['one', 'two'] then the df.one.name will be 'one'.If you set df.one.name = 'three' then df.columns will still give you ['one', 'two'], and df.one.name will give you 'three'.pd.DataFrame(df.one) will returnBecause Pandas reuses the .name of the already defined Series.Pandas has ways of doing multi-layered column names. There is not so much magic involved, but I wanted to cover this in my answer too since I don't see anyone picking up on this here.This is easily achievable by setting columns to lists, like this:\", \"Let's understand renaming by a small example...Renaming columns using mapping:Renaming index/Row_Name using mapping:\", \"Many of pandas functions have an inplace parameter. When setting it True, the transformation applies directly to the dataframe that you are calling it on. For example:Alternatively, there are cases where you want to preserve the original dataframe. I have often seen people fall into this case if creating the dataframe is an expensive task. For example, if creating the dataframe required querying a snowflake database. In this case, just make sure the the inplace parameter is set to False.If these types of transformations are something that you do often, you could also look into a number of different pandas GUI tools. I'm the creator of one called Mito. It\u2019s a spreadsheet that automatically converts your edits to python code.\", 'Suppose your dataset name is df, and df has.So, to rename these, we would simply do.', \"Let's say this is your dataframe.You can rename the columns using two methods.Using dataframe.columns=[#list]The limitation of this method is that if one column has to be changed, full column list has to be passed. Also, this method is not applicable on index labels.\\nFor example, if you passed this:This will throw an error. Length mismatch: Expected axis has 5 elements, new values have 4 elements.Another method is the Pandas rename() method which is used to rename any index, column or rowSimilarly, you can change any rows or columns.\", \"If you've got the dataframe, df.columns dumps everything into a list you can manipulate and then reassign into your dataframe as the names of columns...Best way? I don't know. A way - yes.A better way of evaluating all the main techniques put forward in the answers to the question is below using cProfile to gage memory and execution time. @kadee, @kaitlyn, and @eumiro had the functions with the fastest execution times - though these functions are so fast we're comparing the rounding of 0.000 and 0.001 seconds for all the answers. Moral: my answer above likely isn't the 'best' way.\", \"If your new list of columns is in the same order as the existing columns, the assignment is simple:If you had a dictionary keyed on old column names to new column names, you could do the following:If you don't have a list or dictionary mapping, you could strip the leading $ symbol via a list comprehension:\", 'pandas.DataFrame.rename', 'If you already have a list for the new column names, you can try this:', \"Another way we could replace the original column labels is by stripping the unwanted characters (here '$') from the original column labels.This could have been done by running a for loop over df.columns and appending the stripped columns to df.columns.Instead, we can do this neatly in a single statement by using list comprehension like below:(strip method in Python strips the given character from beginning and end of the string.)\", 'It is real simple. Just use:And it will assign the column names by the order you put them in.', '', 'You could use str.slice for that:', 'Another option is to rename using a regular expression:', 'My method is generic wherein you can add additional delimiters by comma separating delimiters= variable and future-proof it.Working Code:Output:', 'Note that the approaches in previous answers do not work for a MultiIndex. For a MultiIndex, you need to do something like the following:', 'If you have to deal with loads of columns named by the providing system out of your control, I came up with the following approach that is a combination of a general approach and specific replacements in one go.First create a dictionary from the dataframe column names using regular expressions in order to throw away certain appendixes of column names and then add specific replacements to the dictionary to name core columns as expected later in the receiving database.This is then applied to the dataframe in one go.', \"If you just want to remove the '$' sign then use the below code\", 'In addition to the solution already provided, you can replace all the columns while you are reading the file. We can use names and header=0 to do that.First, we create a list of the names that we like to use as our column names:In this case, all the column names will be replaced with the names you have in your list.', \"Here's a nifty little function I like to use to cut down on typing:Here is an example of how it works:\"]",
            "url": "https://stackoverflow.com/questions/11346283"
        },
        {
            "tag": "python",
            "question": [
                "How can I remove a key from a Python dictionary?"
            ],
            "votes": "2706",
            "answer": "[\"To delete a key regardless of whether it is in the dictionary, use the two-argument form of dict.pop():This will return my_dict[key] if key exists in the dictionary, and None otherwise. If the second parameter is not specified (i.e. my_dict.pop('key')) and key does not exist, a KeyError is raised.To delete a key that is guaranteed to exist, you can also use:This will raise a KeyError if the key is not in the dictionary.\", 'Specifically to answer \"is there a one line way of doing this?\"...well, you asked ;-)You should consider, though, that this way of deleting an object from a dict is not atomic\u2014it is possible that \\'key\\' may be in my_dict during the if statement, but may be deleted before del is executed, in which case del will fail with a KeyError.  Given this, it would be safest to either use dict.pop or something along the lines ofwhich, of course, is definitely not a one-liner.', 'It took me some time to figure out what exactly my_dict.pop(\"key\", None) is doing. So I\\'ll add this as an answer to save others googling time:If key is in the dictionary, remove it and return its value, else\\nreturn default. If default is not given and key is not in the\\ndictionary, a KeyError is raised.Documentation', \"del my_dict[key] is slightly faster than my_dict.pop(key) for removing a key from a dictionary when the key existsBut when the key doesn't exist if key in my_dict: del my_dict[key] is slightly faster than my_dict.pop(key, None). Both are at least three times faster than del in a try/except statement:\", \"If you need to remove a lot of keys from a dictionary in one line of code, I think using map() is quite succinct and Pythonic readable:And if you need to catch errors where you pop a value that isn't in the dictionary, use lambda inside map() like this:or in python3, you must use a list comprehension instead:It works. And 'e' did not cause an error, even though myDict did not have an 'e' key.\", \"You can use a dictionary comprehension to create a new dictionary with that key removed:You can delete by conditions. No error if key doesn't exist.\", \"We can delete a key from a Python dictionary by the some of the following approaches.Using the del keyword; it's almost the same approach like you did though -OrWe can do like the following:But one should keep in mind that, in this process actually it won't delete any key from the dictionary rather than making a specific key excluded from that dictionary. In addition, I observed that it returned a dictionary which was not ordered the same as myDict.If we run it in the shell, it'll execute something like {'five': 500, 'four': 400, 'three': 300, 'two': 200} - notice that it's not the same ordered as myDict. Again if we try to print myDict, then we can see all keys including which we excluded from the dictionary by this approach. However, we can make a new dictionary by assigning the following statement into a variable:Now if we try to print it, then it'll follow the parent order:OrUsing the pop() method.The difference between del and pop is that, using pop() method, we can actually store the key's value if needed, like the following:Fork this gist for future reference, if you find this useful.\", \"You can use exception handling if you want to be very verbose:This is slower, however, than the pop() method, if the key doesn't exist.It won't matter for a few keys, but if you're doing this repeatedly, then the latter method is a better bet.The fastest approach is this:But this method is dangerous because if 'key' is removed in between the two lines, a KeyError will be raised.\", 'I prefer the immutable version', 'Another way is by using items() + dict comprehension.items() coupled with dict comprehension can also help us achieve the task of key-value pair deletion, but it has the drawback of not being an in place dict technique. Actually a new dict if created except for the key we don\u2019t wish to include.Output:', 'If you want to do that without KeyError, you can declare a temporary class and set it as default value in dict.get, if the value is equal to that class if means that key does not exist']",
            "url": "https://stackoverflow.com/questions/11277432"
        },
        {
            "tag": "python",
            "question": [
                "Check if a given key already exists in a dictionary"
            ],
            "votes": "2679",
            "answer": "['in tests for the existence of a key in a dict:Use dict.get() to provide a default value when the key does not exist:To provide a default value for every key, either use dict.setdefault() on each assignment:or use defaultdict from the collections module:', \"Use key in my_dict directly instead of key in my_dict.keys():That will be much faster as it uses the dictionary's O(1) hashing as opposed to doing an O(n) linear search on a list of keys.\", 'You can test for the presence of a key in a dictionary, using the in keyword:A common use for checking the existence of a key in a dictionary before mutating it is to default-initialize the value (e.g. if your values are lists, for example, and you want to ensure that there is an empty list to which you can append when inserting the first value for a key). In cases such as those, you may find the collections.defaultdict() type to be of interest.In older code, you may also find some uses of has_key(), a deprecated method for checking the existence of keys in dictionaries (just use key_name in dict_name, instead).', 'You can shorten your code to this:However, this is at best a cosmetic improvement. Why do you believe this is not the best way?', \"For additional information on speed execution of the accepted answer's proposed methods (10\\xa0million loops):Therefore using in or defaultdict are recommended against get.\", 'I would recommend using the setdefault method instead.  It sounds like it will do everything you want.', \"A dictionary in Python has a get('key', default) method. So you can just set a default value in case there isn't any key.\", 'Using the Python ternary operator:', 'Use EAFP (easier to ask forgiveness than permission):See other Stack Overflow posts:', \"Check if a given key already exists in a dictionaryTo get the idea how to do that we first inspect what methods we can call on dictionary.Here are the methods:The brutal method to check if the key already exists may be the get() method:The other two interesting methods items() and keys() sounds like too much of work. So let's examine if get() is the right method for us. We have our dict d:Printing shows the key we don't have will return None:We use that to get the information if the key is present or no.\\nBut consider this if we create a dict with a single key:None:Leading that get() method is not reliable in case some values may be None.This story should have a happier ending. If we use the in comparator:We get the correct results.We may examine the Python byte code:This shows that in compare operator is not just more reliable, but even faster than get().\", \"The ways in which you can get the results are:Which is better is dependent on 3 things:Read More: http://paltman.com/try-except-performance-in-python-a-simple-test/Use of try/block instead of 'in' or 'if':\", 'You can use the has_key() method:', 'Just an FYI adding to Chris. B\\'s (best) answer:Works as well; the reason is that calling int() returns 0 which is what defaultdict does behind the scenes (when constructing a dictionary), hence the name \"Factory Function\" in the documentation.', 'A Python dictionary has the method called __contains__. This method will return True if the dictionary has the key, else it returns False.', \"Another way of checking if a key exists using Boolean operators:This returnsExplanationFirst, you should know that in Python, 0, None, or objects with zero length evaluate to False. Everything else evaluates to True. Boolean operations are evaluated left to right and return the operand not True or False.Let's see an example:Since 'Some string' evaluates to True, the rest of the or is not evaluated and there is no division by zero error raised.But if we switch the order 1/0 is evaluated first and raises an exception:We can use this for pattern for checking if a key exists.does the same asThis already returns the correct result if the key exists, but we want it to print 'boo' when it doesn't. So, we take the result and or it with 'boo'\", 'You can use a for loop to iterate over the dictionary and get the name of key you want to find in the dictionary. After that, check if it exist or not using if condition:']",
            "url": "https://stackoverflow.com/questions/1602934"
        },
        {
            "tag": "python",
            "question": [
                "How do I parse a string to a float or int?"
            ],
            "votes": "2668",
            "answer": "['', 'For the Python3 version of is_float see: Checking if a string can be converted to float in PythonA longer and more accurate name for this function could be: is_convertible_to_float(value)The below unit tests were done using python2.  Check it that Python3 has different behavior for what strings are convertable to float.  One confounding difference is that any number of interior underscores are now allowed:  (float(\"1_3.4\") == float(13.4)) is TrueYou think you know what numbers are? You are not so good as you think! Not big surprise.Catching broad exceptions this way, killing canaries and gobbling the exception creates a tiny chance that a valid float as string will return false.  The float(...) line of code can failed for any of a thousand reasons that have nothing to do with the contents of the string.  But if you\\'re writing life-critical software in a duck-typing prototype language like Python, then you\\'ve got much larger problems.', '', \"This is another method which deserves to be mentioned here, ast.literal_eval:This can be used for safely evaluating strings containing Python expressions from untrusted sources without the need to parse the values oneself.That is, a safe 'eval'\", 'You should consider the possibility of commas in the string representation of a number, for cases like  float(\"545,545.2222\") which throws an exception. Instead, use methods in locale to convert the strings to numbers and interpret commas correctly. The locale.atof method converts to a float in one step once the locale has been set for the desired number convention.Example 1 -- United States number conventionsIn the United States and the UK, commas can be used as a thousands separator.  In this example with American locale, the comma is handled properly as a separator:Example 2 -- European number conventionsIn the majority of countries of the world,  commas are used for decimal marks instead of periods.  In this example with French locale, the comma is correctly handled as a decimal mark:The method locale.atoi is also available, but the argument should be an integer.', '', \"If you aren't averse to third-party modules, you could check out the fastnumbers module. It provides a function called fast_real that does exactly what this question is asking for and does it faster than a pure-Python implementation:\", 'Users codelogic and harley are correct, but keep in mind if you know the string is an integer (for example, 545) you can call int(\"545\") without first casting to float.If your strings are in a list, you could use the map function as well.It is only good if they\\'re all the same type.', 'In Python, how can I parse a numeric string like \"545.2222\" to its corresponding float value, 542.2222? Or parse the string \"31\" to an integer, 31?\\n  I just want to know how to parse a float string to a float, and (separately) an int string to an int.It\\'s good that you ask to do these separately. If you\\'re mixing them, you may be setting yourself up for problems later. The simple answer is:\"545.2222\" to float:\"31\" to an integer:Conversions from various bases, and you should know the base in advance (10 is the default). Note you can prefix them with what Python expects for its literals (see below) or remove the prefix:If you don\\'t know the base in advance, but you do know they will have the correct prefix, Python can infer this for you if you pass 0 as the base:If your motivation is to have your own code clearly represent hard-coded specific values, however, you may not need to convert from the bases - you can let Python do it for you automatically with the correct syntax.You can use the apropos prefixes to get automatic conversion to integers with the following literals. These are valid for Python 2 and 3:Binary, prefix 0bOctal, prefix 0oHexadecimal, prefix 0xThis can be useful when describing binary flags, file permissions in code, or hex values for colors - for example, note no quotes:If you see an integer that starts with a 0, in Python 2, this is (deprecated) octal syntax.It is bad because it looks like the value should be 37. So in Python 3, it now raises a SyntaxError:Convert your Python 2 octals to octals that work in both 2 and 3 with the 0o prefix:', 'The question seems a little bit old. But let me suggest a function, parseStr, which makes something similar, that is, returns integer or float and if a given ASCII string cannot be converted to none of them it returns it untouched. The code of course might be adjusted to do only what you want:', 'float(\"545.2222\") and int(float(\"545.2222\"))', 'The YAML parser can help you figure out what datatype your string is. Use yaml.load(), and then you can use type(result) to test for type:', 'I use this function for thatIt will convert the string to its type', '', '', 'You could use json.loads:As you can see it becomes a type of float.', 'You need to take into account rounding to do this properly.i.e. - int(5.1) => 5\\nint(5.6) => 5  -- wrong, should be 6 so we do int(5.6 + 0.5) => 6', 'To typecast in Python use the constructor functions of the type, passing the string (or whatever value you are trying to cast) as a parameter.For example:Behind the scenes, Python is calling the objects __float__ method, which should return a float representation of the parameter. This is especially powerful, as you can define your own types (using classes) with a __float__ method so that it can be casted into a float using float(myobject).', \"Handles hex, octal, binary, decimal, and floatThis solution will handle all of the string conventions for numbers (all that I know about).This test case output illustrates what I'm talking about.Here is the test:\", 'Pass your string to this function:It will return int, float or string depending on what was passed.', 'There is also regex, because sometimes string must be prepared and normalized before casting to a number:Usage:And by the way, something to verify you have a number:', '', \"This is a corrected version of Totoro's answer.This will try to parse a string and return either int or float depending on what the string represents. It might rise parsing exceptions or have some unexpected behaviour.\", 'If you are dealing with mixed integers and floats and want a consistent way to deal with your mixed data, here is my solution with the proper docstring:Output:', 'Use:This is the most Pythonic way I could come up with.', 'You can simply do this byFor more information on parsing of data types check on python documentation!', \"This is a function which will convert any object (not just str) to int or float, based on if the actual string supplied looks like int or float. Further if it's an object which has both __float and __int__ methods, it defaults to using __float__\", 'By using int and float methods we can convert a string to integer and floats.', 'For numbers and characters together:First import re:For easy model:', \"If you don't want to use third party modules the following might be the most robust solution:It might not be the fastest, but it handles correctly literal numbers where many other solutions fail, such as:\"]",
            "url": "https://stackoverflow.com/questions/379906"
        }
    ]
}